<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Go源码——Sync.Mutex</title>
      <link href="/2022/10/08/sync-mutex/"/>
      <url>/2022/10/08/sync-mutex/</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h1><p><code>sync.Mutex</code>是我们常用到的一把锁。网上讲这个锁的文章也比较多，这里面主要是为了简单做个自我总结。</p><p><code>Sync.Mutex</code> 慢路径底层依赖的是<code>runtime_SemacquireMutex</code>和<code>runtime_Semrelease</code>，对这个不了解可以先去看下 <a href="https://fanlv.wiki/2022/10/06/runtime-sema/">runtime.semaphore</a> 。</p><h1 id="二、Sync-Mutex-源码"><a href="#二、Sync-Mutex-源码" class="headerlink" title="二、Sync.Mutex 源码"></a>二、Sync.Mutex 源码</h1><h2 id="2-1-发展历史"><a href="#2-1-发展历史" class="headerlink" title="2.1 发展历史"></a>2.1 发展历史</h2><p><code>sync.Mutex</code>第一版 <a href="https://github.com/golang/go/commit/bf3dd3f0efe5b45947a991e22660c62d4ce6b671#diff-a8c424f9dc7e3acf3f180a5cbf3f7748e6fd39c6f1eab0b4fd7ec11c548cdbeb">代码</a> 是<code>2008</code>年的时候 <a href="https://github.com/rsc">@rsc</a> 提交的。最早的实现比较简单，是通过简单的<code>CAS</code>加<code>信号量</code>的方式来实现的。信号量具体可以参考 <a href="https://fanlv.wiki/2022/10/06/runtime-sema/">runtime-sema</a> 这篇文章。</p><p><a href="https://github.com/dvyukov">@dvyukov</a> <code>2011</code>年的时候，提交了第一次优化了 <a href="https://codereview.appspot.com/4631075/">sync: improve Mutex to allow successive acquisitions</a>，这一版中加入了<code>mutexWoken</code>唤醒状态和等待者计数的概念。</p><p><a href="https://github.com/dvyukov">@dvyukov</a> <code>2015</code>年的时候，新增了第二次优化 <a href="https://go-review.googlesource.com/c/go/+/5430/">sync: add active spinning to Mutex</a>，这一版里面主要是加了自旋逻辑。</p><p><a href="https://github.com/dvyukov">@dvyukov</a> <code>2016</code>年的时候，新增了第三次优化 <a href="https://go-review.googlesource.com/c/go/+/34310/">sync: make Mutex more fair</a>，这一版加入了饥饿模式，让锁在更公平一些。</p><h2 id="2-2-Mutex结构分析"><a href="#2-2-Mutex结构分析" class="headerlink" title="2.2 Mutex结构分析"></a>2.2 Mutex结构分析</h2><p>先看<code>Mutex</code>的 <a href="https://github.com/golang/go/blob/release-branch.go1.18/src/sync/mutex.go#L42">注释</a>：</p><pre><code>// Mutex fairness.//// Mutex can be in 2 modes of operations: normal and starvation.// In normal mode waiters are queued in FIFO order, but a woken up waiter// does not own the mutex and competes with new arriving goroutines over// the ownership. New arriving goroutines have an advantage -- they are// already running on CPU and there can be lots of them, so a woken up// waiter has good chances of losing. In such case it is queued at front// of the wait queue. If a waiter fails to acquire the mutex for more than 1ms,// it switches mutex to the starvation mode.//// In starvation mode ownership of the mutex is directly handed off from// the unlocking goroutine to the waiter at the front of the queue.// New arriving goroutines don't try to acquire the mutex even if it appears// to be unlocked, and don't try to spin. Instead they queue themselves at// the tail of the wait queue.//// If a waiter receives ownership of the mutex and sees that either// (1) it is the last waiter in the queue, or (2) it waited for less than 1 ms,// it switches mutex back to normal operation mode.//// Normal mode has considerably better performance as a goroutine can acquire// a mutex several times in a row even if there are blocked waiters.// Starvation mode is important to prevent pathological cases of tail latency.</code></pre><p>翻译如下：</p><pre><code>// 公平锁//// 锁有两种模式：正常模式和饥饿模式。// 在正常模式下，所有的等待锁的 goroutine 都会存在一个先进先出的队列中（轮流被唤醒）// 但是一个被唤醒的goroutine并不是直接获得锁，而是仍然需要和那些新请求锁的（new arrivial）// 的goroutine竞争，而这其实是不公平的，因为新请求锁的goroutine有一个优势——它们正在CPU上// 运行，并且数量可能会很多。所以一个被唤醒的goroutine拿到锁的概率是很小的。在这种情况下，// 这个被唤醒的goroutine会加入到队列的头部。如果一个等待的goroutine有超过1ms// 都没获取到锁，那么就会把锁转变为饥饿模式。//// 在饥饿模式中，锁的所有权会直接从释放锁(unlock)的goroutine转交给队列头的goroutine，// 新请求锁的goroutine就算锁是空闲状态也不会去获取锁，并且也不会尝试自旋。它们只是排到队列的尾部。//// 如果一个goroutine获取到了锁之后，它会判断以下两种情况：// 1. 它是队列中最后一个goroutine；// 2. 它拿到锁所花的时间小于1ms；// 以上只要有一个成立，它就会把锁转变回正常模式。// 正常模式会有比较好的性能，因为即使有很多阻塞的等待锁的goroutine，// 一个goroutine也可以尝试请求多次锁。// 饥饿模式对于防止尾部延迟来说非常的重要。</code></pre><p>在看看下<code>Mutex</code>结构体<a href="https://github.com/golang/go/blob/release-branch.go1.18/src/sync/mutex.go#L21">代码</a>：</p><pre><code>type Mutex struct {    state int32    sema  uint32}const (    mutexLocked = 1 &lt;&lt; iota // 表示当前是否已经上锁，1是锁定，0是无锁    mutexWoken // 当前是不是唤醒状态, 1 表示唤醒    mutexStarving // 当前是否是饥饿状态，1 表示是饥饿    mutexWaiterShift = iota // state 右移3位表示 Waiter的个数    starvationThresholdNs = 1e6 // 等待时间超过这个数就变饥饿模式。)</code></pre><p><code>sema</code>这个字段比较简单，就是调用<code>runtime_SemacquireMutex</code>和<code>runtime_Semrelease</code>需要传的参数。<code>state</code>里面不同的位表示不同的含义，如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-14e319d3d5f4c349.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="2-3-Lock"><a href="#2-3-Lock" class="headerlink" title="2.3 Lock"></a>2.3 Lock</h2><pre><code>// 如果已经上锁了，这里会阻塞当前的goroutine直到mutex可用func (m *Mutex) Lock() {    // 快路径，先尝试CAS把state从0改成锁定    if atomic.CompareAndSwapInt32(&amp;m.state, 0, mutexLocked) {        if race.Enabled {            race.Acquire(unsafe.Pointer(m))        }        return    }        // 慢路径    m.lockSlow()}func (m *Mutex) lockSlow() {    var waitStartTime int64    starving := false    awoke := false    iter := 0    old := m.state    for {        // old&amp;(mutexLocked|mutexStarving) 表示保留Locked和Starving两个bit位上的数据，其他的全部清空        // old&amp;(mutexLocked|mutexStarving) == mutexLocked 表示是锁定状态但是不是饥饿状态。        // runtime_canSpin主要判断能不能自旋，它做了几件事        // 1. 自旋次数 &lt; 4        // 2. 必须是多核CPU 且 GOMAXPROCS&gt;1        // 3. P 并且本地运行队列为空.        if old&amp;(mutexLocked|mutexStarving) == mutexLocked &amp;&amp; runtime_canSpin(iter) {            // 当前“唤醒” 标记为 0 ，然后还有其他g处于等待状态           // CAS 尝试设置唤醒状态标记位 = 1           // 告诉其他的 g ，我目前正在处于自旋抢锁状态            if !awoke &amp;&amp; old&amp;mutexWoken == 0 &amp;&amp; old&gt;&gt;mutexWaiterShift != 0 &amp;&amp;                atomic.CompareAndSwapInt32(&amp;m.state, old, old|mutexWoken) {                awoke = true            }            // runtime_doSpin 就是调用的 procyield(active_spin_cnt)            // procyield 可以看 https://fanlv.wiki/2022/10/05/runtime-mutex/#2-4-procyield-%E5%8A%9F%E8%83%BD            runtime_doSpin()            iter++            old = m.state // 读取下 m.state 新的值，可能已经被其他 g 改变了。            continue // 设置失败尝试继续自旋        }                new := old                if old&amp;mutexStarving == 0 {           // 不是饥饿状态，尝试加锁           // 是加锁状态，就不用设置了，下面Waiter+1，然后乖乖排队去就行了            new |= mutexLocked        }                // 如果mutexLocked 或者 mutexStarving = 1        // Waiter 数量加一        if old&amp;(mutexLocked|mutexStarving) != 0 {            new += 1 &lt;&lt; mutexWaiterShift        }                // 如果当前是 mutexLocked = 1(是锁定状态)        // 然后 starving = true （下面加锁等待时间超过1ms）        // 这个时候需要把 mutexStarving 标记位设置为 1        // 如果不是锁定状态，我就不设置了饥饿状态了。搞不好下面CAS一把设置就成功了。        if starving &amp;&amp; old&amp;mutexLocked != 0 {            new |= mutexStarving        }        if awoke {            // 如果已经设置为唤醒状态, 需要清除唤醒标记, 因为后面要么获得了锁，要么进入休眠.            if new&amp;mutexWoken == 0 {                throw("sync: inconsistent mutex state")            }            new &amp;^= mutexWoken        }                // CAS 更新状态        if atomic.CompareAndSwapInt32(&amp;m.state, old, new) {            if old&amp;(mutexLocked|mutexStarving) == 0 {               // 老的状态是没有加锁，也不是饥饿，那表示我们直接加锁成功了               // 直接返回了                break // locked the mutex with CAS            }                        // 走到这里，表示之前的锁可能是加锁状态，之前的说也可能是饥饿状态            // 无论是否是加锁、或者饥饿状态，都要调用信号量，去排队。                        // waitStartTime != 0 表示是 sleep 以后被唤醒的 goroutine            queueLifo := waitStartTime != 0            if waitStartTime == 0 {                waitStartTime = runtime_nanotime()            }                        // 请求信号量            // queueLifo = true 会放到 semTable suodg队列的头部。            // 信号量相关的可以看这个 https://fanlv.wiki/2022/10/06/runtime-sema/            // 如果没有可以用的信号量会阻塞到这句代码，底层其实是调用 gopark 休眠这个 g            runtime_SemacquireMutex(&amp;m.sema, queueLifo, 1)                        // 这里表示有人释放了锁/信号量，我们这个g被唤醒了。            // 虽然我们是在队列头部被唤醒了，但是如果这个时候，业务代码有新的请求过来，刚刚好有代码调用 Lock。我们这个刚刚被唤醒的g，是要跟新的Lock调用场景去抢锁的。            // 等待时间超过 1ms ，直接设置starving=true            starving = starving || runtime_nanotime()-waitStartTime &gt; starvationThresholdNs            old = m.state // 读取一下最新的 state 状态。现在也不知道被改成什么了。            if old&amp;mutexStarving != 0 {                 // 当前是饥饿状态 我们也不用再去抢锁了，默认就是给我们执行了                if old&amp;(mutexLocked|mutexWoken) != 0 || old&gt;&gt;mutexWaiterShift == 0 {                  // 饥饿状态下不可能有（mutexWoken=0&amp;&amp; mutexLocked==0）这种情况                  // mutexWaiter 也不可能 = 0 ，因为下面 mutexWaiter = 1 时候就退出了饥饿状态                    throw("sync: inconsistent mutex state")                }                                // 下面这个位操作，一个AddInt32 改变三个标记位状态，很骚，很难看懂。                                // 设置第一位是1，然后 waiter - 1                delta := int32(mutexLocked - 1&lt;&lt;mutexWaiterShift)                if !starving || old&gt;&gt;mutexWaiterShift == 1 {                    // 没有等待了，就要退出了                    delta -= mutexStarving                }                // 修改state的状态。                atomic.AddInt32(&amp;m.state, delta)                break            }            awoke = true            iter = 0        } else {            // atomic.CompareAndSwapInt32(&amp;m.state, old, new)            // CAS 失败，重新读下当前状态，然后再循环来一次。            old = m.state        }    }    if race.Enabled {        race.Acquire(unsafe.Pointer(m))    }}</code></pre><h2 id="2-4-Unlock"><a href="#2-4-Unlock" class="headerlink" title="2.4 Unlock"></a>2.4 Unlock</h2><pre><code>func (m *Mutex) Unlock() {    if race.Enabled {        _ = m.state        race.Release(unsafe.Pointer(m))    }    // Fast path: CAS 取消无锁状态，成功就结束    // 没有成功就进入 slow path    // 注意这里无论成功与否，mutexLocked状态已经没有了    new := atomic.AddInt32(&amp;m.state, -mutexLocked)    if new != 0 {        // Outlined slow path to allow inlining the fast path.        // To hide unlockSlow during tracing we skip one extra frame when tracing GoUnblock.        m.unlockSlow(new)    }}func (m *Mutex) unlockSlow(new int32) {    if (new+mutexLocked)&amp;mutexLocked == 0 {       // new = m.state-mutexLocked       // m.state&amp;mutexLocked == 0 表示无锁。        // 如果是无锁，上面slow path就成功了.       // 所以理论不会有这种情况        fatal("sync: unlock of unlocked mutex")    }    if new&amp;mutexStarving == 0 { // 不是饥饿状态        old := new        for {            // 如果锁没有waiter,或者锁有其他以下已发生的情况之一，则后面的工作就不用做了，直接返回            // 1. 锁处于锁定状态，表示锁已经被其他goroutine获取了            // 2. 锁处于被唤醒状态，这表明有等待goroutine被唤醒，不用再尝试唤醒其他goroutine            // 3. 锁处于饥饿模式，那么锁之后会被直接交给等待队列队头goroutine            if old&gt;&gt;mutexWaiterShift == 0 || old&amp;(mutexLocked|mutexWoken|mutexStarving) != 0 {                return            }            // 代码走到这，说明当前锁是空闲状态，等待队列中有waiter，且没有goroutine被唤醒                        // waiter - 1 然后设置唤醒状态 = 1            new = (old - 1&lt;&lt;mutexWaiterShift) | mutexWoken            if atomic.CompareAndSwapInt32(&amp;m.state, old, new) {// 设置成功                runtime_Semrelease(&amp;m.sema, false, 1) // 唤醒一个信号量                return            }            old = m.state // 对一下最新状态        }    } else {     // 饥饿模式下，唤醒信号量等待队列的头部的sudog。    // 饥饿状态过来的g都会放到信号量队列的尾部。        runtime_Semrelease(&amp;m.sema, true, 1)    }}</code></pre><p>饥饿模式下做了个优化，会调用 <a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/sema.go#L194">readyWithTime</a> 把队列头部的<code>g</code>放到<code>pp.runnext</code>里面。然后再调用<a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/sema.go#L212">goyield</a> 把当前的<code>g</code>放到<code>p runnable queue</code>的尾部，然后调用 <a href="https://github.com/golang/go/blob/947091d31ccda14b0a362adff37b6e037f0f59f3/src/runtime/proc.go#L3438">schedule</a> 函数，这样就可以优先执行等待队列中的<code>g</code>了。</p><p>详情可以看这个<code>CR</code>：<a href="https://go-review.googlesource.com/c/go/+/206180">sync: yield to the waiter when unlocking a starving mutex</a></p><h1 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h1><p>理解<code>Sync.Mutex</code>主要先理解 <a href="https://fanlv.wiki/2022/10/06/runtime-sema/">runtime.semaphore</a> ，然后再根据注释理解一下<code>normal</code>和<code>starving</code>模式就好了。</p><p>没有一定的技术深度，要设计一个<code>bugfree</code>而且高性能的锁还是挺难的。理解是一回事，离自己要去实现一个锁还差十万八千里。</p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Golang </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Go源码——runtime.semaphore</title>
      <link href="/2022/10/06/runtime-sema/"/>
      <url>/2022/10/06/runtime-sema/</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h1><p><code>sync.Mutex</code>里面用了<code>runtime_SemacquireMutex</code>和<code>runtime_Semrelease</code>，所以看下这个<code>runtime</code>的信号量是如何实现的。</p><h1 id="二、基础知识"><a href="#二、基础知识" class="headerlink" title="二、基础知识"></a>二、基础知识</h1><h2 id="2-1-信号量"><a href="#2-1-信号量" class="headerlink" title="2.1 信号量"></a>2.1 信号量</h2><p>信号量（英语：<code>semaphore</code>）又称为信号标，是一个同步对象，用于保持在<code>0</code>至指定最大值之间的一个计数值。当线程完成一次对该<code>semaphore</code>对象的等待（<code>wait</code>）时，该计数值减一；当线程完成一次对<code>semaphore</code>对象的释放（<code>release</code>）时，计数值加一。当计数值为<code>0</code>，则线程等待该<code>semaphore</code>对象不再能成功直至该<code>semaphore</code>对象变成<code>signaled</code>状态。<code>semaphore</code>对象的计数值大于<code>0</code>，为<code>signaled</code>状态；计数值等于<code>0</code>，为<code>nonsignaled</code>状态。</p><p>信号量的概念是由荷兰计算机科学家艾兹赫尔·戴克斯特拉（<code>Edsger W. Dijkstra</code>）发明的，广泛的应用于不同的操作系统中。在系统中，给予每一个进程一个信号量，代表每个进程目前的状态，未得到控制权的进程会在特定地方被强迫停下来，等待可以继续进行的信号到来。如果信号量是一个任意的整数，通常被称为计数信号量（<code>Counting semaphore</code>），或一般信号量（<code>general semaphore</code>）；如果信号量只有二进制的<code>0</code>或<code>1</code>，称为二进制信号量（<code>binary semaphore</code>）。</p><p>计数信号量具备两种操作动作，称为<code>V</code>（<code>signal()</code>）与<code>P</code>（<code>wait()</code>）（即部分参考书常称的<code>PV操作</code>）。<code>V</code>操作会增加信号标<code>S</code>的数值，<code>P</code>操作会减少它。</p><ul><li><code>P原语</code>：<code>P</code>是荷兰语<code>Proberen</code>(测试)的首字母。为阻塞原语，负责把当前进程由运行状态转换为阻塞状态，直到另外一个进程唤醒它。操作为：申请一个空闲资源(把信号量减<code>1</code>)，若成功，则退出；若失败，则该进程被阻塞；</li><li><code>V原语</code>：<code>V</code>是荷兰语<code>Verhogen</code>(增加)的首字母。为唤醒原语，负责把一个被阻塞的进程唤醒，它有一个参数表，存放着等待被唤醒的进程信息。操作为：释放一个被占用的资源(把信号量加<code>1</code>)，如果发现有被阻塞的进程，则选择一个唤醒之。</li></ul><p><a href="https://en.wikipedia.org/wiki/Semaphore_(programming)">Semaphore - wiki</a></p><h2 id="2-2-Treap"><a href="#2-2-Treap" class="headerlink" title="2.2 Treap"></a>2.2 Treap</h2><p><code>Treap</code>是<code>Binary Search Tree</code>+<code>Heap</code>的组合。</p><p>二叉查找树（<code>Binary Search Tree</code>），它或者是一棵空树，或者是具有下列性质的二叉树：若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值；若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 它的左、右子树也分别为二叉排序树。</p><p>堆有<code>大顶堆</code>和<code>小顶堆</code>：</p><ul><li><code>大顶堆</code>:每个节点的值都大于或者等于他的左右孩子节点的值。</li><li><code>小顶堆</code>:每个结点的值都小于或等于其左孩子和右孩子结点的值。</li></ul><p><code>Treap</code>既是一棵二叉查找树，也是一个二叉堆。但是这两种数据结构貌是矛盾的存在，如果是二叉查找树，就不能是一个堆，如果是一个堆，那么必然不是二叉查找树。</p><p>所以<code>Treap</code>用了一个很巧妙的方式解决这个问题：<strong>给每个键值一个随机附加的优先级，让键值满足二叉查找树的结构，让优先级满足二叉堆的结构</strong>。</p><p><code>Treap</code>它的最大优点就是实现简单，没有太多复杂的操作，但是我们前面也说了，它是通过随机的<code>priority</code>来控制树的平衡的，<strong>那么它显然无法做到完美平衡，只能做到不落入最坏的情况</strong>。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-32b6a94cb2ee19e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><a href="https://zhuanlan.zhihu.com/p/348383884">Treap——堆和二叉树的完美结合，性价比极值的搜索树</a></p><h2 id="2-3-x-sync-semaphore"><a href="#2-3-x-sync-semaphore" class="headerlink" title="2.3 x/sync/semaphore"></a>2.3 x/sync/semaphore</h2><p><code>Go</code>的 <a href="https://github.com/golang/go/wiki/X-Repositories">X-Repositories</a> 提供了一种带权重的信号量实现方式 <a href="https://github.com/golang/sync/blob/master/semaphore/semaphore.go">sync.semaphore</a>。这个跟<code>runtime.semaphore</code>其实没太大关系。主要是提供了个<code>high-level</code>的信号量给<code>Go</code>开发者使用。实现方式如下：</p><pre><code>type Weighted struct {    size    int64 // 资源的总数，Acquire(n) 的时候会消耗这个资源，    cur     int64 // 当前已申请资源数，Acquire(n)成功的话，cur=cur+n    mu      sync.Mutex // 互斥锁，所有Acquire、Release 都要加锁    waiters list.List // 阻塞的队列}type waiter struct {    n     int64    ready chan&lt;- struct{} // 使用 channle 来通信}Release(n) 释放n个资源，然后去</code></pre><p><code>Weighted</code>就是一个权重的信号量，主要提供<code>Acquire(n)</code>和<code>Release(n)</code>两个操作。实现逻辑比较简单。</p><p><code>Acquire(n)</code>申请<code>n</code>个资源，申请成功的话会设置<code>cur=cur+n</code>，如果没有资源可以申请了，会<code>new</code>一个<code>waiter</code>，然后把这个<code>waiter</code>加到<code>waiters</code>这个等待的队列中，并阻塞在<code>waiter.ready</code>的读上面。</p><p><code>Release(n)</code>释放<code>n</code>个资源，然后设置<code>cur=cur-n</code>，在<code>waiters</code>这个等待的队列中，循环取出取<code>waiters</code>头部的<code>waiter</code>（直到<code>s.size-s.cur &lt; w.n</code>终止），调用<code>close(waiter.ready)</code>，这样阻塞在<code>waiter.ready</code>读上的<code>goroutine</code>会被唤醒。</p><p>下面写了个简单<code>Demo</code>：</p><pre><code>func main() {    var (        wg  sync.WaitGroup        ctx = context.Background()    )    cpuNum := runtime.GOMAXPROCS(0)    sem := semaphore.NewWeighted(int64(cpuNum)) // 设置 goroutine 最大并发数 = cpuNum    for i := 0; i &lt; 100; i++ {        wg.Add(1)        err := sem.Acquire(ctx, 1)        if err != nil {            panic(err)        }        go func(i int) {            fmt.Println("ng: ", runtime.NumGoroutine(), " i = ", i)            sem.Release(1)            wg.Done()        }(i)    }    wg.Wait()}</code></pre><h2 id="2-4-sudog"><a href="#2-4-sudog" class="headerlink" title="2.4 sudog"></a>2.4 sudog</h2><p><code>sudog</code>表示为了一个等待队列中的<code>goroutine</code>，比如因为读写<code>channel</code>阻塞，或者<code>Lock</code>导致阻塞（<code>Lock</code>底层其实就是<code>semaphore</code>）等的<code>goroutine</code>，由于这些阻塞的<code>goroutine</code>不确定什么时候能被调度（取消阻塞比如<code>unlock</code>、读写<code>channel</code>），所以这种阻塞的<code>goroutine</code>不适合一直放在<code>p</code>的本地运行队列中，这个时候会把阻塞的<code>gorutine</code>打包成一个<code>sudog</code>里面会有一些这个<code>g</code>运行的上下文。然后存到另外一个地方，比如<code>channel</code>是存在 <a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/chan.go#L42">recvq</a> 和 <a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/chan.go#L43">sendq</a> 中，而阻塞在信号量上的<code>goroutine</code>是存在 <a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/sema.go#L49">semTable</a> 中。</p><p>具体<code>sudog</code>的<a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/runtime2.go#L336">结构体如下</a>：</p><pre><code>// sudog 代表一个处于等待队列的g，比如阻塞在读写 channle上 的 goroutine// sduog 和 g 是多对多的关系，一个 g 可以在多个 wait lists 上，所以一个 g 可以对应多个 sduog// sudogs 会有两级缓存，优先去p的sudogcache取，取不到则去全局的sudogcache取一批（直到本地容量达到50%）// sudogs 通过 acquireSudog 和 releaseSudog 去申请或释放type sudog struct {    // 在channel的场景中sudog中所有字段都受hchan.lock保护    g *g    next *sudog // 双向链表，指向下一个 sduog    prev *sudog // 双向链表，指向上一个 sduog        // channel场景存的是，读写的数据。    // semaphore 场景存的是信号量的地址。    elem unsafe.Pointer     // 下面这些字段不会被并发访问    // For channels, waitlink is only accessed by g.    // For semaphores, 所有字段需要拿到semaRoot的lock才能访问    acquiretime int64 // semaphore 场景使用的，记录获取信号量时间    releasetime int64 // 释放时间    ticket      uint32 // treap 里面堆用的随机的权重    // 只在select 场景使用的字段，表明当前g是否被选中，然后唤醒    isSelect bool    // 只在channel场景使用的    success bool    // 只在 semaphore 场景使用    parent   *sudog // semaRoot binary tree    waitlink *sudog // semaRoot 节点对应的等待列表    waittail *sudog // semaRoot 等待列表的尾部节点    c        *hchan // 只在channel场景使用的，关联的channel}</code></pre><p>从<code>sodug</code>结构体看出来，<code>sudog</code>里面的字段分别跟<code>channel</code>、<code>semaphore</code>、<code>select</code>几个场景有关。某些字段只有在特点场景才会用到，感觉全部都耦合在一个<code>Struct</code>不够<code>优雅</code>。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-47020d0b509ec767.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sudog on channel"></p><h2 id="2-5-semtable"><a href="#2-5-semtable" class="headerlink" title="2.5 semtable"></a>2.5 semtable</h2><p><a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/sema.go#L49">semTable</a> 是一个长度为<code>251</code>的全局数组，每个<code>semaRoot</code>指向一个<code>treap</code>，主要用于存放阻塞在信号量(<code>semaphore</code>)上的<code>sudog</code></p><pre><code>var semtable [semTabSize]struct {    root semaRoot    pad  [cpu.CacheLinePadSize - unsafe.Sizeof(semaRoot{})]byte // 防止 flase sharding}</code></pre><p><code>semaRoot</code>最早是双向链表，在某些场景下性能比较查，所以优化成了<code>treap</code>，具体可以看 <a href="https://go-review.googlesource.com/c/go/+/37103">CR37103</a>    </p><p>优化之后<code>semtable</code>存储的结构大概是这样：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b35dab034e457bd4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="2-6-mutex"><a href="#2-6-mutex" class="headerlink" title="2.6 mutex"></a>2.6 mutex</h2><p><code>runtime</code>包里面的锁都是使用的<code>runtime</code>内部实现的<code>mutex</code>，具体是使用<code>CAS</code>+<code>futex</code>来实现的。更多详见<a href="https://fanlv.wiki/2022/10/05/runtime-mutex/">Go源码——runtime.mutex</a></p><h1 id="三、semaphore-源码分析"><a href="#三、semaphore-源码分析" class="headerlink" title="三、semaphore 源码分析"></a>三、semaphore 源码分析</h1><p><code>semaphore</code>基本操作，在 <a href="https://github.com/golang/go/blob/release-branch.go1.18/src/sync/runtime.go#L8">src/sync/runtime.go</a> 定义了下面几个方法：</p><pre><code>// Semacquire等待*s &gt; 0，然后原子递减它。// 它是一个简单的睡眠原语，用于同步库使用。func runtime_Semacquire(s *uint32)// SemacquireMutex类似于Semacquire,用来阻塞互斥的对象// 如果lifo为true，waiter将会被插入到队列的头部// skipframes是跟踪过程中要省略的帧数func runtime_SemacquireMutex(s *uint32, lifo bool, skipframes int)// Semrelease会自动增加*s并通知一个被Semacquire阻塞的等待的goroutine// 它是一个简单的唤醒原语，用于同步库// 如果handoff为true, 传递信号到队列头部的waiter// skipframes是跟踪过程中要省略的帧数，从这里开始计算func runtime_Semrelease(s *uint32, handoff bool, skipframes int)</code></pre><p>这个几个函数具体的实现在 <a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/sema.go#L53">src/runtime/sema.go</a>。</p><pre><code>//go:linkname sync_runtime_Semacquire sync.runtime_Semacquirefunc sync_runtime_Semacquire(addr *uint32) {    semacquire1(addr, false, semaBlockProfile, 0)}//go:linkname poll_runtime_Semacquire internal/poll.runtime_Semacquirefunc poll_runtime_Semacquire(addr *uint32) {    semacquire1(addr, false, semaBlockProfile, 0)}//go:linkname sync_runtime_Semrelease sync.runtime_Semreleasefunc sync_runtime_Semrelease(addr *uint32, handoff bool, skipframes int) {    semrelease1(addr, handoff, skipframes)}//go:linkname sync_runtime_SemacquireMutex sync.runtime_SemacquireMutexfunc sync_runtime_SemacquireMutex(addr *uint32, lifo bool, skipframes int) {    semacquire1(addr, lifo, semaBlockProfile|semaMutexProfile, skipframes)}//go:linkname poll_runtime_Semrelease internal/poll.runtime_Semreleasefunc poll_runtime_Semrelease(addr *uint32) {    semrelease(addr)}</code></pre><h2 id="3-1-获取信号量"><a href="#3-1-获取信号量" class="headerlink" title="3.1 获取信号量"></a>3.1 获取信号量</h2><p><code>semaphore</code>获取信号量操作步骤如下：</p><ol><li> 调用<code>runtime_SemacquireMutex</code> （比如<code>sync.Mutex.Lock()</code>场景）</li><li><code>sync_runtime_SemacquireMutex</code> </li><li><code>semacquire1</code> </li><li><code>CAS(addr, v, v-1)</code>状态成功就返回，失败继续往下</li><li> 缓存池拿一个<code>sudog</code>，或者<code>new</code>一个<code>sudog</code>（<code>acquireSudog</code>） </li><li>把<code>g</code>相关的数据存到<code>sudog</code>中。</li><li>循环 <ul><li>对当前<code>semaRoot</code>加锁 </li><li><code>nwait++</code> </li><li><code>cansemacquire/CAS(addr, v, v-1)</code> </li><li><code>sudog</code>加到<code>semaRoot</code>的<code>treap</code>中/<code>root.queue()</code> </li><li>可能要调整树的结构（左旋<code>rotateRight</code>/右旋<code>rotateLeft</code>）防止树退化为链表</li><li><code>goparkunlock</code>让出当前<code>g</code>的执行</li><li>被唤醒 </li><li><code>CAS</code>成功或者<code>s.ticket != 0</code>（当前没有其他竞争者了） 认为成功 </li><li>否则继续循环</li></ul></li><li>最后释放<code>sudog</code>/<code>releaseSudog </code></li></ol><p><a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/sema.go#L98">具体源码如下</a>：</p><pre><code>func semacquire1(addr *uint32, lifo bool, profile semaProfileFlags, skipframes int) {    gp := getg()    if gp != gp.m.curg {// 判断下g是不是当前m绑定的g        throw("semacquire not on the G stack")    }    // CAS(addr, v, v-1) 成功就直接成功否则一直循环，如果 *addr = 0 返回 false 走下面 slowpath    if cansemacquire(addr) {        return    }        // 走到这里表示当前g要阻塞    // 下面逻辑，就是把g封装成sudog,然后存到semTable中。    // 最后调用 gopark 让出当前g    s := acquireSudog() // 这个先去 p.sudogcache 拿，没拿到去全局sudohgcache拿    root := semroot(addr) // 根据sema的地址，算出用到semTable中哪个semaRoot    t0 := int64(0)    s.releasetime = 0    s.acquiretime = 0    s.ticket = 0    if profile&amp;semaBlockProfile != 0 &amp;&amp; blockprofilerate &gt; 0 {        t0 = cputicks()        s.releasetime = -1    }    if profile&amp;semaMutexProfile != 0 &amp;&amp; mutexprofilerate &gt; 0 {        if t0 == 0 {            t0 = cputicks()        }        s.acquiretime = t0    }    for {        lockWithRank(&amp;root.lock, lockRankRoot) // 加锁，方面下面修改 semaRoot的属性        // 对等待的计数加1，这样sema_release时候不会走快路径        atomic.Xadd(&amp;root.nwait, 1)        // 看下是否有其他的goroutine调用了sema_release        // 在尝试 CAS(addr, v, v-1) 试下        if cansemacquire(addr) {            atomic.Xadd(&amp;root.nwait, -1)            unlock(&amp;root.lock)            break        }                // 这里，就是这个新的 sudog 加到 semaTable中的        root.queue(addr, s, lifo)        goparkunlock(&amp;root.lock, waitReasonSemacquire, traceEvGoBlockSync, 4+skipframes) // 这你会让出当前的goroutine                        // goroutine 被调度回来了，表示有 sema_release 以后唤醒了这个 sema        // s.ticket != 0 表示是等待队列头部的 sudog，当前队列只有一个sudog了，所以直接结束        // CAS(addr, v, v-1) 成功也结束        if s.ticket != 0 || cansemacquire(addr) {            break        }    }    if s.releasetime &gt; 0 {        blockevent(s.releasetime-t0, 3+skipframes)    }    releaseSudog(s) // 释放 sudog}func cansemacquire(addr *uint32) bool {    for {        v := atomic.Load(addr)        if v == 0 {            return false        }        if atomic.Cas(addr, v, v-1) {            return true        }    }}func acquireSudog() *sudog {    // 设置禁止抢占    mp := acquirem()    pp := mp.p.ptr()    //当前本地sudog缓存没有了，则去全局缓存中拉取一批    if len(pp.sudogcache) == 0 {        lock(&amp;sched.sudoglock)        // 首先尝试从全局缓存中获取sudog，直到本地容量达到50%        for len(pp.sudogcache) &lt; cap(pp.sudogcache)/2 &amp;&amp; sched.sudogcache != nil {            s := sched.sudogcache            sched.sudogcache = s.next            s.next = nil            pp.sudogcache = append(pp.sudogcache, s)        }        unlock(&amp;sched.sudoglock)        // 如果全局缓存为空，则分配创建一个新的sudog        if len(pp.sudogcache) == 0 {            pp.sudogcache = append(pp.sudogcache, new(sudog))        }    }    n := len(pp.sudogcache)    s := pp.sudogcache[n-1]    pp.sudogcache[n-1] = nil    pp.sudogcache = pp.sudogcache[:n-1]    if s.elem != nil {        throw("acquireSudog: found s.elem != nil in cache")    }    //解除抢占限制    releasem(mp)    return s}</code></pre><h2 id="3-2-释放信号量"><a href="#3-2-释放信号量" class="headerlink" title="3.2 释放信号量"></a>3.2 释放信号量</h2><p><code>semaphore</code>释放信号量操作步骤如下：</p><ol><li>调用<code>runtime_Semrelease</code>，比如<code>sync.Mutex.Unlock()</code>场景。</li><li><code>sync_runtime_Semrelease</code>  </li><li><code>semrelease1</code></li><li> 原子<code>*addr++</code></li><li><code>nwait=0</code>，表示没有阻塞在这个信号量上的<code>g</code>直接返回。</li><li>有阻塞的<code>g</code>在<code>semTable</code>中找到对应的<code>semaRoot</code>，然后<code>对</code>semaRoot`加锁。</li><li>再次<code>check</code>下<code>nwait=0</code>，等于<code>0</code>直接返回。</li><li>拿到<code>sema</code>的<code>addres</code>在<code>semTable</code>中对应的队列头部的<code>seamRoot</code>。</li><li><code>dequeue</code>是否需要调整左旋<code>rotateLeft</code>或者右旋<code>rotateRight</code>调整树结构。</li><li><code>readyWithTime</code>，调用<code>goread</code>唤醒<code>sudog</code>绑定的<code>g</code>。</li><li><code>goyield</code></li></ol><p><a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/sema.go#L155">semrelease源码如下：</a></p><pre><code>func semrelease1(addr *uint32, handoff bool, skipframes int) {    root := semroot(addr)    atomic.Xadd(addr, 1)    // 没有等待者直接返回    if atomic.Load(&amp;root.nwait) == 0 {        return    }    //查找一个等待着并唤醒它    lockWithRank(&amp;root.lock, lockRankRoot)    if atomic.Load(&amp;root.nwait) == 0 {        //计数已经被其他goroutine消费，所以不需要唤醒其他goroutine        unlock(&amp;root.lock)        return    }    s, t0 := root.dequeue(addr) //查找第一个出现的addr    if s != nil {        atomic.Xadd(&amp;root.nwait, -1)    }    unlock(&amp;root.lock)    if s != nil { // 可能比较慢 甚至被挂起所以先unlock        acquiretime := s.acquiretime        if acquiretime != 0 {            mutexevent(t0-acquiretime, 3+skipframes)        }        if s.ticket != 0 {            throw("corrupted semaphore ticket")        }        if handoff &amp;&amp; cansemacquire(addr) {            s.ticket = 1        }        //goready(s.g,5)标记runnable 等待被重新调度        readyWithTime(s, 5+skipframes)        if s.ticket == 1 &amp;&amp; getg().m.locks == 0 {            // 直接切换G            // readyWithTime已经将等待的G作为runnext放到当前的P            // 我们现在调用调度器可以立即执行等待的G            // 注意waiter继承了我们的时间片：这是希望避免在P上无限得进行激烈的信号量竞争            // goyield类似于Gosched，但是它是发送“被强占”的跟踪事件，更重要的是，将当前G放在本地runq            // 而不是全局队列。            // 我们仅在饥饿状态下执行此操作(handoff=true),因为非饥饿状态下，当我们yielding/scheduling时，            // 其他waiter可能会获得信号量，这将是浪费的。我们等待进入饥饿状体，然后开始进行ticket和P的手递手交接            // See issue 33747 for discussion.            // https://go-review.googlesource.com/c/go/+/206180            goyield()        }    }}</code></pre><h1 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h1><p>获取信号量操作主要尝试把<code>sema</code>地址<code>CAS</code>方式原子减<code>1</code>，成就直接返回，失败以后会把当前<code>g</code>打包成<code>sudog</code>然后保存到<code>semTable</code>，然后调用<code>gopark</code>让出当前的<code>goroutine</code>。</p><p>释放信号量操作就是吧<code>sema</code>地址加<code>1</code>，然后看有没有等待中的<code>g</code>，没有直接返回，有的话去<code>semaTable</code>的等待队列取出然后调用<code>goready</code>唤醒对应的<code>g</code>。</p><p>主要理解<code>semaTable</code>里面存储<code>sudog</code>的方式就好了。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzA3NDgxNQ==&amp;mid=2257484775&amp;idx=1&amp;sn=9003cf2c4693c75094f6fd2dd5c7ea6e&amp;chksm=9c6e4af7ab19c3e1f0db5aa4f4205659a12448889ef945d999215548a4d727719c19f7514c75&amp;scene=21#wechat_redirect">手摸手Go 并发编程基建Semaphore</a></p><p><a href="https://www.zhangshengrong.com/p/q0arAJWQ1x/">一文读懂go中semaphore(信号量)源码</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Golang </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Go源码——runtime.mutex</title>
      <link href="/2022/10/05/runtime-mutex/"/>
      <url>/2022/10/05/runtime-mutex/</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h1><p>在<code>Go</code>的<code>runtime</code>包中封装了一个 <a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/runtime2.go#L161">mutux</a> ，这个<code>mutex</code>被<code>runtime</code>包中大量组件使用，比如 <a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/chan.go#L51">channel</a>、<a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/netpoll.go#L98">netpoll</a>、<a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/proc.go#L3279">Runtime 调度的 checkTimers</a> 等等。</p><p><strong>sync.Mutex和runtime.mutext区别：</strong>简单说就是<code>sync.Mutex</code>是上层的锁，<code>Lock</code>拿锁失败会造成<code>goroutine</code>阻塞（会调用<code>gopark</code>）。<code>runtime.mutex</code> 是给 <code>runtime</code>使用的锁，<code>Lock</code>拿锁失败，会造成<code>m</code>阻塞（线程阻塞，底层调用的<code>futex</code>）。</p><h1 id="二、基础知识"><a href="#二、基础知识" class="headerlink" title="二、基础知识"></a>二、基础知识</h1><h2 id="2-1-Mutex"><a href="#2-1-Mutex" class="headerlink" title="2.1 Mutex"></a>2.1 Mutex</h2><p><code>Mutex</code> 全称是<code>Mutual Exclusion</code> ，俗称互斥体或者互斥锁。是一种用于多线程编程中，防止两条线程同时对同一公共资源（比如全局变量）进行读写的机制。该目的通过将代码切片成一个一个的临界区域（<code>critical section</code>）达成。临界区域指的是一块对公共资源进行访问的代码，并非一种机制或是算法。一个程序、进程、线程可以拥有多个临界区域，但是并不一定会应用互斥锁。</p><h2 id="2-2-mmap-函数"><a href="#2-2-mmap-函数" class="headerlink" title="2.2 mmap 函数"></a>2.2 mmap 函数</h2><p><code>mmap</code>它的主要功能是将一个<code>虚拟内存区域</code>与一个<code>磁盘上的文件</code>关联起来，以初始化这个虚拟内存区域的内容，这个过程成为内存映射（<code>memory mapping</code>）。</p><p>直白一点说，就是可以将<code>一个文件</code>，映射到一段<code>虚拟内存</code>，写内存的时候操作系统会自动同步内存的内容到文件。内存同步到磁盘，还涉及到一个<code>PageCache</code>的概念，这里不去过度发散。</p><p><code>文件</code>可以是磁盘上的一个<code>实体文件</code>，比如<code>kafka</code>写日志文件的时候，就用了<code>mmap</code>。</p><p><code>文件</code>也可以是一个<code>匿名文件</code>，这种场景<code>mmap</code>不会去写磁盘，主要用于内存申请的场景。比如调用<code>malloc</code>函数申请内存，当申请的大小超过<code>MMAP_THRESHOLD</code>（默认是<code>128K</code>）大小，内核就会用<code>mmap</code>去申请内存。再比如<code>TCMalloc</code>也是通过<code>mmap</code>来申请一大块内存（<code>匿名文件</code>），然后切割内存，分配给程序使用。</p><p>网上很多资料一介绍<code>mmap</code>，就会说到<code>zero copy</code>，就是相对于<code>标准IO</code>来说少了一次内存<code>Copy</code>的开销。让大多数人忽略了<code>mmap</code>本质的功能，认为<code>mmap=zero copy</code></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-58f26fcf756d90b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>还有一个值得一说的<code>mmap</code>申请的内存不在虚拟地址空间的<code>堆区</code>，在<code>内存映射段（Memory Mapping Region）</code></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1987f229490dbaf5.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Application.jpg"></p><h2 id="2-3-Futex"><a href="#2-3-Futex" class="headerlink" title="2.3 Futex"></a>2.3 Futex</h2><p><code>Futex</code>是<code>Fast Userspace Mutexes</code>的缩写。是一个在<code>Linux</code>上实现锁定和构建高级抽象锁如信号量和<code>POSIX</code>互斥的基本工具。</p><p><code>Futex</code>由一块能够被多个进程共享的内存空间（一个对齐后的整型变量）组成；这个整型变量的值能够通过汇编语言调用<code>CPU</code>提供的原子操作指令来增加或减少，并且一个进程可以等待直到那个值变成正数。<code>Futex</code>的操作几乎全部在用户空间完成；只有当操作结果不一致从而需要仲裁时，才需要进入操作系统内核空间执行。这种机制允许使用<code>Futex</code>的锁定原语有非常高的执行效率：由于绝大多数的操作并不需要在多个进程之间进行仲裁，所以绝大多数操作都可以在应用程序空间执行，而不需要使用（相对高代价的）内核系统调用。</p><p><code>futex</code>的基本思想是竞争态总是很少发生的，只有在竞争态才需要进入内核，否则在用户态即可完成。<code>futex</code>的两个目标是：</p><ol><li>尽量避免系统调用；</li><li>避免不必要的上下文切换（导致的<code>TLB</code>失效等）。</li></ol><p><strong>Futex总结</strong></p><p>简单一句话说就是，<code>futex</code>基于<code>mmap</code>来映射一段内存记录锁的状态，使用<code>mmap</code>有两个好处，1）支持跨进程同步锁状态。2）用户态和内核态可以共用一块内存（<code>zero copy</code>也是说的这个），这样在用户态可以直接修改锁状态不用切换到内核态。<code>futex</code>加锁和解锁，都是先通过<code>CAS</code>（这个<code>CPU</code>支持的指令<code>CMPXCHGQ</code>，不需要系统调用）尝试设置把状态置<code>1</code>（加锁）或者置<code>0</code>（解锁），如果成功了，就正常返回，如果<code>CAS</code>失败，就会进行系统调用。</p><p>伪代码如下：</p><pre><code>/*val 0: unlockval 1: lock, no waitersval 2: lock , one or more waiters*/int val = 0;void lock(){    int c    if ((c = cmpxchg(val, 0, 1)) != 0) {        if (c != 2)            c = xchg(val, 2);        while (c != 0) {            futex_wait((&amp;val, 2); // 系统调用            c = xchg(val, 2);        }    }}       void unlock(){       if (atomic_dec(val) != 1){        val = 0;               futex_wake(&amp;val, 1); // 系统调用    }}//uaddr指向一个地址，val代表这个地址期待的值，当*uaddr==val时，才会进行waitint futex_wait(int *uaddr, int val);//唤醒n个在uaddr指向的锁变量上挂起等待的进程int futex_wake(int *uaddr, int n);</code></pre><p>其实这种操作现在挺常见的，<code>Sync.Mutex</code> 也有<code>fastpath</code>和<code>slowpath</code>，<code>fastpath</code>就是先尝试自旋<code>4</code>次<code>CAS</code>方式加锁。</p><p><a href="https://zh.m.wikipedia.org/zh/Futex">Futex-Wiki</a></p><p><a href="http://blog.foool.net/2021/04/futex-%E7%BB%BC%E8%BF%B0/">futex-综述</a></p><p><a href="https://blog.csdn.net/weixin_44062361/article/details/118357206">Futex系统调用</a></p><p><a href="https://jishuin.proginn.com/p/763bfbd55ad5">golang并发底层</a></p><h2 id="2-4-procyield-功能"><a href="#2-4-procyield-功能" class="headerlink" title="2.4 procyield 功能"></a>2.4 procyield 功能</h2><p><a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/asm_amd64.s#L729">实现代码</a>如下：</p><pre><code>TEXT runtime·procyield(SB),NOSPLIT,$0-0    MOVL    cycles+0(FP), AXagain:    PAUSE    SUBL    $1, AX    JNZ    again    RET</code></pre><p>由上面代码可以知道，就是执行<code>n</code>次<code>PAUSE</code>指令，<code>n</code>是函数调用传入的参数。</p><p><code>PAUSE</code>指令的功能。查了下 <a href="https://c9x.me/x86/html/file_module_x86_id_232.html">X86 指令集 - PAUSE</a>，主要是<strong>提高自旋的性能</strong>。</p><h2 id="2-5-osyield"><a href="#2-5-osyield" class="headerlink" title="2.5 osyield"></a>2.5 osyield</h2><p><a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/sys_linux_amd64.s#L647">实现代码</a>如下：</p><pre><code>#define SYS_sched_yield     24TEXT runtime·osyield(SB),NOSPLIT,$0    MOVL    $SYS_sched_yield, AX    SYSCALL    RET</code></pre><p><code>osyield</code>主要做了个系统调用，<code>AX = 24</code>，查下 <a href="https://chromium.googlesource.com/chromiumos/docs/+/HEAD/constants/syscalls.md#x86_64_24">Linux System Call Table</a>，可以知道调用的<code>sched_yield</code>这个函数。看下 <a href="https://man7.org/linux/man-pages/man2/sched_yield.2.html">sched_yield</a> 的描述，主要功能是：让当前线程放弃CPU，把线程移到队列尾部，让优先执行其他线程。</p><h2 id="2-6-futexsleep-和-futexwakeup"><a href="#2-6-futexsleep-和-futexwakeup" class="headerlink" title="2.6 futexsleep 和 futexwakeup"></a>2.6 futexsleep 和 futexwakeup</h2><p><code>Go</code>的<code>futexsleep</code>和<code>futexwakeup</code>就是对<code>futex</code>的封装，<a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/os_linux.go#L53">实现代码如下</a> ：</p><pre><code>// 如果 *addr == val { 当前线程进入sleep状态 } ；不会阻塞超过ns，ns&lt;0表示永远休眠futexsleep(addr *uint32, val uint32, ns int64)//如果任何线程阻塞在addr上，则唤醒至少cnt个阻塞的任务futexwakeup(addr *uint32, cnt uint32) </code></pre><p><code>futex</code>的<a href="https://github.com/golang/go/blob/947091d31ccda14b0a362adff37b6e037f0f59f3/src/runtime/sys_linux_amd64.s#L542">具体实现</a>：</p><pre><code>// int64 futex(int32 *uaddr, int32 op, int32 val,//    struct timespec *timeout, int32 *uaddr2, int32 val2);TEXT runtime·futex(SB),NOSPLIT,$0    MOVQ    addr+0(FP), DI    MOVL    op+8(FP), SI    MOVL    val+12(FP), DX    MOVQ    ts+16(FP), R10    MOVQ    addr2+24(FP), R8    MOVL    val3+32(FP), R9    MOVL    $SYS_futex, AX    SYSCALL    MOVL    AX, ret+40(FP)    RET</code></pre><h1 id="三、runtime-mutex-源码分析"><a href="#三、runtime-mutex-源码分析" class="headerlink" title="三、runtime.mutex 源码分析"></a>三、runtime.mutex 源码分析</h1><h2 id="3-1-mutex-结构"><a href="#3-1-mutex-结构" class="headerlink" title="3.1 mutex 结构"></a>3.1 mutex 结构</h2><p><code>runtime</code>的 <a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/runtime2.go#L161">mutex</a> 定义在<code>runtime/runtime2.go</code>中。定义如下：</p><pre><code>type mutex struct {    // Empty struct if lock ranking is disabled, otherwise includes the lock rank    lockRankStruct    // Futex-based impl treats it as uint32 key,    // while sema-based impl as M* waitm.    // Used to be a union, but unions break precise GC.    key uintptr}</code></pre><p><code>lockRankStruct</code>这个是给<code>runtime</code>做<a href="https://go-review.googlesource.com/c/go/+/207348">死锁</a>检测用的，只有设置了<code>GOEXPERIMENT=staticlockranking</code>才<code>lockRankStruct</code>才会有<a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/lockrank_on.go#L18">具体实现</a>，否则的话这个结构体只会是个<a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/lockrank_off.go#L9">空Struct</a>，空的<code>Struct</code>只要不是最后一个字段是不会占用任何空间的（详见<a href="https://gfw.go101.org/article/unofficial-faq.html#final-zero-size-field">final-zero-size-field</a>），具体<code>lockrank</code>的<code>CR</code>，可以看这个<a href="https://go-review.googlesource.com/c/go/+/207619">提交</a>。<code>lookrank</code>主要通过加锁<a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/lockrank.go#L187">顺序</a> 来判断是否会死锁，如果加锁顺序不符合预期就会<code>throw</code>异常（注意这个不是<code>panic</code>不能被<code>recover</code>）。<a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/lockrank_on.go#L120">具体代码</a>如下：</p><pre><code>// checkRanks checks if goroutine g, which has mostly recently acquired a lock// with rank 'prevRank', can now acquire a lock with rank 'rank'.////go:systemstackfunc checkRanks(gp *g, prevRank, rank lockRank) {    rankOK := false    if rank &lt; prevRank {        // If rank &lt; prevRank, then we definitely have a rank error        rankOK = false    } else if rank == lockRankLeafRank {        // If new lock is a leaf lock, then the preceding lock can        // be anything except another leaf lock.        rankOK = prevRank &lt; lockRankLeafRank    } else {        // We've now verified the total lock ranking, but we        // also enforce the partial ordering specified by        // lockPartialOrder as well. Two locks with the same rank        // can only be acquired at the same time if explicitly        // listed in the lockPartialOrder table.        list := lockPartialOrder[rank]        for _, entry := range list {            if entry == prevRank {                rankOK = true                break            }        }    }    if !rankOK {        printlock()        println(gp.m.procid, " ======")        printHeldLocks(gp)        throw("lock ordering problem")    }}</code></pre><h2 id="3-2-lock-实现"><a href="#3-2-lock-实现" class="headerlink" title="3.2 lock 实现"></a>3.2 lock 实现</h2><p>在<code>macOS</code>和<code>Windows</code>上<code>runtime.mutex</code>是基于<code>pthread_mutex_lock</code>来实现的，详见 <a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/lock_sema.go#L35">lock_sema</a>。而在<code>Linux</code>上<code>lock</code>是基于<code>futex</code>来实现的，详见 <a href="https://github.com/golang/go/blob/release-branch.go1.18/src/runtime/lock_futex.go#L46">lock_futex</a>。这里我们只关注<code>Linux</code>下的实现。</p><pre><code>func lock(l *mutex) {    lockWithRank(l, getLockRank(l))}func lockWithRank(l *mutex, rank lockRank) {    lock2(l)}func lock2(l *mutex) {    gp := getg() // 获取当前的 goroutine    if gp.m.locks &lt; 0 {        throw("runtime·lock: lock count")    }    gp.m.locks++ // g绑定的m的lock数量加1      // l.key 只有三种状态 mutex_unlocked、mutex_locked、mutex_sleeping      // mutex_unlocked 表示无锁状态      // mutex_locked 正常加锁状态      // mutex_sleeping 表示有线程调用futexsleep阻塞了    // 设置状态为 mutex_locked ，注意这里是直接设置，不是CAS    v := atomic.Xchg(key32(&amp;l.key), mutex_locked)    if v == mutex_unlocked { // 之前的状态是 mutex_unlocked 表示加锁成功了        return    }     // 走到这里，表示没有加锁成功     // 这里 v 不是 mutex_unlocked 所以只能是 MUTEX_LOCKED 或 MUTEX_SLEEPING    // 所以 wait 可能是 MUTEX_LOCKED 或 MUTEX_SLEEPING    // 如果我们将 l-&gt;key 从 MUTEX_SLEEPING 更改为其他值，我们必须小心在返回之前将其更改回 MUTEX_SLEEPING    wait := v    // 多核情况下尝试自旋4次，单个就不用自旋了    spin := 0    if ncpu &gt; 1 {        spin = active_spin // active_spin = 4    }    for {        for i := 0; i &lt; spin; i++ {                 // 注意我们上面设置了 l.key = mutex_locked                // 这里如果 key = mutex_unlocked，表示肯定是其他持有锁的线程进行了锁的释放            for l.key == mutex_unlocked {                    // CAS 抢锁成功直接返回，否则再尝试自旋                if atomic.Cas(key32(&amp;l.key), mutex_unlocked, wait) {                    return                }            }            procyield(active_spin_cnt) // 执行 active_spin_cnt = 30 次 PAUSE指令        }        // passive_spin = 1 ，再尝试抢一次锁。        for i := 0; i &lt; passive_spin; i++ {            for l.key == mutex_unlocked {                if atomic.Cas(key32(&amp;l.key), mutex_unlocked, wait) {                    return                }            }            osyield() // CAS 失败，系统调用`sched_yield`让出CPU        }                v = atomic.Xchg(key32(&amp;l.key), mutex_sleeping)        if v == mutex_unlocked {               // 注意这里，如果是从 mutex_unlocked =&gt; mutex_sleeping 也认为是加锁成功，然后直接返回，不会走futexsleep阻塞当前线程。               // 造成的影响就是，解锁的时候执行，执行 futexwakeup了，但是没有需要唤醒的线程（功能上应该没有影响）            return         }        wait = mutex_sleeping // 设置 wait 状态为 mutex_sleeping 下次循环会设置为 mutex_sleeping 状态        // l.key == mutex_sleeping 就 sleep，直到被唤醒。        // 不然继续循环        futexsleep(key32(&amp;l.key), mutex_sleeping, -1)    }}</code></pre><p><strong>lock主要步骤如下：</strong></p><ol><li>调用<code>atomic.Xchg</code>直接设置<code>key</code>的状态为<code>mutex_locked</code>（注意这里不是<code>CAS</code>，是直接设置）。</li><li>根据<code>atomic.Xchg</code>返回的状态<code>v</code>，来判断是否加锁成功了，如果<code>v = mutex_unlocked</code>表示加锁成功了（这个时候可以直接返回了）。否则就是加锁失败，这个时候<code>v</code>可能是<code>MUTEX_LOCKED</code>或者<code>MUTEX_SLEEPING</code>的状态。</li><li>如果是多核的话，会尝试自旋<code>4</code>，把<code>l.key</code>从状态<code>mutex_unlocked</code>改成<code>wait</code>。注意，我们在<code>步骤1</code>里面直接设置了<code>key</code>为<code>mutex_locked</code>，如果这里<code>l.key = mutex_unlocked</code>,只能说明是其他持有锁的线程释放了锁。这个<code>CAS</code>成功，表示加锁成功。如果加锁失败，会调用下<code>procyield</code>优化下自旋性能。</li><li>自旋<code>4</code>次失败，会再尝试一次<code>CAS</code>，失败的话会调用<code>osyield</code>让出<code>CPU</code>。</li><li><code>osyield</code>完成以后，继续执行，这个时候直接调用<code>atomic.Xchg</code>设置<code>l.key = mutex_sleeping</code>,表示当前准备调用<code>futexsleep</code>进行<code>sleep</code>。</li><li>使用系统调用<code>futexsleep</code>，如果<code>l.key == mutex_sleeping</code>,则当前线程进入失眠状态，直到有其他地方调用<code>futexwakeup</code>来唤醒。如果这个时候<code>l.key != mutex_sleeping</code>，说明在<code>步骤5</code>设置完这短短时间内，其他线程设置又重新设置了<code>l.key</code>状态比如设置为了<code>mutex_locked</code>或者<code>mutex_unlocked</code>。这个时候不会进入<code>sleep</code>，而是会去循环执行<code>步骤1</code>。</li></ol><h2 id="3-2-unlock-实现"><a href="#3-2-unlock-实现" class="headerlink" title="3.2 unlock 实现"></a>3.2 unlock 实现</h2><pre><code>func unlock(l *mutex) {    unlockWithRank(l)}func unlockWithRank(l *mutex) {    unlock2(l)}func unlock2(l *mutex) {    // 设置 l.key = mutex_unlocked    v := atomic.Xchg(key32(&amp;l.key), mutex_unlocked)    if v == mutex_unlocked {// 重复调用 unlock，直接抛出异常。        throw("unlock of unlocked lock")    }    if v == mutex_sleeping { // 之前的状态是 mutex_sleeping，说明其他有线程在`sleep`，唤醒一个`sleep`的对象。        futexwakeup(key32(&amp;l.key), 1)    }    gp := getg()    gp.m.locks--    if gp.m.locks &lt; 0 {        throw("runtime·unlock: lock count")    }    if gp.m.locks == 0 &amp;&amp; gp.preempt { // restore the preemption request in case we've cleared it in newstack        gp.stackguard0 = stackPreempt    }}</code></pre><p><strong>unlock 实现总结：</strong></p><ol><li>调用<code>atomic.Xchg</code>设置<code>l.key = mutex_unlocked</code>。</li><li>如果设置之前的状态就是<code>mutex_unlocked</code>，直接抛异常程序退出。</li><li>如果之前状态是<code>mutex_sleeping</code>，则唤醒一个阻塞在<code>futexsleep</code>的线程。</li><li><code>m</code>的锁数量减一，如果锁数量等<code>0</code>且当前<code>g</code>是被抢占状态，要标记<code>gp.stackguard0</code>为<code>stackPreempt</code>，下次发生函数调用的时候，会主动让出这个<code>g</code>。</li></ol><h1 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h1><p><code>runtime.mutex</code> 主要是使用了<code>CAS</code>自旋配合<code>procyield</code>和<code>osyield</code>，最多尝试<code>5</code>次，自旋失败就使用<code>futex</code>系统调用来实现，整体代码逻辑比较简单易懂。</p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Golang </tag>
            
            <tag> GoSourceCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Go源码——Sync.Map的前生今世</title>
      <link href="/2022/10/01/sync-map/"/>
      <url>/2022/10/01/sync-map/</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h1><p>前段时间有个朋友来问我<code>Go</code>的<code>Sync.Map</code>性能怎么样，一般什么场景推荐使用。一句话介绍的话，就是<code>Sync.Map</code>底层有两个<code>map</code>，一个是<code>read</code>，一个是<code>dirty</code>，读写<code>read</code>中数据不需要加锁，读写<code>dirty</code>不用需要加锁，适用于读多写少的场景。</p><h2 id="碎碎念"><a href="#碎碎念" class="headerlink" title="碎碎念"></a>碎碎念</h2><p>其实<code>2020</code>年的时候<code>Go</code>源码里面一些比较常用的包都大致看了一遍，当时跟<code>槊槊</code>、<code>大飞哥</code>、<code>周老板</code>空闲时间天天讨论各种技术细节，包括但不仅限于<code>操作系统</code>、<code>MySQL</code>、<code>Redis</code>、<code>分布式</code>、<code>Go</code>、<code>项目架构方法论</code>等。很多时候观点不合还会争的面红耳赤，最后还会上升到<strong>人生攻击</strong>，你不服我，我也不服你（实际上互有对错，我也被打过几次脸）。因为有的东西，网上有很多错误的资料，导致我养成了一个习惯，找资料的时候我一般都是去看一些<code>权威的技术书</code>或者直接去看<code>开源组件源码</code>，能用代码说的话绝不跟你多<code>BB</code>（<code>talk is cheap, show you the code</code>），用这些东西去反驳别人的观点。虽然有过很多次争吵，但是我们所有人的感觉都是一样，大家都觉得个人的<code>技术能力</code>、<code>技术眼界</code>都有了质的提升，精神上也有很大的满足感。现在三个老板，都去了其他大厂，也拿到了自己期望的<code>PKG</code>了，也在各自忙着自己的工作了，技术交流的世界也少了。</p><p>关于这些成长经历，就想说三点，一个是<strong>费曼教学法</strong>，你学会的东西并不代表你一定懂了，你教懂别人才能表示你彻底懂了，因为别人会站他思考的角度来思考一些你可能没考虑到的细节，用这些细节来反问你。当你把这些细节都了解通，并能解答被人的全部问题了，你才是真的懂了。二是，<strong>持续跟身边优秀的人去交流沟通</strong>，你一定会有很大的成长。如果你觉得自己足够优秀了，那你去开源社区逛逛，那你一定会发现你远远没你想的那么优秀。三是，<strong>选择大于努力</strong>，有时候你总会觉得有的人能力不如你，确混的比你好。一是可能你没看到别人在其他方面的努力，还有一个就是别人路选择的比你好。每次做选择的时候都要想清楚自己的<code>tradeoff</code>是什么，选择以后就接受现实，别再纠结为什么要这样选，记得有个本书叫<strong>《高效人士的七个习惯》</strong>，其他的都记不清楚了，就记得一个观点，一个人有自己的<code>关注圈</code>和<code>影响圈</code>，我们应该花更多的精力去做好自己能改变的<code>影响圈</code>，比如提高<code>个人能力</code>，减少去关注一些自己不能改变的东西比如<code>经济形势</code>、<code>就业形势</code>、<code>个人工资</code>等。</p><p>人的精力是有限的，当你过度聚焦一件事，其他的事情可能很容易被忽略，比如聚焦管理技巧、沟通技巧，那你的技术成长速度就跟不上其他人。我个人更多的精力是关注<code>技术</code>和<code>技术本身能带来的业务价值</code>，所以我对<code>开毫无意义的会</code>、<code>权利的游戏</code>、<code>无意义的social</code>都不感兴趣，但是我喜欢跟有技术追求的人<code>social（交流技术）</code>，在字节这几年的确也碰到了不优秀的同事和老板。我给自己的定位是工作上成为业务专家，之前的飞书老板就是很好的一个业务专家，整个飞书<code>IM</code>架构基本上都是他设计的。很多时候做方案的时候，只有足够的<code>Context</code>，他大多时候给你选择一个最合适的方案。工作之外脱离业务，希望自己，能成为某个方面的技术专家，以后能持续做开源相关的事情。</p><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>后面可能会系统性对<code>Go</code>源码做总结，产出<code>blog</code>，方便以后自己快速查阅。</p><p>还想对<code>Go</code>的内存管理和<code>GC</code>也做个系统总结。今年估计大概率弄不完，一个是工作太忙，周末休息还要陪娃。最近还打算看下<code>Rust</code>（作为一个<code>Geeker</code>，这么多大佬都在推崇这个，不了学习下说不过去）。</p><h1 id="二、Sync-Map-产生的背景"><a href="#二、Sync-Map-产生的背景" class="headerlink" title="二、Sync.Map 产生的背景"></a>二、Sync.Map 产生的背景</h1><h2 id="2-1-Sync-RWMutex-多核的伸缩性问题"><a href="#2-1-Sync-RWMutex-多核的伸缩性问题" class="headerlink" title="2.1 Sync.RWMutex 多核的伸缩性问题"></a>2.1 Sync.RWMutex 多核的伸缩性问题</h2><p>早在<code>2016</code>的时候，<a href="https://github.com/bcmills">@bcmills</a>（这个哥们是<code>Go</code>项目主要维护者之一） 在<code>Go</code>的 <code>Github</code> 上提出了一个<a href="https://github.com/golang/go/issues/17973">sync: RWMutex scales poorly with CPU count</a> 的<code>Issue</code>给大家讨论。简单说就是 <code>Sync.RWMutex</code>这个读写锁，多核情况下扩展性很差。他贴的 <code>Benchmark</code> 测试代码如下：</p><pre><code>func BenchmarkRWMutex(b *testing.B) {    for ng := 1; ng &lt;= 256; ng &lt;&lt;= 2 { // ng 表示，开多少个 goroutine        b.Run(fmt.Sprintf("name[%d]", ng), func(b *testing.B) {            var mu sync.RWMutex            mu.Lock()            var wg sync.WaitGroup            wg.Add(ng)            n := b.N        // n 表示下面要执行多少次 RLock 和 RUnlock             quota := n / ng // quota 表示分摊到每个 goroutine 上需要执行多少次 Lock 和 RUnlock             for g := ng; g &gt; 0; g-- {                if g == 1 { //  n / ng 不是整除的话，剩下余出来的数据，在g=1 的时候全部减掉，不然下面 n 不会等于0                    quota = n                }                go func(quota int) {                    for i := 0; i &lt; quota; i++ { // 一个循环执行一次 RLock 和 RUnlock                        mu.RLock()                        mu.RUnlock()                    }                    wg.Done()                }(quota)                n -= quota            }            if n != 0 {                b.Fatalf("Incorrect quota assignments: %v remaining", n)            }            b.StartTimer() // 从这里开始计时            mu.Unlock()    // 这里释放写锁，上面所有阻塞在 RLock 的 goroutine 同时唤醒去执行 RLock            wg.Wait()      // 所有 goroutine 的 RLock 和 RUnlock 都执行完毕            b.StopTimer()  // 从这里结束计时        })    }}    </code></pre><p><code>Benchmark</code>的结果可以看出，在多个<code>Gorutine</code>并发下，可以看到<code>CPU</code>核数越多，<code>RWLock</code>的性能越差。</p><pre><code># ./benchmarks.test -test.bench . -test.cpu 1,4,16,64testing: warning: no tests to runBenchmarkRWMutex/1      20000000                72.6 ns/opBenchmarkRWMutex/1-4    20000000                72.4 ns/opBenchmarkRWMutex/1-16   20000000                72.8 ns/opBenchmarkRWMutex/1-64   20000000                72.5 ns/opBenchmarkRWMutex/4      20000000                72.6 ns/opBenchmarkRWMutex/4-4    20000000               105 ns/opBenchmarkRWMutex/4-16   10000000               130 ns/opBenchmarkRWMutex/4-64   20000000               160 ns/opBenchmarkRWMutex/16     20000000                72.4 ns/opBenchmarkRWMutex/16-4   10000000               125 ns/opBenchmarkRWMutex/16-16  10000000               263 ns/opBenchmarkRWMutex/16-64   5000000               287 ns/opBenchmarkRWMutex/64     20000000                72.6 ns/opBenchmarkRWMutex/64-4   10000000               137 ns/opBenchmarkRWMutex/64-16   5000000               306 ns/opBenchmarkRWMutex/64-64   3000000               517 ns/opBenchmarkRWMutex/256                    20000000                72.4 ns/opBenchmarkRWMutex/256-4                  20000000               137 ns/opBenchmarkRWMutex/256-16                  5000000               280 ns/opBenchmarkRWMutex/256-64                  3000000               602 ns/opPASS</code></pre><p>为什么多核下面会更慢？其实很简单，就是资源竞争会增加额外开销。<code>RLock</code>和<code>RUnlock</code>，底层实现是<code>atomic.AddInt32</code>，<code>atomic.AddInt32</code>对应的<a href="https://github.com/golang/go/blob/dev.boringcrypto.go1.8/src/runtime/internal/atomic/asm_amd64.s#L81">汇编代码</a>如下：</p><pre><code>// uint32 Xadd(uint32 volatile *val, int32 delta)// Atomically://    *val += delta;//    return *val;TEXT ·Xadd(SB), NOSPLIT, $0-20    MOVQ    ptr+0(FP), BX    MOVL    delta+8(FP), AX    MOVL    AX, CX    LOCK    XADDL    AX, 0(BX)    ADDL    CX, AX    MOVL    AX, ret+16(FP)    RET</code></pre><p>可以看到里面有 <code>LOCK</code> 前缀的指令，<code>Lock</code>其实就是<code>CPU</code>层面的一个锁，锁的单位是<code>Cache Line</code> 具体可以参考 <a href="https://fanlv.wiki/2020/06/09/golang-memory-model/#Atomic">Golang Memory Model</a> 里面的详细介绍。多个核都要同时更新这个<code>Cacheline</code>，所以性能就有所下降。</p><h2 id="2-2-如何去优化？"><a href="#2-2-如何去优化？" class="headerlink" title="2.2 如何去优化？"></a>2.2 如何去优化？</h2><p>我们知道，在业务中遇到锁的性能瓶颈时候，我们一般会下面几个方面去考虑优化锁。</p><ol><li>优化锁的粒度</li><li>读写分离</li><li>减少锁持有时间。</li><li>使用<code>CAS</code></li></ol><p>2、3、4 在这个读写锁场景都不试用（已经是读写锁了，且瓶颈也在<code>CAS</code>对<code>cacheline</code>的资源竞争），所以只能从锁的粒度方向考虑。</p><h3 id="2-2-1-distributedrwmutex"><a href="#2-2-1-distributedrwmutex" class="headerlink" title="2.2.1 distributedrwmutex"></a>2.2.1 distributedrwmutex</h3><p><a href="https://github.com/dvyukov">@dvyukov</a>（<code>Go</code>小组成员之一） 提出了一个<a href="https://codereview.appspot.com/4850045/diff2/1:3001/src/pkg/co/distributedrwmutex.go">分布式读写锁的方案</a>，<br>核心原理就是，一个<code>P</code>对应一个读写锁，这样读锁在多核情况就没有竞争的问题了，因为每个核的读锁是独立的，互不影响（有点类似 <code>ThreadLocal</code> 的概念）。具体核心代码如下：</p><pre><code>func (m *DistributedRWMutex) RUnlock() {        l := m.getLocal()        l.RUnlock()}func (m *DistributedRWMutex) getLocal() *sync.RWMutex {        v := runtime.GetProcLocal(m.slot)        p := (*sync.RWMutex)(unsafe.Pointer(uintptr(*v)))        if p == nil {                p = new(sync.RWMutex)                atomic.AddUint64(v, uint64(uintptr(unsafe.Pointer(p))))        }        return p}</code></pre><p>不过这个实现方式也有一个问题需要注意。就是<code>Goroutine</code>和<code>P</code>不是强绑定的。有可能你在某个<code>P</code>执行<code>Lock</code>以后，做了<code>系统调用</code>这个时候<code>M、G</code>和<code>P</code>可能会解绑，系统调用完成回来的时候，可能绑定的是一个新的<code>P</code>了。这个时候再去调用<code>getLocal</code>可能拿到的已经是不一样的锁对象了，再去用这个锁对象去调用<code>RUnlock</code>是有问题的。一般这种需要在<code>Goroutine</code>里面直接拿到<code>RWLock</code>锁对象。类似下面这种：</p><pre><code>// ...balabala...go func() {    rwx := rw.RLocker() // 这里拿到当前P对应的ReadLocker    rwx.Lock()    defer rwx.Unlock()    // ... balabala...    // syscall 这里切换P了也没影响}()// ...balabala...</code></pre><p>还有一个 <a href="https://github.com/jonhoo/drwmutex/">drwmutex</a> 库也是这个思想，这里不过多赘述。</p><p><a href="https://github.com/bcmills">@bcmills</a> 的回复说，老的<code>RWMutex</code>接口，是允许在不同的<code>Goroutine</code>或者<code>P</code>里面调用<code>RLock / RUnlock</code>，考虑兼容性问题，不太想做这样的改造。</p><h3 id="2-2-2-Atomic-Value"><a href="#2-2-2-Atomic-Value" class="headerlink" title="2.2.2 Atomic.Value"></a>2.2.2 Atomic.Value</h3><p>还有更大的问题是当时（<code>GO1.8</code>）一些基础库中大量使用了<code>RWMutex</code>作为包级锁。比如<a href="https://github.com/golang/go/blob/release-branch.go1.8/src/reflect/type.go#L1434">reflect</a>、<a href="https://github.com/golang/go/blob/release-branch.go1.8/src/net/http/server.go#L1412">http.statusMu</a>、<a href="https://github.com/golang/go/blob/release-branch.go1.8/src/encoding/json/encode.go#L336">json.encoderCache</a>、<a href="https://github.com/golang/go/blob/release-branch.go1.8/src/mime/type.go#L15">mime.mimeLock</a></p><p><a href="https://github.com/dvyukov">@dvyukov</a> 指出这些场景其实可以用<code>Atomic.Value</code>去实现，类似场景有<a href="https://github.com/golang/go/blob/release-branch.go1.8/src/encoding/json/encode.go#L1266">encoding/json/encode.go:cachedTypeFields</a></p><pre><code>// cachedTypeFields is like typeFields but uses a cache to avoid repeated work.func cachedTypeFields(t reflect.Type) []field {    m, _ := fieldCache.value.Load().(map[reflect.Type][]field)    f := m[t]    if f != nil {        return f    }    // Compute fields without lock.    // Might duplicate effort but won't hold other computations back.    f = typeFields(t)    if f == nil {        f = []field{}    }    fieldCache.mu.Lock()    m, _ = fieldCache.value.Load().(map[reflect.Type][]field)    newM := make(map[reflect.Type][]field, len(m)+1)    for k, v := range m {        newM[k] = v    }    newM[t] = f    fieldCache.value.Store(newM)    fieldCache.mu.Unlock()    return f}</code></pre><p>PS：<code>Atomic.Load</code>转汇编其实就是简单的<code>MOV</code>指令，没有<code>LOCK</code>所以没有<code>Cacheline</code>资源竞争的问题。</p><p><a href="https://go-review.googlesource.com/c/go/+/2641/">mime: use atomic.Value to store mime types</a> 这个<code>CL</code>也是尝试用<code>atomic.Value</code>去替代<code>sync.RWMutex</code>。</p><p>这个实现，虽然读的时候没有资源竞争的问题。但是写的时候是<code>O(n)</code>的开销。这个方案对写太不友好。</p><h3 id="2-2-3-基于二叉树实现-dmap"><a href="#2-2-3-基于二叉树实现-dmap" class="headerlink" title="2.2.3 基于二叉树实现 - dmap"></a>2.2.3 基于二叉树实现 - dmap</h3><p><a href="https://github.com/ianlancetaylor">@ianlancetaylor</a> 基于二叉树实现了<code>dmap</code>，<code>dmap</code>的插入时间复杂度是<code>O(LogN)</code>，<code>insert</code>就是常规的写入操作，这里就不过多去赘述了。 </p><pre><code>// Insert inserts a key/value pair into a dmap.func (d *dmap) insert(key, val interface{}) {    var n *node    for { // 判断根节点是不是为空。为空直接加锁然后写Root，否则就拿到根节点        root, _ := d.root.Load().(*node)        if root != nil {            n = root            break        }        root = &amp;node{            key: key,            val: val,        }        d.mu.Lock()        if d.root.Load() == nil {            d.root.Store(root)            d.mu.Unlock()            return        }        d.mu.Unlock() // 走到这表示，有其他 goroutine 写了根节点，会循继续去 load 根节点    }    // 到这里，n 表示是 root 节点        for {        cmp := d.compare(key, n.key) // 判断两个 key是否相等        if cmp == 0 {            if val != n.val {                panic("invalid double-insert")            }            return        }        p := &amp;n.left         if cmp &gt; 0 { // key 大于当前节点key。就找右节点            p = &amp;n.right        }        n2, _ := (*p).Load().(*node)        if n2 != nil { // 当前节点不为空，继续重新走循环，比较key和 n.key 大小            n = n2         } else { // 当前节点为空，尝试写入，写入失败，就继续重新走循环逻辑             n2 = &amp;node{                key: key,                val: val,            }            n.mu.Lock()            if (*p).Load() == nil {                (*p).Store(n2)                n.mu.Unlock()                return            }            n.mu.Unlock()        }    }}</code></pre><p>查找的实现，有<code>fastpath</code>和<code>slowpath</code>两个路径，<code>fastpath</code>用的是<code>map</code>来查找，命中的话就直接返回，时间复杂度是<code>O(1)</code>的，<code>map</code>中没查到的话，会去二叉树里面查，时间复杂度是<code>O（LogN）</code>。有个<code>tricky</code>的地方是，没有命中<code>map</code>但是在二叉树中查到这个<code>key</code>的话，会对这个<code>key</code>的<code>count+1</code>，如果这个<code>key</code>的<code>miss count</code>大于<code>map</code>的长度的话，会复制一下<code>map</code>然后把新的<code>map</code>回写到<code>Atomic.Value</code>里面。</p><pre><code>// Lookup looks up a key in the distributed map.func (d *dmap) lookup(key interface{}) interface{} {    // Common values are cached in a map held in the root.    m, _ := d.m.Load().(map[interface{}]interface{})    if val, ok := m[key]; ok { // map里面找到了，直接返回        return val    }    n, _ := d.root.Load().(*node)    for n != nil {        cmp := d.compare(key, n.key)        if cmp == 0 {            count := atomic.AddUint32(&amp;n.count, 1)            // Add this key/val pair to the map in the root,            // but only if it's worth copying the existing map.            if count &lt; 0 || (count &gt; 1 &amp;&amp; int(count) &gt; len(m)) {                newm := make(map[interface{}]interface{}, len(m)+1)                for k, v := range m {                    newm[k] = v                }                newm[key] = n.val                // It's possible that some other                // goroutine has updated d.m since we                // loaded it.  That means we did extra                // work but it's otherwise OK.                // 这里如果有多个 goroutine 写会导致有互相覆盖的问题                d.m.Store(newm)            }            return n.val        }        p := &amp;n.left        if cmp &gt; 0 {            p = &amp;n.right        }        n, _ = (*p).Load().(*node)    }    return nil}</code></pre><h3 id="2-2-4-两个map"><a href="#2-2-4-两个map" class="headerlink" title="2.2.4 两个map"></a>2.2.4 两个map</h3><p><a href="https://github.com/bcmills">@bcmills</a> 基于上面 <a href="https://github.com/ianlancetaylor">@ianlancetaylor</a> 的二叉树加<code>map</code>的思想优化了下。用<code>map</code>替代了二叉树。<a href="https://go-review.googlesource.com/c/sync/+/33852/">具体实现</a>如下：</p><pre><code>// Map is a key-value map from which entries can be read without external// synchronization.type Map struct {    tenured        atomic.Value // 年老代 map    liveNotTenured uint32 // 记录 miss count    mu   sync.RWMutex // 对 live 读写的时候，需要用到这个读写锁    live map[interface{}]interface{}}</code></pre><p>读的话，先去<code>tenured</code>里面去读，读<code>tenured</code>不用加锁，读写<code>live</code>用的是读写锁，然后根据<code>misscount</code>把<code>live</code>复制给<code>tenured</code></p><pre><code>func (b *Map) Load(key interface{}) (value interface{}, ok bool) {    m, _ := b.tenured.Load().(map[interface{}]interface{})    if value, ok = m[key]; ok {        return value, true    }    b.mu.RLock()    promote := false    if b.live != nil {        value, ok = b.live[key]        lnt := atomic.AddUint32(&amp;b.liveNotTenured, 1)        if lnt &gt;= 1&lt;&lt;31 || int(lnt) &gt;= len(b.live) {            promote = true        }    }    b.mu.RUnlock()    if !promote {        return value, ok    }    b.mu.Lock()    lnt := atomic.LoadUint32(&amp;b.liveNotTenured)    if b.live != nil &amp;&amp; (lnt &gt;= 1&lt;&lt;31 || int(lnt) &gt;= len(b.live)) {        b.tenured.Store(b.live)        b.live = nil        atomic.StoreUint32(&amp;b.liveNotTenured, 0)    }    b.mu.Unlock()    return value, ok}</code></pre><p>写的话，很简单，只写<code>live</code>。</p><pre><code>func (b *Map) StoreOrLoad(key, value interface{}) (actualValue interface{}, dup bool) {    b.mu.Lock()    if b.live == nil {        m, _ := b.tenured.Load().(map[interface{}]interface{})        b.live = make(map[interface{}]interface{}, len(m)+1)        for k, v := range m {            b.live[k] = v        }    }    actualValue, dup = b.live[key]    if !dup {        b.live[key] = value        actualValue = value    }    b.mu.Unlock()    return actualValue, dup}</code></pre><h1 id="三、Sync-Map-的最终实现"><a href="#三、Sync-Map-的最终实现" class="headerlink" title="三、Sync.Map 的最终实现"></a>三、Sync.Map 的最终实现</h1><p>经过一轮讨论以后，<a href="https://github.com/bcmills">@bcmills</a> 单独发了一个提案：<a href="https://github.com/golang/go/issues/18177">sync: add a Map to replace RWLock+map usage</a> 最终决定不去修复<code>RWLock</code>的伸缩性问题，而是提供一个可伸缩并发安全的<code>Map</code>来做。 这个并发安全的<code>Map</code>实现方案就是用的上面双<code>Map</code>实现。然后这个并发安全的<code>Map</code>会先放在 <a href="https://github.com/golang/go/wiki/X-Repositories">x-Repositories</a> 包中经过一段时间迭代，如果没问题了再收敛到<code>Go</code>源码包中。具体可以看 <a href="https://go-review.googlesource.com/c/sync/+/33912/">syncmap: add a synchronized map implementation</a>。</p><p><a href="https://github.com/bcmills">@bcmills</a> 基于<a href="https://go-review.googlesource.com/c/sync/+/33852/">双map的demo</a>，做了一些优化，新增了一些<code>API</code>，比如<code>Delete</code>、<code>Range</code>等。<a href="https://go-review.googlesource.com/c/sync/+/33912/">提交了一个正式的 CR</a>：</p><pre><code>// A Map must not be copied after first use.type Map struct {    mu sync.Mutex    // clean 是 fastpath 用的，读的时候不用加锁，没有cacheline竞争问题    clean atomic.Value // map[interface{}]interface{}        // dirty 读写都需要加锁    dirty map[interface{}]interface{}        // 如果clean没有查到，这个时候misses会加1    // 当 misses &gt;= len(dirty)，会把dirty赋值给clean，然后情况dirty    misses int}</code></pre><p>我们再来看下数据读取的实现，这个里面有几点需要注意，跟上面<a href="https://go-review.googlesource.com/c/sync/+/33852/">双map的demo</a> 不同的事，这里的实现是<code>clean</code>和<code>dirty</code>两个<code>map</code>只会有一个不为空。所以读的时候，如果<code>clean</code>不为空就直接读<code>clean</code>，并不会再去<code>dirty</code>读一次。如果<code>dirty</code>不为<code>nil</code>，读取以后还会调用一下<code>m.missLocked</code>，这个函数主要的作用是判断对<code>m.misses</code>加<code>1</code>，然后判断要不要把<code>dirty</code>赋值给<code>clean</code>，然后清空<code>dirty</code>。</p><pre><code>// Load returns the value stored in the map for a key, or nil if no// value is present.// The ok result indicates whether value was found in the map.func (m *Map) Load(key interface{}) (value interface{}, ok bool) {    clean, _ := m.clean.Load().(map[interface{}]interface{})    if clean != nil {        value, ok = clean[key]        return value, ok    }    m.mu.Lock()    if m.dirty == nil {        clean, _ := m.clean.Load().(map[interface{}]interface{})        if clean == nil {            // Completely empty — promote to clean immediately.            m.clean.Store(map[interface{}]interface{}{})        } else {            value, ok = clean[key]        }        m.mu.Unlock()        return value, ok    }    value, ok = m.dirty[key]    m.missLocked()    m.mu.Unlock()    return value, ok}func (m *Map) missLocked() {    if m.misses++; m.misses &gt;= len(m.dirty) {        m.clean.Store(m.dirty)        m.dirty = nil    }}</code></pre><p><code>Store</code>的函数就比较简单了。如果写入的时候，直接加锁，然后判断<code>dirty</code>是否为空，如果是空，需要把<code>clean</code>数据复制一份给<code>dirty</code>然后清空<code>clean</code>，然后再把数据赋值给<code>dirty</code>。</p><pre><code>// Store sets the value for a key.func (m *Map) Store(key, value interface{}) {    m.mu.Lock()    m.dirtyLocked()    m.dirty[key] = value    m.mu.Unlock()}// dirtyLocked prepares the map for a subsequent write.// It ensures that the dirty field is non-nil and clean is nil by making a deep// copy of clean.func (m *Map) dirtyLocked() {    m.misses = 0    if m.dirty != nil {        return    }    clean, _ := m.clean.Load().(map[interface{}]interface{})    m.dirty = make(map[interface{}]interface{}, len(clean))    for k, v := range clean {        m.dirty[k] = v    }    m.clean.Store(map[interface{}]interface{}(nil))}</code></pre><p><strong>这个实现其实有个很大的问题，就是如果有频繁读写交替的话，会导致数据一直在<code>clean</code>和<code>dirty</code>两个<code>map</code>中来回<code>copy</code>，如果<code>map</code>很大的话，这个性能会很差，还会阻塞其他线程的读写，但是这个CR当时的场景是期望提供给Runtime包中一些读多写少的场景使用，所以看<code>benchmark</code>跑的性能还是有很大的提升的。</strong></p><p>代码合入的时候 <a href="https://github.com/rsc">@rsc</a> 提了两点优化建议</p><ol><li>如果允许<code>clean != nil and dirty != nil</code>会更好。</li><li>如果一个<code>key</code>没有被覆盖或者删除的话，它命中了<code>lock-free path</code>后续理论上应该一直命中<code>lock-free path</code>会更好一些。</li></ol><h2 id="3-1-进一步优化"><a href="#3-1-进一步优化" class="headerlink" title="3.1 进一步优化"></a>3.1 进一步优化</h2><p>过了几个月，基于 <a href="https://github.com/rsc">@rsc</a> 之前合码的时候给的建议，<a href="https://github.com/bcmills">@bcmills</a>  <a href="https://go-review.googlesource.com/c/sync/+/37342/">又优化了一版</a>，整个<code>sync.Map</code>的结构变成了下面这样：</p><pre><code>type Map struct {    mu sync.Mutex    read atomic.Value // readOnly    dirty map[interface{}]*entry        misses int}type readOnly struct { // readOnly 的 map    m       map[interface{}]*entry    amended bool // amended=true m没有全部key的数据，没查到还需要去dirty查下.}var expunged = unsafe.Pointer(new(interface{})) // 表示这个数据已经不在dirty中了。type entry struct {    p unsafe.Pointer // *interface{}}</code></pre><p>主要改动只读的<code>map</code>，之前叫<code>clean</code>类型是<code>map[interface{}]interface{}</code>，现在改成了<code>read</code>，类型是<code>readOnly struct</code>，<code>readOnly</code>还有个<code>amended</code>表示当前<code>readOnly.m</code>是不是全量数据。我们继续往下<code>Store</code>的代码</p><pre><code>func (m *Map) Store(key, value interface{}) {    // fast-path    read, _ := m.read.Load().(readOnly)    if e, ok := read.m[key]; ok &amp;&amp; e.tryStore(&amp;value) {        // 如果这个 key 在 read 里面找到了以后，尝试直接调用 tryStore 去更新 value 数据       // tryStore 里面会做两件事       // 1. 判断当前 entry.p 是不是等于 expunged，等于 expunged 就不能更新，直接返回false。下面会走 slow-path去更新       // 2. 如果不是 expunged ，那就尝试更新 entry.p = &amp;value，如果 CAS 设置成功了就返回。       // 如果是 expunged 状态，表面 dirty 里面已经没有这个 key了，如 read 里面更新这个东西，下次 dirty数据全量提升为 read 的时候，这个数据就会丢失。        return     }    // 下面是 slow-path    m.mu.Lock()    read, _ = m.read.Load().(readOnly)    if e, ok := read.m[key]; ok {        if e.unexpungeLocked() { // e.unexpungeLocked 尝试CAS(&amp;e.p, expunged, nil)            m.dirty[key] = e // 把 e 赋值给 dirty        }        // 到这里 e.p 肯定不是等于 expunged 了        e.storeLocked(&amp;value) // 设置 e.p = &amp;value    } else if e, ok := m.dirty[key]; ok {        e.storeLocked(&amp;value) // 如果只在dirty里面有，直接设置 e.p = &amp;value    } else {        if !read.amended { // 如果目前 read 有全量数据，但是 read 和 dirty 都没有这个 key            m.dirtyLocked() // dirtyLocked 这个函数主要做的就是，把 read 里面的 e.p != nil &amp;&amp; e.p != expunged 的元素 copy 一份赋值给 dirty            m.read.Store(readOnly{m: read.m, amended: true})        }        m.dirty[key] = newEntry(value) // dirty 保存这个 kv    }    m.mu.Unlock()}</code></pre><p>总结下<code>Store</code>主要做了下几件事：</p><ol><li><code>fast-path</code> 路径<ul><li>看下 <code>read</code> 中是否有这个<code>key</code>，有的话尝试调用<code>tryStore</code>，把设置的<code>value</code>保存到<code>entry</code>对象中去。</li><li><code>tryStore</code> 里面会判断<code>entry.p</code>是不是<code>expunged</code>状态，是的话就不能设置，需要走<code>slow-path</code></li><li>如果不是的话保存成功就直接返回。</li></ul></li><li><code>slow-path</code>路径<ul><li>会先加互斥锁</li><li>看下 <code>read</code> 中是否有这个<code>key</code>，有的话尝试调用<code>unexpungeLocked</code>，<code>CAS</code>方式清除<code>entry.p</code>的<code>expunged</code>状态。如果清楚成功，会在<code>dirty</code>里面添加这个数据。如果没有清楚成功，说明状态不是<code>expunged</code>，可以直接更新<code>read</code>的<code>entry.p=&amp;value</code>就行了。</li><li>不在<code>read</code>里面，在<code>dirty</code>里面，直接设置<code>entry.p=&amp;value</code>就行了。</li><li><code>read</code>和<code>dirty</code>都没有找到这个<code>key</code>，先看下<code>read</code>是不是有全量数据，是的话，就调用<code>m.dirtyLocked</code>，把<code>read</code>数据<code>copy</code>一份到<code>dirty</code>，并设置<code>read.amended=true</code>,表示 <code>read</code>里面已经没有全量数据了，需要去<code>drity</code>里面找。</li><li>最后设置 <code> m.dirty[key] = newEntry(value)</code>，dirty 保存这个 kv</li></ul></li></ol><p>在来看下<code>Load</code>相关代码：</p><pre><code>func (m *Map) Load(key interface{}) (value interface{}, ok bool) {    read, _ := m.read.Load().(readOnly)    e, ok := read.m[key]    if !ok &amp;&amp; read.amended { // 如果 read 没找到，且 read 没有全量数据        m.mu.Lock()        read, _ = m.read.Load().(readOnly)        e, ok = read.m[key] // 加锁以后，这里需要 double check一下        if !ok &amp;&amp; read.amended {            e, ok = m.dirty[key] // 去 dirty map 读            m.missLocked() // 这里面对 misscount+1 ，然后看下是否需要把 dirty 全部给 read，然后设置 dirty 为 nil。        }        m.mu.Unlock()    }    if !ok {        return nil, false    }    return e.load() // 如果 e.p != nil &amp;&amp; e.p == expunged , 就把 e.p 的指向的值转成Interface返回}</code></pre><p>最后再来看下<code>Delete</code>的怎么做的，<code>Delete</code>其实比较简单，就是设置，在<code>read</code>里面找到这个<code>entry</code>然后设置<code>e.p=nil</code>,如果在<code>dirty</code>中就直接调用<code>delete</code>方法删除这个<code>key</code>。</p><pre><code>func (m *Map) Delete(key interface{}) {    read, _ := m.read.Load().(readOnly)    e, ok := read.m[key]    if !ok &amp;&amp; read.amended {        m.mu.Lock()        read, _ = m.read.Load().(readOnly)        e, ok = read.m[key]        if !ok &amp;&amp; read.amended {            delete(m.dirty, key)        }        m.mu.Unlock()    }    if ok {        e.delete()    }}func (e *entry) delete() (hadValue bool) {    for {        p := atomic.LoadPointer(&amp;e.p)        if p == nil || p == expunged {            return false        }        if atomic.CompareAndSwapPointer(&amp;e.p, p, nil) {            return true        }    }}</code></pre><h2 id="3-2-思考：dirty-能否不全量拷贝-read？"><a href="#3-2-思考：dirty-能否不全量拷贝-read？" class="headerlink" title="3.2 思考：dirty 能否不全量拷贝 read？"></a>3.2 思考：dirty 能否不全量拷贝 read？</h2><p>正常思路，为了节省内存，<code>dirty</code> 里面只存增量数据，可以吗？反向推理下如果<code>dirty</code>只存增量的数据，那就不需要<code>read</code>到<code>dirty</code>的数据同步操作了，那也不需要<code>expunged</code>状态了。所以<code>read</code>的中元素<code>e.p=nil</code>的时候，表示删除了，由于没有了<code>read</code>到<code>dirty</code>的复制，所以需要定期滤掉<code>read</code>中删除的数据（<code>e.p = nil</code>）并重新给<code>read</code>赋值，那<code>Store</code>的时候，如果<code>read</code>的<code>e.p=nil</code>的话就不能再更新了。因为定期过滤掉<code>read</code>中删除的数据可能会把这个<code>entry</code>给删除掉，导致这个<code>key</code>对应的数据丢失了。所以<code>Store</code>和<code>Load</code>伪代码如下：</p><pre><code>func (m *Map) Store(key, value interface{}) {    read, _ := m.read.Load().(readOnly)    if e, ok := read.m[key]; ok &amp;&amp; e.p != nil {        ok:= CAS(e.p, old, &amp;value) // 注意这里要是 old = nil 时候不能再继续尝试 CAS        if ok{            return        }        // cas 失败继续往下走    }    m.mu.Lock() // 加锁    read, _ = m.read.Load().(readOnly)    if e, ok := read.m[key]; ok {        if atomic.Load(e.p) != nil{           atomic.Store(e.p,&amp;value)            return        }    } else if e, ok := m.dirty[key]; ok {        e.storeLocked(&amp;value)    }     // read 查到了 e 但是 e.p == nil    // read 和 dirty 都没查到    m.dirty[key] = newEntry(value)    noNilMap := fliterNilIFNeed(read.m) // 过滤掉read.m 中为空的数据，如果没有空数据直接返回nil    m.read.Store(readOnly{m: noNilMap, amended: true})    m.mu.Unlock()}func (m *Map) Load(key interface{}) (value interface{}, ok bool) {    read, _ := m.read.Load().(readOnly)    e, ok := read.m[key]    if !ok &amp;&amp; read.amended || (ok &amp;&amp; atomic.Load(e.p) == nil){        m.mu.Lock()        read, _ = m.read.Load().(readOnly)        e, ok = read.m[key]        if !ok &amp;&amp; read.amended || (ok &amp;&amp; atomic.Load(e.p) == nil){            e, ok = m.dirty[key]            m.misses++            if m.misses &gt;= len(m.dirty) {                noNilMap := fliterNilIFNeed(read.m) // 过滤掉read.m 中为空的数据，如果没有空数据直接返回nil                allDataMap := merge(noNilMap,m.dirty)                m.read.Store(readOnly{m: allDataMap})                m.dirty = nil                m.misses = 0            }        }        m.mu.Unlock()    }    if !ok {        return nil, false    }    return e.load()}</code></pre><p>这样实现逻辑上好像也没有问题，不过每次<code>Load</code>和<code>Store</code>一个<code>read</code>中的<code>nil</code>，都需要加锁，然后会过滤<code>read</code>的<code>nil</code>数据，都有数据的拷贝操作。如果在删除以后立即读的场景性能可能会非常差。</p><p><strong>总结：dirty 全量拷贝 read 数据，就是好一个空间换时间的操作。</strong></p><h2 id="3-3-runtime-库使用sync-Map-优化"><a href="#3-3-runtime-库使用sync-Map-优化" class="headerlink" title="3.3 runtime 库使用sync.Map 优化"></a>3.3 runtime 库使用sync.Map 优化</h2><p><code>syncmap</code>在<a href="https://github.com/golang/go/wiki/X-Repositories">x-Repositories</a> 里面给其他用户试用几个月以后，最终被合入了<code>Go</code>源码包中，详见<a href="https://go-review.googlesource.com/c/go/+/36617/">sync: import Map from x/sync/syncmap</a></p><p>然后又优化了一波源码中使用<code>RWMutex</code>的代码，都改成了<code>Sync.Map</code>，修改以后代码相对之前也更简单一些了。</p><p><a href="https://go-review.googlesource.com/c/go/+/41871/">reflect: use sync.Map instead of RWMutex for type caches</a></p><p><a href="https://go-review.googlesource.com/c/go/+/41930/">expvar: replace RWMutex usage with sync.Map and atomics</a></p><p><a href="https://go-review.googlesource.com/c/go/+/41990/">mime: use sync.Map instead of RWMutex for type lookups</a></p><p><a href="https://go-review.googlesource.com/c/go/+/41990/">mime: use sync.Map instead of RWMutex for type lookups</a></p><p><a href="https://go-review.googlesource.com/c/go/+/41991/">encoding/xml: replace tinfoMap RWMutex with sync.Map</a></p><p><a href="https://go-review.googlesource.com/c/go/+/42110/">net/http: use sync.Map instead of RWMutex for ServeMux.m</a></p><p><a href="https://go-review.googlesource.com/c/go/+/42112/">net/rpc: use a sync.Map for serviceMap instead of RWMutex</a></p><p><a href="https://go-review.googlesource.com/c/go/+/42113/">archive/zip: replace RWMutex with sync.Map</a></p><h1 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h1><p>很多时候都没有十全十美的方案，方案设计的越<code>general</code>，需要考虑的场景就越多，需要做<code>tradeoff</code>的地方也就越多。</p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Golang </tag>
            
            <tag> GoSourceCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《五蠹》摘抄</title>
      <link href="/2022/09/29/wudu/"/>
      <url>/2022/09/29/wudu/</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h1><p>看 《Clean Architecture》的时候，看到书里面介绍什么架构师，大致内容如下。</p><blockquote><p>首先，软件架构师自身需要是程序员，并且必须一直坚持做一线程序员，绝对不要听从那些说应该让软件架构师从代码中解放出来以专心解决高阶问题的伪建议。不是这样的！软件架构师其实应该是能力最强的一群程序员，他们通常会在自身承接编程任务的同时，逐渐引导整个团队向一个能够最大化生产力的系统设计方向前进。也许软件架构师生产的代码量不是最多的，但是他们必须不停地承接编程任务。如果不亲身承受因系统设计而带来的麻烦，就体会不到设计不佳所带来的痛苦，接着就会逐渐迷失正确的设计方向。</p></blockquote><p>然后就吐槽了下某些 <strong>PPT架构师</strong> 只会脱离业务去谈一些高大上的架构，完全都落不了地。然后某个博览群书的技术大佬推荐我看下<strong>《五蠹》</strong>说里面就有映射这种人。然后就学习了下 <strong>《韩非子 * 五蠹》</strong>，虽然赞同文中全部的观点，但是部分观点还是觉得满认可的。</p><h1 id="二、摘抄"><a href="#二、摘抄" class="headerlink" title="二、摘抄"></a>二、摘抄</h1><blockquote><p>上古之世，人民少而禽兽众，人民不胜禽兽虫蛇，有圣人作，构木为巢，以避群害，而民悦之，使王天下，号之曰有巢氏。民食果蓏蚌蛤，腥臊恶臭而伤害腹胃，民多疾病，有圣人作，钻燧取火以化腥臊，而民说之，使王天下，号之曰燧人氏。中古之世，天下大水，而鲧、禹决渎。近古之世，桀、纣暴乱，而汤、武征伐。今有构木钻燧於夏后氏之世者，必为鲧、禹笑矣。有决渎於殷、周之世者，必为汤、武笑矣。然则今有美尧、舜、汤、武、禹之道於当今之世者，必为新圣笑矣。是以圣人不期脩古，不法常可，论世之事，因为之备。</p></blockquote><p>在上古时代，人口稀少，鸟兽众多，人民受不了禽兽虫蛇的侵害。这时候出现了—位圣人，他发明在树上搭窝棚的办法，用来避免遭到各种伤害；人们因此很爱戴他，推举他来治理天下，称他为有巢氏。当时人民吃的是野生的瓜果和蚌蛤，腥臊腐臭，伤害肠胃，许多人得了疾病。这时候又出现了一位圣人，他发明钻木取火的方法烧烤食物，除掉腥臊臭味；人们因而很爱戴他，推举他治理天下，称他为隧人氏。到了中古时代，天下洪水泛滥，鲧和他的儿子禹先后负责疏通河道，排洪治灾。近古时代，夏桀和殷纣的统治残暴昏乱，于是商汤和周武王起兵讨伐。如果到了夏朝，还有人用在树上搭窝棚居住和钻木取火的办法生活，那一定会被鲧、禹耻笑了；如果到了殷周时代，还有人要把挖河排洪作为要务的话，那就一定会被商汤、武王所耻笑。既然如此，那么在今天要是还有人推崇尧、舜、禹、场、武王的政治并加以实行的人，定然要被现代的圣人耻笑了。因此，圣人不期望照搬古法，不死守陈规旧俗，而是根据当前社会的实际情况，进而制定相应的政治措施。</p><blockquote><p> 宋人有耕田者，田中有株，兔走，触株折颈而死，因释其耒而守株，冀复得兔，兔不可复得，而身为宋国笑。今欲以先王之政，治当世之民，皆守株之类也。</p></blockquote><p>有个宋人在田里耕作；田中有一个树桩，一只兔子奔跑时撞在树桩上碰断了脖子死了。从此这个末人便放下手中的农具，守在树桩旁边，希望再捡到死兔子。他当然不可能再得到兔子，自己倒成了宋国的一个笑话。现在假使还要用先王的政治来治理当代的民众，那就无疑属于守株待兔之类的人了。</p><blockquote><p>古者丈夫不耕，草木之实足食也；妇人不织，禽兽之皮足衣也。不事力而养足，人民少而财有馀，故民不争。是以厚赏不行，重罚不用而民自治。今人有五子不为多，子又有五子，大父未死而有二十五孙，是以人民众而货财寡，事力劳而供养薄，故民争，虽倍赏累罚而不免於乱。</p></blockquote><p>在古代，男人不用耕种，野生的果实足够吃的；妇女不用纺织，禽兽的皮足够穿的。不用费力而供养充足。人口少而财物有余，所以人们之间用不着争夺。因而不实行厚赏，不实行重罚，而民众自然安定无事。现在人们养有五个儿子并不算多，每个儿子又各有五个儿子，祖父还没有死就会有二十五个孙子。因此，人口多了，而财物缺乏；费尽力气劳动，还是不够吃用。所以民众互相争夺，即使加倍地奖赏和不断地惩罚。结果仍然免不了要发生混乱。</p><blockquote><p>尧之王天下也，茅茨不翦，采椽不斫，粝粢之食，藜藿之羹，冬日麑裘，夏日葛衣，虽监门之服养，不亏於此矣。禹之王天下也，身执耒臿以为民先，股无胈，胫不生毛，虽臣虏之劳不苦於此矣。以是言之，夫古之让天子者，是去监门之养而离臣虏之劳也，古传天下而不足多也。今之县令，一日身死，子孙累世絜驾，故人重之；是以人之於让也，轻辞古之天子，难去今之县令者，薄厚之实异也。夫山居而谷汲者，膢腊而相遗以水；泽居苦水者，买庸而决窦。故饥岁之春，幼弟不饟；穰岁之秋，疏客必食；非疏骨肉爱过客也，多少之心异也。是以古之易财，非仁也，财多也；今之争夺，非鄙也，财寡也；轻辞天子，非高也，势薄也；重争土橐，非下也，权重也。故圣人议多少、论薄厚为之政，故罚薄不为慈，诛严不为戾，称俗而行也。故事因於世，而备適於事。</p></blockquote><p>尧统治天下的时候，住的是没经修整的茅草房，连栋木椽子都不曾刨光；吃的是粗粮，喝的是野菜场；冬天披坏小鹿皮，夏天穿着麻布衣。就是现在看门奴仆的生活，也不比这差。禹统治天下的时候，亲自拿着锹锄带领人们干活，累得大腿消瘦，小腿上的汗毛都磨没了，就是奴隶们的劳役也不比这苦。这样说来，古代把天子的位置让给别人，不过是逃避看门奴仆般的供养，摆脱奴隶样的繁重苦劳罢了；所以把天下传给别人也并不值得赞美。如今的县令，一旦死了，他的子孙世世代代总有高车大马，所以人们都很看重。因此，人们对于让位这件事，可以轻易地辞掉古代的天子，却难以舍弃今天的县官；原因即在其间实际利益的大小很不—样。居住在山上要到谷底汀水的人，逢年过节用水作为礼品互相赠送；居住在洼地饱受水涝灾害的人，却要雇人来挖渠排水。所以在荒年青黄不接的时候，就连自己的幼弟来了也不肯管饭；在好年成的收获季节，即使是疏远的过客也总要招待吃喝。不是有意疏远自己的骨肉而偏爱过路的客人，而是因为存粮多少的实际情况不同。因此，古人轻视财物。并不是因为仁义，而是由于财多；今人互相争夺，并不是因为卑鄙，而是由于财少。古人轻易辞掉天子的职位，并不是什么风格高尚，而是因为权势很小；今人争夺官位或依附权势，也不是什么品德低下，而是因为权大势重。所以圣人要衡量财物多少、权势大小的实况制定政策。刑罚轻并不是仁慈，刑罚重并不是残暴，适合社会状况行动就是了。因此，政事要根据时代变化，措施要针对社会事务。</p><blockquote><p>古者文王处丰、镐之间，地方百里，行仁义而怀西戎，遂王天下。徐偃王处汉东，地方五百里，行仁义，割地而朝者三十有六国，荆文王恐其害己也，举兵伐徐，遂灭之。故文王行仁义而王天下，偃王行仁义而丧其国，是仁义用於古不用於今也。故曰：“世异则事异。”当舜之时，有苗不服，禹将伐之，舜曰：“不可。上德不厚而行武，非道也。”乃修教三年，执干戚舞，有苗乃服。共工之战，铁銛矩者及乎敌，铠甲不坚者伤乎体，是干戚用於古不用於今也。故曰：“事异则备变。”上古竞於道德，中世逐於智谋，当今争於气力。齐将攻鲁，鲁使子贡说之，齐人曰：“子言非不辩也，吾所欲者土地也，非斯言所谓也。”遂举兵伐鲁，去门十里以为界。故偃王仁义而徐亡，子贡辩智而鲁削。以是言之，夫仁义辩智，非所以持国也。去偃王之仁，息子贡之智，循徐、鲁之力使敌万乘，则齐、荆之欲不得行於二国矣。</p></blockquote><p>古代周文王地处丰、镐一带，方圆不过百里，他施行仁义的政策感化了西戎。进而统治了天下。徐偃王统治着汉水东面的地方，方圆有五百里，他也施行仁义的政策，有三十六个国家向他割地朝贡。楚文王害怕徐国会危害到自己，便出兵伐徐灭了徐国。所以周文王施行仁义得了天下，而徐偃王施行仁义却亡了国；这证明仁义只适用于古代而不适用于今天。所以说；时代不同了，政事就会随之不同。在舜当政的时候，苗族不驯服，禹主张用武力去讨伐，舜说：“不行。我们推行德教还不够深就动用武力，不合乎道理。”于是便用三年时间加强德教，拿着盾牌和大斧跳舞，苗族终于归服了。到了共工打仗的时候，武器短的会被敌人击中，销甲不坚固的便会伤及身体；这表明拿着盾牌和大斧跳舞的德政方法只能用于古代而不能用于当今。所以说：情况变了，措施也要跟着改变。上古时候人们在道德上竞争高下，中古时候人们在智谋上角逐优劣，当今社会人们在力量上较量输赢。齐国准备进攻鲁国，鲁国派子贡去说服齐人。齐人说：“你的话说得不是不巧妙，然而我想要的是土地，不是你所说的这套空话。”于是出兵攻打鲁国，把齐国的国界推进到距鲁国都城只有十里远的地方。所以说徐偃王施行仁义而徐亡了国，子贡机智善辩而鲁失了地。由此说来，仁义道德、机智善辩之类，都不是用来保全国家的正道。如果当初抛弃徐偃王的仁义，不用子贡的巧辩，而是依靠徐、鲁两国的实力，去抵抗有万辆兵车的强敌，那么齐、楚的野心也就不会在这两个国家里得逞了。</p><blockquote><p>夫古今异俗，新故异备，如欲以宽缓之政、治急世之民，犹无辔策而御騛马，此不知之患也。今儒、墨皆称“先王兼爱天下”，则视民如父母。何以明其然也？曰：“司寇行刑，君为之不举乐；闻死刑之报，君为流涕。”此所举先王也。夫以君臣为如父子则必治，推是言之，是无乱父子也。人之情性，莫先於父母，皆见爱而未必治也，虽厚爱矣，奚遽不乱？今先王之爱民，不过父母之爱子，子未必不乱也，则民奚遽治哉！且夫以法行刑而君为之流涕，此以效仁，非以为治也。夫垂泣不欲刑者仁也，然而不可不刑者，法也。先王胜其法不听其泣，则仁之不可以为治亦明矣。</p></blockquote><p>古今社会风俗不同，新旧政治措施也不一样。如果想用宽大和缓的政策去治理剧变时代的民众，就好比没有缰绳和鞭子却要去驾驭烈马一样，这就会产生不明智的祸害。现在，儒家和墨家都称颂先王，说他们博爱天下一切人，就如同父母爱子女一样。用什么证明先王如此呢？他们说：“司寇执行刑法的时候，君主为此停止奏乐；听到罪犯被处决的报告后，君主难过得流下眼泪。”这就是他们所赞美的先王。如果认为君臣关系能像父子关系一样，天下必能治理得好，由此推论开去，就不会存在父子之间发生纠纷的事了。从人类本性上说，没有什么感情能超过父母疼爱子女的，然而大家都一样疼爱子女，家庭却未必就和睦。君主即使深爱臣民，何以见得天下就不会发生动乱呢？何况先王的爱民不会超过父母爱子女，子女不一定不背弃父母，那么民众何以就能靠仁爱治理好呢？再说按照法令执行刑法，而君主为之流泪；这不过是用来表现仁爱罢了，却并非用来治理国家的。流泪而不想用刑，这是君主的仁爱；然而不得不用刑，这是国家的法令。先王首先要执行法令，并不会因为同情而废去刑法，那么不能用仁爱来治理国家的道理也就明白无疑了。</p><blockquote><p>且民者固服於势，寡能怀於义。仲尼，天下圣人也，修行明道以游海内，海内说其仁，美其义，而为服役者七十人，盖贵仁者寡，能义者难也。故以天下之大，而为服役者七十人，而仁义者一人。鲁哀公，下主也，南面君国，境内之民莫敢不臣。民者固服於势，诚易以服人，故仲尼反为臣，而哀公顾为君。仲尼非怀其义，服其势也。故以义则仲尼不服於哀公，乘势则哀公臣仲尼。今学者之说人主也，不乘必胜之势，而务行仁义则可以王，是求人主之必及仲尼，而以世之凡民皆如列徒，此必不得之数也。</p></blockquote><p>况且人们一向就屈服于权势，很少能被仁义感化的。孔子是天下的圣人，他修养身心，宣扬儒道，周游列国，可是天下赞赏他的仁、颂扬他的义并肯为他效劳的人才七十来个。可见看重仁的人少，能行义的人实在难得。所以天下这么大，愿意为他效劳的只有七十人，而倡导仁义的只有孔子一个。鲁哀公是个不高明的君主，面南而坐，统治鲁国，国内的人没有敢于不服从的。民众总是屈服于权势，权势也确实容易使人服从；所以孔子反倒做了臣子，而鲁哀公却成了君主。孔子并不是服从于鲁哀公的仁义，而是屈服于他的权势。因此，要讲仁义，孔子就不会屈服于哀公；要讲权势，哀公却可以使孔子俯首称臣。现在的学者们游说君主，不是要君主依靠可以取胜的权势，而致力于宣扬施行仁义就可以统治天下；这就是要求君主一定能像孔子那样，要求天下民众都像孔子门徒。这在事实上是肯定办不到的。</p><blockquote><p>今有不才之子，父母怒之弗为改，乡人谯之弗为动，师长教之弗为变。夫以父母之爱，乡人之行，师长之智，三美加焉而终不动，其胫毛不改；州部之吏，操官兵、推公法而求索奸人，然后恐惧，变其节，易其行矣。故父母之爱不足以教子，必待州部之严刑者，民固骄於爱，听於威矣。故十仞之城，楼季弗能逾者，峭也；千仞之山，跛牂易牧者，夷也。故明王峭其法而严其刑也。布帛寻常，庸人不释；铄金百溢，盗跖不掇。不必害则不释寻常，必害手则不掇百溢，故明主必其诛也。是以赏莫如厚而信，使民利之；罚莫如重而必，使民畏之；法莫如一而固，使民知之。故主施赏不迁，行诛无赦。誉辅其赏，毁随其罚，则贤不肖俱尽其力矣。</p></blockquote><p>现在假定有这么一个不成材的儿子，父母对他发怒，他并不悔改；乡邻们加以责备，他无动于衷；师长教训他，他也不改变。拿了父母的慈爱、乡邻的帮助、师长的智慧这三方面的优势同时加在他的身上，而他却始终不受感动，丝毫不肯改邪归正。直到地方上的官吏拿着武器，依法执行公务，而搜捕坏人的时候，他这才害怕起来，改掉旧习，变易恶行。所以父母的慈爱不足以教育好子女，必须依靠官府执行严厉的刑法；这是由于人们总是受到慈爱就娇纵，见到威势就屈服的缘故。因此，七丈高的城墙，就连善于攀高的楼季也不能越过，因为太陡；干丈高的大山，就是瘸腿的母羊也可以被赶上去放牧，因为坡度平缓。所以明君总要严峻立法并严格用刑。十几尺布帛，一般人见了也舍不得放手；熔化着的百镒黄金，即使是盗跃也不会伸手去拿。不一定受害的时候，十几尺的布帛也不肯丢掉；肯定会烧伤手时，就是百镒黄金也不敢去拿。所以明君—定要严格执行刑罚。因此，施行奖赏最好是丰厚而且兑，使人们有所贪图；进行刑罚最好严厉而且肯定，使人们有所畏惧；法令最好是一贯而且固定，使人们都能明白。所以君主施行奖赏不随意改变，执行刑罚不轻易赦免5对受赏的人同时给予荣誉，对受罚的人同时给予谴责。这样一来，不管贤还是不贤的人，都会尽力而为了。</p><blockquote><p>今则不然。以其有功也爵之，而卑其士官也；以其耕作也赏之，而少其家业也；以其不收也外之，而高其轻世也；以其犯禁也罪之，而多其有勇也。毁誉、赏罚之所加者相与悖缪也，故法禁坏而民愈乱。今兄弟被侵必攻者廉也，知友被辱随仇者贞也，廉贞之行成，而君上之法犯矣。人主尊贞廉之行，而忘犯禁之罪，故民程於勇而吏不能胜也。不事力而衣食则谓之能，不战功而尊则谓之贤，贤能之行成而兵弱而地荒矣。人主说贤能之行，而忘兵弱地荒之祸，则私行立而公利灭矣。</p></blockquote><p>现在就不是这样。正是因为他有功劳才授予他爵位的，却又鄙视他做官；因为他从事耕种才奖赏他，却又看不起他经营家业；因为他不肯为公干事才疏远他，却又推祟他不羡慕世俗名利；因为他违犯禁令才给他定罪，却又称赞他勇敢。是毁是誉，是赏是罚。执行起来竞如此自相矛盾；所以法令遭到破坏，民众更加混乱。现在假如自己的兄弟受到侵犯就一定帮他反击的人，被认为是正直；知心的朋友被侮辱就跟随着去报仇的人，被认为是忠贞。这种正直和忠贞的风气形成了，而君主的法令却被冒犯了。君主推崇这种忠贞正直的品行，却忽视了他们违犯法令的罪责，所以人们敢于逞勇犯禁，而官吏制止不住。对于不从事耕作就有吃有穿的人．说他有本事；对于没有军功就获得官爵的人，说他有才能。这种本事和才能养成了，就会导致国家兵力衰弱、土地荒芜了。君主赞赏这种本事和才能，却忘却兵弱地荒的祸害；结果谋私的行为就会得逞，而国家的利益就要落空。</p><blockquote><p>儒以文乱法，侠以武犯禁，而人主兼礼之，此所以乱也。夫离法者罪，而诸先生以文学取；犯禁者诛，而群侠以私剑养。故法之所非，君之所取；吏之所诛，上之所养也。法趣上下四相反也，而无所定，虽有十黄帝不能治也。故行仁义者非所誉，誉之则害功；工文学者非所用，用之则乱法。楚之有直躬，其父窃羊而谒之吏，令尹曰：“杀之。”以为直於君而曲於父，报而罪之。以是观之，夫君之直臣，父之暴子也。鲁人从君战，三战三北，仲尼问其故，对曰：“吾有老父，身死莫之养也。”仲尼以为孝，举而上之。以是观之，夫父之孝子，君之背臣也。故令尹诛而楚奸不上闻，仲尼赏而鲁民易降北。上下之利若是其异也，而人主兼举匹夫之行，而求致社稷之福，必不几矣。</p></blockquote><p>儒家利用文献扰乱法纪，游侠使用武力违犯禁令，而君主却都要加以礼待，这就是国家混乱的根源。犯法的本该判罪，而那些儒生却靠着文章学说得到任用；犯禁的本该处罚，而那些游侠却靠着充当刺客得到豢养。所以，法令反对的，成了君主重用的；官吏处罚的，成了权贵豢养的。法令反对和君主重用，官吏处罚和权贵豢养，四者互相矛盾，而没有确立一定标准，即使有十个黄帝，也不能治好天下。所以对于宣扬仁义的人不应当加以称赞，如果称赞了，就会妨害功业；对于从事文章学术的人不应当加以任用，如果任用了，就会破坏法治。楚国有个叫直躬的人，他的父亲偷了人家的羊，他便到令尹那儿吉发，令尹说：“杀掉他：”认为他对君主虽算正直而对父亲却属不孝。结果判了他死罪。由此看来，君主的忠臣倒成了父亲的逆子。鲁国有个人跟随君土去打仗，屡战屡逃；孔子向他询问原因，他说：“我家中有年老的父亲，我死后就没人养活他了。”孔子认为这是孝子，便推举他做丁官。由此看来。父亲的孝子恰恰是君主的叛臣。所以令尹杀了直躬，楚国的坏人坏事就没有人再向上告发了；孔子奖赏逃兵．鲁国人作战就要轻易地投降逃跑。君臣之间的利害得失是如此不同，而君主却既赞成谋求私利的行为。又想求得国家的繁荣富强，这是肯定没指望的。</p><blockquote><p>古者苍颉之作书也，自环者谓之私，背私谓之公，公私之相背也，乃苍颉固以知之矣。今以为同利者，不察之患也。然则为匹夫计者，莫如脩行义而习文学。行义脩则见信，见信则受事；文学习则为明师，为明师则显荣；此匹夫之美也。然则无功而受事，无爵而显荣，有政如此，则国必乱，主必危矣。故不相容之事，不两立也。斩敌者受赏，而高慈惠之行；拔城者受爵禄，而信廉爱之说；坚甲厉兵以备难，而美荐绅之饰；富国以农，距敌恃卒，而贵文之士；废敬上畏法之民，而养游侠私剑之属。举行如此，治强不可得也。国平养儒侠，难至用介士，所利非所用，所用非所利。是故服事者简其业，而游学者日众，是世之所以乱也。</p></blockquote><p>古时候，苍颉创造文字，把围着自己绕圈子的叫做“私”。与“私”相背的叫做“公”。公和私相反的道理，是苍颉就已经知道厂的。现在还有人认为公私利益相同，这是犯了没有仔细考察的错误。那么为个人打算的话，没有什么比修好仁义、熟悉学术的办法更好了。修好仁义就会得到君主信任。得到君主信任就可以做官；熟悉学术就可以成为高明的老师。成了高明的老师就会显荣。对个人来说。这是最美的事了。然而没有功劳的就能做官。没有爵位就能显荣，形成这样的政治局面。国家就一定陷入混乱，君主就一定面临危险了。所以，互不相容的事情，是不能并存的。杀敌有功的人本该受赏，却又崇尚仁爱慈惠的行为；攻城大功的人本该授予爵禄。却又信奉兼爱的学说：采用坚固的铠甲、锋利的兵器来防备战乱，却又提倡宽袍大带的服饰；国家富足靠农民。打击敌人靠士兵，却又看重从事于文章学术事业的儒生；不用那些尊君守法的人，而去收养游侠刺客之类的人。如此理政，要想使国家太平和强盛足不可能的。国家太平的时候收养儒生和游侠，危难来临的时候要用披坚执锐的士兵；国家给予利益的人并不是国家所要用的人．而国家所要用的人又得不到任何好处。结果从事耕战的人荒废了自己的事业，而游侠和儒生却—天天多了起来，这就是社会陷于混乱的原因所在。</p><blockquote><p>且世之所谓贤者，贞信之行也。所谓智者，微妙之言也。微妙之言，上智之所难知也。今为众人法，而以上智之所难知，则民无从识之矣。故糟糠不饱者不务梁肉，短褐不完者不待文绣。夫治世之事，急者不得，则缓者非所务也。今所治之政，民閒之事，夫妇所明知者不用，而慕上知之论，则其於治反矣。故微妙之言，非民务也。若夫贤良贞信之行者，必将贵不欺之士。不欺之士者，亦无不欺之术也。布衣相与交，无富厚以相利，无威势以相惧也，故求不欺之士。今人主处制人之势，有一国之厚，重赏严诛，得操其柄，以修明术之所烛，虽有田常、子罕之臣，不敢欺也，奚待於不欺之士？今贞信之士不盈於十，而境内之官以百数，必任贞信之士，则人不足官，人不足官则治者寡而乱者众矣。故明主之道，一法而不求智，固术而不慕信，故法不败，而群官无奸诈矣。</p></blockquote><p>况且社会上所说的贤。是指忠贞不欺的行为；所说的智，是指深奥玄妙的言辞。那些深奥玄妙的言辞，就连最聪明的人也难以理解。现在制定民众都得遵守的法令，却采用那些连最聪明的人也难以理解的言辞，那么民众就无从弄懂了。所以，连糟糠都吃不饱的人，是不会追求精美饭菜的；连粗布短衣都穿不上的人，是不会期望华丽衣衫的。治理社会事务，如果紧急的还没有办好，那么可从缓的就不必忙着去办。现在用来治理国家的政治措施，凡属民间习以为常的事。或普通人明知的道理不加采用，却去期求连最聪明的人都难以理解的说教，其结果只能是适得其反了。所以那些深奥玄妙的言辞，并不是人民所需要的。至于推崇忠贞信义的品行。必将尊重那些诚实不欺的人；而诚实不欺的人，也没有什么使人不行欺诈的办法。平民之间彼此交往，没有大宗钱财可以互相利用，没有大权重势可以互相威胁。所以才要寻求诚实不欺的人。如今君主处于统治地位，拥有整个国家的财富，完全有条件掌握重赏严罚的权力，可以运用法术来观察和处理问题；那么即使有田常、子罕—类的臣子也是不敢行欺的，何必寻找那些诚实不欺的人呢？现今的忠贞信义之十不满十个。而国家需要的官吏却数以百计；如果一定要任用忠贞信义之士。那么合格的人就会不敷需要；合格的人不敷需要，那么能够把政事治理好的官就少，而会把政事搞乱的官就多了。所以明君的治国方法，在于专实行法治，而不寻求有智的人；牢牢掌握使用官吏的权术。而不欣赏忠信的人。这样，法治就不会遭到破坏而官吏们也不敢胡作非为了。</p><blockquote><p>今人主之於言也，说其辩而不求其当焉；其用於行也，美其声而不责其功焉。是以天下之众，其谈言者务为辩而不周於用，故举先王言仁义者盈廷，而政不免於乱；行身者竞於为高而不合於功，故智士退处岩穴，归禄不受，而兵不免於弱，政不免於乱，此其故何也？民之所誉，上之所礼，乱国之术也。今境内之民皆言治，藏商、管之法者家有之，而国愈贫，言耕者众，执耒者寡也；境内皆言兵，藏孙、吴之书者家有之，而兵愈弱，言战者多，被甲者少也。故明主用其力，不听其言；赏其功，必禁无用；故民尽死力以从其上。夫耕之用力也劳，而民为之者，曰：可得以富也。战之为事也危，而民为之者，曰：可得以贵也。今修文学、习言谈，则无耕之劳而有富之实，无战之危而有贵之尊，则人孰不为也？是以百人事智而一人用力，事智者众则法败，用力者寡则国贫，此世之所以乱也。</p></blockquote><p>现在君主对于臣下的言论。喜欢悦耳动听而不管是否恰当；对于臣下的行事，仅欣赏他的名声而不责求做出成效。因此天下很多人说起话来总是花言巧语，却根本不切合实用，结果弄得称颂先王、高谈仁义的人充满朝廷，而政局仍不免于混乱；立身处世的人竞相标榜清高，不去为国家建功立业。结果有才智的人隐居山林，推辞俸禄而不接受，而兵力仍不免于削弱。政局不免于混乱，这究竟是怎么造成的呢？因为民众所称赞的，君主所优待的，都是些使国家混乱的做法。现在全国的民众都在谈论如何治国，每家每户都藏有商鞅和管仲的法典，国家却越来越穷，原因就在于空谈耕作的人太多，而真正拿起农具种地的人太少。全国的民众都在谈论如何打仗，每家每户都藏有孙子和吴起的兵书，国家的兵力却越来越弱；原因就在于空谈打仗的人太多．而真正穿起铠甲上阵的人太少。所以明君只使用民众的力量，不听信高谈阔论；奖赏人们的功劳，坚决禁止那些无用的言行。这样民众就会拼命为君主出力。耕种是需要花费气力吃苦耐劳的事情。而民众印愿意去干，因为他们认为可以由此得到富足。打仗是十外危险的事情。而民众却愿意去于。因为他们认为可以出此获得显贵。如今只要擅长文章学术，能说会道。无需有耕种的劳苦就可以获得富足的实惠。无需冒打仗的危险便可以得到尊贵的官爵，那么人们谁不乐意这样干呢？结果就出现了一百个人从事于智力活动，却只有一个人致力于耕战事业的状况。从事于智力活动的人多了，法治就要遭到破坏；致力于耕战事业的人少了，国家就会变得贫穷。这就是社会所以混乱的原因。</p><blockquote><p>故明主之国，无书简之文，以法为教；无先王之语，以吏为师；无私剑之捍，以斩首为勇。是境内之民，其言谈者必轨於法，动作者归之於功，为勇者尽之於军。是故无事则国富，有事则兵强，此之谓王资。既畜王资而承敌国之亹，超五帝，侔三王者，必此法也。</p></blockquote><p>因此，在明君的国家里，不用有关学术的文献典籍。而以法令为教本；禁绝先王的言论，而以官吏为老师；没有游侠刺客的凶悍，而只以杀敌立功为勇敢。这样，国内民众的一切言论都必须遵循法令，—切行动都必须归于为国立功，一切勇力都必须用到从军打仗上。正因如此。太平时期国家就富足，战争时期兵力就强盛，这便奠定了称王天下的资本。既拥有称五天下的资本，义善于利用敌国的弱点；建立超过五帝、赶上三王的功业，一定得采用这种办法。</p><blockquote><p>今则不然，士民纵恣於内，言谈者为势於外，外内称恶以待强敌，不亦殆乎！故群臣之言外事者，非有分於从衡之党，则有仇之忠，而借力於国也。从者，合众弱以攻一强也；而衡者，事一强以攻众弱也；皆非所以持国也。今人臣之言衡者皆曰：“不事大则遇敌受祸矣。”事大未必有实，则举图而委，效玺而请兵矣。献图则地削，效玺则名卑，地削则国削，名卑则政乱矣。事大为衡未见其利也，而亡地乱政矣。人臣之言从者皆曰：“不救小而伐大则失天下，失天下则国危，国危而主卑。”救小未必有实，则起兵而敌大矣。救小未必能存，而交大未必不有疏，有疏则为强国制矣。出兵则军败，退守则城拔，救小为从未见其利，而亡地败军矣。</p></blockquote><p>现在却不是这样。儒士、游侠在国内恣意妄为，纵横家在国外大造声势。内外形势尽行恶化，就这样来对付强敌。不是太危险了吗？所以那些谈论外交问题的臣子们，不是属于合纵或连衡中的哪一派，就是怀有借国家力量来报私仇的隐衷。所谓合纵。就是联合众多弱小国家去攻打一个强大国家；所谓连衡，就是依附于一个强国去攻打其他弱国。这都不是保全国家的好办法。现在那些主张连衡的臣子都说：“不依附大国，一遇强敌就得遭殃。”侍奉大国不一定有什么实际效应，倒必须先献出本国地图，呈上政府玺印，这样才得以请求军事援助。献出地图，本国的版域就缩小了；呈上空印，君主的声望就降低了。版域缩小。国家就削弱了；声望降低。政治上就混乱了。侍奉大国实行连衡。还来不及看到什么好处，却已丧失了国土，搞乱了政治。那些主张合纵的臣子都说：“不救援小国去进攻大国，就失了各国的信任；失去了各国的信任，国家就面临危险；国家面临危险。君主地位就降低了。”援救小国不一定有什么实惠可言。倒要起兵去和大国为敌。援救小国木必能使它保存下来。而进攻大国未必就不失误一有失误，就要被大国控制了。出兵的话，军队就要吃败仗；退守的话，城池就会被攻破。援救小国实行合纵。还来不及看到什么好处，却已使国土被侵吞，军队吃败仗。</p><blockquote><p>是故事强则以外权士官於内，救小则以内重求利於外，国利未立，封土厚禄至矣；主上虽卑，人臣尊矣；国地虽削，私家富矣。事成则以权长重，事败则以富退处。人主之於其听说也，於其臣，事未成则爵禄已尊矣；事败而弗诛，则游说之士，孰不为用砫缴之说而徼悻其后？故破国亡主以听言谈者之浮说，此其故何也？是人君不明乎公私之利，不察当否之言，而诛罚不必其后也。皆曰：“外事大可以王，小可以安。”</p></blockquote><p>所以，侍奉强国，只能使那些搞连衡的人凭借外国势力在国内捞取高官；援救小国，只能使那些搞合纵的人凭借国内势力从国外得到好处。国家利益没有确立起来，而臣下倒先把封地和厚禄都弄到手了。尽管君主地位降低了，而臣下反而抬高了；尽管国家土地削减了，而私家却变富了。事情如能成功．纵横家们就会依仗权势长期受到重用；事情失败的话，纵横家们就会凭借富有引退回家享福。君主如果听信臣下的游说，事情还没办成就已给了他们很高的爵位俸禄，事情失败得不到处罚；那么，那些游说之士谁不愿意用猎取名利的言辞不断去进行投机活动呢？所以国破君亡局面的出现，都是因为听信了纵横家的花言巧语造成的。这是什么缘故呢？这是因为君主分不清公私利益，不考察言论是公正确，事败之后也没有坚决地实行处罚。纵横家们都说：“进行外交活动，收效大的可以统—天下，收效小的也可以保证安全。”</p><blockquote><p>夫王者，能攻人者也；而安，则不可攻也。强，则能攻人者也；治，则不可攻也。治强不可责於外，内政之有也。今不行法术於内，而事智於外，则不至於治强矣。</p></blockquote><p>所谓统—天下，提的是能够打败别国；所谓保旺安全，指的是本国不受侵犯。兵强就能打败别国。国安就不可能被人侵犯。而国家的强盛和安定并不能通过外交活动取得，只能靠搞好内政。现在不在国内推行法术，却要一心在外交上动脑筋。就必然达下到国家安定富强的目的了。</p><blockquote><p>鄙谚曰：“长袖善舞，多钱善贾。”此言多资之易为工也。故治强易为谋，弱乱难为计。故用於秦者十变而谋希失，用於燕者一变而计希得，非用於秦者必智，用於燕者必愚也，盖治乱之资异也。故周去秦为从，期年而举；卫离魏为衡，半岁而亡。是周灭於从，卫亡於衡也。使周、卫缓其从衡之计，而严其境内之治，明其法禁，必其赏罚，尽其地力以多其积，致其民死以坚其城守，天下得其地则其利少，攻其国则其伤大，万乘之国、莫敢自顿於坚城之下，而使强敌裁其弊也，此必不亡之术也。舍必不亡之术而道必灭之事，治国者之过也。智困於内而政乱於外，则亡不可振也。</p></blockquote><p>乡间谚语说：“长袖善舞，多钱善贾。”这就是说，物质条件越好越容易取得功效。所以国家安定强盛，谋事就容易成功；国家衰弱混乱，计策就难以实现。所以用于秦国的计谋，即使改变十次也很少失败；用于燕国的计谋，即使改变一次也很难成功。这并不是被秦国任用的人智慧必高，被燕国任用的人脑子必笨，而是因为这两个国家的治乱条件大不相同。所以西周背弃秦国参予合纵，只一年工夫就被吞灾了；卫国背离魏国参与连衡，仅半年工夫就被消灭了。这就是说合纵灭了西周，连衡亡了卫国。假使西周和卫国不急于听从合纵连横的计谋，而将国内政治严加整顿，明定法律禁令，信守赏罚制度，努力开发土地来增加积累，使民众拼死去坚守城池；那么．别的国家夺得他们的土地吧，好处不多。而进攻这个国家吧，伤亡很大。拥有万乘兵车的大国不敢自我拖累在坚城之下，从而促使强敌自己去衡量其中的害处，这才是保证本国必然不会灭亡的办法。丢掉这种必然不会亡国的办法，却去搞势必会招致亡国的事情，这是治理国家的人的过错。外交努力陷于困境，内政建设陷于混乱，那么国家的灭亡就无法挽救了。</p><blockquote><p>民之故计，皆就安利如辟危穷。今为之攻战，进则死於敌，退则死於诛则危矣。弃私家之事而必汗马之劳，家困而上弗论则穷矣。穷危之所在也，民安得勿避。故事私门而完解舍，解舍完则远战，远战则安。行货赂而袭当涂者则求得，求得则私安，私安则利之所在，安得勿就？是以公民少而私人众矣。</p></blockquote><p>人们的习惯想法，都是追求安逸和私利而避开危险和穷苦。如果让他们去打仗。前进会被敌人杀死，后退要受军法处置，就处于危险之中了。放弃个人的家业，承受作战的劳苦，家里有困难而君主不予过问，就置于穷困之中了。穷困和危险交加，民众怎能不逃避呢？所以他们投靠私门贵族，求得免除兵役，兵役免除了就可以远离战争，远离战争也就可以得到安全了。用钱财贿赂当权者就可以达到个人欲望，欲望一旦达到也就得到了实际利益。平安有利的事情明摆在那里，民众怎能不去追求呢？这样一来，为公出力的人就少了，而依附私门的人就多了。</p><blockquote><p>夫明王治国之政，使其商工游食之民少而名卑，以寡趣本务而趋末作。今世近习之请行则官爵可买，官爵可买则商工不卑也矣；奸财货贾得用於市则商人不少矣。聚敛倍农而致尊过耕战之士，则耿介之士寡而高价之民多矣。</p></blockquote><p>明君治理国家的政策，总是要使工商业者和游手好闭的人尽量减少。而且名位卑下；以免从事农耕的人少而致力于工商业的人多。现在社会上向君主亲近的侍臣行贿托情的风气很流行，这样官爵就可以用钱买到；官爵可以用钱买到，那么工商业者的地位就不会低贱了。投机取巧非法获利的活动可以在市场上通行，那么商人就不会少了。他们搜括到的财富超过了农民收入的几倍，他们获得的尊贵地位也远远超过从事耕战的人，结果刚正不阿的人就越来越少，而经营商业的人就越来越多。</p><blockquote><p>是故乱国之俗，其学者则称先王之道，以籍仁义，盛容服而饰辩说，以疑当世之法而贰人主之心。其言古者，为设诈称，借於外力，以成其私而遗社稷之利。其带剑者，聚徒属，立节操，以显其名而犯五官之禁。其患御者，积於私门，尽货赂而用重人之谒，退汗马之劳。其商工之民，修治苦窳之器，聚弗靡之财，蓄积待时而侔农夫之利。此五者，邦之蠹也。人主不除此五蠹之民，不养耿介之士，则海内虽有破亡之国，削灭之朝，亦勿怪矣。</p></blockquote><p>因此，造成国家混乱的风气是：那些著书立说的人，称引先王之道来宣扬仁义道德；讲究仪容服饰而文饰巧辩言辞，用以扰乱当今的法令，从而动摇君主的决心。那些纵横家们，弄虚作假，招摇撞骗，借助于国外势力来达到私人目的，进而放弃了国家利益。那些游侠刺客，聚集党徒，标榜气节，以图显身扬名，结果触犯国家禁令。那些逃避兵役的人，大批依附权臣贵族，肆意行贿，而借助于重臣的请托，逃避从军作战的劳苦。那些工商业者，制造粗劣器具，积累奢侈资财。囤积居奇，待机出售，希图从农民身上牟取暴利。上述这五种人，都是国家的蛀虫。君主如果不除掉这五种像蛀虫一样的人，不广罗刚直不阿的人，那么。天下即使出现破败沦亡的国家，地削名除的朝廷，也不足为怪了。</p><h1 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h1><p>五蠹指几类人：</p><ol><li>学者，那些著书立说的人，称引先王之道来宣扬仁义道德；讲究仪容服饰而文饰巧辩言辞，用以扰乱当今的法令，从而动摇君主的决心。 </li><li>言古者，那些纵横家们，弄虚作假，招摇撞骗，借助于国外势力来达到私人目的，进而放弃了国家利益。</li><li>带剑者，那些游侠刺客，聚集党徒，标榜气节，以图显身扬名，结果触犯国家禁令。</li><li>患御者，那些逃避兵役的人，大批依附权臣贵族，肆意行贿，而借助于重臣的请托，逃避从军作战的劳苦。</li><li>商工之民，那些工商业者，制造粗劣器具，积累奢侈资财。囤积居奇，待机出售，希图从农民身上牟取暴利。</li></ol><p>上述这五种人，都是国家的蛀虫。</p><p>工作上团队的蛀虫：</p><ol><li>天天吹嘘方法论，满口黑话的人。</li><li>方案设计的天花乱坠，实际方案落不了地，产出等于0人.</li><li>自己天天消极做事情，还天天在团队传播负能量的人。</li><li>基础素养差，技术能力不行，别人建议也不听，还自我感觉良好的人。</li><li>什么事情都凑合，思维不严谨，写的代码上线一堆问题的人。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Literature </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GO非类型安全指针-Unsafe.Pointer</title>
      <link href="/2022/09/12/unsafe-pointer/"/>
      <url>/2022/09/12/unsafe-pointer/</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h1><p>朋友发了一段测试代码里面不正确的使用了<code>atomic.StorePointer</code>，导致<code>GC</code>的时候程序<code>Panic</code>了。</p><pre><code>var current int64atomic.StorePointer((*unsafe.Pointer)(unsafe.Pointer(&amp;current)), unsafe.Pointer(&amp;latest))</code></pre><p>为什么会<code>Panic</code>这里先按下不表。之前对 <a href="https://github.com/golang/go/blob/master/src/unsafe/unsafe.go">unsafe.Pointer</a> 用的并不多，也没有系统了解过。所以就想系统看下。看了下 <code>unsafe.Pointer</code> <a href="https://golang.google.cn/pkg/unsafe/">官方文档</a>还挺详细的，可能只之前使用出错的人太多了，所以 <a href="https://github.com/rsc">rsc</a> 单独提了一个 <a href="https://go-review.googlesource.com/c/go/+/18640/">CR</a> 来说明<code>unsafe.Pointer</code>的用法。</p><h1 id="二、unsafe-Pointer"><a href="#二、unsafe-Pointer" class="headerlink" title="二、unsafe.Pointer"></a>二、unsafe.Pointer</h1><p><code>unsafe.Pointer</code>表示指向任意类型的指针，主要可以做下面<code>4</code>个操作：</p><ol><li>任意类型的指针值都可以转换为<code>unsafe.Pointer</code>。</li><li><code>unsafe.Pointer</code>可以转换为任意类型的指针值。</li><li><code>uintptr</code>可以转换为<code>unsafe.Pointer</code>。</li><li><code>unsafe.Pointer</code>可以转换为<code>uintptr</code>。</li></ol><h2 id="2-1-场景一-类型转换"><a href="#2-1-场景一-类型转换" class="headerlink" title="2.1 场景一 类型转换"></a>2.1 场景一 类型转换</h2><p><code>unsafe.Pointer</code>支持<code>*T1</code>到<code>*T2</code>类型的转换，前提是<code>T2</code>类型要小于<code>T1</code>类型大小，比如<code>reflect.SliceHeader</code>转为<code>reflect.StringHeader</code></p><pre><code>func SliceByteToString(b []byte) string {    return *(*string)(unsafe.Pointer(&amp;b))}func Float64bits(f float64) uint64 {    return *(*uint64)(unsafe.Pointer(&amp;f))}func Float64frombits(b uint64) float64 {    return *(*float64)(unsafe.Pointer(&amp;b))}</code></pre><h2 id="2-2-场景二-unsafe-Pointer-转换为-uintptr"><a href="#2-2-场景二-unsafe-Pointer-转换为-uintptr" class="headerlink" title="2.2 场景二 unsafe.Pointer 转换为 uintptr"></a>2.2 场景二 unsafe.Pointer 转换为 uintptr</h2><p>将指针转换为<code>uintptr</code> 生成指向值的内存地址，作为整数。 这种<code>uintptr</code>的通常用途是打印它。</p><ul><li>将<code>uintptr</code>转换回<code>Pointer</code>通常是无效的。（编译器会有<code>Possible misuse of 'unsafe.Pointer' 警告</code>）</li><li><code>uintptr</code>是一个整数，而不是一个引用。</li><li>即使 <code>uintptr</code>保存了某个对象的地址，垃圾收集器也不会更新该<code>uintptr</code>的值。</li></ul><pre><code>func main() {    type User struct{ age int }    var t User    fmt.Printf("%p\n", &amp;t)           // 0xc000018270    println(&amp;t)                      // 0xc000018270    p := uintptr(unsafe.Pointer(&amp;t)) // c000018270    fmt.Printf("Ox%x\n", p)          // 0xc000018270}</code></pre><h2 id="2-3-场景三-计算-uintptr-得到-Pointer"><a href="#2-3-场景三-计算-uintptr-得到-Pointer" class="headerlink" title="2.3 场景三 计算 uintptr 得到 Pointer"></a>2.3 场景三 计算 uintptr 得到 Pointer</h2><p>如果<code>p</code>指向一个已分配的对象，则可以通过转换为<code>uintptr</code>、添加偏移量和转换回<code>Pointer</code>来推进该对象。此模式最常见的用途是访问结构中的字段或数组的元素：</p><pre><code>func main() {    type Num struct {        i string        j int64    }    n := Num{i: "test", j: 1}    nPointer := unsafe.Pointer(&amp;n)    niPointer := (*string)(nPointer)    *niPointer = "dr"    njPointer := (*int64)(unsafe.Pointer(uintptr(nPointer) + unsafe.Offsetof(n.j)))    *njPointer = 2    fmt.Println(n) // {dr 2}          // equivalent to e := unsafe.Pointer(&amp;x[i])    // e := unsafe.Pointer(uintptr(unsafe.Pointer(&amp;x[0])) + i*unsafe.Sizeof(x[0]))}</code></pre><h3 id="注意一：不要读到-Struct-String-Byte-尾部数据"><a href="#注意一：不要读到-Struct-String-Byte-尾部数据" class="headerlink" title="注意一：不要读到 Struct/String/[]Byte 尾部数据"></a>注意一：不要读到 Struct/String/[]Byte 尾部数据</h3><pre><code>// INVALID: end points outside allocated space.var s thingend = unsafe.Pointer(uintptr(unsafe.Pointer(&amp;s)) + unsafe.Sizeof(s))// INVALID: end points outside allocated space.b := make([]byte, n)end = unsafe.Pointer(uintptr(unsafe.Pointer(&amp;b[0])) + uintptr(n))</code></pre><h3 id="注意二：不要存储-uintptr-到变量【重要】"><a href="#注意二：不要存储-uintptr-到变量【重要】" class="headerlink" title="注意二：不要存储 uintptr 到变量【重要】"></a>注意二：不要存储 uintptr 到变量【重要】</h3><pre><code>// INVALID: uintptr cannot be stored in variable// before conversion back to Pointer.u := uintptr(p)p = unsafe.Pointer(u + offset)func main() {    type User struct{ age int }    var t User    fmt.Printf("%p\n", &amp;t)    p := uintptr(unsafe.Pointer(&amp;t))    fmt.Println((*User)(unsafe.Pointer(p))) // 执行 go vet, 这一行会有警告：possible misuse of unsafe.Pointer}</code></pre><h4 id="为什么不支持用变量存储-uintptr-，然后转unsafe-Pointer"><a href="#为什么不支持用变量存储-uintptr-，然后转unsafe-Pointer" class="headerlink" title="为什么不支持用变量存储 uintptr ，然后转unsafe.Pointer"></a>为什么不支持用变量存储 <code>uintptr</code> ，然后转<code>unsafe.Pointer</code></h4><p><strong>1. 一个值的生命范围可能并没有代码中看上去的大</strong></p><pre><code>type T struct {x int; y *[1&lt;&lt;23]byte}func bar() {    t := T{y: new([1&lt;&lt;23]byte)}    p := uintptr(unsafe.Pointer(&amp;t.y[0]))        // 一个聪明的编译器能够觉察到值t.y将不会再被用到而回收之。    *(*byte)(unsafe.Pointer(p)) = 1 // 危险操作！    println(t.x) // ok。继续使用值t，但只使用t.x字段。}</code></pre><p><strong>2. 栈扩容的时候地址会发生变化</strong>    </p><pre><code>func f(i int) int {    if i == 0 || i == 1 {        return i    }    return f(i - 1)}func main() {    var num uint64    xAddr := uintptr(unsafe.Pointer(&amp;num))     println("before stack copy num : ", num, " num pointer: ", &amp;num)    f(10000000)    xPointer := (*uint64)(unsafe.Pointer(xAddr)) // 这里有警告 possible misuse of unsafe.Pointer    atomic.AddUint64(xPointer, 1)    println("after stack copy num : ", num, " num pointer:", &amp;num)} // 输出内容如下：    before stack copy num :  0  num pointer:  0xc000044768after stack copy num :  0  num pointer: 0xc0200fff68</code></pre><h3 id="注意三：请注意，Pointer必须指向已分配的对象，因此它可能不是-nil。"><a href="#注意三：请注意，Pointer必须指向已分配的对象，因此它可能不是-nil。" class="headerlink" title="注意三：请注意，Pointer必须指向已分配的对象，因此它可能不是 nil。"></a>注意三：请注意，Pointer必须指向已分配的对象，因此它可能不是 nil。</h3><pre><code>// INVALID: conversion of nil pointeru := unsafe.Pointer(nil)p := unsafe.Pointer(uintptr(u) + 1) fmt.Println(p1) // 0x1</code></pre><h3 id="注意四：-unsafe-Pointer是一个类型安全指针类型"><a href="#注意四：-unsafe-Pointer是一个类型安全指针类型" class="headerlink" title="注意四：*unsafe.Pointer是一个类型安全指针类型"></a>注意四：*unsafe.Pointer是一个类型安全指针类型</h3><pre><code>func main() {    type T struct {x int}    var p *T    var unsafePPT = (*unsafe.Pointer)(unsafe.Pointer(&amp;p))    atomic.StorePointer(unsafePPT, unsafe.Pointer(&amp;T{123}))    fmt.Println(p) // &amp;{123}}</code></pre><h2 id="2-4-场景四-在调用-syscall-Syscall-时将指针转换为-uintptr"><a href="#2-4-场景四-在调用-syscall-Syscall-时将指针转换为-uintptr" class="headerlink" title="2.4 场景四 在调用 syscall.Syscall 时将指针转换为 uintptr"></a>2.4 场景四 在调用 syscall.Syscall 时将指针转换为 uintptr</h2><p><code>syscall</code> 包中的 <code>Syscall</code> 函数将它们的 <code>uintptr</code> 参数直接传递给操作系统，然后操作系统可能会根据调用的细节将其中一些重新解释为指针。也就是说，系统调用实现隐式地将某些参数转换回从 <code>uintptr</code> 到指针。</p><p>如果必须将指针参数转换为 uintptr 以用作参数，则该转换必须出现在调用表达式本身中：</p><pre><code>syscall.Syscall(SYS_READ, uintptr(fd), uintptr(unsafe.Pointer(p)), uintptr(n))</code></pre><p>编译器在调用汇编中实现的函数的参数列表中处理转换为 <code>uintptr</code> 的指针，方法是安排引用的分配对象（如果有）在调用完成之前保留并且不移动，即使从类型仅在通话期间似乎不再需要该对象。具体见 <a href="https://go-review.googlesource.com/c/go/+/18584/">CR</a></p><p>为了让编译器识别这种模式，转换必须出现在参数列表中，下面这种方式是无效的：</p><pre><code>// INVALID: uintptr cannot be stored in variable// before implicit conversion back to Pointer during system call.u := uintptr(unsafe.Pointer(p))syscall.Syscall(SYS_READ, uintptr(fd), u, uintptr(n))</code></pre><h2 id="2-5-场景五-reflect-Value-Pointer-或-reflect-Value-UnsafeAddr-的结果从-uintptr-到-Pointer-的转换。"><a href="#2-5-场景五-reflect-Value-Pointer-或-reflect-Value-UnsafeAddr-的结果从-uintptr-到-Pointer-的转换。" class="headerlink" title="2.5 场景五 reflect.Value.Pointer 或 reflect.Value.UnsafeAddr 的结果从 uintptr 到 Pointer 的转换。"></a>2.5 场景五 reflect.Value.Pointer 或 reflect.Value.UnsafeAddr 的结果从 uintptr 到 Pointer 的转换。</h2><p><code>reflect.Value.Pointer</code> 和 <code>reflect.Value.UnsafeAddr</code> 返回的是<code>uintptr</code>也不能用变量存储。同场景三的注意事项二。</p><pre><code>p := (*int)(unsafe.Pointer(reflect.ValueOf(new(int)).Pointer())) // ok// INVALID: uintptr cannot be stored in variable// before conversion back to Pointer.u := reflect.ValueOf(new(int)).Pointer() // uintptrp := (*int)(unsafe.Pointer(u))</code></pre><h2 id="2-6-场景六-reflect-SliceHeader-和-reflect-StringHeader-转换"><a href="#2-6-场景六-reflect-SliceHeader-和-reflect-StringHeader-转换" class="headerlink" title="2.6 场景六 reflect.SliceHeader 和 reflect.StringHeader 转换"></a>2.6 场景六 reflect.SliceHeader 和 reflect.StringHeader 转换</h2><p>场景一说了，大<code>size</code>的<code>struct</code>转小<code>size</code>的<code>struct</code>没有任何问题。比如：</p><pre><code>func SliceByteToString(b []byte) string {    return *(*string)(unsafe.Pointer(&amp;b))}</code></pre><p><code>reflect.StringHeader</code>转<code>reflect.SliceHeader</code>很多场景就会有问题，注意<strong>不要凭空生成SliceHeader和StringHeader，要从切片和字符串转换出它们。</strong> 详见 Runtime <a href="https://github.com/golang/go/blob/master/src/unsafe/unsafe.go#L160">代码注释</a></p><blockquote><p>As in the previous case, the reflect data structures SliceHeader and StringHeader declare the field Data as a uintptr to keep callers from changing the result to  an arbitrary type without first importing “unsafe”. However, this means that SliceHeader and StringHeader are only valid when interpreting the content of an actual slice or string value.</p></blockquote><pre><code>func main() {    fmt.Printf("main : %s\n", gcStr())}func gcStr() []byte {    defer runtime.GC()    x := []byte("1234567890")    return StringToSliceByte(string(x))}// 这个方法是凭空生成的一个reflect.SliceHeader，所以 s 被 gc 回收了， main 输出乱码func StringToSliceByte(s string) []byte {    l := len(s)    return *(*[]byte)(unsafe.Pointer(&amp;reflect.SliceHeader{        Data: (*(*reflect.StringHeader)(unsafe.Pointer(&amp;s))).Data,        Len:  l,        Cap:  l,    }))}func StringToSliceByte2(s string) []byte {    var b []byte // 这里申明了一个 slice 所以没有问题    stringHeader := (*reflect.StringHeader)(unsafe.Pointer(&amp;s))    sliceHeader := (*reflect.SliceHeader)(unsafe.Pointer(&amp;b))    sliceHeader.Data = stringHeader.Data    sliceHeader.Len = stringHeader.Len    sliceHeader.Cap = stringHeader.Len    return b}</code></pre><p><code>StringToSliceByte2</code> 是好的，是因为编译器对 <code>reflect.StringHeader</code> 做了<a href="https://github.com/golang/go/issues/19168">优化</a> 如果使用自定义的<code>StringHeader</code>和<code>SliceHeader</code> 依然有问题。</p><pre><code>type StringHeader struct {    Data uintptr // unsafe.Pointer    Len  int}type SliceHeader struct {    Data uintptr // unsafe.Pointer    Len  int    Cap  int}// https://groups.google.com/g/golang-nuts/c/Zsfk-VMd_fU/m/qJzdycRiCwAJ?pli=1func StringToSliceByte3(s string) []byte {    var b []byte // 这里申明了一个 slice 所以没有问题    stringHeader := (*StringHeader)(unsafe.Pointer(&amp;s))    sliceHeader := (*SliceHeader)(unsafe.Pointer(&amp;b))    sliceHeader.Data = stringHeader.Data    sliceHeader.Len = stringHeader.Len    sliceHeader.Cap = stringHeader.Len    return b}</code></pre><p>如果<code>StringHeader</code>和<code>SliceHeader</code>的<code>Data</code>改成<code>unsafe.Pointer</code>，那<code>StringToSliceByte3</code>也能正常<code>work</code>。所以 <a href="https://github.com/mdempsky">mdempsky</a> 就提议过，在<code>unsafe</code>包里面新增<code>Slice</code>和<code>String</code>类型，方便做<code>string</code>和<code>slice</code>的转换。</p><pre><code>type Slice struct {    Data Pointer    Len int    Cap int}type String struct {    Data Pointer    Len int}func makestring(p *byte, n int) string {    // Direct conversion of unsafe.String to string.    return string(unsafe.String{unsafe.Pointer(p), n})}func memslice(p *byte, n int) (res []byte) {    // Direct conversion of *[]byte to *unsafe.Slice, without using unsafe.Pointer.    s := (*unsafe.Slice)(&amp;res)    s.Data = unsafe.Pointer(p)    s.Len = n    s.Cap = n    return}</code></pre><p>推荐使用<code>gin</code>的<a href="https://github.com/gin-gonic/gin/blob/ee4de846a894e9049321e809d69f4343f62d2862/internal/bytesconv/bytesconv.go#L12">转换方式</a>，简介明了：</p><pre><code>func StringToBytes(s string) []byte {    return *(*[]byte)(unsafe.Pointer(        &amp;struct {            string            Cap int        }{s, len(s)},    ))}</code></pre><h1 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h1><p><code>unsafe.Pointer</code>和<code>uintptr</code> 坑挺多的，使用的时候一定要注意。</p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Golang </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Go 自定义引用包的域名</title>
      <link href="/2022/09/03/go-custom-import-domain/"/>
      <url>/2022/09/03/go-custom-import-domain/</url>
      
        <content type="html"><![CDATA[<h1 id="一、-背景"><a href="#一、-背景" class="headerlink" title="一、 背景"></a>一、 背景</h1><p>最近在看 <code>Go</code>源码的时候，发下部分库最早是在 <a href="https://go.dev/doc/faq#x_in_std">x-pkg</a> 里面的，经过一段时间迭代才进了<code>runtime</code>包里面。</p><p> <a href="https://github.com/golang/go/wiki/X-Repositories">x-pkg</a> 里面介绍了用途和源码地址。</p><p> <a href="https://pkg.go.dev/golang.org/x/sync@v0.0.0-20220819030929-7fc1605a5dde#section-readme">golang.org/x 文档</a></p><p>我发现 <a href="https://github.com/golang/go/wiki/X-Repositories">x-pkg</a> 的源码地址都在 <a href="https://go.googlesource.com/">https://go.googlesource.com</a>， 但是我们项目里面导入某个<code>x-pkg</code>库的路径确是</p><pre><code>import "golang.org/x/sync/semaphore"</code></pre><p>比较好奇，这<code>import</code>的别名是在哪里做的，感觉是个挺冷门的知识，于是搜了下相关资料。</p><h1 id="二、实现步骤"><a href="#二、实现步骤" class="headerlink" title="二、实现步骤"></a>二、实现步骤</h1><p>找到了官网相关资料： <a href="https://pkg.go.dev/cmd/go#hdr-Remote_import_paths">hdr-Remote_import_paths</a></p><p>简单说就是在你的网址里面加入如下信息。</p><pre><code>&lt;meta name="go-import" content="example.org git https://code.org/r/p/exproj"&gt;</code></pre><p><code>go get</code>的时候就知道真正的<code>git</code> 地址是在 <code>https://code.org/r/p/exproj</code>，然后去这个地址去拉取。</p><p>我<code>curl</code>了<code>https://golang.org/x/sync</code> 看了下返回内容如下：</p><pre><code>➜  Desktop curl --location --request GET 'https://golang.org/x/sync'&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv="Content-Type" content="text/html; charset=utf-8"/&gt;&lt;meta name="go-import" content="golang.org/x/sync git https://go.googlesource.com/sync"&gt;&lt;meta name="go-source" content="golang.org/x/sync https://github.com/golang/sync/ https://github.com/golang/sync/tree/master{/dir} https://github.com/golang/sync/blob/master{/dir}/{file}#L{line}"&gt;&lt;meta http-equiv="refresh" content="0; url=https://pkg.go.dev/golang.org/x/sync"&gt;&lt;/head&gt;&lt;body&gt;&lt;a href="https://pkg.go.dev/golang.org/x/sync"&gt;Redirecting to documentation...&lt;/a&gt;&lt;/body&gt;&lt;/html&gt;</code></pre><h2 id="demo-验证"><a href="#demo-验证" class="headerlink" title="demo 验证"></a>demo 验证</h2><ol><li><p><code>github</code>创建一个 <a href="https://github.com/fanlv/gopkg">Go项目</a> ,然后调用<code>go mod init fanlv.wiki/gopkg</code>。然后随便添加一个<code>Go Func</code>，方便拉取以后调用就可以了。</p></li><li><p>在<code>fanlv.wiki/gopkg</code>发布一个静态页面，页面内容如下：</p><pre><code> &lt;!DOCTYPE html&gt; &lt;html&gt;  &lt;head&gt;     &lt;meta charset="UTF-8" /&gt;     &lt;meta name="viewport" content="width=device-width" /&gt;     &lt;meta name="go-import" content="fanlv.wiki/gopkg git https://github.com/fanlv/gopkg"&gt;     &lt;meta name="go-source"         content="fanlv.wiki/gopkg https://github.com/fanlv/gopkg https://github.com/fanlv/gopkg/tree/master{/dir} https://github.com/fanlv/gopkg/blob/master{/dir}/{file}#L{line}"&gt;     &lt;title&gt;fanlv/gopkg&lt;/title&gt; &lt;/head&gt;  &lt;body&gt;     &lt;a href="https://github.com/fanlv/gopkg"&gt; https://github.com/fanlv/gopkg&lt;/a&gt; &lt;/body&gt;  &lt;/html&gt;</code></pre></li><li><p>在本地新建一个项目，然后<code>go get fanlv.wiki/gopkg</code> 尝试下正常。</p></li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f49bba6e4fe99ced.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h1><p>挺简单的一个东西。没啥好说的。</p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Golang </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《人月神话》</title>
      <link href="/2022/08/13/man-month/"/>
      <url>/2022/08/13/man-month/</url>
      
        <content type="html"><![CDATA[<p>什么是人月神话？</p><p><strong>一个项目有10人/月的工作量，把人增加到100人，所以只用3天就能开发完，这就是人月神话。</strong></p><p><strong>一个谬误的思考方式是在估计和进度安排中使用的工作量单位：人月</strong>。成本的确随开发产品的人数和时间的不同，有着很大的变化，进度却不是如此。因此我认为用人月作为衡量一项工作的规模是一个危险和带有欺骗性的神话。它暗示着人员数量和时间是可以相互替换的。</p><h1 id="一、焦油坑（The-Tar-Pit）"><a href="#一、焦油坑（The-Tar-Pit）" class="headerlink" title="一、焦油坑（The Tar Pit）"></a>一、焦油坑（The Tar Pit）</h1><p>史前史中，没有别的场景比巨兽在焦油坑中垂死挣扎的场面更令人震撼。上帝见证着恐龙、猛犸象、剑齿虎在焦油中挣扎。它们挣扎得越是猛烈，焦油纠缠得越紧，没有任何猛兽足够强壮或具有足够的技巧，能够挣脱束缚，它们最后都沉到了坑底。</p><p><strong>过去几十年的大型系统开发就犹如这样一个焦油坑，</strong>很多大型和强壮的动物在其中剧烈地挣扎。他们中大多数开发出了可运行的系统–不过，其中只有非常少数的项目满足了目标、时间进度和预算的要求。各种团队，大型的和小型的，庞杂的和精干的，一个接一个淹没在了焦油坑中。</p><p><strong>编程系统产品（Program System Product）</strong></p><p>演化进程：程序-&gt;编程系统（接口、系统集成）-&gt;编程系统产品；程序-&gt;编程产品（通用化、测试、文档、维护）-&gt;编程系统产品。这两条线都节约了成本。</p><p><strong>编程的乐趣</strong></p><ul><li>首先是一种创建事物的纯粹快乐。如同小孩在玩泥巴时感到愉快一样，成年人喜欢创建事物，特别是自己进行设计。我想这种快乐是上帝创造世界的折射，一种呈现在每片独特、崭新的树叶和雪花上的喜悦1。</li><li>其次，快乐来自于开发对其他人有用的东西。内心深处，我们期望其他人使用我们的劳动成果，并能对他们有所帮助。从这个方面，这同小孩用粘土为”爸爸办公室”捏制铅笔盒没有本质的区别。</li><li>第三是整个过程体现出魔术般的力量–将相互啮合的零部件组装在一起，看到它们精妙地运行，得到预先所希望的结果。比起弹珠游戏或点唱机所具有的迷人魅力，程序化的计算机毫不逊色。</li><li>第四是学习的乐趣，来自于这项工作的非重复特性。人们所面临的问题，在某个或其它方面总有些不同。因而解决问题的人可以从中学习新的事物：有时是实践上的，有时是理论上的，或者兼而有之。</li><li>最后，乐趣还来自于工作在如此易于驾驭的介质上。程序员，就像诗人一样，几乎仅仅工作在单纯的思考中。程序员凭空地运用自己的想象，来建造自己的”城堡”。很少有这样的介质–创造的方式如此得灵活，如此得易于精炼和重建，如此得容易实现概念上的设想。（不过我们将会看到，容易驾驭的特性也有它自己的问题）然而程序毕竟同诗歌不同，它是实实在在的东西；可以移动和运行，能独立产生可见的输出；能打印结果，绘制图形，发出声音，移动支架。神话和传说中的魔术在我们的时代已变成了现实。在键盘上键入正确的咒语，屏幕会活动、变幻，显示出前所未有的或是已经存在的事物。</li></ul><p><strong>编程的苦恼</strong></p><ul><li>首先，必须追求完美。因为计算机也是以这样的方式来变戏法：如果咒语中的一个字符、一个停顿，没有与正确的形式一致，魔术就不会出现。（现实中，很少的人类活动要求完美，所以人类对它本来就不习惯。）实际上，我认为学习编程的最困难部分，是将做事的方式往追求完美的方向调整。</li><li>其次，是由他人来设定目标，供给资源，提供信息。编程人员很少能控制工作环境和工作目标。用管理的术语来说，个人的权威和他所承担的责任是不相配的。不过，似乎在所有的领域中，对要完成的工作，很少能提供与责任相一致的正式权威。而现实情况中，实际（相对于正式）的权威来自于每次任务的完成。</li><li>概念性设计是有趣的，但寻找琐碎的bug却只是一项重复性的活动。</li><li>人们发现调试和查错往往是线性收敛的，或者更糟糕的是，具有二次方的复杂度。结果，测试一拖再拖，寻找最后一个错误比第一个错误将花费更多的时间。</li><li>当投入了大量辛苦的劳动，产品在即将完成或者终于完成的时候，却已显得陈旧过时。可能是同事和竞争对手已在追逐新的、更好的构思；也许替代方案不仅仅是在构思，而且已经在安排了。</li></ul><h1 id="二、人月神话（The-Mythical-Man-Month）"><a href="#二、人月神话（The-Mythical-Man-Month）" class="headerlink" title="二、人月神话（The Mythical Man-Month）"></a>二、人月神话（The Mythical Man-Month）</h1><p><strong>项目滞后的主要原因是缺乏进度的合理安排：</strong></p><ul><li>对估算技术缺乏有效研究。乐观主义，通常假设一切都将运作良好；</li><li>混淆工作量和进度，以为人月可以互换；</li><li>对自己的估算缺乏信心，项目经理不会耐心持续的进行估算；</li><li>缺少进度跟踪和监督；</li><li>进度偏移时，第一反应是协调人力。</li></ul><ol><li>乐观主义，乐观主义是程序员的职业病。</li><li>人月，人员之间的需要沟通交流，新派人手可能成为负担；而且任务还有先后顺序。</li><li>系统测试，过程时间占比经验：<ul><li>计划（1/3）：包含技术调研、概念设计（功能规格）</li><li>编码（1/6）：最容易估计时间的部分</li><li>构件测试和早起系统测试（1/4）：单元测试</li><li>系统测试（1/4）：集成测试</li></ul></li><li>空泛的估算，在外部的压力下，项目经理在估算时容易屈服。开发并推行生产率图表、缺陷率图表、估算规则等工具可以使得项目估计更有依据，相比经验也更能有说服力。</li><li>重复产生的进度灾难，进度延迟后考虑的做法：<ul><li>重新安排进度：注意在开发过程中及时重新估计和调整进度（不一定是延后，而是根据实际情况，部分任务缩减，部分任务延长），任务的粒度也需要注意，不需要太细，也不能太粗。</li><li>削减任务：讨论功能的重要性和优先级，考虑分版本发布，先完成重要、紧急的功能。</li></ul></li></ol><p><strong>Brooks法则</strong>：想落后的项目中增加人手，只会使得进度更加落后。</p><h1 id="三、外科手术队伍（The-Surgical-Team）"><a href="#三、外科手术队伍（The-Surgical-Team）" class="headerlink" title="三、外科手术队伍（The Surgical Team）"></a>三、外科手术队伍（The Surgical Team）</h1><ul><li>优秀程序员的生产率是一般程序员的10倍</li><li>小型精干的团队是最好的</li><li>对于大型的系统，小型精干的团队速度太慢了</li><li>类似外科手术的团队能很好解决大型系统开发的问题</li></ul><ol><li>问题，小型精干队伍的人数一般在10人以内。但是对于大型系统，比如需要1000人年，完成项目的进度显得太长对于大型系统，需要划分模块来完成。而且概念设计部分需要由少而精的人员完成。</li><li>Mills的建议（外科手术队伍）：<ul><li>外科医生（架构师）：定义功能、性能技术说明书，设计程序，编写源代码，测试，写技术文档；</li><li>副手（开发组长）：能完成任何一部分工作，但经验较少。设计的思考者、讨论者、评估人员；</li><li>管理员（项目经理）：管理财务、人员、办公设备等，可以一个人管两个项目组；</li><li>编辑：组织文档；</li><li>两个文秘：管理员和编辑各一个（偶也想要啊）</li><li>程序职员：维护所有团队的技术记录</li></ul></li><li>如何运作，概念来自外科医生和副手，保证了完整性并节约了沟通成本，外科医生在金字塔顶端，可以统一分歧，其余人员分工专业化，更为高效</li><li>团队的扩建，大型系统需要一个系统结构师自上而下的进行所有设计，也有利于协调。概念设计团队的人也要少而精。</li></ol><h1 id="四、贵族专制、民主政治和系统设计（Aristocracy-Democracy-and-System-Design）"><a href="#四、贵族专制、民主政治和系统设计（Aristocracy-Democracy-and-System-Design）" class="headerlink" title="四、贵族专制、民主政治和系统设计（Aristocracy, Democracy, and System Design）"></a>四、贵族专制、民主政治和系统设计（Aristocracy, Democracy, and System Design）</h1><ul><li>概念完整性是系统设计中最重要的因素</li><li>为了保证概念完整性，设计必须由一个人或一个小型的团队</li><li>软件开发要实现设计和具体实现的分离</li><li>限制和规则能激发创造性</li><li>概念统一的系统能更快的开发和测试</li></ul><p><strong>概念完整性</strong></p><p>概念完整性应该是系统设计最重要的考虑因素。保证概念完整性最好的方法是由一个人来完成设计，对于后来的设计者，则要求他们牺牲一些创意，以获得纯粹的设计。</p><p><strong>获得概念完整性</strong></p><p>系统设计的最终测试标准：易用性 = 功能 / 理解复杂度。</p><p>软件不仅要功能越多越好，而且需要简洁、直接。简洁和直白来自概念完整性。</p><p>设计的一致性是易用性的另一重要因素。设计一致性要求：每个部分必须反映相同的原理需求的一致平衡（语法上使用的技巧相同，语义上也要具有相似性）。</p><p><strong>贵族专制和民主政治</strong></p><p>概念的完整性要求设计必须由一个人，或者少数互有默契的人员来实现。但进度压力要求系统要由很多人来开发。解决这一矛盾的方法有两个：仔细对设计方法/体系结构与具体实现进行分工；第三章所述的团队的组建方式。</p><p>系统的体系结构（achitecture）指的是完整和详细的用户接口说明。系统的结构师（架构师）需要运用专业技术知识来支持最终用户的利益。</p><p>体系结构师（架构师）的贵族专制统治：体系结构师代表最终用户的利益；其产物的生命周期比实现人员更长；为了概念完整性，系统设计必须由少数人（架构师）控制。</p><p>具体设计实现在创造性方面并不亚于体系结构设计工作：具体实现更大程度决定了产品的成本和性能；而结构设计偏向于易用性。但是具体实现需要依赖于结构设计。</p><p><strong>在等待时，实现人员应该做什么</strong></p><p>软件开发的三个阶段（Blaauw）：体系结构（achitecture）、设计实现（implementation）、物理实现（realization）。他们彼此存在依赖，但往往可以同时开始和并发进行。</p><p>上述的划分是水平的划分，实际中再进行垂直的（功能）划分并不会在概念完整性等方面造成不利影响。上述的三个阶段确实存在一定的向上依赖，但是垂直的划分使得工作可以并行进行；另外各个阶段中都有部分工作是不存在向上依赖的。</p><h1 id="五、画蛇添足（The-Second-System-Effect）"><a href="#五、画蛇添足（The-Second-System-Effect）" class="headerlink" title="五、画蛇添足（The Second-System Effect）"></a>五、画蛇添足（The Second-System Effect）</h1><ul><li><p>不要过分设计</p><p> 结构师的主要职责在于只能功能规格说明，他没有产品开发时间、成本方面的责任。理论上结构师的创造性可以是无限的，那么如何来约束这种热情呢？</p></li></ul><p><strong>结构师的交互准则和机制</strong></p><p>今早的交流和持续沟通：使得结构师有成本意识，也让开发人员对设计有信心，不会混淆责任分工。</p><p>产生估算过高时，结构师有两个选择：削减设计；建议实现人员使用成本更低的实现方法。</p><p><strong>交互准则：</strong></p><ul><li>牢记开发人员承担实现的责任，所以只能建议，不能支配；</li><li>时刻准备为所指定的功能建议一种实现方法，同样准备接受其他任何能够达到目标的方法；</li><li>对上述的建议保持低调和不公开；</li><li>准备放弃坚持所做的改进建议；</li><li>在体系结构上的修改往往会引起开发人员的反对——实现工作展开时，一些次要特性的修改会造成意想不到的成本开销。</li></ul><p><strong>自律——开发第二个系统所带来的后果</strong></p><p>结构师设计的第二个系统往往是最危险的。</p><p>由于对任务不够了解而产生的谨慎，结构师的第一个系统倾向于精炼和简洁；而第二个系统往往会被过分设计（太多的修饰功能、想法）；后面的项目会在之前的经验上形成对比总结，所以在简洁性和功能方面，比较容易达到平衡。避免过分设计的方法在于：自律和程序经理的监督。</p><h1 id="六、贯彻执行（Passing-the-Word）"><a href="#六、贯彻执行（Passing-the-Word）" class="headerlink" title="六、贯彻执行（Passing the Word）"></a>六、贯彻执行（Passing the Word）</h1><ul><li>设计必须由一个或两个人负责，确保设计的一致性</li><li>具体精确的定义系统和功能</li></ul><p><strong>文档化的规格说明——手册</strong></p><p>手册需要描述和规定用户所见的每一个细节，同时需要避免描述用户不可见的部分。结构师必须为自己描述的任何特性准备一种实现方法，但不应试图支配实现过程。</p><p>手册（规格说明）的风格必须清晰、完整和准确。一致性很重要。</p><p><strong>形式化定义</strong></p><p>形式化（术语化、公式化）定义是精确的，倾向完整；但是不易理解，需要记叙性文字的辅助。注意在同一个手册中需要在谁主谁辅上保持一致。</p><p>实现可以作为一种形式化定义方法（如）。实现作为形式化定义的优点：所有问题可以通过试验清晰的得到答案，快捷迅速、无需争辩商讨；缺点：实现作为定义时，体现了过多的内容，即不但描述了系统必须做什么，同时还声明了自己做了些什么（多余的要求）。</p><p><strong>直接整合</strong></p><p>设计好模块间接口的原型，要求在实现过程中按照原型开发。</p><p><strong>会议和大会</strong></p><p>周例会：每周半天，由首席结构师主持，所有结构师参加、实现人员代表和市场计划人员（代表用户）参与。参与人员都可以提出问题和修改意见，但需要提前准备并分发书面建议书。讨论通过的建议/问题解决方案由相关结构师进行修改，并纳入变更建议说明书中。问题结论由首席架构师发布。</p><p>周例会优点：相同人员数月内每周交流一次，对项目内容都比较了解，无需额外培训；参与人员与产品密切相关，深刻理解所面对的问题，每个人都承担义务；出现问题时，在界线内部和外部同时寻求解决方案；正式的书面建议集中了注意力，强制了决策的制定和执行；明确授予首席结构师决策权力，避免妥协拖延。</p><p>年度（或半年）大会，在功能规格制定前进行。出了周例会成员，程序经理、实现人员、市场人员都会参与，由项目经理主持。集中解决遗留的各种问题，耗时两周，每天需要通知前一天的各项决定。</p><p>年度大会优点：解决了部分决策上的问题，同时使决策更容易被接受和理解。</p><p><strong>多重实现</strong></p><p>多重实现时，根据机器修改手册的消耗可能比根据手册修改机器要高。</p><p><strong>电话日志</strong></p><p>实现人员对规格说明容易理解不当或规格说明本身描述就不够精确，所以结构师鼓励实现人员电话沟通。在此条件下，结构师需要保存电话日志，记录每个问题和相应的解答，并每周进行合并整理，分发给用户和实现人员。</p><p><strong>产品测试</strong></p><p>测试组的工作是根据规格说明检查程序，需要由独立的技术监督部门完成，以保证公正性。</p><h1 id="七、为什么巴比伦塔会失败？（Why-Did-theTower-of-Babel-Fail-）"><a href="#七、为什么巴比伦塔会失败？（Why-Did-theTower-of-Babel-Fail-）" class="headerlink" title="七、为什么巴比伦塔会失败？（Why Did theTower of Babel Fail?）"></a>七、为什么巴比伦塔会失败？（Why Did theTower of Babel Fail?）</h1><ul><li>失败的原因：交流</li><li>团队应该尽可能多的交流</li><li>制定良好的工作手册</li><li>每一个团队成员都应该能看到所有材料</li><li>每个团队两个领导，技术负责人和产品负责人</li></ul><p>　巴比伦塔（圣经中的典故）：开始，人们使用的是同一种语言，他们建造房屋，并想着建造一个有高塔的城市，所有人都可以聚集在高塔里。上帝听说后，认为有必要让人类经历些挫折，于是他在人类的语言中制造了混淆，相互无法听懂。这样人类就不得不停止制造巴比伦塔。</p><p><strong>巴比伦塔的管理教训</strong></p><p>人类有清晰的目标、人力充足、材料齐全、时间充裕、技术完备。但失败的原因在于：缺乏交流以及交流的结果（组织）。</p><p><strong>大型编程项目中的交流</strong></p><p>团队之间的交流沟通需要利用所有可能的途径：</p><ul><li>非正式途径：电话、口头、即时通讯、邮件等；</li><li>会议：各团队逐一进行简要的技术陈述。很容易澄清细小的误解；</li><li>工作手册：在项目开始阶段即应该准备</li></ul><p><strong>项目工作手册</strong></p><p>定义：对项目必须产出的一系列文档进行组织的一种结构，而非独立的一篇文档。即定义各类文档存放在什么位置。</p><p>备忘录的管理：编号、整理，便于检索。</p><p>当编程人员仅了解自己负责的部分，而非整个系统的开发细节时，工作效率最高。此方法的先决条件在于精确完整的定义所有接口。</p><p><strong>大型变成项目的组织架构</strong></p><p>团队组织的目的：减少所需的交流和合作的数量。</p><p>减少交流的方法：人力划分和限定职责范围。</p><p>树状组织架构是作为权力和责任的结构出现，不适合交流和沟通（网状）。</p><p>树状编程队伍中，每颗子树必备的基本要素：任务（mission）；产品负责人（producer）；技术主管或结构师（techenical director or architecture）；进度（schedule）；人力划分（division of labor）；各部分间的接口定义（interface definitions among the parts）。</p><p>产品负责人：组建团队、划分工作、制定进度表。争取和保证必要的资源（外部向上或水平沟通）；建立内部沟通/报告方式；确保进度目标的实现，根据环境变化调整资源和团队架构。</p><p>技术主管：对设计进行构思，识别系统的字部分，设计内部结构和外部功能；提供整个设计的一致性和概念完整性；控制系统的复杂程度。提供复杂问题的解决方案，或根据需要调整系统设计。</p><p>建议技术主管为主，产品负责人为辅的组织架构；产品负责人为主时需要预先声明技术主管在技术决策方面的权威（亦可通过办公条件等方面暗示）。</p><h1 id="八、胸有成竹（Calling-the-Shot）"><a href="#八、胸有成竹（Calling-the-Shot）" class="headerlink" title="八、胸有成竹（Calling the Shot）"></a>八、胸有成竹（Calling the Shot）</h1><ul><li>仅仅靠编码时间乘以系数是无法得到完成时间的</li><li>小项目的数据不适用于大项目，开发时间随项目大小呈指数增长</li><li>使用高级语言的效率是使用汇编语言5倍</li></ul><p>系统编程的时间估计，工作量估计方法：比率估算法（不能仅用编码阶段估计）；小型程序数据不适合编程系统产品。</p><p>他发现他的编程队伍落后进度大约1/2，每项工作花费的时间大约是估计的两倍。这些估计通常是非常仔细的，由很多富有经验的团队完成。他们对PERT图上数百个子任务估算过（用人小时作单位）。当偏移出现时，他要求他们仔细地保存所使用时间的日志。日志显示事实上他的团队仅用了百分之五十的工作周，来进行实际的编程和调试，估算上的失误完全可以由该情况来解释。其余的时间包括机器的当机时间、高优先级的无关琐碎工作、会议、文字工作、公司业务、疾病、事假等等。简言之，项目估算对每个人年的技术工作时间数量做出了不现实的假设。我个人的经验也在相当程度上证实了他的结论6。</p><h1 id="九、削足适履（Ten-Pounds-in-a-Five-Pound-Sack）"><a href="#九、削足适履（Ten-Pounds-in-a-Five-Pound-Sack）" class="headerlink" title="九、削足适履（Ten Pounds in a Five-Pound Sack）"></a>九、削足适履（Ten Pounds in a Five-Pound Sack）</h1><ul><li>仅仅靠编码时间乘以系数是无法得到完成时间的</li><li>小项目的数据不适用于大项目，开发时间随项目大小呈指数增长</li><li>使用高级语言的效率是使用汇编语言5倍</li></ul><h1 id="十、提纲挈领（The-Documentary-Hypothesis）"><a href="#十、提纲挈领（The-Documentary-Hypothesis）" class="headerlink" title="十、提纲挈领（The Documentary Hypothesis）"></a>十、提纲挈领（The Documentary Hypothesis）</h1><p><strong>在一片文件的汪洋中，少数文档形成了关键的枢纽，每件项目管理的工作都围绕着它们运转。它们是经理们的主要个人工具。</strong></p><ul><li>文档的关键，目标，用户手册，内部文档，进度，预算，组织结构图和工作空间分配</li><li>为每个关键文档提供状态监督和语境机制</li><li>项目经理的职责是使每个人都朝着同一个方向前进</li><li>项目经理的日常工作是沟通而不是做决定</li></ul><p>前提：在堆积如山的文档中，少数是关键枢纽，每一件项目管理工作都围绕着他们运转。这些文档是项目经理最重要的个人工具。</p><p>技术、周边组织机构、行业传统等因素定义了项目必须准备的一些文书工作。这些文书工作看起来令人厌烦，但是他们往往包含了一些管理方面的内容：每份文档的准备工作是集中考虑，并使各种讨论意见明朗化的主要时刻；文档有利于明确工作阶段；文档的跟踪维护是项目监督和预警机制；文档本身可以作为检查列表、状态控制，也可以作为汇报的数据基础。</p><p>任何管理任务的关注焦点都是：时间、地点、人员、项目内容、资金。</p><ol><li>软件项目文档</li></ol><p>内容：目标。定义待完成的目标、迫切需要的资源、约束和优先级；</p><p>内容：产品技术说明。以建议书开始，用户手册和内部文档结束。性能说明是关键部分；</p><p>时间：进度；</p><p>资金：预算；</p><p>地点：工作空间分配；</p><p>人员：组织图。人员与接口说明相互依存。组织结构需要根据系统设计自由变化。</p><ol start="2"><li>为什么要有正式的文档</li></ol><p>书面记录决策是必要的。记录文档可以作为同其他人沟通的渠道。项目经理的基本职责是使每个人都想着相同的方向前进，其主要工作是沟通，非决定。</p><p>文档可以作为数据基础和检查列表。通过文档能够搞清项目所处状态，需要做的更改和调整。</p><p>文档非常重要，但是不存在“完全信息管理系统”。因为管理人员80%的时间都用在沟通上：倾听、报告、讲授、规劝、讨论和鼓励。</p><p>项目经理的任务是制定并实现计划，只有书面计划是精确和可沟通的。</p><h1 id="十一、未雨绸缪（Plan-to-Throw-One-Away）"><a href="#十一、未雨绸缪（Plan-to-Throw-One-Away）" class="headerlink" title="十一、未雨绸缪（Plan to Throw One Away）"></a>十一、未雨绸缪（Plan to Throw One Away）</h1><p><strong>不变只是愿望，变化才是永恒。</strong></p><ul><li>开发人员交付的是用户满意度而不是实际的产品</li><li>用户的实际需要会不断变化</li><li>前进两步，后退一步，维护会增加系统的复杂性和引入新的Bug。</li></ul><ol><li>试验性工厂和增大规模</li></ol><p>　　　　必须为舍弃而计划。第一个版本必然会存在很多问题，所以做好把其当做一个试验品的准备。要注意试验品是否应当发布给用户。</p><ol start="2"><li>唯一不变的是变化本身</li></ol><p>　　　　接受变化，但并非客户所有的需求变更都应该整合到设计中。</p><ol start="3"><li><p>为变更计划系统</p><p> 技术手段：设计细致的模块化、可扩展的函数、精确完整的模块间接口设计和完备的文档。</p><p> 变更的阶段化：版本号、日程表、冻结日期、变更内容。</p></li><li><p>为变更计划组织架构</p><p> 选择高效的技术人员，解决各种技术问题；对技术人员进行培训；废除职位头衔，让成员能够接受职务变更。</p></li><li><p>前进两步，后退一步</p><p> 缺陷修复总会以20%-50%的概率引入新的Bug；</p><p> 回归测试的成本很高；</p><p> 选择程序设计方法时，需要考虑其副作用，并指明。</p></li><li><p>前进一步，后退一步</p><p> 在系统维护过程中，更多关注的是功能实现，系统原有设计日益被忽略，每次的修改都可能导致潜在的新问题，修改成本日益增加。必要时，需要考虑代码重构或重新设计。</p></li></ol><h1 id="十二、干将莫邪（Sharp-Tools）"><a href="#十二、干将莫邪（Sharp-Tools）" class="headerlink" title="十二、干将莫邪（Sharp Tools）"></a>十二、干将莫邪（Sharp Tools）</h1><p><strong>巧匠因为他的工具而出名。</strong></p><ul><li>项目成员要使用通用开发工具</li></ul><p>工具要通用化，个性化的工具妨碍沟通。建议每个团队一个工具管理人员，工具小组方式则相对效率更低。</p><p>考虑使用高级语言、成熟框架，性能测试工具，调试工具，测试用例生成工具等。</p><h1 id="十三、整体部分（The-Whole-and-the-Parts）"><a href="#十三、整体部分（The-Whole-and-the-Parts）" class="headerlink" title="十三、整体部分（The Whole and the Parts）"></a>十三、整体部分（The Whole and the Parts）</h1><ul><li>有时必须推翻顶层，重新开始</li></ul><p><strong>剔除Bug的设计</strong></p><p>系统各个部分的开发者都会做出假设，这些假设的不匹配会导致致命或难以察觉的Bug。细致的功能定义、仔细的规格说明、规范化的功能描述说明很有必要。</p><p>测试规格说明：完整性和明确性</p><p>自上而下的设计，横向划分为：体系结构设计、设计实现、物理实现；纵向以模块划分，且模块从大到小不断细化。优点：</p><ul><li>清晰的结构和表达方式更容易对需求和模块功能进行精确的描述；</li><li>模块分割和模块独立性避免了系统级Bug；</li><li>细节的抑制使得结构上的缺陷容易暴露出来；</li><li>设计在每个精化步骤上都是可测试的。</li><li>遇到极端问题时，可能需要考虑逆转过程，直至推翻顶层设计，重新开始。</li></ul><p><strong>单元调试</strong></p><p>程序调试过程在过去的二十年中有过很多反复，甚至在某些方面，它们又回到了出发的起点。整个调试过程有四个步骤，跟随这个过程来检验每个步骤各自的动机是一件很有趣的事情。　　</p><p><strong>系统集成调试</strong></p><p>需要在各个部分都能够正常运行之后开始。这个阶段耗费的时间往往比预计的要长。</p><p>使用伪构件（dummy component，包含接口、可能的伪数据和一些小的测试用例）。</p><p>控制变更。控制构件单元版本。</p><p>一次添加一个构件。能够有利于问题定位。</p><p>阶段化变更。定期进行大范围的构件更新等，周期不宜太短。</p><h1 id="十四、祸起萧墙（Hatching-a-Catastrophe）"><a href="#十四、祸起萧墙（Hatching-a-Catastrophe）" class="headerlink" title="十四、祸起萧墙（Hatching a Catastrophe）"></a>十四、祸起萧墙（Hatching a Catastrophe）</h1><p><strong>带来坏消息的人不受欢迎。</strong></p><ul><li>项目进度的落后是潜移默化的，因此必须设置阶段目标</li></ul><p><strong>里程碑还是沉重的负担</strong></p><p>里程碑澄清了划分的比较模糊的阶段，定义了确定时间点需要的完整的产物。里程碑的边界需要是明显的，无歧义的。里程碑需要能够正确的反应损失的时间，避免慢性进度偏离影响士气。</p><p>建议活动之前开始估计，每两周对计划进行一次仔细的修订。</p><p><strong>关键路径法（Pert图）</strong></p><p>那些任务的滞后会影响最终的完成日期。非关键任务允许一定的落后时间。</p><p>相对对每个任务估计完成时间，关键路径法更能够鼓励超前完成，也指明了补偿其他任务滞后时间的方法。</p><p><strong>地毯的下面</strong></p><p>程序经理发现计划偏离时，存在于老板的利益冲突：老板需要行动计划，分析的状态数据；程序经理会认为非致命性的进度偏离是可通过团队内部解决的，老板的行动可能会影响计划或个人威信。冲突（让程序经理公开信息）的解决方法：</p><p>减少角色冲突：老板避免对程序经理能够解决的问题进行干预，绝不在检查状态报告时做安排。</p><p>猛地拉开地毯：评审机制。每周小屏，每月大评。项目经理需要解释延迟原因，并指出应对方案和需要的帮助。</p><h1 id="十五、另外一面（The-other-face）"><a href="#十五、另外一面（The-other-face）" class="headerlink" title="十五、另外一面（The other face）"></a>十五、另外一面（The other face）</h1><ul><li>文档很重要，不论程序员还是用户</li><li>自文档化技术，文档编写要和源代码结合起来，文档以注释的形式存在</li></ul><p>对开发人员光是介绍文档的重要性，并讲解写文档的技巧和要点，还不如做一份给他们看。</p><p><strong>需要什么样的文档</strong></p><p>使用程序的文档内容：</p><ul><li>目的（程序的主要功能是什么，解决了什么问题）；</li><li>环境（机器、硬件配置、操作系统等）；</li><li>输入范围，输出范围；</li><li>实现功能和使用的算法；</li><li>输入-输出格式（必须是确切完整的）；</li><li>操作指令；</li><li>功能选项；</li><li>运行时间；</li><li>精度和校验；</li><li>精度及如何校验精度。</li><li>验证程序的文档内容：</li><li>测试用例。给程序使用者提供信心。</li><li>修改程序文档内容：</li><li>流程图或子系统的结构图；</li><li>对所用算法的完整描述；</li><li>对所有文件规划的解释；</li><li>数据流处理的概要描述，以及每个处理过程中晚餐的操作；</li><li>初始设计中，对已遇见修改的讨论；特性、功能回调以及出口的位置；原作者对可能会修改的地方及可能处理方案的一些意见。</li></ul><p><strong>流程图</strong></p><p>从代码生成流程图非良好实践。</p><p><strong>自文档化的程序</strong></p><p>维护文档和代码的一致性是费力不讨好的事情。建议将注释以文字记录到代码头部，注意格式。维护记录也可记录其中。</p><h1 id="十六、没有银弹——软件工程中的根本和次要问题"><a href="#十六、没有银弹——软件工程中的根本和次要问题" class="headerlink" title="十六、没有银弹——软件工程中的根本和次要问题"></a>十六、没有银弹——软件工程中的根本和次要问题</h1><p><strong>在未来十年内，无论是在技术还是管理方法上，都看不出有任何突破性的进步，能够独自保证在十年内大幅度地提高软件的生产率、可靠性和间接性</strong>。</p><p>软件活动包括：</p><ul><li>根本任务，打造构成抽象软件实体的复杂概念结构</li><li>次要任务，使用编程语言表达这些抽象实体，在空间和时间限制内将它们映射成机器语言。</li></ul><p>作者出于必要任务考虑的四点建议：</p><ul><li>仔细进行市场调研，避免开发已上市的产品；</li><li>获取和制定软件需求时，将快速原型开发作为迭代计划的一部分；</li><li>有机地更新软件，随着系统的运行、使用和测试，逐渐添加越来越多的功能；</li><li>不断挑选和培养杰出的概念设计人员。</li></ul><p><strong>根本困难</strong></p><p>软件开发困难的部分：规格说明、设计和测试这些概念结构，而非对概念进行表达和对实现逼真程度进行验证。</p><p>软件内在特性：复杂度、一致性、可变性和不可见性。</p><p><strong>次要问题上的突破</strong></p><ul><li>高级语言；</li><li>统一的编程环境。</li></ul><p><strong>银弹的希望</strong></p><ul><li>高级语言；</li><li>面向对象编程；</li><li>人工智能；</li><li>专家系统；</li><li>“自动”编程；</li><li>图形化编程；</li><li>程序验证；</li><li>环境和工具。</li></ul><p><strong>针对根本问题颇有前途的方法</strong></p><ul><li>购买而非自行开发</li><li>需求精炼和快速原型</li><li>增量开发——增长而非搭建系统。</li><li>卓越的设计人员</li><li>杰出设计人员的培养方法：</li><li>尽可能早的，有系统的识别顶级的设计人员；</li><li>为设计人员指派一位职业导师，负责他们技术方面的成长，仔细为他们规划职业生涯；</li><li>为每个方面制定和维护一份职业计划，包括设计大师的精挑细选的学习过程，正式的高级教育和短期的课程；</li><li>为成长中的设计人员提供相互交流和激励的机会。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Literature </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《原则：应对变化中的世界秩序》</title>
      <link href="/2022/08/06/principle/"/>
      <url>/2022/08/06/principle/</url>
      
        <content type="html"><![CDATA[<h1 id="一、大周期概述"><a href="#一、大周期概述" class="headerlink" title="一、大周期概述"></a>一、大周期概述</h1><p><strong>人类的生产率是驱动世界总财富、权力和生活水平逐步提高的最重要的力量</strong>。随着时间的推移，生产率稳步提高（生产率是人均产出，受人类的学习、积累和发明的驱动）。然而，不同群体的生产率增速有差异，其原因总是一样的：不同的教育质量、创造力、职业道德以及将想法转化为产出的经济体制。决策者需要理解这些原因，以使自己的国家获得可能实现的最佳结果。投资者和公司也需要理解这些原因，以便确定最佳的长期投资对象。</p><p><strong>下图及下页图显示过去500年的人均产出［即实际GDP（国内生产总值）估值］和预期寿命</strong>。这可能是两种公认的福祉衡量方法（但数据可能并不精确）。你可以看到这些数据渐进的上行幅度相对于围绕趋势线的波动幅度。</p><p>与围绕趋势线的波动相比，这些上行趋势非常明显，这一事实表明，相对于其他任何事物，人类的创造力极其强大。从整体格局来看，人均产出正在稳步提高，不过早期增速非常缓慢，但从19世纪开始加快了速度，那时的上行曲线明显走陡，反映了生产率的加速提升。<strong>生产率提升的主要原因是，大多数人能够通过更好的方式获取知识，并将知识转化为生产率</strong>。这是很多因素促成的，可以追溯到15世纪中期欧洲的谷登堡印刷机（当时印刷术在中国已经使用了几个世纪）。随着谷登堡印刷机的问世，更多人获得了学习和教育的机会，它对一系列重要运动都做出了贡献，例如文艺复兴、科学革命、启蒙运动、资本主义诞生，以及英国的第一次工业革命。我们稍后将会深入探讨这些方面。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e4c0fa57c06b00ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fd3b8397e39d43ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>获取知识和提高生产率固然意义重大，但这个过程是渐进的，不会给财富和权力及其持有者带来巨大的突变。巨大的突变来自经济繁荣、经济萧条、革命和战争，这些时期主要由周期决定，而逻辑因果关系驱动这些周期</strong>。例如，19世纪末的标志是生产率提升、企业家精神和资本主义壮大，而这些因素也导致了贫富悬殊和过度负债，造成经济衰退，使20世纪上半叶出现了反资本主义和共产主义势力，国家内部和外部都发生了严重的财富和权力斗争。我们可以看到大周期围绕进化的上行趋势线向前推进。<strong>● 自古至今，成功的体系一直是受过良好教育的人们文明相处，提出创新想法，通过资本市场获得资金，将创新转化为生产和资源分配，从中获利而得到回报。但是，长期来看，资本主义造成了财富和机会差距，助长过度负债，导致经济衰退，引发了革命和战争，进而改变了国内和世界秩序。</strong></p><p><strong>●拥有大量储蓄、低债务和强大储备货币的国家能更好地抵御经济和信贷崩溃；储蓄少、大量负债，没有强大储备货币的国家抵御能力则较弱。</strong>同样，如果一个国家拥有强有力的、有能力的领导者和国民素质，它就比不具备这些条件的国家更容易管理。如果一个国家更有创造力，它就有更强的适应力。正如稍后将会探讨的，这些因素是可以测量的永恒普适的原则。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-37880316d36561ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="不同类别的全球死亡人数"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0fa2f3e456c2c5f1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="冲突造成的死亡率估值"></p><p><strong>因为相比人类适应力和创造力的逐步上行轨迹，这些动荡时期造成的影响微乎其微，几乎不能从前面的人均GDP和预期寿命图中分辨出来，只体现为一些小波动。不过在我们看来，这些波动的幅度似乎非常显著，因为我们是如此渺小，生命也是如此短暂。</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e807b9025d02f4fd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>历史上的大多数周期是出于大致相同的原因发生的。</strong>例如，1907年恐慌开始的1907—1919年，与1929—1932年货币和信贷危机（继“咆哮的20年代”之后）一样，都是繁荣时期（美国的“镀金时代”、欧洲大陆的“美好时代”和英国的“维多利亚时代”都是同一时期）产生债务融资泡沫而导致经济和市场下跌的结果。在经济和市场下行的同时，贫富悬殊导致了大规模的财富再分配，进而引发了世界大战。为了进行像1930—1945年那样的财富再分配，政府需要大幅增加税收和支出，承担巨额赤字，大幅调整货币政策，将赤字货币化。接下来，西班牙流感加剧了压力测试和由此产生的重组格局。在这些压力测试与全球经济和地缘政治重组的作用下，新的世界秩序（表述在《凡尔赛和约》中）于1919年建立，开启了20世纪20年代债务融资拉动的繁荣时期，接下来是1930—1945年，同样的事情再次发生。</p><p><strong>这些破坏/重建时期重创了弱者，也确定了谁是强者。强者建立全新的规则（即新的世界秩序），为繁荣时期奠定了基础。在繁荣时期，随着债务泡沫滋生，贫富差距扩大，最终超过极限，导致债务泡沫破裂、新的压力测试开始，从而进入破坏/重建时期（即战争），开启新的秩序。最终，强者又比弱者获得更大的权力，等等。</strong></p><p><strong>下图显示过去500年里11个主要国家的相对财富和权力。</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e12a7937c64d3c77.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们仔细研究后会发现，较粗的四条线代表四个最重要的国家：荷兰、英国、美国和中国。三个国家持有了最近的三种储备货币——当前是美元，之前是英镑，再之前是荷兰盾。将中国纳入最主要国家的原因是，中国已成为世界第二大国家，它在大约1850年以前的大部分时间里一直很强大。以下简述上页图反映的情况：</p><ul><li><strong>中国曾经在几百年里一直占据主导地位（在经济和其他领域一直领先于欧洲），不过从19世纪的第一个十年开始急剧衰落；</strong></li><li><strong>荷兰作为一个相对很小的国家，在17世纪的第一个十年成为世界储备货币帝国；</strong></li><li><strong>英国的发展轨迹与荷兰类似，在19世纪的第一个十年达到鼎盛；</strong></li><li><strong>最后，美国崛起，在过去150年里成为世界超级大国，不过其主导地位在二战期间和战后尤为显著；</strong></li><li><strong>当前，随着中国再次崛起，美国的主导地位相对下降。</strong></li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b32f5916aeb566f3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>需要明确的是，尽管这项研究中的主要国家都是最富裕和最强大的国家，但它们不一定是最优越的国家。原因有二。其一，虽然财富和权力是大多数人想要获得的，也最容易引发人们之间的争斗，但一些国家和人民并不认为财富和权力是最重要的，也不会为此而与他人争斗</strong>。一些国家和人民认为享受和平与生活更重要，不会为了获取大量财富和权力而奋力争斗。本项研究不包括这些国家，尽管其中一些国家比那些争夺财富和权力的国家享受更多的和平（顺便说一下，我认为把享受和平与生活置于获取财富和权力之上有很多优势。有趣的是，一个国家的财富和权力与国民的幸福几乎没有关联，这是另一个研究课题）。其二，这项研究并不包含我称之为“小而精的国家”（如瑞士和新加坡），这些国家拥有大量财富，生活水平很高，但是因其规模较小，不足以被归入世界主要国家的行列。</p><p><strong>财富和权力的8个决定因素</strong>：（1）教育，（2）竞争力，（3）创新和技术，（4）经济产出，（5）世界贸易份额，（6）军事实力，（7）金融中心实力，（8）储备货币地位。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8f5fc4b5ca05e6b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>上页图中的曲线清楚地显示出国家兴衰的原因和过程。我们可以看到，提高教育实力可以推进创新和技术，进而扩大世界贸易份额和军事实力，增加经济产出，建设世界领先的金融中心，在一定时期之后，构建起作为储备货币的货币体系。我们还可以看到，在相当长的时间里，其中大部分决定因素保持强劲，之后依据类似的次序跌落。世界储备货币就像世界通用语言一样，往往在国家开始衰落后仍然存在，由于人们已经习惯于使用这种货币，货币存在的时间通常长于国家实力持续的时间。</strong></p><p><strong>我将这种具有周期性和相关性的上行或下行变化称为大周期。</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e15c380f50b974e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>上升阶段：</strong></p><p>上升阶段是新秩序建立之后的繁荣建设时期。在这个阶段，国家的基本面较为强劲，因为（a）债务水平相对较低，（b）财富、价值观和政治差距相对较小，（c）人们通过有效合作来创造繁荣，（d）具备良好的教育和基础设施，（e）拥有强有力且有能力的领导者，（f）一个或多个世界主要大国主导着和平的世界秩序，进而发展到……</p><p><strong>顶部阶段：</strong></p><p>顶部阶段以各种形式的过度状况为特征：（a）高负债，（b）财富、价值观和政治差距巨大，（c）教育水平和基础设施不断下滑，（d）国家内部不同群体之间发生冲突，（e）过度扩张的国家受到新兴对手的挑战，引发国家之间的争斗，这导致……</p><p><strong>下跌阶段：</strong></p><p>这是一个痛苦的阶段，充满各种争斗和结构性重组，从而导致严重冲突和巨大变革，推动内部和外部新秩序的建立，为下一个新秩序和繁荣建设时期埋下了种子。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d0557123d5f4ed6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>综上所述，围绕着生产率增长的上行趋势线（使财富不断增加、生活水平提高），存在着产生繁荣建设时期的周期。在这个时期，国家的基本面较为强劲，因为债务水平相对较低，财富、价值观和政治差距相对较小，人们融洽合作以共同创造繁荣，构建良好的教育体制和基础设施，拥有强有力且有能力的领导者，以及一个或多个世界大国主导着和平的世界秩序。这是令人愉悦的繁荣时期。但是，繁荣发展一旦变得过度（以往情况总是如此），国家就会进入破坏和重组的萧条时期。届时，国家的基本面处于疲弱态势，负债水平处于高位，财富、价值观和政治差距巨大，不同派系之间无法进行有效合作，教育和基础设施落后。在新兴竞争对手的挑战下，国家难以维持过度的扩张，从而陷入充满争斗、毁灭和重组的痛苦阶段，进而推动新秩序的建立，为新的建设时期奠定了基础。</strong></p><p><strong>由于这些周期是按照符合逻辑的次序、依据永恒普适的因果关系展开的，因此，我们可以通过考察所有这些指标，构建一个反映国家当前形势的健康指数。如果该指数表现强劲/良好，这个国家就处于强劲/良好态势，在未来一段时期更可能表现强劲/良好；如果健康指数表现疲弱/不佳，那么该国就处于疲弱/不佳态势，未来一段时期也更有可能表现疲弱/不佳。</strong></p><p>下页表显示这些指标，我把大部分指标配以不同的颜色和深度。深绿色是非常有利的读数，而深红色是非常不利的读数。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5711ea793ff140e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fdaf67598dbf5123.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-85ab874c096d8621.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如前所述，1930—1945年是最后一个主要的破坏和重组时期，此后迎来了建设时期和世界新秩序（始于1945年）、新的全球货币体系（于1944年在美国新罕布什尔州的布雷顿森林建立），以及美国主导的世界治理体系（位于纽约的联合国、位于华盛顿的世界银行和国际货币基金组织）。新的世界秩序是以下因素的自然结果：美国成为世界上最富有的国家（当时美国拥有全球2/3的黄金储备，当时黄金是货币），美国是占据主导地位的经济强国（美国的产出占全球整体水平的大约一半），美国也是最强大的军事大国（当时美国垄断了核武器和最强大的常规武</p><p><strong>主要的旧国家（即主要的储备货币国家）正接近长期债务周期的终点，积累了大量债务，而传统的货币政策已经失效。为了填补财政缺口，政治分裂的中央政府最近发放借来的大量资金，央行设法通过大规模印钞来提供融资（即将政府债务货币化）。同时，财富和价值观存在巨大差距，崛起的世界大国与主要世界大国在贸易、科技发展、资本市场和地缘政治方面展开竞争。雪上加霜的是，在我撰写本书时，我们还需要应对一场大流行病。</strong></p><h1 id="二、决定因素"><a href="#二、决定因素" class="headerlink" title="二、决定因素"></a>二、决定因素</h1><p>为了应对袭面而来的现实情况，我的工作过程是……</p><ul><li>与这台历史机器进行互动，试图了解它的运转原理。</li><li>记录下来我对其运转方式的观察，总结我学到的应对原则。</li><li>检验这些原则的长期适用性。</li><li>将这些原则转化成方程式，编入计算机程序，使其帮助我做出决策。</li><li>从我的经验、教训及反思中继续学习，进一步完善我的原则。</li><li>一直重复这个过程。</li></ul><p><strong>● 自古至今，所有人都有管理人们相处关系的体制或秩序。我把国家内部的体制称为“内部秩序”，把国家之间的体制称为“外部秩序”，把适用于全世界的体制称为“世界秩序”。这些秩序相互影响，而且总在改变。</strong>这样的秩序存在于家庭、公司、城市、州和国家以及世界的各个层面。它们决定谁拥有何种权力，以及如何做出决策，包括如何分配财富和权力。这些秩序的根本性质和运作方式取决于人类的本性、文化和环境。例如，在当前的民主体制内部，美国存在一套现有的政治秩序，但在永恒普适因素的推动下，这些秩序和体制都在不断变化。</p><p><strong>在我看来，任何时刻都存在着（1）包括现有内部和世界秩序在内的一组现有情况，以及（2）导致这些情况发生变化的永恒普适力量。大多数人往往过分关注现有情况，而忽视了改变情况的永恒普适力量。我的做法恰恰相反，因为我需要预测未来的变化。已经发生的一切和将要发生的一切都有使其发生的决定因素。如果我们能理解这些决定因素，就能理解历史机器的运作原理，预测接下来可能会遇到的情况</strong>。</p><p><strong>三大周期：</strong></p><p>我介绍了三大周期、国家及其货币兴衰的8个最重要的决定因素。同时考虑所有这些决定因素及其相互作用，过于复杂。<strong>我们不妨重点关注这三大周期：（1）有利和不利的金融周期（例如资本市场周期）；（2）内部秩序和混乱周期（取决于合作程度与财富和权力斗争，后者主要缘于财富和价值观差距）；（3）外部秩序和混乱周期（取决于现有大国在财富和权力斗争中的竞争力）。</strong>希望读者尝试理解这三个周期，了解各国在周期中所处的阶段。历史和逻辑表明，如果这三个周期同时处于有利阶段，国家就会强大而不断崛起；如果这三个周期都处于不利阶段，国家就会疲弱而走向衰落。</p><p><strong>另外两个决定因素也值得注意，即（4）创新和技术发展步伐，这有助于人类解决问题和做出改进；及（5）天灾，最重要的是干旱、洪水和疾病。通过创新和技术进步，我们可以解决大多数问题，推动进化过程。历史上的天灾（如干旱、洪水、疾病）都产生了巨大的影响。因此，以上是5个最重要的因素，我称之为“五大力量”。当5个决定因素朝着同一个方向出现改善或恶化时，大多数其他因素都会出现相同的走势。</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-09cd96173ae8a0da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>需要关注的决定因素</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-57dbb9468b1e7863.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>在我看来，驱动事件发生的决定因素和影响机制可以分为两类：</strong></p><ol><li><strong>继承的决定因素</strong>：包括一个国家的地理、地质和自然现象，如气候和疾病。</li><li><strong>人力资本的决定因素</strong>：人们对待自己和彼此的方式。这些因素受人类本性和不同文化的影响（影响方式各不相同）。</li></ol><p><strong>继承的决定因素</strong></p><p><strong>所谓继承的决定因素，是指地理、地质、自然现象和系谱。这些因素关系到国家的福祉。它们是决定每个国家和民族发展的重要因素。</strong>例如，要想理解美国的成功，你需要认识到美国与欧洲和亚洲的大国以两大海洋相隔，它拥有一个繁荣昌盛和自给自足的社会所需的大部分矿产、金属和其他自然资源（包括表土、水和温带气候）。因此，美国能够做到粮食自给自足。鉴于这些因素，美国在一个多世纪前主要实行孤立主义政策，同时，由于投资于教育、基础设施和创新，美国变得越发强大。下文简述这些因素。</p><p><strong>1.地理。一个国家的位置、周围环境、地形地貌等都是重要的决定因素</strong>。例如，美国和中国均拥有辽阔无垠的国土，环绕边界的是河流和山脉构成的巨大天然屏障，这样的地理条件使两国都更容易形成一个庞大的整体，拥有更强的国民共性（例如共同的语言、政府、文化等）。相反，欧洲的地理（自然边界较多），对其划分成不同的主权/国家起到强化作用，使欧洲人之间的共性减少（例如不同的语言、政府和文化等）。<br><strong>2.地质。一个国家地表和地下的自然资源至关重要，但相对于人力资本，地质因素不应被高估。</strong>历史告诉我们，每一种大宗商品的价值都在下降（经通胀调整后），围绕这一下降趋势的是巨大的上行和下行周期。这是因为发明创造改变了需求的对象（新能源取代旧能源，光缆取代铜线，等等），而自然资源则在逐步枯竭。许多中东国家的财富和权力及其与其他地区的关联随着石油重要性的上升而增加，可能也随着世界逐渐摆脱化石燃料而下降。最脆弱的定位是高度依赖一种或几种大宗商品，因为大宗商品具有高度的周期性，有时会完全失去价值。<br><strong>3.天灾。天灾存在多种形式，例如流行病、洪水和干旱。纵观历史，天灾对国家的福祉及其演变过程的影响甚至超过战争和经济萧条。</strong>在1350年左右，黑死病夺去了0.75亿~2亿人的生命。在20世纪，天花夺去了3亿多人的生命，这是战争身亡人数的两倍多。干旱和洪水造成了大范围的饥荒和死亡。这些灾难往往突如其来，难以预料，它们充当压力测试，揭示了社会的潜在优势和劣势。<br><strong>4.系谱。关于系谱，我不是遗传学专家，只能说来到这个世界的所有人都有遗传基因，这些基因在某种程度上影响人们的行为。因此，一个国家的人口基因构成应该对其结果产生一定的影响。</strong>话虽如此，但我还应该指出，大多数证据表明，从不同群体之间的行为差异来看，只有一小部分（15%或更少）可以用基因差异来解释。因此，与我提到的其他影响因素相比，遗传基因似乎是一个相对次要的决定因素。</p><p><strong>人力资本的决定因素</strong></p><p><strong>● 虽然一个国家继承的资产和负债非常重要，但历史表明，人们对待自己和他人的方式是最重要的决定因素。</strong>换句话说，人们是否保持较高的行为准则，是否严于律己，是否与他人以礼相待，是否努力成为富有成效的社会成员，这些都是最重要的因素。这些品质加上灵活性和韧性（即适应“不利”和“有利”条件的能力）会使挫折最小化，机会最大化。大多数人的性格、判断力、创造力和思考力可以造就一个富有成效的社会。</p><p>因为资本是可以产生收入的资产，所以人力资本可以被定义为产生收入的个人。<strong>●如果一个人能够使自己的收入超过支出，就拥有良好的人力资本，可以自给自足。</strong>我称之为“自给自足+”，这是所有个人、公司和国家都应该努力做到的，因为这会加强个人和集体的财务实力。如果一个社会注重素质教育、努力工作和相互合作的文化及相关培训等，它就更有可能拥有良好的人力资本，做到“自给自足+”。如果一个社会缺乏良好的人力资本，就会消耗自身的资源，或者陷入无法偿还的债务中（这注定带来麻烦）。</p><p><strong>●虽然许多国家都可以利用自己的自然资源，但人力资本是最可持续的资本，因为继承下来的资产最终会消失，而人力资本可以永远存在。</strong></p><p><strong>最重要的人性决定因素</strong></p><p><strong>5.自身利益。对大多数人、组织和政府来说，自身利益（尤其是自我生存能力）是最强大的推动力。</strong>但最重要的是何种自身利益（是个人、家庭还是国家，等等），这是社会成功的主要决定因素。</p><p><strong>6.获得并维持财富和权力的动力。</strong>对财富和权力的追求是个人、家庭、公司、州和国家拥有的强大动力，尽管这并不完全正确，因为相对于其他事物，不同的个人、家庭、公司、州和国家对财富和权力的重视程度不同。对一些人来说，财富和权力并不像生活中的其他事情那么重要。但对大多数人来说，对财富和权力的追求是他们全身心投入的大事，对那些最富有、最有权力的人来说尤其如此。<strong>从长远来看，一个国家要想取得成功，其收入至少需要与支出相当。一些国家的收入和支出适中且有盈余；另一些国家的收入和支出要多得多，但有赤字。前者更可能取得可持续的成功。历史表明，一个人、组织、国家或帝国的支出一旦超过收入，就离苦难和动荡不远了。请参阅本章附录提供的更多信息。</strong></p><p><strong>7.资本市场。储蓄和通过资本市场获得购买力的能力对一个国家的健康状态至关重要。</strong>因此，资本市场的发达程度是一个国家成功的重要决定因素。</p><p><strong>8.借鉴历史的能力。</strong>大多数人不具备这种能力，这是一种障碍。这在不同的社会各有差异。例如，中国人很擅长借鉴历史。仅从自己的经历中学习是不够的，因为正如上文所解释的，许多最重要的教训在人的一生中不会出现。事实上，未来遇到的许多情况会与以前遇到的情况不同，而非相似。因为周期初期的和平与繁荣时期与周期末期的萧条、革命与战争时期是相反的，人们在生命后期面临的情况与早期遇到的情况不同，而不是相似。更具体地说，在我看来，如果你不了解至少1900年以来的历史及其与当前形势的关联，那么你很可能会遇到麻烦。</p><p><strong>9.多代人心理周期。由于经历不同，每代人都有不同的想法、不同的决策方式，从而影响他们和后代的命运。</strong>有句谚语“富不过三代”就是这个意思。三代人的时间也大致相当于一个典型的长期债务周期。但历史表明，如果这些周期管控得当（例如，许多代都拥有强大的人力资本），财富和权力就可以延续许多代。本章附录描述了这一跨越几代人的周期的几个阶段。</p><p><strong>10.将即时满足置于长远福祉之上。</strong>这是另一个可以辨别个人和社会是否会成功的因素。那些看重长远福祉（而非即时满足）的个人和社会往往处于更好的境况。人类选择即时享受（而非长远福祉）的倾向自然夸大了周期的高峰和低谷，因为这样做是以牺牲未来为代价而提前享受美好时光。这种情况会以许多有害的方式发生，最典型的方式是产生债务繁荣和萧条周期。鉴于政治体制的运作方式，政府尤其容易受到这种影响。更具体地说，（a）政客们有动力优先考虑短期利益，而不是长远利益；（b）他们不希望受到约束，也不愿意面临艰难的财政权衡（比如，是将财政资金用于军事国防还是用于社会项目）；（c）通过征税来获取他人资金的手段，会对他们造成政治威胁，导致一系列政治和其他问题。</p><p><strong>11.人类的创造力。人类最伟大的能力是推动进化，这表现为生产率和生活水平的提高。</strong>与其他物种不同，人类具备吸取知识和思维理解的独特能力。此外，人类的发明创造从实质上改变了生存环境，推进了各项进步。这些进步产生了第1章所述的螺旋式上行走势。我们看看其他物种就可以想象，如果没有这种能力，那么人类会是什么样子。如果人类没有独特的发明创造力，新事物、惊喜和进步就会减少，几代人的生活可能都大致相同，事实上，人类历史上的一些时期就接近这种状态。然而，不同的社会各有差异。要想了解更多信息，请参阅本章的附录。</p><p><strong>文化形成的决定因素</strong></p><p><strong>12.文化。俗话说，“文化就是命运”。文化差异是人们对应该如何对待彼此所持的不同看法，这一差异会产生巨大的影响。每个社会的文化都是基于人们对现实世界的看法而形成的，文化提供指导人们应对现实的原则，最重要的是指导人们如何对待彼此的原则。</strong>文化以正式和非正式的方式推动社会的运转。自古以来，人们在著作中表达自己对人生的态度。这些人包括耶稣、孔子、穆罕默德、佛陀、摩诃毗罗、古鲁那纳克、柏拉图、苏格拉底、马克思等。他们的著作包括《希伯来圣经》、《新约》、《犹太法典》、《古兰经》、“四书五经”、《奥义书》、《薄伽梵歌》、《大梵天经》、《沉思录》、《理想国》、《形而上学》、《国富论》、《资本论》等。这些著作加之各种发现（来自科学家、艺术家、政治家、外交家、投资者、心理学家等），让我们看到人们如何以各自的方式面对和适应现实，这些因素决定了一个民族的文化。</p><p><strong>13.对全球思维的开放态度。这是一个可以很好地反映国家实力的先行指标。因为自我孤立的实体往往会错过时机，不能了解到世界上的最佳做法，从而削弱自身实力，而了解世界上的最佳做法可以帮助人们成为最好的自己。</strong>如果一个国家自我孤立，就不能面对世界最佳竞争者的挑战，也无法从中受益。历史上有很多实行孤立主义政策的案例，一个国家选择闭关锁国的原因各不相同，有时是为了保护自己的文化（例如中国的唐末、明末和日本的江户时代），有时是因为自然灾害和内部争斗等情况。这两个原因都会导致技术落后，造成可怕的恶果。事实上，这是国家和王朝失败最常见的原因之一。</p><p><strong>14.领导力。</strong>到目前为止，我所提到的一切都受到领导人的影响。人生就像一盘国际象棋或中国围棋，每走一步都有可能决定最终结果，一些棋手会比另一些棋手技高一筹。未来，这些走法会日益借助于计算机，但目前它们仍然由人来完成。在阅读历史的过程中，你会反复看到，历史的进程如何被主要领域（如政府、科学、金融和商业、艺术等）的相对少数人的独特性改变（有时带来卓越的成就，有时带来可怕的恶果）。在每一代人中，大约有几百人改变了一切。研究这些关键人物所发挥的作用，考察他们在不同情况下所采取的措施和带来的后果，可以帮助我们理解这台永动机的工作原理。</p><p><strong>个体和群体相互作用形成的决定因素</strong></p><p><strong>15.贫富差距。</strong>贫富差距扩大往往导致冲突加剧，特别是在经济形势恶化、人们为不断缩小的经济蛋糕而争斗的时期。</p><p><strong>16.价值观差距。</strong>虽然财富很重要，但它并不是人们唯一争夺的东西。价值观（例如宗教和意识形态）也很重要。历史告诉我们，如果价值观差距扩大（特别是当经济承压时），那么国家往往步入冲突加剧的时期，而如果价值观差距缩小，那么国家往往进入更为和谐的时期。这些趋势缘于这样一个事实：由于彼此之间存在共性，人们倾向于结成一伙，以非正式的方式组成部落。自然地，这样的部落根据共同的价值观相互合作。在困难时期，价值观差距越大，人们之间的冲突就越大。人们经常将其他部落的成员妖魔化，他们没有认识到其他部落的成员与自己一样，都是以各自所知的最佳方式，做着符合自身利益的事情。</p><p><strong>17.群体斗争。●自古至今，所有国家的人民都被归入不同“群体”（尽管程度不同），其原因要么是他们选择与自己相似的人在一起，要么是他们被其他人归入特定群体。</strong>权力通常由三四个群体掌控，这些人加起来也只占人口的一小部分。人们所处的群体通常决定了谁是朋友和盟友，谁是敌人。人们被刻板地划分到不同的群体中，无论他们喜欢与否。富人和贫困者是最常见的群体划分，此外还有许多其他的重要类别，如种族、民族、宗教、性别、生活方式、地理位置（如城市与农村）和政治（右倾与左倾）。在大周期的早期阶段，由于形势较好，不同群体之间通常比较和谐；在大周期的后期阶段，由于形势不佳，各群体之间会发生更多争斗。群体斗争对内部秩序有着深远的影响，我将在第5章中进行探讨。本章的附录更详细地解析了这一决定因素。</p><p><strong>18.左倾/右倾政治周期。</strong>在所有社会中，政治都在左倾势力与右倾势力之间更迭，这些更迭决定财富和权力的分配方式。有的更迭是以和平方式进行的，有的则充满暴力。这些一直都是值得我们了解的情况。一般来说，资本市场周期与财富、价值观和群体划分周期一起，驱动左倾/右倾政治周期，它们共同构成了政治变革的动力。在资本市场和经济繁荣时期，贫富差距通常会扩大。虽然一些社会在左倾/右倾政治之间取得了相对合理且稳定的平衡，但更常见的是，左倾/右倾政治之间发生周期性更迭。在国家兴衰的整个过程中，这些更迭持续发生，周期大约是10年。重大的经济危机标志着大周期的结束，往往预示着革命的到来。本章的附录提供了更多的解析。</p><p><strong>19.要想维系和平，就必须解决囚徒困境。</strong>囚徒困境是博弈论中的一个概念，它解释了为什么尽管彼此合作是对双方来说最好的选择，但对一方来说，先杀死对方的做法更合乎逻辑。这是因为生存高于一切，虽然你不能确定你的对手是否会攻击你，但你可以确定，在你打败对手之前，对手会先打败你，因为这符合他方的利益。正是出于这个原因，双方最好相互保证，不给对方造成生存危害，从而避免致命性的战争。要想进一步降低发生冲突的风险，双方就需要交换利益，建立相互依赖的关系，使自身不能承受失去这种关系的后果。</p><p><strong>20.是双赢关系还是双输关系，这取决于双方做出的选择。</strong>个人和国家关系的各个层面都是如此。实际上，双方可以选择是相互合作的双赢关系，还是相互威胁的双输关系（即是盟友还是敌人），但双方的行动将决定其关系和结果。需要明确的是，只要一方不给另一方带来生存风险（参见囚徒困境），竞争对手之间就可以存在双赢关系。双方必须明确并尊重对方的生存红线。在双赢关系中，双方可能会以相互尊重为基础进行艰难的谈判，就像集市上的商人和气地讨价还价，或者奥运会上的运动队友好竞赛一样。拥有双赢关系显然比两败俱伤好，但有时存在不可调和的分歧，双方必须为之而战，因为无法通过谈判解决分歧。</p><p><strong>21.权力平衡大周期，它驱动着国家内部和国家之间的和平/战争大周期。</strong>权力平衡机制是盟友和敌人争取财富和权力的永恒普适机制。从办公室政治到地方政治，从国家政治到地缘政治，权力平衡驱动着几乎所有权力斗争。在不同的文化中，权力平衡的方式略有不同。西方社会的做法更像下国际象棋，而亚洲社会的做法更像下围棋，尽管目标都是支配对方。无论是过去还是现在，权力平衡都无处不在，并似乎是通过一系列具有连贯性的步骤而进行的。在第5章讨论内部秩序时，我将进行更详细的描述（尽管这些因素同样适用于内部和外部的权力斗争）。本章的附录更完整地解释了权力平衡的具体机制。</p><p><strong>22.军事实力与和平/战争周期</strong>。历史告诉我们，军事实力（无论是自身的还是通过联盟获得的）是决定最终结果的关键因素。有时武力本身就是威胁，有时必须动用武力。军事实力是很容易观察和衡量的，也可以定性评估。在国际关系方面，军事实力尤为重要，因为国际上没有有效的司法和执法体系，各国需要通过战争来检验各自的相对实力，这就产生了战争与和平周期。我在第6章讨论外部秩序周期时将会进行解释。</p><p><strong>●与其他事物一样，内部秩序和世界秩序也在不断演变，将现有情况向前推进；现有情况相互作用，这些作用又产生新的情况。</strong></p><p>直至今日，依然只有一小部分人（他们来自少数群体）拥有大部分的财富、权力，这些人作为“精英”统治社会。在我看来，很明显，在大多数国家，资产阶级拥有最大的财务权力；在民主国家，政治权力掌握在选民手里；在专制国家，政治权力掌控在通过某种程序被选定的少数人手里。[插图]在当今的大多数国家，这些人是“统治者”和“精英”，他们监督现有的国内秩序，但目前受到攻击，所以现有格局可能会改变。例如，美国正在掀起一项大规模的运动，目的是明显加大对不同群体成员的包容度，无论在赚钱的资本家领域还是在政治领域都是如此。这些转变可能带来正面的影响，也可能产生负面的影响，这取决于处理转变的方式（是和平还是暴力，是明智还是愚蠢）。通过研究历史，我发现了这样一个永恒普适的真理：自孔子生活的公元前500年左右以来，<strong>●那些最广泛地使用人才，并根据个人业绩而不是特权来赋予公民责任的社会最可能取得可持续性成功。这样做可以（1）找到最佳人选来很好地完成工作，（2）从多种视角看待问题，（3）让公民认为自己得到最公平的待遇，从而促进社会稳定。</strong></p><p><strong>左倾/右倾政治周期。资本家（即右倾势力）和社会主义者（即左倾势力</strong>）不仅有着不同的自身利益，而且有着不同的意识形态，二者都愿意为自己根深蒂固的信念而奋斗。</p><p><strong>右倾/资本主义者的典型观点是</strong>，自给自足，从而获取效益，限制政府干预，允许个人保留自己的成果，允许个人选择社会道德标准。他们还认为，私营部门比公共部门更有效率，资本主义最适合大多数人，白手起家的亿万富翁对社会的贡献最大。资本家通常不能容忍为那些缺乏生产和盈利能力的人提供财务支持。对他们来说，赚钱 = 取得效益 = 得到自己应得的东西。他们不太注意经济机器是否为大多数人创造了机会和繁荣。他们也可能忽略了这样一个事实：他们的盈利形式不利于实现大多数人的目标。例如，在一个纯粹的资本主义体系中，提供优质的公共教育并非当务之急，但这显然是整个社会提高生产力和增加财富的一个主要动力。</p><p><strong>左倾/社会主义者的典型观点是</strong>，人们互相帮助，政府支持民众，分享财富和机会是高尚道德，而且有利于社会。他们认为，私营部门基本上是由贪婪的资本家经营的，而普通劳动者（如教师、消防员和工人）对社会的贡献更大。社会主义者和共产主义者强调妥善分配经济资源。他们支持政府增加干预，相信政府官员会比资本家更公平，资本家只想剥削他人以赚取更多的钱财。</p><p><strong>●赚钱、储蓄和将储蓄转化成资本（即资本主义）的能力是激励人们的有效动力，也帮助人们合理配置资源，提高生活水平。但资本主义也会造成财富和机会差距，这种差距是不公平的，可能产生负面作用，具有高度的周期性，可能破坏社会的稳定性。在我看来，决策者面临的最大挑战是，构建一个资本主义经济体系，在不加剧不平等和不稳定的情况下，提高生产力和生活水平。</strong></p><p><strong>权力平衡大周期，它驱动着国家内部和国家之间的和平/战争大周期：</strong></p><p><strong>第一步：联盟的形成。</strong>当权力失去平衡时（例如，在美国，如果民主党的权力比共和党大得多，反之亦然），实力较强的政党就会利用并控制实力较弱的政党。为了削弱较强的政党，较弱的政党自然会去寻找其他政党加入自己的阵营，共同对抗较强的政党。这样它们可能拥有与反对党相当或更大的权力。实力较弱的政党通过给其他政党以它们想要的好处来换取它们的支持。如果以前实力较弱的政党通过这种做法，获得了比以前实力较强的政党更多的权力，后者就会与其他政党交涉，以求与它们结盟，从而消除反对党的优势。其结果是有着不同既得利益的盟友们团结起来，反对共同的敌人。</p><p><strong>第二步：战争决定胜负。当双方势均力敌且存在差异时，通常会发生剧烈的争斗；当双方势力不相当时，往往不会发生激烈的交战。</strong>因为明显较弱的实体去抗衡明显较强的实体是愚蠢之举，如果双方真的交战，那么规模也会很小。然而，有时即使双方的势力旗鼓相当，也可能出现僵持不下，而不是剧烈战斗，因为在试图打败对方的过程中，双方可能伤害自己的威胁大于从生死搏斗中获得的收益。例如，如果双方交战的结果肯定是相互毁灭（如美国和苏联当时面临的情况），双方就更可能僵持不下，而不是战斗。</p><p><strong>第三步：胜者之间的斗争。</strong>历史告诉我们，在打败了共同敌人的权力斗争之后，那些联手击败共同敌人的赢家通常会在内部争夺权力，而输家也会这样做，他们想要卷土重来。我把权力平衡机制的这一形态称为“清洗”。这种情况在所有案例中都发生过，其中最著名的是法国的恐怖统治。同样的争斗也发生在国家之间，比如二战中曾是盟友的美国和苏联。类似地，抗日战争结束后，中国共产党和国民党的抗日统一战线立即变成内战。</p><p><strong>第四步：步入和平与繁荣，但最终变得过度扩张，主要表现为财富和机会差距扩大与负债过度。</strong>历史告诉我们，由于这样的机制，最好的时期（即和平与繁荣时期）通常发生在战争之后，这时，领导者和权力结构明显确立，国家内部、与其他国家之间没有激烈的权力斗争，因为一个实力明显更强大的实体，能够让那些不太强大的实体安稳度日。</p><p><strong>第五步：不断激化的冲突导致国内和国际秩序的巨变。</strong>只有大多数人能享受和平与繁荣（前提条件是制度公平，大多数人自律和高效），和平与繁荣时期才能延续下去。然而，如前所述，和平与繁荣时期也往往助长贫富悬殊和债务泡沫。随着繁荣消退和新矛盾的出现，冲突就会爆发。</p><h2 id="三、闪货币、信贷、债务和经济活动的大周期"><a href="#三、闪货币、信贷、债务和经济活动的大周期" class="headerlink" title="三、闪货币、信贷、债务和经济活动的大周期"></a>三、闪货币、信贷、债务和经济活动的大周期</h2><p><strong>货币和信贷的永恒普适的基本要素：</strong></p><p><strong>●无论是过去还是现在，所有实体（个人、公司、非营利组织和政府）都要面对同样的基本财务现实。这些实体有资金的流入（即收入）和流出（即支出），两者相抵后构成净收入。这些资金流用数字来衡量，显现在损益表中。如果一个实体的收入超过支出，这就会产生利润，储蓄就会增加。如果一个实体的支出超过收入，储蓄就会减少，或者它需要弥补差额，通过借款或以其他方式获得资金。如果一个实体的资产远远超过负债（即拥有高额的净资产），它就可以出售资产，使支出超过收入，直到资金耗尽，届时，它必须削减开支。如果一个实体的资产并不比负债多很多，而且收入低于运营和偿债所需的金额，该实体就必须削减开支，否则将会出现违约或者需要重组债务。</strong></p><p>一个实体的所有资产和负债（即债务）都可以在资产负债表中显示出来。无论这些数字是否有记录，每个国家、公司、非营利组织和个人都有资产和负债。例如，如果经济学家把每个实体的收入、支出和储蓄综合起来，就得到了所有实体的收入、支出和储蓄。<strong>●各个实体在损益表和资产负债表中反映的整体财务管理方式是内部和世界秩序变化的最大驱动因素。理解了自己的收入、支出和储蓄，你就可以将其应用于其他实体，把这些实体综合起来，你就可以看到整个系统的运作方式。</strong></p><p>例如，<strong>由于一个实体的支出是另一个实体的收入，当一个实体削减支出时，这不仅会伤害自身实体，也会伤害依赖这一支出获得收入的其他实体。同样，由于一个实体的债务是另一个实体的资产，债务违约会使其他实体的资产减少，迫使它们削减支出。这会使债务问题和经济萎缩加速恶化，当人们为如何分配缩减了的经济蛋糕发生争辩时，这就会酿成政治问题。</strong></p><p>作为一个原则，<strong>●债务吞噬资产净值。我的意思是，你必须最先偿还债务。例如，如果你拥有一个房子（即拥有“房屋净值”所有权），但不能支付抵押贷款，你的房子就会被卖掉或收走。也就是说，债权人比房主优先得到偿付。因此，如果你的收入低于支出，你的资产低于负债（即债务），你就不得不卖掉你的资产。</strong></p><p>与大多数人凭直觉想象的不同，<strong>货币和信贷并不存在固定的数量。央行可以很容易地创造货币和信贷。个人、公司、非营利组织和政府都希望央行发行大量货币和信贷，因为这会增强其消费能力。如果货币和信贷用于消费，大多数商品、服务和投资资产的价格就会上涨。这也产生了必须偿还的债务，需要个人、公司、非营利组织和政府削减支出，最终使支出低于收入，这个过程是艰难和痛苦的。这就是为什么货币、信贷、债务和经济活动具有内在的周期性。在信贷创造阶段，商品、服务、投资资产的需求和生产都很强劲，而在债务偿还阶段，两者都很疲弱。</strong></p><p>总之，这种考察基本财务现实的方法适用于所有个人、公司、非营利组织和政府，也适用于你我，但存在一大例外情况（我在上文提到过）。所有国家都可以发行货币和信贷，把资金发放给国民，将其用于消费或放贷。通过发行货币，并将其发放给需要资金的债务人，央行可以避免债务危机（我刚解释过）。出于这个原因，我将把前面的原则改为：<strong>●债务吞噬资产净值，但央行可以通过印钞来为债务融资。一旦债务危机恶化到一定程度，使吞噬资产净值的大量债务在政治上无法接受，同时带来相应的经济痛苦，政府印钞就不足为奇了。</strong></p><p><strong>在世界各国被广泛接受的钞票（即货币）被称为储备货币。</strong></p><p><strong>●在储备货币存在期间，拥有储备货币的国家受益匪浅。因为储备货币赋予一个国家极大的借款和消费能力，也给予该国巨大的权力，它能决定哪个国家获得国际交易所需的资金和信贷。</strong>然而，拥有储备货币通常会让一个国家播下失去储备货币地位的种子。这是因为储备货币国家可以借到超出自身偿付能力的资金。为了偿还债务，储备货币国家发行大量货币和信贷，导致货币贬值，最终丧失储备货币地位，而失去储备货币地位极为糟糕。这是因为<strong>●拥有储备货币是一个国家所能拥有的最大权力之一，它赋予这个国家巨大的购买力和地缘政治实力。</strong></p><p>货币是什么：</p><p><strong>货币是一种交换媒介，也可以是财富贮藏的手段。</strong></p><p><strong>大多数货币和信贷（尤其是目前存在的法定货币）都没有内在价值。它们只是会计系统中的日记账目，很容易被更改。货币和信贷体系旨在帮助有效分配资源，提高生产率，使借贷双方都能获利，但这一体系会出现周期性失灵。发生这种情况时（亘古以来，这种情况总会发生），货币供应就采用“货币化”[插图]，货币不是被贬值就是被摧毁，财富将发生巨大的转移，对整个经济和市场都造成冲击。</strong></p><p>货币、信贷和财富：</p><p><strong>货币和信贷与财富有关，但它们不是一回事。因为货币和信贷可以用来购买财富（即商品和服务），所以一个人拥有的货币和信贷总值与其拥有的财富总值看起来几乎是一样的，但是一个人无法单纯通过创造更多的货币和信贷来创造更多的财富。要想创造更多的财富，就必须提高生产率。货币和信贷创造与财富创造之间的关系经常被混淆，但这种关系是经济周期的最大驱动力。因此我们来详细考察这种关系。</strong></p><p><strong>就像金融经济和实体经济容易被混淆一样，价格和价值的关系也令人困惑。价格和价值的走势往往相同，所以可能会被混为一谈。因为当人们拥有更多的货币和信贷时，他们就拥有了增加支出的能力和意愿。如果支出增加了经济产出，提高了商品、服务和金融资产的价格，这就相当于增加了财富，因为以我们核算财富的方式来衡量，拥有这些资产的人变得“更富有”了。然而，这种财富增加更多的是种错觉，而不是现实，原因有二：（1）尽管信贷增长推高了价格和生产，但这些信贷是需要偿还的，在所有条件不变的情况下，信贷到期而需要偿还时，会对财富产生反向作用；（2）物品的内在价值不会只因价格上涨而增加。</strong></p><p>你可以这么想：如果你拥有一套房子，那么在政府创造了大量货币和信贷时，许多急切购房的买家可能会推高你的房价，但房子仍是同一套房子，你的实际财富并没有增加，只是财富的估值有所增加。</p><p>你可以这么想：<strong>央行有一瓶兴奋剂，可以根据需要将其注入经济。当市场和经济增长下滑时，央行注入货币和信贷兴奋剂，从而提振市场和经济；当市场和经济过热时，央行减少或停止注入兴奋剂。</strong>这些举措使货币、信贷、商品、服务和金融资产的数量与价格出现周期性涨跌。这些走势通常表现为短期债务周期和长期债务周期。短期债务周期起起落落，通常持续8年左右（会有出入）。持续时间取决于刺激政策需要多长时间，才能把需求提高到实体经济达到产能极限的水平。大多数人见过足够多的短期债务周期（通常被称为“经济周期”），对其有一定认知，因此他们错误地认为，债务周期将永远以这种方式持续运转。</p><p><strong>我称之为短期债务周期，是为了区别于长期债务周期。长期债务周期通常持续50<del>100年（因此包括6</del>10个短期债务周期）。[插图]长期债务周期在人的一生中只出现一次，大多数人对其会毫无预料</strong>。因此其到来往往令人措手不及，让很多人遭受损失。我们正处于长期债务周期的后期阶段，目前的长期债务周期是1944年在新罕布什尔州的布雷顿森林设计的世界货币体系的结果，这一货币体系在1945年二战结束后付诸实施，标志着美元/美国主导的世界秩序的开始。</p><p><strong>长期债务周期</strong></p><p>长期债务周期分为6个阶段。</p><p><strong>第一阶段：最初（a）并不存在债务，或者债务很少，（b）人们使用硬通货。</strong></p><p>通过进行债务重组和债务货币化，上个周期的债务负担基本上被消除了。考虑到这些后果（特别是通胀），人们又开始使用硬通货（例如黄金和白银，有时还有铜和其他金属，如镍）或者与硬通货挂钩的货币。例如，在德国魏玛共和国的债务和货币崩溃后，以黄金计价的资产和土地成为货币的后盾，而且货币与美元挂钩。20世纪80年代末，阿根廷比索大幅贬值后，开始与美元挂钩。</p><p>在这个阶段，硬通货很重要，因为它们的交易不需要涉及信任或信用。任何交易都可以就地结算，即使买卖双方是陌生人或冤家。有句老话说：“黄金是唯一不是他人负债的金融资产。”你从买家那里收到金币后，可以把金币熔化掉，因为它存在内在价值，你仍然可以得到几乎同样的价值，而不像债务资产（是交付价值的承诺），例如纸币（因为很容易印制，所以纸币算不上什么承诺）。在战争期间，各方不信任彼此的支付意图和能力，故而仍然使用黄金来完成交易。因此，黄金（其次是白银）既可以作为安全的交换媒介，也可以作为安全的财富贮藏手段。</p><p><strong>第二阶段：后来出现了硬通货债权票据（又称票据或纸币）。</strong></p><p>由于随身携带大量金属货币既有风险又不方便，而且信用创造对贷款人和借款人都有吸引力，因此一些可信赖的机构出现了，它们把货币存放在安全的地方，然后给存放人出具债权票据。这些机构后来被称为“银行”，不过最初包括人们信任的各种机构，如中国的寺庙。很快，人们将这些“货币票据”视为实际货币。毕竟，这些票据可以被兑换成有形货币，也可以直接用于购物。这种货币体系被称为挂钩货币体系，因为货币的价值与某种实物的价值挂钩，这种实物通常是硬通货，如黄金和白银。</p><p><strong>第三阶段：后来是债务增加。</strong></p><p><strong>起初，硬通货债权票据的数量与银行里储存的硬通货一样多。之后，债权票据的持有者和银行共同发现了信贷和债务的奥妙。</strong>票据持有者将这些债权票据借给银行，可以得到银行支付的利息。借入这些债权票据的银行很乐意，因为它们可以把钱借给其他能支付更高利息的人，从而从中获利。从银行借来票据的人也高兴，因为他们获得了以前没有的购买力。随着资产价格和生产双双提升，整个社会皆大欢喜。由于受到所有人的青睐，这种做法逐渐盛行起来。贷款和借款日益增多，掀起了借贷热潮，以至货币债权（即债务资产）数量超过了可供购买的实际商品和服务数量。最终，货币债权远远超过银行储存的硬通货。</p><p><strong>一旦出现以下问题，麻烦就来了：人们没有足够的收入来偿还债务，或者人们持有的债权数量（他们指望通过出售这些资产，换来购买商品和服务的货币）比商品和服务数量增长得更快，以致债务资产（如债券）无法兑换成货币以购买商品和服务。而这两个问题往往结伴而来。</strong></p><p><strong>第四阶段：然后会发生债务危机、违约和货币贬值，导致印钞和与硬通货脱钩。</strong></p><p>关于第二个问题，如果债权人认为他们不能从债务中获得足够的回报（相对于其他财富贮藏手段及商品和服务成本），上述情况就会发生。债务资产（如债券）是由投资者持有的，他们将这些资产视为财富贮藏手段，以备未来出售换取资金，然后购买商品。当债务资产持有者想要换成实物货币与实物商品和服务却发现无法转换时，挤兑就会出现，即债务资产持有者竞相将债务资产转换成货币、商品、服务和其他金融资产。届时，无论是私人银行还是央行，都会面临这样的选择：要么允许资金从债务资产流出，这会推升利率，导致债务和经济问题恶化；要么“印钞”，发行债券，并充分购买债券，防止利率上升，以期资金流回债务资产。在这种情况下，央行不可避免地与硬通货脱钩，印发货币，并让货币贬值。因为不这样做，就会导致无法忍受的通缩性经济萧条。这个阶段的关键是，创造足够的货币和让货币贬值，从而抵消通缩带来的经济萧条，但又不至于造成通胀螺旋式上升。如果行之有效，这就会实现我所称的“和谐的去杠杆化”（我在《债务危机》一书中对此进行了更全面的阐述）。有时购债举措暂时奏效，但一旦货币债权（债务资产）与实际存在的硬通货以及可供购买的商品和服务的比率过高，银行将处于无法自救的困境，因为它没有足够的硬通货来偿付货币债权。当央行发生这种情况时，它可以选择违约，或者与硬通货脱钩，发行货币，让货币贬值。央行总会不可避免地选择让货币贬值。但如果这些债务重组和货币贬值的规模过大，就会导致货币体系失灵乃至被摧毁。债务（即货币债权及商品和服务债权）越多，货币贬值的必要性和可能性就越大。</p><p><strong>第五阶段：然后是法定货币，最终导致货币贬值。</strong></p><p>央行希望尽可能延长货币和信贷周期，使其长久持续下去，因为这比其他选择要好得多。因此，当硬通货和硬通货债权的货币体系变得过于受限时，政府通常会放弃该体系，转而采用所谓的法定货币体系。法定货币体系不涉及硬通货，央行可以无限制地印制纸币，所以不会面临硬通货储备缩水而被迫违约的风险。届时，央行的风险在于，由于不再受制于有形黄金、白银或其他硬资产的供给，印钞机掌管者（即与商业银行合作的央行）不断创造更多的货币、债务资产和负债，以至超过商品和服务的供应数量。最终，持有大量债务的债权人设法出售这些债务，换取商品和服务，从而产生与银行挤兑相同的效果，导致债务违约或货币贬值。</p><p><strong>当信贷周期达到极限时，为了维持经济运行，中央政府及央行会创造大量债务，还会印钞，并用于购买商品、服务和投资资产，这是合乎逻辑的典型对策。</strong>这就是2008年全球金融危机期间的做法，当时利率已降为零，不能再下调了。2020年，为了应对新冠肺炎疫情造成的经济危机，政府也大规模地采取了这一举措。这也是美国政府应对1929—1932年的债务危机的做法，当时利率也同样降为零。在我撰写本书的时候，债务和货币的规模超过二战以来的任何时候。</p><p>在几乎所有情况下，政府的举措都会造成债务累积，政府自身也变成负债累累的债务人。当债务泡沫破裂时，政府需要救助自己和其他人，它们购买资产和/或增印货币，让货币贬值。债务危机越严重，越是如此。这些举措虽然不是上策，但其背后的原因是可以理解的。<strong>●当政府能够创造货币和信贷，并发放给民众而取得好评时，政府很难抵抗这种诱惑。[插图]这是一种典型的金融手法。纵观历史上的统治者，他们都会积累大量债务，而这些债务在他们统治结束后很久才会到期，所以继任者需要为之买单。</strong></p><p><strong>当政府大量印钞和购债时，货币和债务的价值就会下降。这实质上是对货币和债券持有人征税，减轻债务人和借款人的负担。这种情况发展到一定程度时，货币和债务资产持有者就会意识到真实情况。于是，他们设法出售债务资产和/或借入廉价贷款，再举借可以通过廉价贷款来偿还的债务。他们还常常把财富转移到其他财富贮藏手段上，比如黄金、某些类别的股票或没有这些问题的其他国家。在这种情况下，央行通常会继续印钞，直接或间接地购买债券（例如，让银行代其购买），并禁止资金流入通胀对冲资产、其他货币和其他地方。</strong></p><p><strong>尽管人们趋于相信货币基本上会永远存在，“现金”是可以持有的最安全的资产，但是事实并非如此。●所有货币都会贬值甚至消亡。一旦出现这种情况，现金和债券（即获得货币的承诺）就会贬值甚至变得一文不值。这是因为大规模印钞、降低债务价值是减轻或消除债务负担的最简便方式。</strong>正如下一章所述，只有充分减轻或消除债务负担，信贷/债务扩张周期才能重新启动</p><p>要想使债务和负债相对于偿债所需的收入和现金流下降，决策者可以使用以下4种工具：</p><ol><li>财政紧缩（减少支出）；</li><li>债务违约和重组；</li><li>将资金和信贷从富人向贫困者转移（例如，增税）；</li><li>印钞并使货币贬值。</li></ol><p>出于逻辑原因，这些政策工具通常从一个发展到下一个。</p><ul><li>财政紧缩具有通缩性，不会持续太久，因为这一做法过于痛苦。</li><li>债务违约和重组也具有通缩性，也很痛苦，因为价值减少甚至归零的债务亦是某些人的资产。因此，违约和重组对债务人和债权人来说都很痛苦。债务人破产，资产被收走；债权人因债务减计而遭受财富损失。</li><li>将资金和信贷从富人转移到贫困者（即通过对富人增税进行财富再分配），这一做法虽然具有一定的政治挑战性，但比前两种做法更容易忍受，通常是解决方案的一个选项。</li><li>与其他做法相比，● 印钞是最简便、最不被人了解、最常见的主要债务重组手段。实际上，大多数人认为这种做法利大于弊，因为：<ul><li>印钞有助于缓解债务压力；</li><li>在提供金融财富的过程中，难以分辨出财富被收走的任何受害方（尽管他们是货币和债务资产的持有者）；</li></ul></li></ul><p><strong>第六阶段：回归硬通货。</strong></p><p><strong>在极端情况下，如果做得太过火了，就是说央行过度增印法定货币，债权人就会出售债务资产，引发上文所述的银行挤兑现象，最终导致货币和信贷的价值下降，促使人们逃离现金和债务（如债券）。</strong>历史告诉我们，在这种情况下，人们通常会转向黄金、白银、保值的股票，以及没有这些问题的其他国家的货币和资产。一些人认为，一旦发生资本外逃，国家就需要一种替代储备货币，但事实并非如此。因为同样的情况（即货币体系崩溃、资金流向其他资产）曾经发生在没有替代货币的历史案例中（例如，在古代中国和罗马帝国时期）。当货币贬值时，人们会抢购各种各样的另类资产，例如，在德国魏玛共和国时期，人们会抢购石材（用于建筑）。由于货币贬值，人们逃离货币和以该货币计价的债务，转而选择其他货币和资产。</p><p>如果货币贬值和债务违约变得过于极端，货币和信贷体系就会崩溃。届时，政府通常会被迫回归某种形式的硬通货，重建人们对货币价值作为财富贮藏手段的信心。政府经常（但并非总是）将货币与黄金或某种硬通货挂钩，并允许新货币持有者将其转换为硬通货。例如，在过去几十年中，许多弱势货币国家将其货币与美元挂钩，或者干脆把自己的整个经济美元化（即把美元作为本国的交换媒介和财富贮藏手段）。</p><p><strong>综上所述，在长期债务周期的早期，未偿债务不多，持有赚取利息的债务资产通常带来回报。但是，到了周期的后期，未偿债务大量积累，濒临违约或者贬值，持有债务的风险超过所得的利息。</strong>因此，持有债务（例如债券）有点儿像手握一颗定时炸弹，它在嘀嗒作响时好比债务的积累，但一旦停下来就会把你炸得粉碎。正如我们所见，这种大爆炸（即发生重大违约或货币大幅贬值）每50~100年就发生一次。</p><p><strong>几千年来，世界上一直存在三种类型的货币体系：</strong></p><ul><li>第一类：硬通货（如金属硬币）。</li><li>第二类：纸币（硬通货债权）。</li><li>第三类：法定货币。</li></ul><p><strong>硬通货是最具约束性的货币体系。</strong>除非作为货币的金属或其他具有内在价值的大宗商品的供给增加，否则货币无法创造。在第二类货币体系中，货币和信贷更容易创造，因而硬通货债权与实际持有的硬通货的比率会上升，最终导致银行发生“挤兑”。结果是违约，银行关门，储户失去他们的硬资产和/或货币债权贬值，这意味着储户拿回来的钱少了。在第三类货币体系中，政府可以自由创造货币和信贷，只要人们对货币还有信心，这种体系就一直运营；一旦人们对货币失去信心，这种体系就会失灵。</p><h1 id="四、货币价值的演变"><a href="#四、货币价值的演变" class="headerlink" title="四、货币价值的演变"></a>四、货币价值的演变</h1><p>有的货币贬值有利于体系（但货币和债务持有者总是需要付出高昂代价），而有的货币贬值则有害于体系，这种货币贬值会破坏信贷/资本分配体系，但它是消除债务以建立货币新秩序的必要举措。搞清二者的差别非常重要。</p><p>为此，我先来讲述货币相对于黄金和按消费者价格指数加权的一揽子商品和服务的价值如何发生变化。二者具有可比性，因为黄金是永恒普适的替代货币，而货币是用来购买商品和服务的，因此货币的购买力是最重要的。我还会简要分析货币价值相对于其他货币/债务以及股票的变化，因为这些资产也可以成为财富贮藏手段。在货币大幅贬值的情况下，所有这些指标反映出来的情况基本类似，许多其他物品（房地产、艺术品等）也可以作为财富贮藏手段，但黄金可以很好地说明我的观点。</p><p>储备货币对黄金（现汇汇率）</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9dc4ee7e47148cdf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>现汇汇率对黄金</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-09806c0d531d291b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="五、内部秩序和混乱大周期"><a href="#五、内部秩序和混乱大周期" class="headerlink" title="五、内部秩序和混乱大周期"></a>五、内部秩序和混乱大周期</h1><p><strong>●我发现自古至今，在大多数国家，影响大多数人的最主要因素是人们如何努力创造、获取和分配财富与权力，尽管他们也在其他方面发生斗争，其中最突出的是意识形态和宗教。在时间的长河中，这些斗争以永恒普适的方式出现，给人们生活的各个方面都造成了重要影响（从税收、经济到人们在繁荣与萧条时期、和平与战争时期如何相处），而这些斗争循环往复地出现，就如同潮起潮落。</strong></p><p>究历史使我发现，从内部有序到内部无序再回到内部有序的典型周期分为以下阶段：</p><ul><li>第一阶段：新秩序开始，新领导层巩固权力，接下来……</li><li>……第二阶段：资源配置体系与政府官僚机构建立和完善，如果行之有效，就会……</li><li>……第三阶段：出现和平与繁荣，进而出现……</li><li>……第四阶段：支出和债务严重过度，贫富差距和政治分歧扩大，从而导致……</li><li>……第五阶段：财政状况糟糕，冲突激烈，之后……</li><li>……第六阶段：出现内战/革命，然后再回到……</li><li>……第一阶段：发展到第二阶段，以此类推，整个周期循环往复。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1e8153eaa47965bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f917fdc9f205cbbe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2bfb85d2d61cf783.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>全球实际人均GDP（以2017年美元为基准，取对数值）</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e849930f62245c0f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>深入研究内部周期的6个阶段：</strong></p><p><strong>第一阶段：新秩序开始，新领导层巩固权力</strong></p><p>无论是内战还是革命（即使是和平的革命）都是一场巨大的冲突，最后，一方取胜，另一方落败，国家遭受损伤。第一阶段出现在战争之后，获胜者夺取了控制权，而失败者则必须屈服。在新秩序的第一阶段，获胜者因足够强大而获胜，其必须还有智谋，才能巩固权力并重建家园。</p><p>在赢得控制权之后，新领导者通常会清除残余的反对派，并为争夺权力展开内斗。事实上，也许可以说，革命通常分为两部分：第一部分是推翻已有领导者和体制的斗争，第二部分是消除前领导者的忠诚分子的斗争以及胜者为争夺权力而进行的内斗。我将第二部分称为“清洗”，并在本节进行简要的介绍。</p><p><strong>第二阶段：资源配置体系与政府官僚机构建立和完善</strong></p><p>这一阶段的一个永恒普适原则是，<strong>●一个体制要想成功，就要为大多数人（特别是广大中产阶级）创造繁荣。</strong>正如亚里士多德在《政治学》里说的：“如果中产阶级规模大，那么在可能的情况下，中产阶级比另外两个阶级都强大，这样的国家可能是治理良好的国家……中产阶级规模大的地方，派别纷争出现的可能性最低……因为假如没有中产阶级，而贫困者又过多，麻烦就来了，国家会很快走向灭亡。”</p><p><strong>第三阶段：出现和平与繁荣</strong></p><p><strong>我也将这个阶段称为中期繁荣时期。这是内部秩序周期中的最佳时期。在此期间，人们拥有取得效益的大量机会，并且对此热情高涨，融洽合作，取得许多成果，变得更加富裕，并因成功而受到钦佩</strong>。在此阶段，几乎所有人的生活条件都不断改善，大多数下一代人会比上一代人过得更好，所以人们对于未来普遍乐观且充满向往。历史告诉我们，如果治国有效，人们就会拥有近乎平等的各种教育机会，以及择优录用的工作岗位。国家在最大范围内吸引人才，打造大多数人认为公平的体制。成功的企业家、发明家和冒险家提出新想法，推动社会的进步，因此成为英雄而受到仰慕。因为他们提出了颠覆性的创新想法，改善了人们的生活，所以得到回报。债务增长促进生产率提高，进而推动实际收入增长，减轻债务偿还负担，提供超额利润，取得出色的股本回报率。收入超过支出，储蓄超过负债，储蓄将为未来投资提供资金。第三阶段是一个激动人心的时期，整个社会充满着巨大的创造力、生产力和正能量。</p><p><strong>这是“鼓舞人心的远见卓识者”大展身手的时代。他们可以（a）设想从未存在过的未来，并把这幅激动人心的图景传达给民众；（b）将这一设想付诸实现；（c）利用取得的成就，让更广泛的社会成员享受繁荣，并投资于未来。此外，他们（d）维持稳健的财政状况；（e）建立良好的国际关系。因此，他们可以保护或扩张自己的帝国，而无须投入削弱经济和社会的战争。</strong></p><p><strong>在这个阶段，值得关注的势态发展是，机会、收入、财富和价值观方面的差距扩大，同时大多数人面临不公平的困难处境；精英享有奢华和对他人不公的特权；生产率下滑；过度创造债务导致财政状况糟糕。这些自然发展趋势对国家构成巨大的风险，破坏自我维持的良好结果。伟大的国家能够实现自我维系，避免这些风险，继续留在第三阶段。</strong>如果不能避免这些风险，国家就会发展到第四阶段，即过度时期。在第四阶段，做成（或通过举债做成）一切的诱惑会将一个国家推至冲突边缘。</p><p><strong>第四阶段：过度时期</strong></p><p>我也将这一阶段称为泡沫繁荣时期。</p><p><strong>在这一阶段，典型的最佳领导者是“稳扎稳打、纪律严明的领导者”，他们理解和展示高标准的行为准则，实现经济效益，打造稳健财政，他们对大众的过度行为加以约束。在人们变得富裕后，他们带领国家继续把大量收入和时间再投资于生产性活动。</strong>如前所述，在新加坡前总理李光耀的领导下，新加坡在变得富有之后，仍然保持了良好教育、严明纪律和坚强性格的文化。但这样的领导者凤毛麟角，而且间隔甚久。因为阻止大众激情是很不得人心的。在几乎所有案例中，在变得富裕后，国家（及其领导者）都会堕落，过度消费，通过举债维持过度支出，使国家丧失竞争力。这样的衰落时期以一些堕落的领导者为代表，例如臭名昭著的罗马皇帝尼禄（他利用罗马全城大火的机会没收土地，修建宏伟的宫殿）、法国国王路易十四（在他的权力达到最高峰的时期，尽管生产率下降，民众生活艰难，但他依然扩建了凡尔赛宫）、中国明朝的万历皇帝（他不再积极治理国家，把心思全用于给自己修建宏伟的陵墓）。</p><p><strong>第五阶段：财政状况糟糕，冲突激烈</strong></p><p><strong>●导致重大内部冲突的典型有害因素包括</strong>：（1）一个国家（或州、城市）及其民众处于糟糕的财务状态（如承担大量债务和非债务性义务）；（2）实体内部的收入、财富和价值观存在巨大差距；以及（3）受到严重的负面经济冲击。</p><p><strong>●贫富差距最大、负债最多、收入下滑最显著的地方（城市、州和国家）最有可能发生最严重的冲突。</strong>值得思考的是，美国人均收入和财富水平最高的城市和州，通常也是负债最多、贫富差距最大的城市和州，如纽约市、芝加哥市、旧金山市、康涅狄格州、伊利诺伊州、马萨诸塞州、纽约州和新泽西州。</p><p><strong>●历史表明，在贫富悬殊和经济不佳的情况下，政府进行增税和减支，则尤其是内战或某种类型革命的主要预示信号。</strong>需要明确的是，这些革命不一定是暴力革命，但有这种可能性。</p><p><strong>●相比于受困者的数量和他们的权力，平均水平就不那么重要了。</strong>有些人支持有利于整体的政策，如自由贸易、全球化以及取代人的技术进步，而不考虑如果整体的分配方式不能惠及大多数人，那么社会将会发生怎样的情况。这些人忽视了整体将会面临风险这个事实。<strong>●一个社会要想拥有和平与繁荣，就需要拥有惠及大多数人的生产率。你认为我们今天具备这样的条件吗？</strong></p><p><strong>●在内部秩序周期的早期，官僚主义较轻，但在周期的后期，官僚主义严重，阻碍人们做出明智和必要的决策。</strong>这是因为随着事物的发展，它们往往变得越来越复杂，直到发展到一个地步——连明显有益的事情也无法完成。这时就需要根本性的变革。在一个以法律和契约为基础的体制里（这样的体制有诸多好处），这可能成为一个问题，因为法律可能妨碍人们去做明显有益的工作。我举个身边的例子，这涉及我和我妻子都很关心的一个问题。</p><p><strong>●成功的一个关键要素是，要把创造出来的债务和资金用于促进生产率的提升，创造良好的投资回报，而不是只把资金发放出去，但未能提高生产率和收入，因为如果这样做，货币就会大幅贬值，导致政府及所有人的购买力大幅缩水。</strong></p><p><strong>●历史表明，如果将放贷和支出用于广泛提高生产率、使投资回报超过借贷成本的项目，就可以提高生活水平，同时偿还债务。这样的政策就是好政策。</strong></p><p>在第五阶段，族群斗争激化。因为一般来说，<strong>●当困难和冲突加剧时，人们更倾向于：刻板地把他人看作一个或多个族群的成员</strong>；把这些族群要么看作敌人，要么看作盟友。这一情况在第五阶段变得更加明显，在第六阶段变得危险。</p><p><strong>●如果人们认为自己所热衷的某些追求比决策体系更为重要，那么决策体系危在旦夕。</strong>规则和法律只有在下述两个前提下才能生效：规则和法律必须清晰明了；多数人把在规则和法律的范围内行事看得足够重要，因此愿意为规则和法律行之有效而做出妥协。如果这两个前提都有所欠缺，那么法律体系危在旦夕。如果竞争各方均不愿意理性地对待他方，也不愿意为了维持整体的利益而以文明的方式做出决策（这往往需要他们放弃一些他们想要而且通过争斗有可能获得的东西），某种“内战”就会出现，相关各方以相对实力验证自己。到了这个阶段，不惜一切代价取胜将成为游戏规则，无底线的竞争将成为常态。在第五阶段的后期，人们为激情而抛弃理性。<strong>●在取胜成为唯一重要之事的时候，不道德的争斗就会以强化自身的方式愈演愈烈。如果每个人都为了自身的某种追求而不惜争斗，而且任何人都无法对任何事达成共识，体制就濒临内战/革命。</strong></p><p><strong>●如果有人在战斗中丧生，这就发出了一个警示：几乎可以肯定地说，斗争走向下一个更暴力的内战阶段，这一阶段将持续到胜者和败者明显确定为止。</strong></p><p>这就引出了我的下一个原则：<strong>●若你有疑虑，就要撤离。如果你不想被卷入一场内战或战争，就应该趁早撤离。这种情况通常出现在第五阶段。历史表明，当局势恶化时，想逃脱的人往往已经无法逃离了。投资和资金也是如此，因为各国在这些时期会采取资本管制及其他措施。</strong></p><p><strong>●当一个国家处于第五阶段后期时（如美国目前的情况），最大的疑问是，体制在崩溃之前有多大的韧性</strong>。民主体制基本上尊重民心，也更有韧性。因为民众可以选择替换领导层，而且出了问题只能归咎于自己。在这样的体制下，政权更迭更容易通过和平的方式进行。但一人一票的民主程序有一个缺点：领导者是通过民众支持率选出的，而大多数人不大会慎重地评估候选人的能力（大多数组织在为要职寻找合适人选时会进行慎重评估）。事实证明，这种民主体制在重大冲突时期可能会崩溃。</p><p>民主还需要达成共识的决策和妥协，很多观点不同的人需要在体制内部通力合作。西式民主制度确保了拥有大量选民的政党能派出代表参政，但像所有由观点很不一致（甚至彼此不喜欢）的群体组成的大型委员会一样，这样的决策体制无助于高效的决策。<strong>●西式民主国家面临的最大风险是，在决策的过程中，存在太多的分裂和对立，因而决策可能缺乏效率，产生不佳结果，进而引发由民粹主义者领导的革命，这些专制者代表很大一部分群体，这些群体希望有一个强大的、有能力的领导人控制混乱局面，进而使国家能够很好地为他们造福。</strong></p><p><strong>●要想取得最好的结果，不同的阶段就需要不同类型的领导人。第五阶段是个交叉路口，一条路可能走向内战/革命，另一条路可能走向理想化的和平共存。</strong>和平与繁荣显然是理想之路，但要艰难得多。这条路需要一位“强有力的和平缔造者”，他能把全国人民团结在一起，主动向他方伸出橄榄枝，让各方参与决策，重塑大多数人认为公平和有效的内部秩序（即高效地让大多数人获益）。这样的领导人在历史上寥寥无几，我们为他们祈祷。第二类是一位“强有力的革命者”，带领国家走出内战/革命的深渊。</p><p><strong>第六阶段：出现内战</strong></p><p><strong>●在长期没有经历内战的国家，人们通常认为不会发生内战，但历史告诉我们，内战是不可避免的，因此，我们与其假设“这种情况不会在这里发生”，不如对内战保持警惕，观察那些显示本国离内战有多近的指标。</strong>上一节探讨了内部秩序中的非暴力革命，本节将考察内战和革命的标志和模式。内战和革命几乎总是以暴力的方式推翻旧秩序，建立新秩序。尽管有无数的案例可供我们分析、理解其背后的机制，但如下页表所示，我选择了我认为最重要的29个案例。我把这些案例分为两类，一类使体制/政体发生了巨变，另一类则没有。例如，美国南北战争是一场极血腥的内战，但最终未能推翻体制/秩序，所以它属于第二类，处在该表的下方。那些推翻了体制/秩序的内战与革命则处于该表的上方。当然，这样的分类是不精确的，但和此前一样，我们不会过于求精，从而忽视全局性视野。尽管我们不能保证所有情况，但大多数内战与革命是以本节描述的典型方式发生的。</p><p><strong>●内战和革命注定会发生，从根本上改变内部秩序。这包括财富和政治权力的彻底重组，也包括债务、财务所有权以及政治决策权的彻底重组。</strong>这些变化是需求带来的自然结果，因为已有体制内无法实现重大变革。几乎所有的体制都会经历这些变革。因为几乎所有的体制都以牺牲一些群体为代价，让另一些群体获益，最终前者变得无法忍受，他们通过斗争来改变未来的方向。当贫富和价值观差距变得巨大，又伴随着糟糕的经济形势时，体制就不能惠及很大一部分人，这些人会通过斗争来改变体制。为了获得更多的财富和权力，最贫困的群体与拥有财富和权力、受益于现有体制的群体展开争斗。革命者希望彻底改变体制，所以他们自然愿意推翻当权者要求他们遵守的法律。这些根本性变革通常是通过内战、以暴力的方式发生的，不过如前所述，革命性变革也可能在不推翻体制的情况下以和平的方式发生</p><p><strong>●几乎所有内战都有一些外国势力参与，它们试图影响内战的结果，使其对自己有利。</strong></p><p><strong>●内战/革命何时开始并不清楚，但当人们深陷其中时，则是显而易见的。</strong>历史学家会确定内战开始和结束的日期，但这是主观臆断的。</p><p><strong>内战和革命是极其痛苦的，但往往带来结构性改革。如果其处理得当，就可以为更好的未来奠定基础。内战/革命之后将会出现怎样的状况，这取决于接下来的行动举措。</strong></p><h1 id="六、外部秩序和混乱大周期"><a href="#六、外部秩序和混乱大周期" class="headerlink" title="六、外部秩序和混乱大周期"></a>六、外部秩序和混乱大周期</h1><p><strong>●国际关系更多地取决于原始实力的动态机制。这是因为所有管理体制都需要有效且议定的：（1）法律和立法能力，（2）执法能力（如警察），（3）裁决机制（如法官），以及（4）明确和具体的结果，从而确定罪行并依法执行（例如罚款和监禁）。在外部秩序中，这些规则要么不存在，要么对国际关系的指导不如对国内关系那么有效。</strong></p><p>为了使外部秩序更加遵守规则，各国做出了多种努力（例如，建立国际联盟和联合国），但总的来说，这些努力未能奏效。因为这些组织的财富和权力比不上最强大的国家。<strong>如果一个国家的权力超过国家共同体的权力，那么权力更大的国家将会制定国际秩序</strong>。例如，如果美国、中国或其他国家的权力超过联合国，那么决定未来发展方向的就是美国、中国或其他国家，而不是联合国。因为权力胜过一切，旗鼓相当的各方极少会不经斗争就放弃财富和权力。</p><ol><li>贸易/经济战：冲突涉及关税、进出口限制和从经济上损害对手的其他方法。</li><li> 技术战：冲突涉及哪些技术可以共享，哪些技术属于国家安全保护范畴。</li><li> 地缘政治战：冲突涉及领土和联盟，解决方式是谈判和做出明确或间接的承诺（而不是战争）。</li><li>资本战：冲突涉及施加经济制裁等金融工具（例如，通过切断资金和信贷来惩罚提供资金和信贷的机构和政府），以及限制外资进入资本市场。</li><li>军事战：冲突涉及实际动武和军事力量的部署。</li></ol><p><strong>●全面展开的战争通常发生在生存问题岌岌可危（这些问题对国家来说生死攸关，以至人们愿意为之战斗和牺牲）又无法通过和平手段来解决的情况下。由此引发的战争将会确定哪一方得偿所愿，进而获取霸权地位。规则制定者的明确成为国际新秩序的基础。</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c44d7b0a6c20689f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如图所示<strong>，每个周期都包含相对较长的和平与繁荣时期（如文艺复兴、启蒙运动和工业革命），它们为残酷和暴力的外部战争（如三十年战争、拿破仑战争和两次世界大战）埋下了种子</strong>。无论是上升时期（和平与繁荣时期）还是下跌时期（萧条、革命与战争时期），这些周期都影响着整个世界。其他国家并不是随着主要大国的繁荣而繁荣，因为一些国家的繁荣是以其他国家的利益为代价的。例如，<strong>由于被西方列强和日本欺凌掠夺，中国在1840—1949年陷入衰落（被称为“百年屈辱”）</strong>。</p><p>在阅读下文时，请记住<strong>，●对于战争，最可确信的两点：（1）战争不会按计划进行，（2）战争远比想象的更糟糕。</strong>正是出于这些原因，接下来的许多原则是针对如何避免武力战争的。然而，无论出于何种理由，战争总会发生。需要澄清的是，虽然我认为大多数战争是悲惨的，而且出于荒谬的理由，但有些事情是值得为之奋战的。因为不为之奋战的后果（如失去自由）将是不可容忍的。</p><p><strong>●国内实力和军事实力密切相关。买枪炮（军事实力）需要财力，买黄油（国内社会支出需求）也需要财力。</strong>如果一个国家不能充分提供其中任何一项，就很容易受到国内外反对势力的攻击。研究中国朝代和欧洲帝国使我认识到，<strong>●在财力上超过对手是一个国家能够拥有的最大优势之一。</strong>这就是为什么美国在冷战中打败了苏联。</p><p> 如果主导大国开始衰弱，或者新兴大国与主导大国实力近乎旗鼓相当，或者两者兼具，冲突就会出现。<strong>●在以下两种情况下，爆发军事战争的风险最大：（1）双方的军事实力旗鼓相当，（2）双方在生存问题上存在不可调和的分歧。</strong></p><p> 尽管国际关系中没有规则，唯一的规则是超级大国加于自身的规则，但一些做法产生更好的结果。确切地说，那些更可能带来双赢结果的做法要好过更可能导致两败俱伤的做法。因此，一个至关重要的原则：<strong>●要想获得更多的双赢结果，双方必须进行良好的协商，既考虑到对方也考虑到自身的优先关注点，并懂得妥善地在二者之间进行权衡。[插图][插图]</strong></p><p> 通过巧妙协商、通力合作，双方可以营造双赢关系，有效地增加和分配财富与权力，这样做与一方征服另一方的战争相比，得到的收益要大得多，双方经受的痛苦要小得多。从对手的角度看问题，确认并传达给对手自己的底线（即什么是不可妥协的）是妥善处理分歧的关键。<strong>●获胜意味着在不失去最重要的东西的前提下，得到最重要的东西。所以，如果丧失的生命和金钱超过带来的益处，这样的战争就是愚蠢的。</strong>但是，愚蠢的战争仍在不断发生。我将解释背后的原因。</p><p>** ●在经济低迷时期，加征关税以保护国内企业和就业的做法很常见，但这会导致效率降低，因为生产并未发生在效率最高的地方。**最终，这会加剧全球经济疲弱，因为关税战争使加税国家的出口减少。但这一做法有利于受到关税保护的实体，也能给征收关税的领导人带来政治支持。</p><p> 雪上加霜的是，20世纪30年代，美国和苏联都出现了干旱<strong>。●天灾（如旱灾、洪灾和瘟疫）常常使国家陷入经济困境，加之其他的不利形势，酿成一段严重冲突时期。</strong>此外，由于内部政治斗争和对纳粹德国的恐惧，苏联采取了政治极端主义政策，很多人被指控从事间谍活动，遭到清洗。</p><p>** ●通缩性萧条是债务人不具备偿债所需的资金而导致的债务危机。为了应对这一危机，政府不可避免地采取印钞、债务重组和政府支出计划等措施。这些措施会增加货币和信贷供应，并降低其价值。唯一的问题是政府官员需要花多长时间才会采取这些行动。**</p><p> 以美国为例，从1929年10月股市崩盘到1933年3月罗斯福采取行动，美国花了三年半的时间。在罗斯福执政的前100天里，他出台了许多大规模政府支出计划，这些计划的资金来源是大幅增加的税收和巨额预算赤字（通过债务融资，并由美联储将债务货币化）。他推出了就业计划、失业保险、社会保障支持，以及有利于劳工和工会的各项计划。1935年实施新税法（被普遍称为“富人税”）后，最高个人边际所得税税率升至75%（1930年仅为25%）。到了1941年，最高个人所得税税率为81%，最高公司税税率为31%，而1930年的公司税税率仅有12%。此外，罗斯福也征收了其他一些税负。虽然政府征收了这些税款，经济改善也有助于税收增加，但预算赤字仍从GDP的约1%升至4%左右。这是因为支出的涨幅极大。[插图]从1933年到1936年年底，股市回报率超过200%，美国的经济突飞猛进，平均实际增长率高达9%左右。</p><p> <strong>●在经济严重承压且存在贫富悬殊的时期，国家通常会进行根本性的大规模财富再分配。以和平的方式进行，则是对富人大幅增税，显著增加货币供应（使债务人的负债贬值）；</strong>以暴力的方式进行，则是强制没收资产。美国和英国在进行财富和政治权力再分配的同时，维持了资本主义和民主制度。德国、日本、意大利和西班牙的情况则不然。</p><p> <strong>●真枪实弹的战争爆发之前，通常会有一场经济战。</strong>也像典型情况一样，在宣布全面战争之前，双方会有大约10年的经济、技术、地缘政治和资本战争，在此期间，冲突大国相互威慑，试探对方实力的极限。虽然1939年和1941年被认为是二战和太平洋战争正式打响的时间，但在之前的10年左右，冲突实际上就已经开始了。除了国家内部的经济冲突和由此而来的政治变化外，所有这些国家都面临着越来越多的外部经济冲突，各国竞相在不断缩小的经济蛋糕中争夺更大的份额。因为支配国际关系的是权力，而不是法律，德国和日本愈加走向扩张主义，在争夺资源和领土势力的竞争中，它们日趋挑战英国、美国和法国。</p><p><strong>●当国家处于弱势时，对手国家会利用其弱势为自己谋取利益</strong>。当时，法国、荷兰和英国在亚洲都拥有殖民地。欧洲的战争已经使这些国家不堪重负，它们无力帮助其殖民地抵御日本的侵略。从1940年9月开始，日本入侵了东南亚的几个殖民地，首先占领法属印度支那，其次将“南部资源区”纳入其“大东亚共荣圈”之中。1941年，日本掠夺了荷属东印度群岛的石油储备。</p><p> <strong>战时经济政策：</strong></p><p> 就像经济战争的典型战术值得注意一样，战时经济政策也同样值得关注。在战争时期，随着国家将资源的利用从赢利转向作战，政府几乎控制了一切。例如，政府决定（a）何种物品可以生产，（b）何种物品可以买卖及买卖数量（配额供应），（c）何种物品可以进口和出口，（d）价格、工资和利润，（e）个人是否可以控制自己的金融资产，以及（f）个人是否可以将自己的资金转至国外。由于战争耗资巨大，政府通常（g）发行大量债券，并将债务货币化，（h）依赖于非信贷货币（如黄金）进行国际交易，因为本国的信贷不被接受，（i）实施更加专制的政策，（j）对敌国实施各类经济制裁，包括切断资金渠道，以及（k）面临敌国对己实施的这些制裁。</p><p> 战时经济管控措施</p><p> <img src="https://upload-images.jianshu.io/upload_images/12321605-def0589861cc0d37.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p> 影响资产的管控政策</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5f50771e22077931.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-66f5ff3790df74ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>然而，<strong>并不是所有周期都需要以这样的方式结束。如果国家在富强阶段时，能够保持生产率，让收入高于支出，使体制惠及大多数人，能够与主要竞争对手建立和维持双赢关系，那么就会维持得更久</strong>。许多国家都维持了数百年，而有着245年历史的美国也验证了自己是持续时间最长的国家之一。</p><h1 id="七、从大周期的角度思考投资"><a href="#七、从大周期的角度思考投资" class="headerlink" title="七、从大周期的角度思考投资"></a>七、从大周期的角度思考投资</h1><p>主要大国的资产回报率（实际回报率，10年窗口期，年率）</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ef5a61a7809b27b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0ab56fe0626f1b46.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>的投资经历（所有主要国家）[插图]</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9a60c7bcd428be26.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>财富没收时期[插图]</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-cb5133100ceb9d80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>主要经济体的股市关闭</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d40f06e83002b533.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我在稍前解释了典型债务和资本市场大周期的运行机制。需要强调的是，在周期的上行阶段，债务增加，金融财富和负债增速超过实物财富增速，最终，债务人无法兑现对未来支付的承诺（如现金、债券和股票）。这导致了“银行挤兑”型债务问题的出现。在这种情况下，央行不得不印钞，设法缓解债务违约和股市下跌的局面。央行印钞将导致货币贬值，使金融财富价值相对于实物财富价值下跌，直到金融资产的实际价值（经通胀调整后）低于实物资产的水平。这时，周期重新开始。这是一个非常简化的描述，但情况大致如此。在周期的下行阶段，金融资产相对于实物资产的实际回报处于负值，同时经济陷入困境，这是周期的反资本、反资本主义阶段，这种状况一直持续，直到到达另一个极端。</p><p>美国金融资产占总资产的份额</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2130e556ce8b7941.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>纸币的实际回报率（相对于消费者价格指数）</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c403c50e9264472e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="八、过去500年简述"><a href="#八、过去500年简述" class="headerlink" title="八、过去500年简述"></a>八、过去500年简述</h1><p>1500年的世界与今天大不相同，但运行方式与今天无异。这是因为，虽然自1500年以来事物进化了许多，但事物的进化方式始终未变：进化的上升趋势带来了进步，大周期带来了围绕着上升趋势的波动和颠簸。</p><p>当时的世界要“大”得多。500年前，一个人骑马一天能走大约40千米。今天，人们可以在一天里旅行到世界的另一端。“阿波罗号”宇航员往返月球的时间，远远短于1500年旅行者从巴黎到罗马的时间。因此，当时的地域相关性（例如谁能影响谁的区域）要小得多，所以世界显得大得多。欧洲是一个世界，俄罗斯是另一个世界，中国及其周边区域则是一个更加遥远的世界。现在回头看起来微小而数目众多的邦国，当时看起来却全然不同。因为当时并没有像今天这样的国家边界，邻邦之间几乎不间断地发生财富和权力争斗。</p><p>控制着中国绝大部分地区的明朝是当时世界上最先进、最强大的帝国。像欧洲帝国一样，明朝实行世袭制统治，皇帝自称拥有“天命”。皇帝监督着官僚机构，官僚机构由大臣和军事领袖管理和保护；拥有土地的显贵家族管理着农民，他们与大臣和军事领袖之间存在着共生关系（不过有时会发生争执）。1500年，明朝正接近力量的顶峰，在财富、技术和权势方面远远领先于欧洲。它对包括日本在内的整个东亚都有巨大的文化和政治影响力。[插图]</p><p><strong>讽刺而同时又具有一般性意义的是，明朝巨大的财富和力量可能正是导致其最终衰落的原因之一。明朝皇帝自以为万物已备，别无他求，所以结束了探索世界的行动，关闭门户，安享生活，把管理政府的工作交给了大臣和宦官，这导致了功能失调性的内斗、腐败、虚弱和易于被攻击的脆弱。文化风气从务实的科学研究与创新转向了咬文嚼字的学术。正如我们将在第12章中看到的，这导致了中国相对于欧洲的衰落。</strong></p><p><strong>商业革命是指从单纯以农业为基础的经济形态转变为包括多种商品贸易的经济形态</strong>。这场从12世纪开始直到1500年的演进，以意大利城邦为中心，由于两个因素的共同作用，这些城邦变得极为富有。首先，基督教欧洲与奥斯曼帝国之间的战争使欧洲与世界其他地方的陆路贸易（尤其是香料和奢侈品贸易）的发展大大减缓，这为海上贸易创造了巨大的机遇。其次，一些意大利城邦发展出了以罗马共和国为模板的共和制政府。与欧洲其他地方的政府相比，这些城邦的政府创造力更强，反应更灵敏，这为强有力的商人阶层的发展创造了条件。</p><p><strong>1300年左右，意大利各城邦开始出现一种新的思维方式，这种思维方式在很多方面以古希腊和古罗马为模板。从那时到17世纪，这种思维方式在欧洲流传。这一时期被称为文艺复兴时期。</strong>文艺复兴时期的思想家做出了一个重大转变，以逻辑推理而不是神意来解释世界的运转。这个转变促成了一系列飞快的新发现，给欧洲带来了艺术和技术进步。文艺复兴始于意大利北部的各城邦，在那里，之前的商业革命已创造巨大财富，依靠理智主义和创造活动的帮助，这些财富带来了贸易、生产和银行业的进步。文艺复兴是历史上最重大的自我强化循环的案例之一，我在第5章中描述过这种循环：和平时期里，创造力和商业彼此促进，带来经济繁荣和巨大进步。</p><p><strong>探险时代开始于15世纪，当时欧洲人在世界各地旅行，寻找财富，历史上第一次在许多不同的民族之间建立起广泛的联系，并开始使世界变小。这个时代与文艺复兴时期大致重合，因为文艺复兴非凡的技术成就转变为造船和航海技术的进步，而远航船带回来的财富为文艺复兴的进一步发展提供了资金。</strong></p><p><strong>各国的统治家族支持这些赚钱的探险活动，并与探险者分享利润。</strong>例如，葡萄牙国王的兄弟航海家亨利赞助了一些最早的航行，并在非洲和亚洲建立了一个贸易帝国。西班牙紧随其后，迅速征服了西半球的大片土地，将其变为殖民地，包括盛产贵金属的阿兹特克帝国和印加帝国。尽管葡萄牙和西班牙是竞争对手，但尚未被探索的世界还很广阔，于是当它们发生纠纷时，最终调和了。在16世纪，西班牙是哈布斯堡王朝的一部分，并控制着利润丰厚的银矿，因此强于葡萄牙，而且在从16世纪晚期开始的约60年里，哈布斯堡王朝的君主也统治着葡萄牙。两国都把财富转化为了艺术与技术的黄金时代。西班牙帝国领土日益广阔，最终被称为“日不落帝国”，这一名称后来被用于形容大英帝国。</p><p><strong>明朝曾有过自己的探险时代，但最终放弃了。</strong>从15世纪初开始，明朝的永乐皇帝授权他最信任的海军将领郑和领导了7次大规模的海上远航，游历世界，这被称为“郑和下西洋”。尽管这些远航不是殖民远征（对于其在多大程度上是商业航行，历史学家仍有争议），但这些航海行动扩大了中国的海外影响力。永乐皇帝的海军是世界上规模最大、技术最先进的海军，船舶之大，建造之精良，是欧洲任何国家在接下来的至少100年里所望尘莫及的。</p><p><strong>宗教改革（1517—1648年）在欧洲，从16世纪开始，新教宗教运动发起了一场反对罗马天主教会的革命，这导致了一系列战争，以及当时欧洲的既存秩序被推翻。如前所述，当时的既存秩序由君主、贵族和教会组成，三者结为共生关系。</strong>宗教改革针对罗马天主教会的权势和腐败，寻求建立一种独立的宗教，人们直接与上帝沟通，而不是以教会的规则为中介。当时，许多天主教主教及其他高级教士过着宫殿里的王公一样的生活，天主教会出售“赎罪券”（据说这能缩短人们必须待在炼狱里的时间）。当时罗马天主教会既是宗教，也是国家，直接统治着现代意大利领土的相当大一部分（教皇国）。</p><p>19世纪中期共产主义出现并发展起来，这是对以下因素的回应：资本主义；资本主义带来的贫富差距；工业革命的好处更多地为新技术的拥有者所获得，而不是为工人所获得。19世纪末20世纪初，共产主义者与权力既有者之间的冲突加剧，导致了20世纪的一系列重大革命，包括俄国十月革命和中国革命，共产主义者在这两国执政。</p><h1 id="九、荷兰帝国与荷兰盾的大周期兴衰"><a href="#九、荷兰帝国与荷兰盾的大周期兴衰" class="headerlink" title="九、荷兰帝国与荷兰盾的大周期兴衰"></a>九、荷兰帝国与荷兰盾的大周期兴衰</h1><p><strong>荷兰帝国崛起的原因，就是前文阐述过的所有典型原因。</strong>荷兰在1650年左右达到顶峰，那个时代现在被称为荷兰的黄金时代。虽然荷兰人口不多，领土狭小，无法成为欧洲大陆上的主导性军事大国，但荷兰凭借经济实力、复杂的金融体系和强大的海军，完全弥补了这一劣势。强大的海军能够保护在世界各地拥有交易站和殖民地的荷兰帝国。这使荷兰盾成为历史上第一种全球储备货币。</p><p>下页第一张图展示了推动荷兰崛起并导致其最终衰落的8种实力。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d4c6906e1ee0c63c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>荷兰的发展轨迹（1550—1850年）</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-269c6a1167860b20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>1519—1556年，神圣罗马帝国的皇帝和哈布斯堡王朝的君主是查理五世。他控制的领土包括当今荷兰、比利时、意大利、德国、奥地利和西班牙的大部分地区，这使哈布斯堡王朝成为欧洲最强大的家族统治的朝代。由于在探险时代获得的财富和力量，西班牙尤其强大。[插图]西班牙舰队显然是当时欧洲最强大的海军。西班牙银币几乎成了一种储备货币，连遥远的中国都在使用它。16世纪中期，情况开始变化，哈布斯堡王朝强盛阶段播下的衰落种子开始发芽，一场革命性的力量转移开始酝酿。</p><p>哈布斯堡王朝的衰落符合许多典型的情况。没有财富和权力的人向拥有它们的精英发动了革命，挑战现存秩序。如我在前一章中所述，新的宗教思想以宗教改革的形式出现，这是一场反对罗马天主教会的革命，罗马天主教会被视为腐朽的剥削势力。当时天主教会和神圣罗马帝国是富有而强大的政治力量，是现存秩序的组成部分。当一群被统称为新教的宗教反对派群体挑战现存体系时，这场革命爆发了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e66c9da7cc36bc3a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>从1581年到1625年左右，荷兰帝国兴起</strong>，这一过程遵循着帝国崛起的典型步伐，第1章中对此有过描述。更具体地说：</p><ul><li>在“沉默者威廉”的领导下，荷兰在八十年战争中反抗西班牙统治，最终胜利，1581年荷兰共和国独立。基本上是荷兰国父的威廉是一位有才干的军事指挥官，他联合了荷兰各省反抗西班牙。</li><li>尽管在之后的几十年里西班牙人和荷兰人继续交战，但荷兰人已获得了独立，一个更为统一的荷兰共和国崛起的种子已经播下（尤其是费利佩二世切断了与荷兰的贸易，这迫使荷兰人依靠自身力量在海外拓展）。</li><li>因为荷兰共和国的建立是为了让每个省保持高度的主权，所以荷兰帝国的崛起是由一群政治家推动的，而不是由一个君主或领导人推动的。尽管贵族扮演着最重要的角色，但这个体系带来了制衡和被证明行之有效的伙伴关系。</li><li>荷兰价值观和文化重视教育、储蓄、才能和宽容。• 脱离西班牙使荷兰能建立一个更开放和更具创造性的社会。</li><li>荷兰人发明的船舶能周游世界收集财富。能为这些活动及其他生产性活动提供资金的资本主义以及许多别的突破，使荷兰变得富裕而强大。荷兰人创建了世界上第一家大型公司——荷兰东印度公司，其贸易额占到了全世界的1/3左右。[插图]荷兰人对新思想、新人才和新技术的开放态度帮助他们快速崛起。</li><li>为支持贸易，荷兰政府增加了军事投资，从而在一系列军事冲突中遏止英国，并且控制了更多的贸易。</li><li>荷兰人还创造了除金银之外世界上最早的储备货币荷兰盾。荷兰盾的基础是一套具有创新性的银行和货币体系，该体系是通过阿姆斯特丹银行的成立而形成的。[插图]</li></ul><p>重大发明（每100万人口）</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e897144bcf30982d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>人均实际GDP（以2017年美元为基准）</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-258727692cef1e73.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>重申一遍，荷兰人两个最重要的发明是：（1）效能极其优异的帆船，使他们能在世界各地航行，这一点再加上他们从欧洲战事中获得的军事技能，使他们能收集巨大的财富；（2）为这些行动提供推动力的资本主义。</strong></p><p>荷兰人发明了我们所知的资本主义。对荷兰和全世界来说，这都很了不起，但像大多数伟大发明一样，这也伴随着一些潜在的致命后果。尽管生产、贸易和私有制以前就存在，但大量的人能通过公开股票市场，集体购买以营利为目的的企业的所有权，这在以前并不存在。1602年，荷兰人创立了世界上第一家公开上市的公司（荷兰东印度公司）和第一家股票交易所，从而开了先河。</p><p>像大多数发明一样，这些资本市场方面的发展也源于必要性和自利需求。寻找新贸易路线的世界远航是有风险的事业，所以商人有理由把一些与远航相关的风险出售给其他人，交换条件是获取未来利润中的一部分。16世纪中期，荷兰人在远航事业中引入了股权，这具有革命性。在1600年之前，这些股份只由少数商人持有，在很大程度上缺乏透明性，流动性差，所以这些股份对外部投资者的吸引力有限</p><p>1602年8月阿姆斯特丹股票交易所的成立，以及荷兰东印度公司的上市，使持股人的范围变得更加广泛（持有股票的荷兰成年人的比例超过2%）。而且该交易所对股票的持有和转让有明确规定，使市场变得更透明。荷兰东印度公司是具有同等革命性的发明。这是世界上第一家跨国公司，具有今日公司的许多特征，包括股东、公司标识、董事会等。资本市场使投资者能够储蓄，商人能够筹集资金，人人都有机会进入一个具有流动性的市场，在这个市场中，人们可以轻松、有效地进行资本转移，这催生了一个财富积累的新时代。荷兰东印度公司在18世纪初发展到顶峰，其股息占荷兰总GDP的近1%。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b6616d49b0ad35d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>（A）荷兰宣布脱离西班牙独立。</p><p>（B）荷兰东印度公司、阿姆斯特丹银行和阿姆斯特丹股票交易所成立。</p><p>（C）第一次和第二次英荷战争。</p><p>（D）七年战争和1763年的影子银行危机。</p><p>（E）第四次英荷战争，阿姆斯特丹银行挤兑。</p><p>（F）荷兰东印度公司被国有化，荷兰帝国衰亡。</p><h1 id="十、大英帝国和英镑的大周期兴衰"><a href="#十、大英帝国和英镑的大周期兴衰" class="headerlink" title="十、大英帝国和英镑的大周期兴衰"></a>十、大英帝国和英镑的大周期兴衰</h1><p><strong>世界秩序的变化发生于两个或两个以上实力相当的国家（或国家联盟）斗争时，一方取胜，成为主导者，有能力制定新的规则，这就是新的世界秩序。</strong>在这之前，崛起国需要让本国实力达到与霸权国相当的水平，所以任何大国都是在远未成为大国之前，就开始本国的崛起之旅的。同样，在任何大国不再是大国之后的很长时间里，其衰落之旅还在延续。这一点反映在我之前分享过的弧形图里，那张图展示了荷兰帝国、大英帝国、美帝国与中华帝国兴衰周期的简化版本。我在此处再次分享。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-86615da2280742ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>英国关键决定因素的指标</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2f8848221e5c8700.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>虽然随着18世纪后期荷兰的衰落，英国首要的贸易和金融竞争对手失势了，但直到19世纪早期，英国才完成崛起，因为它还要打败最后一个强大的对手——拿破仑领导的法国。拿破仑急于通过拿破仑战争征服欧洲，成为第一强国。这就带来了我在第2章附录里描述过的常见的大国竞争和均势斗争态势，联盟的组建和局势的升级都进入了一个大高潮。为解释大英帝国的崛起，本章稍后将简要介绍法国的情况，法国的历程也具有典型意义。但在这里我直接跳到重点：英国通过有效的经济战和军事战取得了胜利。接着，遵循确立主宰权的战争之后的典型大周期脚本，出现了一种由胜利者建立的新世界秩序，之后是一段长期的相对和平与繁荣，在这个案例里是100年。这正是英国成为有史以来最大帝国的时候。在英国鼎盛时期，其人口只占全世界的2.5%，但大英帝国创造了全世界总收入的20%以上，并控制着世界上20%以上的陆地和25%以上的人口。</p><p>1600年至今英国的发展轨迹</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3cad4877825e40ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>为阐述英国兴起的背景，我们需要描述17世纪末英国所处的形势，以及大的欧洲背景。对英国和欧洲来说，17世纪早期发生了大规模的冲突，大大改变或者颠覆了之前的所有秩序。如上一章所述，<strong>三十年战争之后，欧洲发生了大规模的破坏和变化，因为这是一场不同意识形态、宗教和经济阶层之间的战争，最终通过《威斯特伐利亚和约》形成了新的欧洲秩序。该和约确立了我们今天所知的国家，并造成了一个分裂的欧洲，导致不同国家做出不同的选择</strong>。英国国内也为争夺财富和权力而发生混乱，即英国内战，这场残酷暴力的战争是之前几百年里不同阶层斗争的延续，接着是光荣革命，它以不那么暴力的形式使荷兰统治者威廉三世成为英格兰国王。这些冲突的共同之处是都削弱了君主并强化了议会，也为英格兰、苏格兰和爱尔兰三国的关系奠定了基础。英国内战导致国王（查理一世）被审判和处决，君主政体被由英格兰、苏格兰和爱尔兰组成的联邦取代，其统治者是领导反王室起义的将领奥利弗·克伦威尔。</p><p><strong>工业革命：受过良好教育的人口，重视发明创造的文化，新想法（尤其是关于如何改进机器的效率，更好地完成此前靠大量人力完成的工作）的发展能够得到资本的经济支持，这些因素创造了一大波竞争力提升与繁荣。英格兰在地质上拥有丰富的铁和煤，这极大地推动了这场被称为第一次工业革命的经济转型。如第8章所述，这场转型改变了欧洲。此前欧洲主要是一个农村和农业社会，大多数人很贫困，权力掌握在地主精英手中。此后欧洲成为城市化和工业化社会，民众整体上富裕多了（尽管精英获得了不成比例的高收益），权力掌握在中央政府官僚和资本家手中。从地缘政治上看，英国取得这些实力后，得以在1750年左右超过荷兰，成为欧洲首屈一指的经济和金融强国。30年后，英国在战争中击败荷兰，明显地成为世界第一帝国。</strong></p><p>这场生产力革命始于农业领域。农业发明提高了生产率，降低了农业的劳动强度。这还使食物种类更丰富、价格更便宜，从而使人口激增。这些因素共同促使人们涌向城市，劳动力供给不断增加，从而惠及工业。工业革命的推动力不仅是蒸汽机等全新的发明，还包括对现有概念的调整和改进，以提高生产效率，例如标准化的投入和将生产从个体工匠那里转移到工厂。充足的劳动力和能源以及相互联系的全球市场，共同推动了创新的爆发。下面列出了英国创新的时间和节奏。</p><ul><li>1712年：蒸汽机发明。</li><li>1719年：丝绸工厂建立。</li><li>1733年：飞梭纺织机（基础性纺织机）发明。</li><li>1764年：珍妮纺织机（多轴纺织机）发明。</li><li>1765年：（用于蒸汽机的）的分离式冷凝器发明。</li><li>1769年：水力纺纱机发明，蒸汽机得到改进。</li><li>1785年：动力织布机发明，铁精炼技术得到发展。</li><li>1801年：带轮子的蒸汽机车发明。</li><li>1816年：铁路蒸汽机车发明获得专利。</li><li>1825年：连接曼彻斯特和利物浦的铁路开始建设。</li></ul><p>通过农业和工业方面革命性的变化，欧洲变得城市化和工业化，商品由城市工厂里的机器生产。新的城市人口需要新型的商品和服务，这要求政府变大，花钱来促进住房、卫生、教育等事业，并建设新的工业资本主义体系发展所需要的基础设施，如法院、监管机构和中央银行。<strong>权力转移到了中央政府的官僚和控制生产资料的资本家手中。</strong></p><p>这一点在英国最为明显，<strong>英国有许多最重要的创新，并利用新的生产方法领先于其他国家，成为世界第一超级大国。</strong>正如人均经济产出所显示的，英国的生活水平在1800年左右赶上荷兰，并于19世纪中期超过荷兰，当时英国在世界经济产出中所占的份额接近顶峰（20%左右）。伴随经济增长的是英国成为世界上占主导地位的贸易国家，在18世纪晚期远超荷兰，并在19世纪一直保持这一地位，这一地位又促进了英国的经济增长。同时，在19世纪的大部分时间里，所有国家的经济产出都在加速增长。当时世界上大多数国家处于内部秩序周期的第三、第四阶段。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fe217daae29103e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>自然，随着英国成为世界经济强国，它需要具备军事斗争能力，以保护和维持自身的利益。<strong>英国的军事实力，特别是海军，帮助它建立殖民地，夺取其他欧洲国家的殖民地，并确保对全球贸易路线的控制。帝国的盈利大于军费开支，因为军费开支支持了经济活动。</strong>由于英格兰银行的金融创新和荷兰盾的衰落，伦敦成为世界金融中心，英镑成为世界储备货币。换言之，大英帝国在沿着崛起帝国的典型大周期步伐前进。</p><p>自然，随着英国成为世界经济强国，它需要具备军事斗争能力，以保护和维持自身的利益。英国的军事实力，特别是海军，帮助它建立殖民地，夺取其他欧洲国家的殖民地，并确保对全球贸易路线的控制。帝国的盈利大于军费开支，因为军费开支支持了经济活动。由于英格兰银行的金融创新和荷兰盾的衰落，伦敦成为世界金融中心，英镑成为世界储备货币。换言之，大英帝国在沿着崛起帝国的典型大周期步伐前进。</p><p>20世纪40年代通常被认为是英镑的“危机岁月”。因为二战，英国不得不从盟国和殖民地大量举债，这些债务必须以英镑偿付。二战结束时，如果不增税或削减政府支出，英国就无法履行偿债义务，因此英国不得不要求前殖民地不得主动出售英国债务资产（即英国发行的债券）。美国迫切希望英国尽快恢复英镑的可兑换性，因为英国的限制措施在减少全球经济中的流动性，影响美国的出口利润。英格兰银行也迫切希望取消资本管制，以恢复英镑作为全球交易货币的地位，增加伦敦金融业的收入，并鼓励国际投资者继续储蓄英镑。1946年，一项协议达成，美国向英国提供37.5亿美元的贷款（约为英国GDP的10%），以缓冲可能出现的英镑遭抛售的情况。和人们预料的一样，当1947年7月部分可兑换实行时，英镑承受了相当大的抛售压力，英国和英镑区国家转而采取紧缩政策，以维持英镑与美元的挂钩。奢侈品进口受到限制，国防开支大幅削减，美元和黄金储备减少，英镑区国家彼此达成协议，不以增持美元的方式将外汇储备多元化。克莱门特·艾德礼首相发表了激动人心的演讲，呼吁国民发扬战时牺牲精神。</p><p>演讲一结束，对英镑的抛售就加快了。到8月底，英镑的可兑换性被暂停，这让美国以及在实行可兑换性前夕购买了英镑资产的其他国际投资者很愤怒。比利时国家银行行长威胁要停止英镑交易，为此相关方不得不进行外交干预。两年后，英美两国的决策者认识到，英镑无法在现有的汇率下恢复可兑换性，于是英镑贬值。接着英国的竞争力恢复，经常账户改善，到20世纪50年代中后期，英镑的完全可兑换性恢复。下图反映了这一情况。</p><p>尽管1949年英镑的贬值在短期内对形势有所帮助，但英镑面临着反复出现的国际收支紧张。国际决策者们对此颇感忧虑，他们担心英镑价值崩溃或迅速转向美元会对新的布雷顿森林货币体系造成巨大损害（尤其是在冷战和国际上担忧共产主义的背景下）。因此，各方做出了许多努力来支撑英镑，维持其作为国际流动性来源的地位。此外，英国规定共同市场内的所有贸易都必须以英镑计价，共同市场内的所有货币都要与英镑挂钩。结果是，在20世纪50年代和60年代初，英国在准确意义上是一个区域性经济强国，英镑是区域性储备货币。但那些措施仍未能解决问题：英国负债太多，竞争力太弱，无法在还债的同时进口所需的东西。1967年，英镑不得不再次贬值。在那之后，即使是英镑区国家也不愿持有英镑储备，除非英国以美元担保英镑的潜在价值。</p><p><strong>二战之后的欧洲：正如我们反复看到的，战争的可怕代价促使各国在战后建立新的世界秩序，以确保这样的战争不再发生。很自然地，新的世界秩序围绕着胜利者展开，这通常是新崛起的帝国。二战后，这显然是美国。</strong></p><p>战后秩序最重要的地缘政治因素包括：</p><ul><li>美国是主导性大国，因而成为事实上的世界警察。很自然地，美国和世界第二强国苏联的关系几乎立即紧张起来。美国及其盟国成立了一个军事联盟，名为北大西洋公约组织，苏联阵营的国家则成立了华沙条约组织，双方在冷战中对峙。</li><li>联合国成立以解决全球争端。和通常的情况一样，其总部位于崛起帝国的心脏（这一案例中为纽约），其主要权力机构安全理事会由战胜国主导，这也符合通常的情况。</li></ul><p>新世界秩序最重要的金融元素包括：</p><ul><li>布雷顿森林货币体系确立美元为世界储备货币。</li><li>国际货币基金组织和世界银行，旨在支持新的全球金融体系。</li><li>纽约成为新的全球金融中心。</li></ul><p><strong>从欧洲的视角看，新世界秩序的关键是这样的转变：原来是一种均势，最强大的欧洲国家位于顶端；现在则是欧洲国家均筋疲力尽，新的超级大国比任何欧洲国家都强大，欧洲光芒不再（特别是在欧洲的殖民地获得独立的情况下）。</strong>鉴于这些压力，以及两次世界大战留下的清晰教训，即分裂会带来巨大代价，欧洲团结的价值显而易见。这成为建立新欧洲秩序的动力，该新秩序逐渐发展为欧盟。</p><h1 id="十一、美国和美元的大周期兴衰"><a href="#十一、美国和美元的大周期兴衰" class="headerlink" title="十一、美国和美元的大周期兴衰"></a>十一、美国和美元的大周期兴衰</h1><p>美国关键决定因素指数</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-32c01d02df89504e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>1750年至今美国的发展轨迹</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2cb686857a8dfa5a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>和所有新国家及王朝一样，<strong>美国经历了通常的革命和革命后进程，建立了一个新的国内秩序。具体过程如下：（1）一个由强有力的领导人组成的协调组织为获得控制权而战斗；（2）这个组织取胜并巩固控制权；（3）新的领导层拥有一个得到民众支持的愿景；（4）新的领导层分裂为不同的派系，就政府应如何行动来实现这一愿景的问题产生冲突；（5）最终这些派系设计出控制系统并将其列明在协议中（在美国的案例中，首先是在《邦联条例》中，其次是在《美国宪法》里）；（6）建立政府的各个部分（如货币和信贷系统、法律系统、立法体系、军队等）；（7）任命人员履行职务，让系统顺利运行。</strong>美国以一种独特的和平方式做到了这一切，它通过谈判、对协议近乎完全的尊重以及良好的治理设计，给国家开了一个好头。</p><p>南北战争之后发生了第二次工业革命，那是一个典型的时代。那段时期，在英国、欧洲大陆和美国，对财富和繁荣的和平追求创造了收入、技术和财富的巨大收益。</p><p><strong>在美国，实现这些收益的资金来自自由市场资本主义体系，和通常的情况一样，该体系既创造了大量财富也导致了巨大的贫富差距。</strong>贫富差距导致了不满和进步时代的政策，这些政策打破了富有和强大的垄断集团（“取缔托拉斯”），并增加了对富人的税收，这一过程始于1913年通过的宪法修正案，其允许征收联邦所得税。美国实力的增强体现为占全球经济产出和世界贸易的份额上升、金融实力的不断增强（以纽约成为世界领先的金融中心为代表）、在创新方面的持续领导地位，以及美国金融产品的大量应用。</p><p>美元成为世界主要储备货币的发展过程远非一帆风顺。在美国建国后最初的100年里，其金融体系完全不发达。如我在第3章和第4章中所述，当时美国的银行业是以和大多数国家一样的典型方式运作的。换言之，硬通货被存入银行，而银行贷出的货币总额远多于自身拥有的货币量。这场庞氏骗局被揭穿后，银行就无法履行自己的承诺，并将资金贬值。美国没有央行来控制金融市场或充当最后贷款人。<strong>美国经历了许多繁荣/萧条周期，典型过程是一场债务融资的投资高潮（投资于土地、铁路等）扩张过度，导致信贷损失和信贷紧缩</strong>。因此，银行系统恐慌成为极为常见的情况。单在纽约，1836—1913年就发生了8次大规模的银行恐慌。区域性银行恐慌也很常见。这是因为高度分散化的银行系统持有的货币量是固定的，没有存款保险制度，储备系统是金字塔式的（少量的大型银行在纽约充当“通汇银行”或者说为全美的大部分银行持有储备），这种系统加剧了单家银行破产所产生的传染风险。</p><p><strong>和伦敦一样，纽约在成为全球金融中心之前早已确立了自己的贸易中心地位，这个变化是在20世纪初之后才发生的。</strong>1913年，只有两家美国银行跻身全球最大的20家银行之列，分别排在第13位和第17位。相比之下，英国银行占据了9个席位，包括最大的5家中的3家。要注意的是，当时美国的经济产出远高于英国，在出口市场上所占的份额也与英国不相上下。</p><p>与欧洲和英国市场相比美国经济更具活力、变化更快这一点，也反映在美国股市上。美国股市从南北战争结束后开始快速增长。如前所述，19世纪下半叶是和平与繁荣时期，被称为“第二次工业革命”、“镀金时代”和“强盗贵族时代”，因为这一时期资本主义和创新蓬勃发展，贫富差距大幅扩大，腐朽颓废现象明显，怨恨不断积累。这种情况引起的反应开始于1900年左右，1907年发生了一场典型的债务危机。这样的动荡导致了1913年美联储中央银行体系的诞生。到1910年，美国的股票市值已经超过英国。新的行业和企业迅速崛起，如美国钢铁公司，该公司成立于1901年，短短15年后就成为美国市值最高的公司。</p><p><strong>接着发生了一战，从1914年开始到1918年结束，几乎没有人预料到这场战争会发生，更没有人预料到它会持续这么久</strong>。在一战的大部分时间里，美国不是参战方，而且是唯一一个在战争期间维持黄金可兑换的大国。不仅战争严重损害了欧洲的经济和市场，而且欧洲各国政府采取的政策也进一步削弱了人们对各国货币的信心。相反，一战提高了美国金融与经济的相对地位。盟国的战时债务主要是向美国举借的，这使更多的人使用美元来为全球政府债务计价。</p><p><strong>按照标准的剧本，战胜国（这次是美国、英国、法国、日本和意大利）在战后聚在一起建立新的世界秩序。那场会议被称为巴黎和会，召开于1919年年初，持续了6个月，最终签订了《凡尔赛和约》</strong>。根据该和约，战败国（德国、奥匈帝国、奥斯曼帝国和保加利亚）的领土被瓜分，被置于战胜国的控制之下。根据规定，战败国要偿付战胜国很多债务，以补偿战胜国的战争费用。这些债务以黄金偿还。</p><p>在地缘政治上，美国也得益了，因为美国对塑造新的世界秩序发挥了关键作用，尽管当时英国在继续扩张和监管其全球殖民帝国，<strong>美国依然更倾向于孤立主义</strong>。一战后初期的货币体系处于不稳定状态。虽然大多数国家在努力恢复黄金的可兑换性，但在经历了一段时期的急剧贬值和通胀之后，货币对于黄金的稳定性才实现。</p><p><strong>和通常的情况一样，战后，随着新世界秩序的出现，一段和平与繁荣时期到来了，这是由伟大的创新、生产力和资本市场繁荣推动的，资本市场繁荣导致上升时期出现巨额债务和巨大的贫富差距。在“咆哮的20年代”，人们为购买投机性资产（尤其是股票）而发行了大量债券（即提供可兑换为黄金的纸币的承诺）。为了遏制这种趋势，美联储在1929年收紧了货币政策，从而导致了泡沫的破裂和全球大萧条的开始。这使几乎所有国家都陷入经济困境，并引发了各国内部和各国之间围绕财富的争斗，最终在10年后导致热战爆发。</strong></p><p><strong>在相对意义上，美国是大赢家，因为美国在战前和战争期间出售了很多东西，出借了很多钱，而且基本上所有战斗都发生在美国本土以外，所以美国没有受到实际损害，而且与大多数其他主要国家相比，美国的死亡人数较少。</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-22944aebe8b8d60b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>正如我在第6章中阐释过的，与国内治理相比，<strong>●国际关系更多地受到原始动力驱动。这是因为国家内部有法律和行为准则，而国家之间最重要的影响因素是原始力量，法律、规则甚至相互达致的条约和仲裁组织（如国际联盟、联合国和世界贸易组织）都不太重要。因此拥有强大的军队和军事联盟是非常重要的。1949年，属于美国阵营的12个国家成立了军事联盟——北大西洋公约组织（后来又有更多国家加入）。1954年，美国、英国、澳大利亚、法国、新西兰、菲律宾、泰国和巴基斯坦成立了东南亚条约组织。苏联阵营中的8个国家在1955年成立了华沙条约组织。</strong></p><p><strong>核武器库存（核弹头，取对数值）</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5d69ada8978acc1f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>●因为军费开支会占用政府的钱，使政府无法把这些钱投入社会项目。因为军事技术与私营部门的技术是相辅相成的，所以对领先的大国来说，最大的军事风险是它们输掉经济战和技术战。</strong></p><p>就战后的新货币和经济体系而言，存在着分别以美国为首和以苏联为首的两个阵营，尽管也有一些不结盟国家，有自己的没有得到广泛接受的不结盟货币。1944年，44个国家的代表聚集在新罕布什尔州的布雷顿森林，建立了一个将美元与黄金挂钩、将其他国家的货币与美元挂钩的货币体系。苏联的体系是建立在卢布的基础上的，而没有哪个国家想要卢布。●国家之间的交易和各国国内交易存在很大不同。各国政府想要控制本国境内使用的货币，因为通过增加和减少货币供给、借贷成本和货币价值，各国政府能拥有巨大的影响力。</p><p><strong>由于布雷顿森林协定，美元成为世界上最主要的储备货币。这很正常，因为两次世界大战使美国成为迄今为止最富有和最强大的国家。</strong>到二战结束时，美国积累的黄金/货币储蓄达到有史以来的最大值，约占全世界政府持有的黄金/货币的2/3，相当于8年的进口购买金额。即使在二战结束后，美国仍继续通过出口赚了很多钱。</p><p><strong>欧洲和日本的经济毁于二战。作为解决方案，并为了对抗共产主义的传播，美国为它们提供了大规模援助（即马歇尔计划和道奇计划），这些援助（a）对这些受战争破坏的国家有好处；（b）对美国经济有好处，因为这些国家用这笔钱购买美国商品；（c）对美国的地缘政治影响力有好处；（d）有助于巩固美元作为世界主要储备货币的地位。</strong></p><p><strong>美国拥有足够的资金来改善教育，发明令人难以置信的技术（例如登月技术），并做许多别的事情。</strong>股市在1966年达到最高点，这标志着长达16年的好日子的结束，尽管当时没有人知道这一点。大约在那时，我自己与历史事件的直接接触开始了。1961年，12岁的我便开始投资。当然，当时我并不知道我在做什么，也没有意识到我们那一代人有多幸运。我的出生可谓天时地利。当时美国是全球领先的制造业国家，因此劳动力宝贵。大多数成年人能得到一份好工作，他们的孩子能接受大学教育，不受限制地向上发展。因为大多数人是中产阶级，所以大多数人很快乐。</p><p>美国采取了所有典型的做法，帮助世界变得更加美元化。美国的银行增加了在国外市场的业务和贷款。1965年，只有13家美国银行拥有国外分行；到1970年，79家美国银行拥有国外分行；到1980年，几乎每家美国大银行都至少拥有了一个国外分行，国外分行总数增至787家。全球信贷繁荣。但也和通常的情况一样，（a）那些发财的人做得过火，在财务上鲁莽行事；（b）全球竞争尤其是来自德国和日本的竞争加剧。结果，随着贸易顺差消失，美国的借贷和财政状况开始恶化。</p><p><strong>美国人从来没有想过太空计划、“向贫困宣战”和越南战争要花多少钱。因为他们觉得自己很富有，而且美元的储备货币地位似乎很稳定，美国人认为他们可以无限期地实行“枪炮加黄油”的财政政策。</strong>20世纪60年代接近尾声时，美国的实际GDP增长率接近于零，通胀率达到6%左右，短期政府利率约为8%，失业率约为4%。20世纪60年代，美国股票的年回报率为8%，债券则落后，与股票波动率匹配的债券的年回报率为—3%。官方黄金价格以美元计算依然是固定的，市场价格在20世纪60年代后期略有回升，大宗商品价格继续疲软，年回报率为1%。</p><p><strong>后布雷顿森林体系</strong></p><p><strong>在1971年美元和其他货币与黄金脱钩后，世界转向一种非锚定的法定货币体系（或者说是第三类货币体系，正如我在第3章中解释的那样），而且美元相对于黄金、其他货币、股票都贬值了，最终几乎相对于一切都贬值了。</strong>这个新货币体系是由美国、德国和日本的主要经济政策制定者共同协商建立的。[插图]当尼克松切断美元与黄金的联系时，保罗·沃尔克是尼克松政府中负责国际货币事务的副国务卿，他在1979—1987年担任美联储主席。一直以来，他在塑造和引导以美元为基础的货币体系方面做得比任何人都多。我非常幸运与他熟识，所以我可以亲自证明他具有伟大的人格和高超的能力与影响力，而且为人谦逊。在一个缺乏英雄/榜样的世界里，特别是在经济公共服务领域，他是一位典型的英雄/榜样。我认为他和他的思想值得更多研究。</p><p><strong>●经济和政治在极端程度不同的左翼和右翼之间摇摆，这是因为任何一个极端的过度行为都变得令人难以接受，同时对反向极端的问题的记忆逐渐淡去。这好比时尚——领带的宽度和裙子的长度随时间的推移而变化。</strong>如果一种极端情况非常流行，人们就应该预期不久之后将朝着相反方向发生类似规模的变动。收紧货币的政策压垮了债务人，并使借贷减少，导致世界经济陷入大萧条以来最严重的低迷。美联储慢慢开始降息，但市场继续下跌。1982年8月，墨西哥出现债务违约。有趣的是，美国股市应声反弹。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-16a54801b0012279.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>2016年，唐纳德·特朗普这位直言不讳的商人、资本家、右翼民粹主义者为了当选总统，掀起了一场针对政治建制派和“精英阶层”的反抗，承诺给那些持有保守主义价值观、失业、生活窘迫的人提供支持。</strong>之后，他削减了公司税，并实行大额预算赤字，美联储也对此提供了支持。虽然这种债务增长为市场和经济相对强劲的增长提供了资金，并使低收入群体的生活有所改善，但随之而来的是贫富差距和价值观差异的进一步扩大，导致“穷人”对“富人”越发不满。与此同时，政治鸿沟加大，一边是日益极端的共和党人，另一边是日益极端的民主党人。下面的两张图反映出上述情况。下图显示参众两院共和党人的保守程度和参众两院民主党人的自由主义程度相对于过去的情况。我们从这一数据可以看出，这两个党派都变得更加极端，他们之间的分歧也变得比以往任何时候都大。虽然我不确定情况是否确实如此，但我认为这一看法基本上是正确的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a06772c4b556ed61.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>历史告诉我们，<strong>●更严重的两极分化相当于两种情况：（a）出现政治风险的僵局更大，这使通过革命性变革纠正问题的可能性降低；（b）某种形式的内战/革命。</strong></p><p>在第5章中，我描述了反映从第五阶段升级到第六阶段可能性的典型标志。我现在看到了三个最重要的标志：<strong>（1）规则被无视，（2）双方相互的情绪化攻击，（3）发生流血事件。</strong></p><h1 id="十二、中国和人民币的大周期兴起"><a href="#十二、中国和人民币的大周期兴起" class="headerlink" title="十二、中国和人民币的大周期兴起"></a>十二、中国和人民币的大周期兴起</h1><p><strong>中国文化是指中国人先天固有的期望，即家庭和社会之间应当如何相处；领导者应如何领导，追随者应如何追随。它们是经过中国几千年许多统治王朝的兴衰、儒家哲学以及其他信仰的发展而形成的。</strong>我反复看到这些典型的中国价值观和操作方式的体现，例如曾长期担任新加坡总理的李光耀和启动中国改革开放的邓小平在经济和领导力方面的做法。他们都将儒家价值观与市场经济相结合，其中邓小平创立了社会主义市场经济理论。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-76451dfb0742f51a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-31d53add51b6b130.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>英国和美国的周期是从崛起开始，然后进入漫长的衰落，但中国过去200年的周期是在漫长的衰落之后出现快速崛起。虽然顺序是相反的，但周期的驱动力是一样的。8种实力指标中有7种在1940—1950年跌至最低点。在那之后，大多数指标，尤其是经济竞争力、教育和军事实力，逐渐提高，直到1980年左右，中国的经济竞争力和贸易开始腾飞。那是在邓小平刚实行改革开放政策之后。这并不是巧合。从1984年我第一次访问中国到2008年左右，债务增长与十分强劲的经济增长保持一致。换言之，经济在没有负债的情况下取得了极快的改善。然后，2008年全球金融危机到来了，像其他国家一样，中国大规模实施一揽子计划以刺激经济，因此债务相对于收入上升。2012年起，中国政府大幅改善了中国的债务和经济管理，延续创新与技术的发展，加强了教育和军事，并与美国产生了更大规模的竞争。<strong>现在，中国在贸易、经济产出、创新与技术方面和美国不相上下，都是领先大国，而且是一个强大且迅速崛起的军事和教育大国。中国是金融领域的新兴大国，但在储备货币和金融中心地位上落后。我们将在本章的后面详细探讨这一切，但为了理解中国的现状，我们首先需要深入中国漫长的历史</strong>。</p><p>下图把各大国图中相同的总体实力衡量指标应用于中国，覆盖了600年至今1 400多年的历史。历史上中国一直是全世界最强大的国家之一，只有在大约1840—1950年是显著的例外，当时中国陷入急剧衰落。内战结束之后，中国开始再次崛起，起初缓慢，后来很迅速。现在中国仅次于美国，并有望超越美国。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3fafb89a6638868e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>不同王朝的衰落中有一些共同的主题，我们在本书中提到的其他一些国家的衰落中也可以看到这些主题。</strong></p><ol><li>在王朝发展过程中日益加剧的不平等和财政问题是衰落的关键驱动因素。通常王朝初创时，随着旧王朝精英的集中财产被重新分配，土地和财富持有更为平等，这有助于防止社会冲突和改善财政状况（因为与广泛存在的小地主相比，精英往往更有能力避税）。但随着时间的推移，土地集中于越来越少的家庭，这些家庭可以避税（通过贿赂、利用官方影响力和寻找其他方法来隐匿财富以躲避税收），这又使他们能进一步积累财富。由此造成的不平等直接导致冲突的产生，同时国家税基的削弱使国家变弱，更容易受到危机冲击。</li><li>货币问题是帝国衰落的常见原因。在宋朝、元朝和明朝，政府努力维持充足的金属货币供应，并诉诸印钞，特别是在战争和天灾人祸时期。征税方面的问题使政府更有动力印钞。这导致高通胀或恶性通胀，使情况变得更糟。</li><li>治理和基础设施的质量往往在每个朝代的早期上升，然后在朝代发展过程中下降。在宋朝、明朝和清朝后期，多年的公共工程投资不足，使中国易受饥荒和洪水的影响。虽然很难对几十个皇帝进行概括，但具有远见卓识的王朝开创者（如拥抱科技的宋朝和元朝开创者）之后的统治者通常更僵化和保守（如清朝统治者），过于关注帝国财富和奢侈品（如北宋的末代统治者）和/或不怎么支持对外贸易（如明朝统治者）。</li><li>内部冲突通常源于经济差异和经济不景气（最典型的诱因是农业问题、高债务、管理不善和自然灾害，有时是与外部力量的冲突）。严重的自然灾害和造成巨大破坏力的气候突变往往伴随着王朝的灭亡。典型的下行螺旋是：（1）技术和投资（包括新项目和维护）不足，使基础设施容易受到自然灾害的影响；（2）发生灾害（在中国通常是干旱和主要河流的洪水），破坏作物产量，在某些情况下会摧毁社会，因为作物产量下降会导致食物短缺甚至饥荒；（3）灾难引发国内民众起义。这一过程在宋、元、明、清朝的衰落中起了重要作用。</li><li>糟糕的条件和巨大的贫富差距导致最严重的起义，起义原因是百姓反抗精英的过度行为（如宋朝的方腊起义、元朝的红巾军起义和清朝的白莲教起义）。与此相反，大多数人的良好境况带来的国内稳定是较为繁荣时期的一个关键特征。</li><li>由于孤立以及强调士胜于农工商和军事实力的儒家文化的影响，中国在商业、技术和军事上的竞争力较弱，这导致中国被更强大的“野蛮人”打败，或者落后于对方，如蒙古人、鸦片战争中的外国列强。</li></ol><p>中国的自然地理和地质也对朝代的兴衰产生了很大影响。这主要是因为中国地形多变，气候多样。例如，北方地势平坦，更冷、更干燥，而南方多山，更温暖、更潮湿，因此中国不同地区的作物产量往往不一致。尽管如此，由于多样化和各区域的协调，统一的中国在很大程度上是自给自足的。然而，这些条件加上清洁水、农田和沿海海洋渔业的短缺，使中国在历史上很容易出现粮食短缺。由于这个原因，中国经常处于粮食不安全状态，甚至在今天也进口大量粮食。<strong>中国还缺乏一些重要的自然资源，如石油、一些矿产和一些食品。中国还有空气污染，给民众健康和农业带来不利影响，尽管中国正在迅速改善这些条件。</strong></p><p><strong>中国历史和哲学，其中最重要的是儒家—道家—法家—马克思主义哲学，对中国人的思维方式影响很大</strong>，比美国历史及其犹太教—基督教—欧洲哲学根源对美国人的思维方式的影响要大得多。一位受人尊敬的中国历史学家告诉我，毛泽东把卷帙浩繁的《资治通鉴》通读数遍，这部编年史有20卷，记录了从公元前400年到公元960年这约1 400年、共约16个朝代的中国历史，他还通读了几遍篇幅更长的《二十四史》，以及许多其他关于中国历史的书籍和外国哲学家的著作，其中最重要的是马克思的著作。他还以哲学的方式写作和言说，写诗，研习书法。如果你对毛泽东的想法感兴趣，或者更重要的是，对他的思维方式感兴趣，我建议你阅读《实践论》和《矛盾论》，当然还有“红宝书”，那是一本涉及许多主题的语录。[插图]</p><p><strong>中国领导人关心的是百年大计，因为100年是一个好的王朝延续的最短时间。他们明白，典型的发展轨迹包含不同阶段，每个阶段持续几十年，他们为此进行规划。</strong></p><p>中国文化是几千年来中国人的经验和教训的延伸。关于事物如何运转，如何最有效地应对现实，人们应该如何相处，政治决策应该如何做出，经济体系应该如何运转，这些都在中国的哲学思想中得以阐明。西方世界的主导哲学是犹太教—基督教、民主和资本主义/社会主义。个人通常从这些哲学中选择，形成适合于自己的组合。中国的主导哲学曾是儒家、道家和法家，在20世纪早期，马克思主义和资本主义加入这个系列。皇帝通常以自己的喜好做选择，将其付诸实施，并不断学习和适应。如果皇帝选的组合行之有效，王朝就会延续和昌盛（用他们的话说这是得了“天命”），否则王朝就会崩溃，被另一个王朝取代。从有记录的历史开始之前，这一过程就开始了，而且只要有人必须决定如何集体做事情，这一过程就会延续。</p><ul><li>儒家寻求实现和谐，确保人们知道在等级制中扮演的角色以及如何扮演好这些角色，从家庭内部（夫妻之间、父子之间、长幼之间等）开始，扩展到统治者和臣民。每个人都尊重和服从上级，上级既仁慈，同时又规定严格的行为标准。期待所有人都善良、诚实、公正。儒家强调和谐、广泛的教育和任人唯贤。</li><li>法家主张由专制领导人快速征服和统一“天下”。法家认为世界是一个无情厮杀、你死我活的丛林，人们必须严格服从皇帝的中央政府，政府不必对民众太过仁慈。</li><li>道家认为，与自然规律和谐相处最为重要。道家认为自然是由对立（即阴阳）组成的，和谐来自妥善地平衡对立。</li></ul><p><strong>古代中国的制度都是等级制和不平等的。</strong>一位杰出的历史学家和不同文化的探索研究者告诉我，美国人和中国人的核心差异是，<strong>美国人最强调个人至上，而中国人则最看重家庭和集体</strong>。美国是自下而上运行的，追求个人利益最大化；中国是自上而下运行的，追求集体利益最大化。他解释说，中文中“国家”这个词包含两个字——国和家，每个人都知道自己的位置。因此，中国人更加谦逊、尊重他人、循规蹈矩，而美国人则更加傲慢、平等、厌恶规则。我注意到中国人对提问和学习更感兴趣，而美国人对畅谈他们的想法更感兴趣。</p><p>与其他曾征服和占领他国的大国不同，中国占领遥远国家的情况相对少见。<strong>中国基本是一个大平原，周围是广阔的自然边界（山脉和海洋），大部分人口分布在平原上</strong>。中国的大部分领土在这些边界之内，它所经历的大多数战争是为了争夺对中国内部的控制，主要是中国人之间的战争，不过有时是外来入侵者和中国人之间的战争。</p><p>传统的中国军事哲学认为，<strong>赢得战争的理想方式不是打仗，而是悄悄发展自己的力量，不战而屈人之兵。</strong>它还主张广泛运用心理战来影响对手的行为。[插图]不过，历朝历代中国内部也发生过许多暴力战争。中国在境外的为数不多的战争是为了建立中国的相对实力和打开贸易机会。</p><p>在20世纪20—30年代的内乱和战争期间，债务急剧增长，这引发一个典型的周期，政府承诺提供的资金远远超出其能力。这造成广泛的违约，像通常的情况一样导致金属本位被放弃，金属货币和私人拥有白银的权利被取缔。如前所述，货币被用于：<strong>（1）国内交易，政府垄断了国内交易（因而交易可以用法定货币甚至假币进行）；（2）国际交易，在国际交易中货币必须具有真实价值，否则不会被接受。衡量一种货币是否具有真实价值的标准是，它是不是被积极使用，是不是在国际和国内以同样的汇率进行交易。当资本管制阻止一种国内货币在国际上自由兑换时，这种货币更容易贬值。按照定义，储备货币不受这样的管制。因此，这里有一项原则：●当你看到一种货币受到资本管制时，特别是当存在严重的国内债务问题时，远离这种货币</strong>。</p><p>20世纪30年代，中国有两种货币：一种是法定的纸币，用于国内交易；另一种是黄金，用于国际支付。法定纸币被大量发行，频繁贬值。在二战和内战的混乱过后，1948年12月，最早的人民币作为法定货币发行，当时的发行量被控制在有限水平上，以结束恶性通胀。1955年，第二套人民币发行，1962年第三套发行。1955—1971年，人民币汇率固定为1美元兑2.46元。20世纪70—80年代出现了又一轮高通胀。原因包括：1971年货币相对于黄金的全球性贬值，全球通胀压力，中国逐渐取消价格控制，宽松信贷，国有企业支出缺乏控制。2005年，人民币结束与美元挂钩。</p><p>中国的通胀率（同比）</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-21c59c59ae44f978.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>1800—1949年的衰落时期：中国1800年之后开始衰落，原因包括：（a）最后一个中国的朝代（清朝）变得衰败孱弱；（b）英国及其他一些西方国家变得强大，这导致英国和其他资本主义——殖民主义者日益在经济上控制中国；（c）无法偿还的债务负担及印钞导致货币价值崩溃，致使中国金融和货币体系崩溃；（d）发生了大规模的国内起义和内战。这场严重的大周期性衰落从1840年左右一直持续到1949年，其间，所有的主要实力因彼此牵动、相互影响而急剧衰落。1945年二战的结束导致大多数在中国的外国人被遣返（香港地区和台湾地区除外），接着中国发生了内战，即一场共产主义者与资本主义者之间的战争，以决定如何分配财富和权力。这场漫长衰落是典型大周期的一个典型案例。接着出现了一场同样典型的大周期性上升：新领导人掌控大局，巩固权力，开始建立基本构架，并由后代人继承，在前人成就的基础上添砖加瓦。</strong> </p><p>如之前几章所述，19世纪早期是英国在全球崛起和扩张的时期，这使崛起的英国与中国产生更多联系。英国东印度公司想从中国获得茶叶、丝绸和瓷器，因为这些货物回销英国非常有利可图。然而，英国人没有中国人想要与之交易的东西，所以不得不为这些货物支付白银，当时白银是全球性货币。当英国人花光白银时，他们从印度向中国走私鸦片，卖鸦片换得白银，用白银来买中国货物。这导致了第一次鸦片战争。<strong>1839—1842年，技术更先进的英国海军击败了中国人。这使英国得以强行与中国签订一项条约，把香港割让给英国，向英国商人开放一系列港口，其中最重要的是上海（在接下来的一系列条约里，中国又向其他列强开放港口）。这又最终导致中国把大片北部领土割让给俄国和日本，并把台湾割让给日本</strong>。</p><p>清政府大举向外国人借钱来对付国内叛乱。赔款，尤其是《辛丑条约》的赔款，也造成了巨额债务。《辛丑条约》约定中国对各国赔款4.5亿两白银，价息合计超过9.8亿两白银，并以关税和盐税等做抵押。清政府财政资源枯竭，在鸦片战争后的几十年里面临许多次起义，它为了筹措镇压起义的经费而花光了积蓄。<strong>被称为“百年屈辱”的这场相互强化并自我强化的衰落的原因包括：（1）没有强有力的领导；（2）没有健全的财政；（3）发生了破坏生产力、耗费金钱、牺牲人命的内部起义；（4）外国入侵，耗资及人员死亡巨大，面临巨额赔款；（5）经历了一些重大自然灾害。</strong></p><p><strong>我们可以很容易地看到，这段时期对塑造中国领导人的观点产生了重大影响：企业通过帝国主义（即通过控制和剥削各国，就像英国及其他资本主义大国对中国所做的那样）追求利润，这剥削了工人，养肥了贪婪的精英。</strong>他们对资本主义的观点与我对资本主义的观点不同，这是因为他们与资本主义打交道的经历和我的经历大为不同。资本主义向我以及我认识的大多数人，包括来自世界各地的移民，提供了大量机会。我成长时所处的美国是一方充满机会的热土，人们可以在此学习，做贡献，得到公正和不设上限的回报。这段通过他人的视角看问题的经历再次提醒我，极度头脑开放和深思熟虑的意见分歧，对探寻真相是多么重要。这促使我学了一点儿马克思主义，以便理解为什么毛泽东及其他人觉得这种哲学有道理。在那之前我倾向于认为，马克思主义明显不切实际，甚至可能是一种威胁，但当时我并不知道马克思到底说了什么。</p><p><strong>马克思最重要的理论/体系被称为“辩证唯物主义”。</strong>“辩证”是指对立面如何相互作用而引起变化，“唯物主义”是指一切事物都以物质的方式存在，以机械的方式与其他事物相互作用。简言之，辩证唯物主义是这样一种系统：通过观察和影响“对立面”的“矛盾”来创造变化，由此带来“斗争”，“斗争”的解决意味着进步的发生。马克思想把辩证唯物主义应用到一切事物上。体现为资本主义与共产主义冲突的两个阶级之间的冲突和斗争，只是许多此类冲突中的一个。</p><p>每个阶段都使中国沿着长期发展的弧线前进，不断巩固之前的成就。简言之，事件的发生情况如下。</p><ul><li>1949—1976年，毛泽东建立了无产阶级专政，奠定了中国的制度、治理和基础设施基石，领导中国，直到1976年去世。当时中国遵循严格的计划经济制度，并实行严格的管控。毛泽东和周恩来去世后，邓小平带来了第二阶段。</li><li>以邓小平为核心的领导集体治理中国，直到他1997年去世。在这一阶段，中国转向更有集体色彩的领导模式，对外开放，引入并发展了市场经济，经济实力大增，并在其他方面变得更强大，而又没有显得威胁到美国及其他国家。当时中美关系被视为共生关系，美国从中国购买价格诱人的商品，中国把钱借给美国人，供其购买这些商品。结果，美国获得了以美元计价的债务，中国获得了以美元计价的资产。邓小平去世后，中国政府延续了这些政策方向，所以中国继续以基本健全的方式变富变强，也没有威胁到美国。2008年全球金融危机爆发，导致美国及其他发达国家围绕财富的社会紧张加剧，民众对制造业工作机会流失到中国的怨愤加剧，同时在包括中国在内的所有国家，通过举债来实现经济增长的做法增多。</li><li>2012年上任，他领导的是一个更富更强的国家，同时中国本身也在应对负债，治理腐败，与美国的冲突加剧。习近平加快了经济改革，直面挑战，在努力遏制债务增长的同时，大力改革经济，支持打造世界领先的技术，并采取了日益全球化的立场。他还更积极地缩小中国的教育和收入差距，保护环境，提高治理能力。随着中国力量增长，中国的目标（如“一带一路”倡议和《中国制造2025》）变得更加明确，尤其是在特朗普（民粹主义和民族主义者，竞选政纲是阻止美国制造业岗位流失到中国）当选总统之后，美国与中国的冲突增多。对美国而言，中国成为一个迅速崛起的、挑战其主导地位的国家。</li></ul><p>“文革”从1966年持续到1976年，这样严重的错误损害了教育，导致中国经济增长缓慢。到20世纪70年代初，在周恩来总理的直接领导下，形势开始稳定下来。1969年，中国和苏联军队在边境发生冲突。</p><p>1971年是中国发生大变化的一年。1971年，中国正受到苏联威胁，当时苏联军力比中国强大得多，东段与中国有4 000多千米的边界线，这导致了越来越多的边界威胁。美国撤离越南后，1975年苏联与越南结盟并运送部队和武器进入越南，越南与中国南部约有1 300多千米的陆地边界线。毛泽东的一个地缘政治原则是确认主要敌人，削弱敌人的盟友，吸引其远离主要敌人。毛泽东确认苏联是中国的主要敌人，并认识到苏联正在与美国争斗，尽管还没有变成热战，但有这个可能。这促使他做出接近美国的战略行动。亨利·基辛格引述中国官员的话说：“美帝国主义者最不想看到的就是苏联修正主义者在一场中苏战争中获胜，因为这将使苏联人建立起在资源和人力方面比美帝国更强大的大帝国。”</p><p>中国希望与美国建立关系，以消除苏联的威胁，并希望这能强化自身的地缘政治和经济地位。<strong>因为在1971年很明显的是，中美建立关系对两国都有利，所以两国都提出了建立关系的建议</strong>。1971年7月基辛格访华，接着1972年2月理查德·尼克松总统访华。1971年10月，中华人民共和国恢复联国合法席位。尼克松访华期间，尼克松和周恩来签署了一份协议（《上海公报》），美国在协议中宣布，美国“<strong>承认在台湾海峡两边的所有中国人都认为只有一个中国，台湾是中国的一部分。美国政府对这一立场不提出异议。它重申它对由中国人自己和平解决台湾问题的关心</strong>”。尽管存在这些保证，但台湾问题始终是最令双方紧张的问题。</p><p>1978年，时年74岁的邓小平成为领导核心。从1978年到1997年去世，<strong>他最重要的政策可以用一个短语来表达——改革开放。“改革”的意思是对内改革，即利用市场来帮助配置资源和激励民众；“开放”的意思是对外开放，即通过与外部世界交流来学习和进步，并进行对外贸易。</strong>市场经济成为政策组合的一部分。当时中国仍很贫困，人均年收入不到200美元。如果希望中国保持弱势的强大外部国家不破坏这些措施的话，那么这些措施将增强中国经济实力。关键是在实施这些措施时要使之有利于外部大国，而不是威胁它们。1979年，中美正式建交。</p><p>邓小平很早就编制了一项70年规划：<strong>（a）实现国民生产总值比1980年翻一番，解决人民的温饱问题；（b）到20世纪末，使国民生产总值再增长一倍，人民生活达到小康水平；（c）到21世纪中期，人均国民生产总值达到中等发达国家水平，人民生活比较富裕，基本实现现代化。他明确表示，中国实现这些目标的途径是实行“社会主义市场经济”，他也称之为“中国特色社会主义”。</strong>他在做出这一重大转变的同时，没有批评马克思列宁主义。事实上，如前所述，他并不认为这两种体制在根本上对立，而是通过辩证唯物主义的视角来看待这两种体制，认为它们是两个可以调和的对立面，调和能带来进步，经过漫长的历程抵达共产主义理想状态。</p><p><strong>在这段全球化时期，中美形成了一种共生关系，中国人以极低的成本生产消费品，同时借钱给美国，以便其购买中国消费品。对美国人来说，这是一笔极有利的“先买后付”交易。中国人也乐见这种交易，因为他们能借此积累美元外汇储备</strong>。令我感到奇怪的是，平均收入只有美国人1/40的中国人会借钱给美国人，因为通常来讲是富人而不是穷人会借钱给别人。这也让我想到，美国人为了支撑自己的过度消费是多么愿意借债，而中国人是多么重视储蓄。这也反映了新兴市场国家想积累主要储备货币国家的债券/债权，会如何导致储备货币国家过度负债。</p><p>1997年2月19日，邓小平去世，<strong>他彻底改变了中国的面貌。邓小平1977年复出时，90%的中国人处于极端贫困状态，到他去世时这个数字削减了一半多</strong>。从1978年邓小平开始改革到1997年他去世，中国经济年均增长率为10%，经济规模增长了6倍多，平均通胀率约为8%。中国的外汇储备从40亿美元增加到近1 500亿美元（考虑通胀因素根据当前美元价值调整后，外汇储备增加逾2 500亿美元）。外汇储备与年进口额的比率从1978年的60%发展到1998年的逾125%（1998年外汇储备与外债利息的比率是近800%）。</p><p>作为二战后和平与繁荣时期通常发生的情况，当领先大国没有受到威胁，崛起国家还不构成威胁时，崛起国家能从领先大国那里学到很多东西，它们以一种共生的方式合作，直到崛起大国变得足够强大，足以威胁到领先大国。除了从学习中受益外，它们还从相互贸易中受益（直到这变得对它们不利），并以互惠互利的方式利用资本市场从中受益（直到这变得对它们不利）。</p><p><strong>更具体地说，1978—2008年中国快速发展的原因有两个。（1）世界仍处于大周期里的和平与繁荣时期，全球化和市场经济是被广泛接受的通往更好世界的道路，这包括如下信念：商品和服务应当在成本最低的地方生产；应当让人才自由流动，不因国籍受歧视；淡化民族主义，重视全球机会平等和追求利润的市场经济。（2）邓小平改革计划经济，实行改革开放政策，取得了很好的效果。这使中国学到了很多东西，吸引了很多外资，成为一个巨大的出口国和储蓄国。</strong></p><p><strong>第三阶段：中美冲突日益凸显，全球化走向终结（2008年至今）</strong></p><p>与典型情况一样，由债务增长支撑的繁荣时期引发了债务泡沫，造成贫富悬殊。2008年，美国的泡沫破裂（像1929年那样），全球经济萎缩，冲击到美国和其他国家的中产阶级（像1929—1932年那样）。利率降至零（像1931年那样），但宽松力度仍然不足，于是，2008年后，各国央行大量印钞，大规模购买金融资产（像1934年那样）。这些举措的结果是，从2009年开始，大多数国家的金融资产价格上涨（像1933—1936年那样）。这使“富人”（金融资产持有者）比“穷人”（没有金融资产的人）更受益，导致贫富差距扩大（类似于1933—1938年）。“穷人”，特别是那些工作被外国人和移民抢走的人，开始反抗受益于全球化的精英阶层。像典型情况一样，经济不景气伴随着贫富悬殊，民粹主义和民族主义在全球泛起（像20世纪30年代那样）。<strong>此时，崛起大国对领先大国构成的威胁变得愈加明显。和平、繁荣和全球化的时代逐步终结，各国国内贫富矛盾激化、崛起国（中国）与世界主导国（美国）发生冲突的时代已经开启。</strong></p><p>多年来，中国政府积极推行改革政策，旨在开放市场和经济，管控债务增长，提高人民币汇率的灵活性，支持创业和以市场为导向的决策（特别是在那些中国希望领先世界的产业），建立合理的监管体制，由高度完善的监管机构实施监管，发展未来科技和工业潜能，扩大经济效益，使其惠及最落后的群体和地区，控制污染和环境退化。但许多人对此仍持有不同观点。我认为原因如下：（a）在进行这些改革的同时，政府收紧了其他管制措施；（b）中小企业得到的一些支持（如信贷可得性）不如国有大企业（主要原因是技术性挑战，而不是政府促进中小企业发展的意愿下降）；（c）政府自上而下地指导经济，银行和企业有时发放不符合经济效益却有社会效益的贷款（政府希望做对国家整体最好的事）；（d）政府扶持企业来实现宏观目标；（e）外国公司在华不能享有与中国公司相同的待遇；（f）与主要储备货币国家相比，中国更多地通过协调财政和货币政策来管控经济。这一切通常不受国外资本主义者的欢迎。</p><p>要理解他们的视角和所处的环境，我建议，不要用刻板印象来看待他们的做法（例如，简单地说这是“共产党员”的做法），而是接受另一个视角：他们在努力而且将继续努力兼顾这两种看似矛盾的东西。在他们看来，市场经济是一种提升大多数人生活水平的方式，不是为资本家服务的。无论是否喜欢这种视角，他们获得的结果是令人印象极为深刻的，所以我们不应期望中国人放弃这种视角，接受美国或西方视角。相反，我们应当研究这种视角，以寻求获益，就像中国研究和借鉴西方一样。毕竟我们面临的是两种做法的竞争，需要理解这两种做法，以更好地进行这场竞争博弈。</p><p><strong>在外交政策方面，中国变得日益积极，而美国变得更具对抗性。具体而言，2012年至今，中国实力不断增强，这一点有目共睹。中国也更公开地展示自己的实力（例如，在《中国制造2025》中，中国公开计划，要主导一些美国目前控制的产业）。这引起了美国的强烈反应，这一点在2016年特朗普当选总统后变得更为明显。</strong></p><h1 id="十三、中美关系和战争"><a href="#十三、中美关系和战争" class="headerlink" title="十三、中美关系和战争"></a>十三、中美关系和战争</h1><p>在我看来，命运和大周期规律将中美两国及其领导人置于目前的境地。在这些力量的作用下，美国经历了多个成功周期相互强化的时期。这些成功周期引发了过度行为，导致一些领域出现疲弱。在同样的力量作用下，中国经历了大衰退周期，陷入困境，这促使中国进行大变革，迎来当前相互强化的上行周期。因此，基于所有常见的原因，目前似乎美国正在衰落，中国正在崛起。</p><p>在命运和大债务周期的作用下，美国目前处于长期债务周期的后期阶段。在这个阶段，由于债务水平过高，政府需要快速举借新债，但无法通过硬通货来支付，而不得不进行债务货币化，通过印钞为政府赤字融资。这是长期债务周期后期阶段的典型情况。讽刺但也符合常规的是，美国陷入当前困境的原因是成功周期导致了过度行为。例如，<strong>正是因为美国在全球取得了巨大成功，美元才成为世界主要储备货币，这使美国可以从全球其他地区（包括中国）过度借债。而过度借债使美国对其他国家（包括中国）大量欠债，将自己置于弱势地位，欠其他国家（包括中国）许多钱。其他国家也处于弱势地位，因为它们持有一个重债国的债务，而这个重债国还在快速增加债务，将债务货币化，向债权国支付的实际利率明显低于零</strong>。换句话说，正是由于典型的储备货币周期，中国希望大量积蓄世界储备货币，便大量借钱给美国，而美国需要大量借钱。这将中国和美国置于大债权国—大债务国的尴尬关系中，而同时中美之间正在发生各种争斗。</p><p><strong>●历史表明，所有国家的成功都依靠不断加强自身实力，避免导致衰落的过度行为。真正成功的国家能够在200~300年中持续发展实力。但没有任何国家可以永远这样做。</strong></p><p>迄今为止，我们考察了过去500年的历史，尤其关注主要储备货币国家（荷兰、英国和美国）的兴衰周期，并简要回顾了过去1 400年里的中华帝国史，再回到现在。我们的目标是从宏观角度分析目前局势，找出事态发展的因果关系，以便更好地了解当前形势。现在，我们需要进一步细化，详尽研究目前的态势，同时把握大势。近距离观察时，许多事件（如华为、美国制裁香港、领事馆关闭、战舰调动、前所未有的货币政策、政治争斗、社会冲突等）回想起来似乎不大，但现在显得大得多。这些事件每天像雪片般飞来，哪怕解析其中的一件事都需要整章篇幅，所以我不会在此赘述，只是简要讨论主要问题。</p><p><strong>历史告诉我们有五大类型的战争：（1）贸易/经济战，（2）技术战，（3）地缘政治战，（4）资本战，（5）军事战。我再加上两种，（6）文化战，（7）自我交战的战争。</strong>所有明智的人都希望这些“战争”未曾发生，而是进行合作。但是，我们必须从实际出发，承认其存在，研究历史案例，了解其实际演变过程，以便思考未来最可能发生的情况和妥善的处理方式。</p><p><strong>贸易/经济战：</strong></p><p>迄今为止，<strong>中美贸易战尚未发展到极其严重的地步</strong>。目前有加征关税、限制进口等典型举措，这是其他类似冲突时期反复出现的情况（如1930年的《斯穆特—霍利关税法》）。双方已经进行了贸易谈判，谈判成果反映在2019年达成的一份非常有限的“第一阶段”贸易协议中，这份协议得到了初步执行。我们看到，这场“谈判”是对彼此实力的考验，而不是依靠全球法律和法官（如世界贸易组织）形成公正的解决方案。所有这些战争都将以实力较量的方式展开。大的问题是双方将在多大程度上以何种方式较量实力。</p><p><strong>●在平安无事时，很容易保持高尚道德。但当冲突激化时，就更容易为以前被视为不道德的行为辩护（把以前被称为不道德的做法说成是道德的）。</strong>随着冲突变得更加激烈，对当下的行为给予理想化的描述（有利于国内公共关系）和为了获取胜利而采取的实用做法之间就会出现分裂。这是因为在战争中，领导人希望选民相信“我们是善人，他们是恶人”。这是赢得民众支持的最有效方式。在极端情况下，他们甚至愿意为此而杀人或牺牲。假如一位务实的领导人说，除了人们加于自身的道德法则外，“战争中没有法律”，“我们必须像对方一样不择手段，不然的话，我们就相当于自缚其手地傻傻作战”，尽管这是真实情况，但这很难激发民众的热情</p><p><strong>一般来说，贸易/经济战最危险的举动是，一方切断另一方的必需品进口。</strong>第6章中阐述的二战前夕美国和日本的案例，对中美关系现状有所启示，因为地理态势和所涉问题是相似的。例如，如果美国切断中国从美国或其他国家进口石油、其他必需品、技术和/或其他必要的进口，那么这将是清晰明显的战争升级信号。同样，中国升级战争的方式是，切断通用汽车（该公司在中国的销售量高于美国）、苹果等公司的业务，或者切断美国进口稀土，稀土是生产许多高科技产品、汽车引擎和国防系统所必需的。我不是说这样的行动将会发生，但我想明确一点：任何一方采取切断必要进口的行动，都标志着局势的严重升级，可能导致更严重的冲突。如果这种情况没有发生，局势就会按照正常情况发展，中美的国际收支平衡状况将主要根据各国竞争力的变化而变化。</p><p><strong>技术战</strong></p><p>技术战要比贸易/经济战严重得多，因为谁能赢得技术战，谁就也可能赢得军事战和其他方面的战争。</p><p>目前，美国和中国是世界大型科技行业的主导力量，而这些大型科技行业是朝阳产业。<strong>中国的科技业迅速发展，为中国人提供服务，在国际市场上也变得有竞争力。与此同时，在技术上，中国仍然高度依赖美国和其他国家。</strong>因此，美国很容易受到中国技术日益发展和竞争力提高的影响，中国很容易受到美国或其他国家切断关键技术的影响。</p><p>总的来说，目前美国的技术能力领先，但各类技术领域的领先程度不同，且正在失去领先地位。比如，美国在先进的人工智能芯片开发方面领先，但在5G（第五代移动通信技术）方面落后。以下数据粗略地反映了这一点：美国科技公司的总市值约为中国的4倍。这一数据低估了中国的相对实力，因为它不包括一些大型民营企业（如华为和蚂蚁集团）和非企业机构（如政府）的技术发展，而中国这些机构的规模大于美国。中国最大的上市科技公司（腾讯和阿里巴巴）已是全球第七大和第八大科技公司，仅次于美国最大的一些科技股“FAAMG”（脸书、苹果、亚马逊、微软、谷歌）。中国目前在一些最重要的技术领域领先。例如，中国拥有40%的世界最大民用超级计算机，在人工智能/大数据竞赛和量子计算/加密/通信竞赛的一些方面领先。中国在其他科技领域也有类似的领先优势，比如，在金融科技领域，中国的电子商务交易和移动支付的交易金额目前位居世界首位，远远超过美国。可能中国还在秘密开发一些技术，甚至情报最灵通的美国情报机构对此也不了解。</p><p>中国可能会比美国更快地提高技术和（受益于这些技术的）决策质量。因为大数据+大型人工智能+大型计算=卓越决策。中国的人均数据收集量远远高于美国（而且人口是美国的4倍多）。中国还大量投资于人工智能和大型计算领域，使其发挥最大作用。中国投入这些领域和其他技术领域的资源比美国多得多。在资金投入方面，风险投资家和政府正在向中国技术开发者提供几乎上不封顶的资金。在提供人才方面，中国从事科技工作的STEM专业的大学毕业生大约是美国的8倍。美国在整体技术上处于领先地位（尽管在某些领域落后），当然也有一些新的大型创新中心，其大多在顶级大学和大型科技公司。虽然美国仍然具有竞争力，但其相对优势正在下降，因为中国的技术创新能力正在加速提高。请记住，37年前的中国人还对我送给他们的手持计算器惊叹不已，想想看他们37年后会怎样。</p><p><strong>目前美国拥有技术优势（尽管优势在快速缩小）。因此，中国目前高度依赖从美国和美国可以影响的其他国家的技术进口。这对中国意味着一个巨大的脆弱性，也是美国的一个强大武器。</strong>这一局面最明显地存在于尖端半导体领域，尽管在其他技术领域也存在。全球领先芯片制造商台积电的举动是许多值得关注的动向之一，尤其是因为该公司位于中国台湾地区。台湾地区为大陆及全世界提供所需的芯片，并受美国影响。中国的健康发展依赖于很多这些技术的进口，而美国的健康发展对从中国进口的依赖度要小得多。如果美国切断中国获得关键技术的渠道，就预示着真实战争的风险大幅上升。另一方面，如果当前态势持续下去，那么在5~10年的时间里，中国将会在技术上比美国更不依赖外界，届时我们会看到这些技术进一步脱钩。相关局面每天都在变化，保持对其了解至关重要。</p><p><strong>地缘政治战</strong></p><p>中国最大的问题可能是主权问题，特别是涉及中国台湾、香港、东海和南海的主权问题。除此之外是一些具有战略经济重要性的区域，如中国“一带一路”倡议的沿线国家。</p><p>台湾问题可能是最危险的主权问题。许多中国人认为，当美国向中国台湾地区出售F16战机和其他武器装备时，美国一点儿也不像是要促进实现中国的和平统一。所以他们认为，只有中国拥有了对抗美国的实力，才能期待美国在面对更强大的中国时能明智地停止干涉中国内政，从而能确保中国的安全和统一。据我了解，中国已经增强在该地区的军事力量。此外，中国可能会以更快的速度增强军力，但发出“相互保证毁灭”的威慑依然是最可能出现的情况。所以，正如我之前提到的，如果“台海危机”再次出现，那么我会非常担心。美国会为台湾而出击吗？这一点不能确定。如果美国不出击，那么这对中国来说将是巨大的地缘政治胜利，对美国来说将是巨大的耻辱。这将标志着美国在太平洋及其他地区的势力衰落，就像英国丢失苏伊士运河标志着大英帝国在中东及其他地方的终结一样。其影响将远远超出这些损失。例如在英国的案例中，苏伊士运河事件标志着英镑作为世界储备货币的终结。美国越表现出支持台湾的姿态，输掉战争或撤退会造成的耻辱就越大。这令人担忧，因为美国一直在做要支持台湾的大戏，而命运似乎很快就会带来一场直接冲突。如果美国真的出战，那么我相信一场以牺牲美国人生命为代价的战争在美国将是极不受欢迎的，而且美国可能输掉这场战争，因此最大的问题是，这是否会引发更大范围的战争。这让每个人都感到恐惧。希望对战争及其破坏性的恐惧将会阻止战争的发生，就像对相互保证毁灭的恐惧将会阻止战争一样。</p><p>至于在全球的影响力，对美国和中国来说，都有一些地区至关重要，这主要基于邻近性（它们最关心距离自己最近的国家和地区）和/或能否获取必需资源（例如，它们最关心能否持续获取重要矿产和关键技术），以及出口市场（相对次要）。对中国人来说，最重要的区域首先是中国领土内的区域，其次是与中国接壤的区域（如东海和南海）和处于关键供应通道的地区（如“一带一路”沿线国家）或关键进口品的供应国，再次是能够成为合作伙伴的、具有经济或战略重要性的其他国家。</p><p><strong>资本战</strong></p><p>历史表明，冲突中最大的风险之一是资金/资本的获取可能被切断。这可能是由于（a）对手的行动和/或（b）自伤其身的有害行为（如负债过多和让货币贬值）导致资本提供者不想提供资本。在第6章，我回顾了资本战的经典举措。其中一些举措正在被使用，而且可能会以更强力的方式被使用，所以必须密切关注这些举措。</p><p><strong>因为美元是世界贸易、资本流动和储备的主导货币，是世界上最主要的储备货币，所以美国处在令人羡慕的地位：能够为世界印钞，并对其敌人实施制裁。美国现在拥有一个制裁武器库，这是美国使用最多的武器库。</strong>截至2019年，美国针对个人、公司和政府实施了约8 000项制裁。通过这样的力量，美国可以获得自身需要的资金，并能通过阻止金融机构和其他国家与敌国打交道，切断敌国获得资金和信贷的渠道。这些制裁绝非完美，也不能面面俱到，但总体上是有效的。</p><p><strong>●历史表明，无论何时，只要（a）货币不受欢迎，（b）没有其他有吸引力的货币可供投资，这些货币就会贬值，资本就会进入其他投资项目（如黄金、大宗商品、股票、房地产等）。因此，存在一个强有力的替代货币并不是货币贬值发生的前提。</strong></p><p>事情将会变化。如果美国和中国发生一场资本战，那么中国货币和资本市场的发展将对美国不利，对中国有利。如果美国没有为削弱中国的货币和资本市场而对其展开攻击，而且/或者中国没有伤害本国货币和资本市场（做出会降低市场吸引力的政策转变），那么中国的货币和资本市场可能会快速发展，与美国市场展开越来越激烈的竞争。这要由美国决策者来决定，即他们是打算通过变得更加强势来打断这一演化路径，还是将接受这场演化。后者可能会导致中国变得相对更强，更加自给自足，更不容易受到美国的挤压。中国伤害美元及美国资本市场的能力较弱，中国最好的举措是增强人民币。</p><p><strong>军事战</strong></p><p><strong>●无法想象下一场大的战争会是什么样子，但有可能比任何人想象的更具破坏性。这是因为，自从上次使用最强大的武器以来，各国一直在秘密研制大量的武器，在各种战争形式中打击对手的创造力和能力都得到了极大的提升，这都在以往战争中体现了出来</strong>。战争的种类比人们想象的更多，而且在每一种战争中，武器系统比任何人知道的都要多。核战争固然是一个可怕的前景，但我也听说生物战、网络战、化学战、太空战及其他战争同样可怕。许多战争形式还未经测试，因此它们存在很多不确定性。</p><p>不过我们确知的是，整体而言，美国和中国在东海和南海的地缘政治斗争正在军事上升级，因为双方都在考验对方的极限。<strong>目前中国在东海和南海的军事力量比美国强大，所以美国可能会在该地区战败，但美国在全球范围内总体上更强大，因而可能会在一场更大规模的战争中“获胜”</strong>。但更大规模的战争太过复杂，因而难以想象，因为有大量未知因素，包括其他国家在这种情况下会如何行动，以及存在着哪些秘密军事技术。大多数知情人士的唯一共识是，这样一场战争将会超乎想象的可怕。</p><p>就潜在的军事冲突地点而言，<strong>台海、东海、南海和朝鲜是最热门的地区，印度和越南次之</strong>（原因此处不赘述）。</p><p>就战争的时机而言，我牢记着这样的原则：<strong>●当国家出现严重的内部混乱时，这是敌对国家积极利用其弱点的合适时机。例如，20世纪30年代，当中国因持续的内战而分裂和疲惫时，日本采取了入侵中国的行动。</strong></p><p><strong>●历史告诉我们，当出现领导层换届和/或领导软弱的情况，同时又存在大规模内部冲突时，敌人发动进攻的风险就应该被视为有所提高。</strong>因为时间在中国这边，所以如果战争发生的话，那么较晚发生有利于中国（5~10年后中国可能会变得更强大和自给自足），而较早发生则有利于美国。</p><p><strong>文化战</strong></p><p><strong>●人们彼此如何相处，是决定他们将如何应对共同面临的情况的最重要因素，而他们所拥有的文化，是决定他们彼此如何相处的最重要因素。</strong>中国人和美国人分别最重视什么，他们对人与人相处之道的看法，决定了在应对我们刚刚探讨过的冲突时，他们彼此将如何对待。因为美国人和中国人有不同的价值观和文化规范，他们会为之战斗，为之牺牲，所以我们要想和平解决我们的分歧，双方就必须理解这些分歧是什么，如何妥善处理这些分歧。</p><p>如前所述，<strong>中国文化促使其领导人和社会以自上而下的方式进行大多数决策，奉行高标准的文明举止，认为集体利益高于个人利益，要求每个人知道自己的角色以及如何好好扮演角色。他们还寻求“无产阶级统治”，通俗地说，这意味着机会和回报的广泛分配。相反，美国文化促使其领导人自下而上地治理国家，要求高水平的个人自由，崇尚个人主义而非集体主义，赞赏革命性的思考和行为，不因人的地位而是因人的想法的质量而尊敬人。这些核心文化价值观决定了每个国家选择的经济和政治制度的类型。</strong></p><p><strong>●怎么做效果最好取决于（a）环境和（b）运用这些系统的人将如何彼此对待。没有一个系统能够持续良好运行，事实上，所有系统都将崩溃。持续良好运行的前提是系统中的人超越个体意愿尊重系统，系统有足够的灵活性来适应时代而不崩溃。</strong></p><p><strong>自我交战的战争：敌人就是我们自己</strong></p><p>在此之前，我们来回顾一下打造伟大帝国所需的具体因素。这些因素包括：</p><p>……足够强大和有能力的领导层，能为成功提供必需要素，这包括……</p><p>……强大的教育。我这里说的强大的教育不仅指教导知识和技能，还包括……</p><p>……性格坚强、有礼貌、强烈的职业道德，这通常需要学校和家庭的共同教育。这能带来公民素质的上升，反映为如下因素……</p><p>……腐败少，高度尊重规则，如法治。</p><p>……人们能融洽合作，对如何相处有共识，并团结在这一共识之下，也很重要。当人们有知识、技能、良好的性格、文明行为素质并融洽合作时，就会出现……</p><p>……良好的资源分配制度，以下因素能显著完善这一制度……</p><p>……开放接纳全球最好的思想，这是让一个国家成功最重要的因素。这将让它获得……</p><p>……全球市场上更强的竞争力，这能带来高于开支的收入，这将让它获得……</p><p>……强有力的收入增长，这将让它获得……</p><p>……更多投资来改善基础设施、教育体系和研发。这将让它获得……</p><p>……快速提高的生产力（每小时经济产出的价值提高）。生产力的提高将使财富增多，生产能力增强。在一国获得更高的生产力水平后，它将能大量发明……</p><p>……新技术。新技术既有商业价值也有军事价值。随着该国在这些方面变得更具竞争力，它自然会获得……</p><p>……世界贸易中显著且不断上升的份额。这要求它拥有……</p><p>……强大的军事力量，以保护本国贸易路线，并影响对其重要的域外国家。在成为经济强国的过程中，它发展出……</p><p>……被广泛使用的坚挺货币、股票市场和信贷市场。在贸易和资本流动方面占主导地位的国家，其货币自然被当作首选的全球交易媒介和财富储存手段，被广泛使用，从而成为储备货币，因此开始建立……</p><p>……至少是全球主要的金融中心之一，吸引和分配资本，在全球扩展贸易。</p><h1 id="十四、未来"><a href="#十四、未来" class="headerlink" title="十四、未来"></a>十四、未来</h1><p>“靠水晶球生活的人注定要吃碎玻璃”，这是我大概14岁时学到的一句市场格言。我的亲身经历表明这句话是对的，所以这影响了我对未来和过去的看法。<strong>我学会了通过回顾过去来（1）判断可能发生的事情；（2）保护我自己和我需要对其负责的人，以免犯错或错过重要的事情。</strong>虽然你、我和其他人可以就本书中描述的模式和因果关系展开争论，但如果你是出于实用目的而不是一时兴起而阅读本书，那么你和我一样，都需要做好这两件事。</p><p>本章旨在分享我对如何面对未来的看法。虽然我对未来的了解极为有限，但我也知道不少。应对未来就是要：<strong>（1）感知并适应正在发生的事情，即使这些事情无法预料；（2）预估可能发生的事情的发生概率；（3）充分了解可能发生的事情，以保护自己免受不可接受的伤害，即便不可能万无一失地做到这一点。</strong></p><p>我的做法基于我对三点的想法：<strong>（a）进化，随时间推移而产生的变化通常带来改进，如生产率提高；（b）周期，即在经济中引起有节奏的起伏（如债务泡沫和泡沫的破裂）和其间的颠簸（非节奏的起伏，如自然灾害）；（c）指标，可以帮助我们看到当下在周期中所处的位置和接下来可能发生的情况。我将简要回顾一下我对每一点的看法。</strong></p><p>总结：</p><ul><li>人类的创造性可能会带来更大的进步，但同时债务/经济周期、内部秩序和混乱周期、外部秩序和混乱周期与不断恶化的自然灾害，几乎可以肯定会构成问题。换言之，人类的创造性和这些其他挑战之间，将存在一场斗争。</li><li>各国内部和各国之间情况极为不同，这将决定哪些国家将以何种方式崛起，哪些国家将以何种方式衰落。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-bd9509060d651c93.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0e61421df153fa54.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>例如，如前所述，<strong>当（1）一个国家的财政状况恶化，同时（2）内部冲突的水平很高（如围绕贫富差距和/或价值观差异的冲突），（3）该国受到一个或多个强大的外国挑战时，这通常会造成（4）一场交互的自我强化的衰落。这是因为该国不断恶化的财政状况使其无法满足国内支出需求和为战争筹资，进而导致更糟糕的结果。</strong></p><p>如一些之前的图所展示的，我们发现：<strong>（1）这些决定因素往往会相互强化，无论是优势变强（例如更好的教育往往会带来更多的收入）还是劣势加剧（例如贸易疲软会导致更多的负债），所以这些因素往往会以周期的形式发生，共同形成大周期；（2）当决定因素不强且在继续弱化时，帝国就处于弱势，而且不断变得更弱。[插图]当许多决定因素增强时，大规模的上升就会发生；当许多决定因素减弱时，大规模的下降就会发生。</strong></p><p><strong>应对已知与未知</strong></p><p><strong>●了解所有的可能性，考虑最坏的情况，然后想办法消除无法忍受的情况。</strong>首先要确定和消除无法忍受的最坏情况。这是因为，在生活或市场的博弈中，最重要的是不要被淘汰出局。</p><p><strong>●分散风险。除了确保我考虑到了所有能想到的最坏情况，我还试图通过有效分散风险，为我想不到的地方做准备。</strong></p><p><strong>●首先考虑延迟满足而不是当下满足，这样你将来会过得更好。</strong></p><p><strong>●与最聪明的人反复沟通。我紧跟我能找到的最聪明的人，这样我就能测试我的想法，并向他们学习。</strong></p><p>使用我给你们的测量方法，或者根据统计数字构建自己的测量方法，来（1）衡量你的国家和你感兴趣的其他国家的健康程度；（2）看其健康程度是在改善还是在变差，以及以何种方式改善或变差；（3）改变未来的决定因素，以赢得更好的未来。</p>]]></content>
      
      
      <categories>
          
          <category> Literature </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《非暴力沟通》</title>
      <link href="/2022/07/18/nonviolent-communication/"/>
      <url>/2022/07/18/nonviolent-communication/</url>
      
        <content type="html"><![CDATA[<h1 id="一、由衷的给予"><a href="#一、由衷的给予" class="headerlink" title="一、由衷的给予"></a>一、由衷的给予</h1><p>非暴力沟通帮助我们与他人和自己建立连结，使我们得以流露与生俱来的慈悲之心。它指引我们通过将注意力聚焦在观察、感受、需要和请求这四个要素，来重新构建表达和聆听的方式。</p><p>非暴力沟通培育深度倾听的能力，让我们带着尊重和同理心对待他人，并且发自内心地相互给予。用非暴力沟通的方式，有的人学会了爱自己，有的人在人际关系中创造了更深厚的连结，有的人在工作或政治领域中建立了更有效的关系。在世界各地，非暴力沟通还被用来调和各个层面的纠纷和冲突。</p><h1 id="二、疏离生命的语言"><a href="#二、疏离生命的语言" class="headerlink" title="二、疏离生命的语言"></a>二、疏离生命的语言</h1><p>由衷的给予和接受，是人类天性所乐见的。然而我们习得了太多疏离生命的语言形式，导致我们的说话和行为方式给他人和自己带来伤害。其中的一种形式是道德评判，即认为那些不符合我们价值观的人是不对的、不好的。另一种形式是做比较，让人们难以升起对人对己的善意。疏离生命的语言还会使我们无法认清：每个人要为自己的想法、感受和行为负责。此外，还有一种形式是用要求来表达我们的诉求。</p><h1 id="三、不带评论的观察"><a href="#三、不带评论的观察" class="headerlink" title="三、不带评论的观察"></a>三、不带评论的观察</h1><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3bdf9e1af017e029.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>非暴力沟通的第一个要素是区分观察与评论。当我们在观察中夹杂着自己的评论时，他人往往会认为我们在批评他们，并因而产生抗拒的心理。非暴力沟通是一种动态的语言，它不鼓励人们做静态、一概而论的陈述，而是提倡我们在描述观察时，清楚地说出特定的时间和情境。例如“汉克在过去20场球赛中未进一球”，而非“汉克是名糟糕的足球运动员”。</p><h1 id="四、体会和表达感受"><a href="#四、体会和表达感受" class="headerlink" title="四、体会和表达感受"></a>四、体会和表达感受</h1><p>非暴力沟通的第二个要素是感受。通过建立表达感受的词汇表，我们可以更清晰明确地体会和表达感受，从而更好地与他人建立连结。允许自己表达感受、袒露脆弱，也会有助于化解冲突。此外，在非暴力沟通中，用来表达实际感受的语言和那些用来陈述想法、评论或诠释观点的语言是不同的。</p><h1 id="五、为自己的感受负责"><a href="#五、为自己的感受负责" class="headerlink" title="五、为自己的感受负责"></a>五、为自己的感受负责</h1><p>非暴力沟通的第三个要素，是看见感受背后的需要。他人的言行也许会刺激我们的感受，但并不是感受的根源。听到不中听的话时，我们可以有四种选择来接收：（1）指责自己；（2）指责他人；（3）体会自己的感受与需要；（4）体会他人在消极的话语下隐藏着什么感受与需要。</p><p>评判、批评、分析和论断他人，都使我们疏远了自己的需要和价值观。当人们听到批评，往往会以自我辩护或反击来回应。我们越能够直接说出自己的感受与需要，他人就越有可能对我们做出善意的回应。</p><p>在我们身处的世界中，我们时常会因为看见和袒露自身需要遭到强烈抨击，因而对表达需要感到害怕。对女性来说，尤其如此。长期以来，社会文化将她们塑造成了只懂照顾他人而无视自己需要的样子。</p><p>在通往情绪自由的成长路上，许多人都会经历三个阶段：（1）“情绪的奴隶”——我们认为自己要为他人的感受负责；（2） “面目可憎”——我们拒绝考虑他人的感受或需要；（3）“情绪的主人”——我们全然地为自己的感受负责，同时我们也意识到，牺牲他人的福祉也无法满足自己的需要。</p><h1 id="六、提出请求，丰盈生命"><a href="#六、提出请求，丰盈生命" class="headerlink" title="六、提出请求，丰盈生命"></a>六、提出请求，丰盈生命</h1><p>非暴力沟通的第四个要素是提出请求，以便让我们的生命更加丰盈。在提出请求时，我们要尽力避免模糊、抽象或模棱两可的语言，说明我们要什么，而不是不要什么。</p><p>在开口时，我们越是把想要得到的回应表达清楚，就越有可能得到这样的回应。由于我们所表达的信息与别人的理解有可能不一致，我们需要学习去发现对方是否已经准确无误地接收到了我们的信息。特别是在团体讨论中，更需要清楚知道和说明我们想要的回应。否则，讨论可能只是在浪费大家的时间。</p><p>一旦人们认为不答应我们的请求就会受到责罚，“请求”就成了“要求”。为了让人们信任我们所提出的是“请求”而非“要求”，可以清楚地表明我们希望人们出于自愿来满足请求。非暴力沟通的意图不是为了改变他人来满足自己，而是帮助双方建立坦诚和有同理心的关系，最终每个人的需要都能得到满足。</p><h1 id="七、以同理心倾听"><a href="#七、以同理心倾听" class="headerlink" title="七、以同理心倾听"></a>七、以同理心倾听</h1><p>所谓“同理”就是带着尊重来理解他人的经历。我们常常有强烈的冲动想给他人建议或安慰，或是解释自己的立场和感受。同理则邀请我们清空先入为主的想法，全身心地聆听他人。</p><p>活在非暴力沟通中意味着，不论他人用什么样的言辞来表达自己，我们都只是聆听他们的观察、感受、需要和请求。接着，我们可以选择反馈他们的意思、复述我们的理解。此外，持续地同理让他人有机会充分表达自己，当他人感到被充分理解后，我们再来关注解决方案或提出请求。</p><p>我们自己也需要获得同理才能更好地同理他人。当意识到自己处于辩解防卫的状态或痛苦得无法同理他人时，我们可以选择（1）停顿，深呼吸，同理自己；（2）“非暴力呐喊”；（3）离开现场。</p><h1 id="八、同理心的力量"><a href="#八、同理心的力量" class="headerlink" title="八、同理心的力量"></a>八、同理心的力量</h1><p>同理他人使得我们敢于呈现自己的脆弱，平息潜在的暴力，让乏味的对话变得有趣，并了解“不”和沉默所传达的感受和需要。一次又一次，我见证了同理倾听帮助人们疗愈心灵的伤痛。</p><h1 id="九、爱自己"><a href="#九、爱自己" class="headerlink" title="九、爱自己"></a>九、爱自己</h1><p>非暴力沟通最关键的应用或许就是让我们学会善待自己。当犯了错误，我们可以运用非暴力沟通的“哀悼”与“自我宽恕”来看清个人可以成长的方向，而不会陷入对自己的道德评判。评价自己的行为时，若着眼于“有哪些未被满足的需要”，我们的改变就能不受羞愧、内疚、愤怒或压抑所驱动，而是由衷地想要对自己和他人的幸福做出贡献。</p><p>同时，在日常生活中，有觉知地根据需要和价值观来选择行动，而不是为了履行职责、获得外在的奖励，或是逃避内疚、羞愧和惩罚，我们便是在培养对自己的爱。当我们重新检视那些无法带着乐趣来做的事情，并将“我不得不做”的想法化为“我选择这么做”，我们将会在生活中发现更多的乐趣，并且成为一个更完整的人。</p><h1 id="十、充分表达愤怒"><a href="#十、充分表达愤怒" class="headerlink" title="十、充分表达愤怒"></a>十、充分表达愤怒</h1><p>生气时，指责和惩罚他人都无法真正传达我们的心声。如果想充分表达愤怒，首先要认识到他人并不需要对我们的愤怒负责。取而代之的是，把注意力放在自己的感受和需要上。相较于评判、指责或惩罚他人，我们若能把自己的需要表达出来，将更有可能让这些需要得到满足。</p><p>表达愤怒的4个步骤是：（1）停下来，深吸一口气；（2）辨识脑海中评判性的想法；（3）与自己的需要连结；（4）表达自己的感受和未满足的需要。有时，在第3和第4步之间，我们也许需要先同理对方。这样，当我们在第4步表达自己时，对方才能更好地听见我们。</p><h1 id="十一、化解冲突，调和纷争"><a href="#十一、化解冲突，调和纷争" class="headerlink" title="十一、化解冲突，调和纷争"></a>十一、化解冲突，调和纷争</h1><p>用非暴力沟通化解冲突时，最重要的是建立冲突双方之间的连结。人与人之间只有建立了连结，才会发自内心地想要了解彼此的感受和需要。唯有如此，非暴力沟通的步骤才能发挥效用。此外，你从一开始就要让双方明白，冲突调解的目标并非让一方服从于另一方。只有当双方都能理解这点，才有可能（甚至很容易）就如何满足大家的需要开展对话。</p><p>非暴力沟通鼓励我们建立人与人之间的连结，这意味着，当我作为第三方调解人（即有一位第三方人员介入冲突的两方进行调和工作）和冲突中的双方（个人或者团体）开展工作时，我的工作方法和职业调解人常采用的方法大不相同。</p><p>理解与表达需要是非暴力沟通冲突解决的核心，让我们来回顾这一贯穿本书（尤其在第五章中）的核心概念。</p><p>要用非暴力沟通来化解冲突，我们需要训练自己——无论他人用什么方式来表达，都听见那背后的需要。如果我们真的想支持他人，就要先学习将任何信息翻译为需要。沉默、否定、评判、肢体语言或请求等都是“信息”的不同呈现形式。我们要磨练的技巧是聆听每个信息中所隐含的需要，即使一开始是通过猜测。</p><h1 id="十二、为了保护使用强制力"><a href="#十二、为了保护使用强制力" class="headerlink" title="十二、为了保护使用强制力"></a>十二、为了保护使用强制力</h1><p>在有些没有机会沟通（例如危险迫在眉睫）的情况下，我们需要采取保护性强制力。这样做的目的是防止人们受到伤害或不公待遇，而不是为了惩罚他人或让他人难受、忏悔或改变。惩罚性强制力会让人产生敌意和抵触心理。惩罚还会伤害他人的善意和自尊，也会让我们只注意行为的外在后果，而忽视行为本身的价值。惩罚和指责也无法让他人按照我们所期望的理由来行动。</p><h1 id="十三、解放自我，协助他人"><a href="#十三、解放自我，协助他人" class="headerlink" title="十三、解放自我，协助他人"></a>十三、解放自我，协助他人</h1><p>非暴力沟通可以帮助我们将内心的负面信息转化成感受和需要，从而改善和自己的沟通。如果有能力做到同理自身的感受和需要，我们就可以转化抑郁情绪。非暴力沟通教导我们如何专注在自己及他人心底真正的渴望上，摆脱在自己或他人身上纠错的习惯，从而创造更加和平的心境。心理辅导与治疗的专业人士也可以用非暴力沟通与来访者建立真诚的关系。</p><h1 id="十四、用非暴力沟通表达感激与赞赏"><a href="#十四、用非暴力沟通表达感激与赞赏" class="headerlink" title="十四、用非暴力沟通表达感激与赞赏"></a>十四、用非暴力沟通表达感激与赞赏</h1><p>我们惯常的赞美方式往往带有评判，哪怕是正向的，有时甚至被用来操纵他人的行为。非暴力沟通所鼓励的是，向他人表达感激时，庆祝是我们唯一的意图。我们表达的内容包括：（1）对方做了什么对我们的福祉做出了贡献；（2）我们有哪些需要得到了满足；（3）我们因此产生了什么样的愉悦感受。</p><p>当我们接收这样的赞赏和感谢时，我们也能免于自负或是假谦虚，而是和对方一起庆祝。</p>]]></content>
      
      
      <categories>
          
          <category> Literature </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一次线上内存使用率异常问题排查</title>
      <link href="/2022/06/02/golang-pprof-mem/"/>
      <url>/2022/06/02/golang-pprof-mem/</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h1><p>朋友的一个服务，某个集群内存的<code>RSS</code>使用率一直在<code>80%</code>左右，他用的是<code>8核16G</code>, 双机房一共<code>206</code>个实例。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5001071b9627f07b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>但是在<code>pprof</code>里面查的堆内存才使用了<code>6.3G</code>左右，程序里面主要用了<code>6G</code>的<code>LocalCache</code>所以<code>heap</code>用了<code>6.3G</code>是符合预期的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9602a9ef42c1af94.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>朋友让我帮忙看下，额外的内存到底是被啥占用了。</p><h1 id="二、基础知识"><a href="#二、基础知识" class="headerlink" title="二、基础知识"></a>二、基础知识</h1><h2 id="2-1-TCMalloc-算法"><a href="#2-1-TCMalloc-算法" class="headerlink" title="2.1 TCMalloc 算法"></a>2.1 TCMalloc 算法</h2><p><a href="http://goog-perftools.sourceforge.net/doc/tcmalloc.html">Thread-Caching Malloc</a> 是<code>Google</code>开发的内存分配算法库，最开始它是作为<code>Google</code>的一个性能工具库<code>perftools</code>的一部分。</p><p><code>TCMalloc</code>是用来替代传统的<code>malloc</code>内存分配函数。它有减少内存碎片，适用于多核，更好的并行性支持等特性。</p><h2 id="2-2-mmap-函数"><a href="#2-2-mmap-函数" class="headerlink" title="2.2 mmap 函数"></a>2.2 mmap 函数</h2><p><code>mmap</code>它的主要功能是将一个<code>虚拟内存区域</code>与一个<code>磁盘上的文件</code>关联起来，以初始化这个虚拟内存区域的内容，这个过程成为内存映射（<code>memory mapping</code>）。</p><p>直白一点说，就是可以将<code>一个文件</code>，映射到一段<code>虚拟内存</code>，写内存的时候操作系统会自动同步内存的内容到文件。内存同步到磁盘，还涉及到一个<code>PageCache</code>的概念，这里不去过度发散，感兴趣朋友可以自己搜下。</p><p><code>文件</code>可以是磁盘上的一个<code>实体文件</code>，比如<code>kafka</code>写日志文件的时候，就用了<code>mmap</code>。</p><p><code>文件</code>也可以是一个<code>匿名文件</code>，这种场景<code>mmap</code>不会去写磁盘，主要用于内存申请的场景。比如调用<code>malloc</code>函数申请内存，当申请的大小超过<code>MMAP_THRESHOLD</code>（默认是<code>128K</code>）大小，内核就会用<code>mmap</code>去申请内存。再比如<code>TCMalloc</code>也是通过<code>mmap</code>来申请一大块内存（<code>匿名文件</code>），然后切割内存，分配给程序使用。</p><p>网上很多资料一介绍<code>mmap</code>，就会说到<code>zero copy</code>，就是相对于<code>标准IO</code>来说少了一次内存<code>Copy</code>的开销。让大多数人忽略了<code>mmap</code>本质的功能，认为<code>mmap=zero copy</code></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-58f26fcf756d90b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>还有一个值得一说的<code>mmap</code>申请的内存不在虚拟地址空间的<code>堆区</code>，在<code>内存映射段（Memory Mapping Region）</code></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1987f229490dbaf5.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Application.jpg"></p><h2 id="2-3-Golang-内存分配"><a href="#2-3-Golang-内存分配" class="headerlink" title="2.3 Golang 内存分配"></a>2.3 Golang 内存分配</h2><p><a href="https://github.com/golang/go/blob/master/src/runtime/malloc.go">Golang的内存分配</a> 是用的 <code>TCMalloc</code>（<code>Thread-Caching Malloc</code>）算法, 简单点说就是<code>Golang</code>是使用 <a href="https://github.com/golang/go/blob/master/src/runtime/mem_linux.go#L185">mmap</a> 函数去操作系统申请一大块内存，然后把内存按照 <code>0~32KB``68</code>个 <code>size</code> 类型的<code>mspan</code>，每个<code>mspan</code>按照它自身的属性 <a href="https://github.com/golang/go/blob/master/src/runtime/sizeclasses.go#L93">Size Class</a> 的大小分割成若干个<code>object</code><a href="https://github.com/golang/go/blob/go1.16.6/src/runtime/sizeclasses.go">（每个span默认是8K）</a>，因为分需要<code>gc</code>的<code>mspan</code>和不需要<code>gc</code>的<code>mspan</code>，所以一共有<code>136</code>种类型。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-17d63fba4dcbecc5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><code>mspan</code>：<code>Go</code>中内存管理的基本单元，是由一片连续的<code>8KB</code>的页组成的大块内存，每个<code>mspan</code>按照它自身的属性<code>Size Class</code>的大小分割成若干个<code>object</code>，<code>mspan</code>的<code>Size Class</code>共有<a href="https://github.com/golang/go/blob/master/src/runtime/sizeclasses.go#L89">68种（算上0）</a> , <a href="https://github.com/golang/go/blob/master/src/runtime/mheap.go#L528"> numSpanClasses = _NumSizeClasses &lt;&lt; 1</a> (因为需要区分需要GC和不需要GC的)</p><p><code>mcache</code>：每个工作线程都会绑定一个<code>mcache</code>，本地缓存可用的<code>mspan</code>资源。</p><p><code>mcentral</code>：为所有 <code>mcache</code>提供切分好的 <code>mspan</code>资源。需要加锁</p><p><code>mheap</code>：代表<code>Go</code>程序持有的所有堆空间，<code>Go</code>程序使用一个<code>mheap</code>的全局对象<a href="https://github.com/golang/go/blob/master/src/runtime/mheap.go#L216">_mheap</a>来管理堆内存。</p><!--![image.png](https://upload-images.jianshu.io/upload_images/12321605-97e487c33ee90c7c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)--><p><code>Go</code>的内存分配器在分配对象时，根据对象的大小，分成三类：小对象（小于等于<code>16B</code>）、一般对象（大于<code>16B</code>，小于等于<code>32KB</code>）、大对象（大于<code>32KB</code>）。</p><p>大体上的分配流程：</p><ul><li><code>&gt;32KB</code> 的对象，直接从<code>mheap</code>上分配；</li><li><code>(16B,32KB]</code> 的对象，首先计算对象的规格大小，然后使用<code>mcache</code>中相应规格大小的<code>mspan</code>分配；</li><li><code>&lt;=16B</code> 的对象使用<code>mcache</code>的<code>tiny</code>分配器分配；</li></ul><p>如果<code>mcache</code>没有相应规格大小的<code>mspan</code>，则向<code>mcentral</code>申请<br>如果<code>mcentral</code>没有相应规格大小的<code>mspan</code>，则向<code>mheap</code>申请<br>如果<code>mheap</code>中也没有合适大小的<code>mspan</code>，则向操作系统申请</p><h2 id="2-4-TCMalloc-的内存浪费"><a href="#2-4-TCMalloc-的内存浪费" class="headerlink" title="2.4 TCMalloc 的内存浪费"></a>2.4 TCMalloc 的内存浪费</h2><p><code>Golang</code>的 <a href="https://github.com/golang/go/blob/master/src/runtime/sizeclasses.go#L6">sizeclasses.go</a> 源码里面已经给我们已经计算了出每个<code>size</code>的<code>tail waste</code>和<code>max waste</code>比例</p><pre><code>// class  bytes/obj  bytes/span  objects  tail waste  max waste  min align//     1          8        8192     1024           0     87.50%          8//     2         16        8192      512           0     43.75%         16//     3         24        8192      341           8     29.24%          8//     4         32        8192      256           0     21.88%         32//     5         48        8192      170          32     31.52%         16//     6         64        8192      128           0     23.44%         64//     7         80        8192      102          32     19.07%         16//     8         96        8192       85          32     15.95%         32//     9        112        8192       73          16     13.56%         16//    10        128        8192       64           0     11.72%        128.... 略//    58      14336       57344        4           0      5.35%       2048//    59      16384       16384        1           0     12.49%       8192//    60      18432       73728        4           0     11.11%       2048//    61      19072       57344        3         128      3.57%        128//    62      20480       40960        2           0      6.87%       4096//    63      21760       65536        3         256      6.25%        256//    64      24576       24576        1           0     11.45%       8192//    65      27264       81920        3         128     10.00%        128//    66      28672       57344        2           0      4.91%       4096//    67      32768       32768        1           0     12.50%       8192</code></pre><p>我们看下<code>tail waste</code>和<code>max waste</code>的计算方式，<a href="https://github.com/golang/go/blob/master/src/runtime/mksizeclasses.go#L238">源码如下</a>：</p><pre><code>    spanSize := c.npages * pageSize    objects := spanSize / c.size    tailWaste := spanSize - c.size*(spanSize/c.size)    maxWaste := float64((c.size-prevSize-1)*objects+tailWaste) / float64(spanSize)    alignBits := bits.TrailingZeros(uint(c.size))    if alignBits &gt; pageShift {        // object alignment is capped at page alignment        alignBits = pageShift    }    for i := range minAligns {        if i &gt; alignBits {            minAligns[i] = 0        } else if minAligns[i] == 0 {            minAligns[i] = c.size        }    }</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-44574dcf46fe8b7c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><code>sizeclase=8</code>的时候<code>obj= 96</code>，所以<code>tailWaste = 8192%96 = 32</code>，<code>maxWaste = ((96-80-1)* 85 + 32)/ 8192 = 0.1595</code></p><h2 id="2-5-Go-查看内存使用情况几种方式"><a href="#2-5-Go-查看内存使用情况几种方式" class="headerlink" title="2.5 Go 查看内存使用情况几种方式"></a>2.5 Go 查看内存使用情况几种方式</h2><ol><li><p>执行前添加系统环境变量<code>GODEBUG='gctrace=1'</code>来跟踪打印垃圾回收器信息，具体打印的内容含义可以参考<a href="https://pkg.go.dev/runtime">官方文档</a>。</p><pre><code> gctrace: 设置gctrace=1会使得垃圾回收器在每次回收时汇总所回收内存的大小以及耗时， 并将这些内容汇总成单行内容打印到标准错误输出中。 这个单行内容的格式以后可能会发生变化。 目前它的格式：     gc # @#s #%: #+#+# ms clock, #+#/#/#+# ms cpu, #-&gt;#-&gt;# MB, # MB goal, # P 各字段的含义：     gc #        GC次数的编号，每次GC时递增     @#s         距离程序开始执行时的时间     #%          GC占用的执行时间百分比     #+...+#     GC使用的时间     #-&gt;#-&gt;# MB  GC开始，结束，以及当前活跃堆内存的大小，单位M     # MB goal   全局堆内存大小     # P         使用processor的数量 如果信息以"(forced)"结尾，那么这次GC是被runtime.GC()调用所触发。  如果gctrace设置了任何大于0的值，还会在垃圾回收器将内存归还给系统时打印一条汇总信息。 这个将内存归还给系统的操作叫做scavenging。 这个汇总信息的格式以后可能会发生变化。 目前它的格式：     scvg#: # MB released  printed only if non-zero     scvg#: inuse: # idle: # sys: # released: # consumed: # (MB) 各字段的含义:     scvg#        scavenge次数的变化，每次scavenge时递增     inuse: #     MB 垃圾回收器中使用的大小     idle: #      MB 垃圾回收器中空闲等待归还的大小     sys: #       MB 垃圾回收器中系统映射内存的大小     released: #  MB 归还给系统的大小     consumed: #  MB 从系统申请的大小</code></pre></li><li><p>代码中使用<code>runtime.ReadMemStats</code>来获取程序当前内存的使用情况</p><pre><code> var m runtime.MemStats runtime.ReadMemStats(&amp;m)</code></pre></li><li><p>通过<code>pprof</code>获取</p><pre><code>  http://127.0.0.1:10000/debug/pprof/heap?debug=1    在输出的最下面有MemStats的信息    # runtime.MemStats # Alloc = 105465520 # TotalAlloc = 334874848 # Sys = 351958088 # Lookups = 0 # Mallocs = 199954 # Frees = 197005 # HeapAlloc = 105465520 # HeapSys = 334954496 # HeapIdle = 228737024 # HeapInuse = 106217472 # HeapReleased = 218243072 # HeapObjects = 2949 # Stack = 589824 / 589824 # MSpan = 111656 / 212992 # MCache = 9600 / 16384 # BuckHashSys = 1447688 # GCSys = 13504096 # OtherSys = 1232608 # NextGC = 210258400 # LastGC = 1653972448553983197</code></pre></li></ol><h2 id="2-6-Sysmon-监控线程"><a href="#2-6-Sysmon-监控线程" class="headerlink" title="2.6 Sysmon 监控线程"></a>2.6 Sysmon 监控线程</h2><p><code>Go Runtime</code>在启动程序的时候，会创建一个独立的<code>M</code>作为监控线程，称为<code>sysmon</code>，它是一个系统级的<code>daemon</code>线程。这个<code>sysmon</code>独立于<code>GPM</code>之外，也就是说不需要<code>P</code>就可以运行，因此官方工具<code>go tool trace</code>是无法追踪分析到此线程。</p><p><code>sysmon</code>执行一个无限循环，一开始每次循环休眠<code>20us</code>，之后（<code>1ms</code>后）每次休眠时间倍增，最终每一轮都会休眠 <code>10ms</code>。</p><p><code>sysmon</code>主要如下几件事</p><ul><li>释放闲置超过<code>5</code>分钟的<code>span</code>物理内存，<code>scavenging</code>。（Go 1.12之前）</li><li>如果超过两分钟没有执行垃圾回收，则强制执行<code>GC</code>。</li><li>将长时间未处理的<code>netpoll</code>结果添加到任务队列</li><li>向长时间运行的<code>g</code>进行抢占</li><li>收回因为<code>syscall</code>而长时间阻塞的<code>p</code></li></ul><h1 id="三、问题排查过程"><a href="#三、问题排查过程" class="headerlink" title="三、问题排查过程"></a>三、问题排查过程</h1><h2 id="3-1-内存泄露？"><a href="#3-1-内存泄露？" class="headerlink" title="3.1 内存泄露？"></a>3.1 内存泄露？</h2><p>服务内存不正常，本能反应是不是内存泄露了？朋友说他们服务内存一周内一直都是在<code>80%~85%</code>左右波动，然后<code>pprof</code>看的<code>heap</code>的使用也是符合预期的。看了下程序的<code>Runtime</code>监控，容器的内存监控，都是正常的。基本可以排除内存泄露的可能性。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4b7d1d580f80422e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-bdf636713d6259f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="3-2-madvise"><a href="#3-2-madvise" class="headerlink" title="3.2 madvise"></a>3.2 madvise</h2><p>排除了内存泄露的可能性，再一个让人容易想到的坑就是<code>madvise</code>，这个感觉是<code>GO 1.12</code> ~ <code>Go 1.15</code> 版本，被提到很多次的问题。</p><h3 id="什么是-madvise-？"><a href="#什么是-madvise-？" class="headerlink" title="什么是 madvise ？"></a>什么是 madvise ？</h3><p><a href="https://man7.org/linux/man-pages/man2/madvise.2.html">madvise()</a> 函数建议内核,在从<code>addr</code>指定的地址开始,长度等于<code>len</code>参数值的范围内,该区域的用户虚拟内存应遵循特定的使用模式。内核使用这些信息优化与指定范围关联的资源的处理和维护过程。如果使用<a href="https://man7.org/linux/man-pages/man2/madvise.2.html">madvise()</a>函数的程序明确了解其内存访问模式,<strong>则使用此函数可以提高系统性能</strong>。”</p><ul><li><code>MADV_FREE</code> ：（<code>Linux 4.5</code>以后开始支持这个特性），内核在当出现内存压力时才会主动释放这块内存。</li><li><code>MADV_DONTNEED</code>：预计未来长时间不会被访问，可以认为应用程序完成了对这部分内容的访问，因此内核可以立即释放与之相关的资源。</li></ul><h3 id="Go-Runtime-对-madvise-的使用"><a href="#Go-Runtime-对-madvise-的使用" class="headerlink" title="Go Runtime 对 madvise 的使用"></a>Go Runtime 对 madvise 的使用</h3><p>在<code>Go 1.12</code>版本的时候，为了提高内存的使用效率，把<code>madvise</code>的参数从<code>MADV_DONTNEED</code>改成<code>MADV_FREE</code>，<a href="https://go-review.googlesource.com/c/go/+/135395/">具体可以看这个CR</a>，然后又加个<code>debug</code>参数来可以控制分配规则改回为<code>MADV_DONTNEED</code>，<a href="https://go-review.googlesource.com/c/go/+/155931/">具体可以看这个CR</a></p><p><code>runtime</code>中调用<code>madvise</code>的<a href="https://github.com/golang/go/blob/master/src/runtime/mem_linux.go#L106">代码如下</a>： </p><pre><code>var adviseUnused = uint32(_MADV_FREE)func sysUnused(v unsafe.Pointer, n uintptr) {    // ... 略        var advise uint32    if debug.madvdontneed != 0 {        advise = _MADV_DONTNEED    } else {        advise = atomic.Load(&amp;adviseUnused)    }    if errno := madvise(v, n, int32(advise)); advise == _MADV_FREE &amp;&amp; errno != 0 {        // MADV_FREE was added in Linux 4.5. Fall back to MADV_DONTNEED if it is        // not supported.        atomic.Store(&amp;adviseUnused, _MADV_DONTNEED)        madvise(v, n, _MADV_DONTNEED)    }}</code></pre><p>使用<code>MADV_FREE</code>的问题是，<code>Golang</code>程序释放的内存，操作系统并不会立即回收，只有操作系统内存紧张的时候，才会主动去回收，而我们的程序，都是跑在容器中的，所以造成了，我们容器内存使用快满了，但是物理机的内存还有很多内存，导致的现象就是用<code>pprof</code>看的内存不一样跟看的<code>RES</code>相差巨大。</p><p>由于<code>MADV_FREE</code>导致的<code>pprof</code>和<code>top</code>内存监控不一致的问题，导致很多开发者在<code>GO</code>的<code>GitHub</code>上提<code>issue</code>，最后<code>Austin Clements</code>（<code>Go</code>开源大佬）拍板，把<code>MADV_FREE</code>改回了<code>MADV_DONTNEED</code>，<a href="https://go-review.googlesource.com/c/go/+/267100">具体可以看这个CR</a></p><p>大佬也在代码里面做了个简单解释如下：</p><pre><code>// On Linux, MADV_FREE is faster than MADV_DONTNEED,// but doesn't affect many of the statistics that// MADV_DONTNEED does until the memory is actually// reclaimed. This generally leads to poor user// experience, like confusing stats in top and other// monitoring tools; and bad integration with// management systems that respond to memory usage.// Hence, default to MADV_DONTNEED.</code></pre><p>该改动已经在 <a href="https://go.dev/doc/go1.16">Go 1.16</a> 合入了。我看了下朋友服务的<code>GO</code>版本是<code>1.17</code>，所以是<code>MADV_FREE</code>的问题基本也可以排除了。</p><h2 id="2-3-memory-scavenging"><a href="#2-3-memory-scavenging" class="headerlink" title="2.3 memory scavenging"></a>2.3 memory scavenging</h2><p>既然排除了<code>内存泄露</code>，然后也不是<code>madvise()</code>的问题，只能猜想是不是<strong>内存是不是还没有归还给操作系统</strong>。</p><p><code>Go</code>把内存归还给系统的操作叫做<code>scavenging</code>。在<code>Go</code>程序执行过程中，当对象释放的时候，对象占用的内存并没有立即返还给操作系统(为了提高内存分配效率，方式归还以后又理解需要申请)，而是需要等待<code>GC</code>（定时或者条件触发）和<code>scavenging</code>（定时或者条件触发）才会把空闲的内存归还给操作系统。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-bb7f35b62139fda7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>当然我们也可以在代码里面调用<code>debug.FreeOSMemory()</code>来主动释放内存。<code>debug.FreeOSMemory()</code>的功能是强制进行垃圾收集，然后尝试将尽可能多的内存返回给操作系统。<a href="https://github.com/golang/go/blob/master/src/runtime/mheap.go#L1573">具体代码实现如下</a>：</p><pre><code>//go:linkname runtime_debug_freeOSMemory runtime/debug.freeOSMemoryfunc runtime_debug_freeOSMemory() {    GC() // 第一步强制 GC    systemstack(func() { mheap_.scavengeAll() }) // 第二步 scavenging}</code></pre><h3 id="GC-触发机制"><a href="#GC-触发机制" class="headerlink" title="GC 触发机制"></a>GC 触发机制</h3><p><code>GO</code>的<code>GC</code>触发可以分为主动触发和被动触发，主动触发就是在代码里面主动执行<code>runtime.GC()</code>，线上环境我们一般很少主动触发。这里我们主要讲下被动触发，被动触发有两种情况：</p><ol><li><p>当前内存分配达到一定比例则触发，可以通过环境变量<code>GOGC</code>或者代码中调用<code>runtime.SetGCPercent</code>来设置，默认是<code>100</code>，表示内存增长<code>1</code>倍触发一次<code>GC</code>。比如一次回收完毕后，内存的使用量为<code>5M</code>，那么下次回收的时机则是内存分配达到<code>10M</code>的时候。</p></li><li><p>定时触发<code>GC</code>，这个是<code>sysmon</code>线程里面干的时区，一般是<code>2</code>分钟（<code>runtime</code>中写死的）内没有触发<code>GC</code>，会强制执行一次<code>GC</code>。<a href="https://github.com/golang/go/blob/master/src/runtime/proc.go#L5250">具体代码如下</a>：</p><pre><code> // forcegcperiod is the maximum time in nanoseconds between garbage // collections. If we go this long without a garbage collection, one // is forced to run. // // This is a variable for testing purposes. It normally doesn't change. var forcegcperiod int64 = 2 * 60 * 1e9</code></pre></li></ol><pre><code>    // gcTriggerTime indicates that a cycle should be started when    // it's been more than forcegcperiod nanoseconds since the    // previous GC cycle.    gcTriggerTime    // check if we need to force a GC    if t := (gcTrigger{kind: gcTriggerTime, now: now}); t.test() &amp;&amp; atomic.Load(&amp;forcegc.idle) != 0 {        lock(&amp;forcegc.lock)        forcegc.idle = 0        var list gList        list.push(forcegc.g)        injectglist(&amp;list)        unlock(&amp;forcegc.lock)    }</code></pre><h3 id="scavenging-触发机制"><a href="#scavenging-触发机制" class="headerlink" title="scavenging 触发机制"></a>scavenging 触发机制</h3><p><code>GO 1.12</code>之前是通过定时触发，<code>2.5min</code>会执行一次<code>scavenge</code>，然后会回收<code>超过5分钟内没有使用过的mspan</code>，<a href="https://github.com/golang/go/blob/release-branch.go1.12/src/runtime/proc.go#L4357">具体源码如下</a>：</p><pre><code>// If a heap span goes unused for 5 minutes after a garbage collection,// we hand it back to the operating system.scavengelimit := int64(5 * 60 * 1e9)// scavenge heap once in a whileif lastscavenge+scavengelimit/2 &lt; now {    mheap_.scavenge(int32(nscavenge), uint64(now), uint64(scavengelimit))    lastscavenge = now    nscavenge++}</code></pre><p>这样会有个问题是，如果不停的有大量内存申请和释放，会导致<code>mspan</code>内存一直不会释放给<code>操作系统</code>（因为不停被使用然后释放），导致堆内存监控和<code>RSS</code>监控不一致。具体可以看 <a href="https://github.com/golang/go/issues/14045">runtime: reclaim memory used by huge array that is no longer referenced</a> 这个<code>Issue</code>，还有一个问题因为内存释放不及时，容易在低内存的设备上<code>OOM</code>，具体可以看 <a href="https://medium.com/samsara-engineering/running-go-on-low-memory-devices-536e1ca2fe8f">Running Go on Low Memory Devices</a> 这个文章。</p><p>基于以上这些问题，<code>Austin Clements</code>大佬提交了一个<code>Issue</code>：<a href="https://github.com/golang/go/issues/16930">runtime: make the scavenger more prompt</a>，<code>Austin Clements</code>提出如果我们只考虑在<code>scavenge</code>阶段需要释放多少个<code>mspan</code>，这个是比较难的。我们应该分离关注点，通过关注<code>释放和重新获得内存的成本</code>，<code>下次GC的堆大小</code>，<code>我们愿意承担的CPU和内存开销</code>来计算出应该释放多少<code>mspan</code>，提议保留的内存大小应该是过去一段时间内，堆内存回收大小的峰值乘以一个常数，计算回收方式如下：</p><pre><code>retain = C * max(current heap goal, max({heap goals of GCs over the past T seconds}))C = 1 + ε = 1.1T = 10 seconds</code></pre><p>这个提议<code>2016.08.31</code>提出以后，但是一直没有人去实现。</p><p>直到<code>2019.02.21</code>的时候<code>Michael Knyszek</code>重新提了一个<code>Proposal</code>：<a href="https://github.com/golang/go/issues/30333">runtime: smarter scavenging</a>。</p><p>这个<code>Proposal</code>目标是：</p><ol><li>降低<code>Go</code>应用程序的<code>RSS</code>平均值和峰值。</li><li>使用尽可能少<code>CPU</code>来持续降低<code>RSS</code>。</li></ol><p><code>runtime</code>做内存回收策略，有三个关键问题</p><ol><li>内存回收的速率是多少？</li><li>我们应该保留多少内存？</li><li>什么内存我们应该归还给操作系统？</li></ol><p>实现方法</p><ol><li><code>Scavenge</code>速度应该与程序<code>Alloc</code>内存的速度保持一致。</li><li>保留的内存大小应该是一个常量乘以过去<code>N</code>次<code>GC</code>的峰值。<a href="https://github.com/golang/go/issues/16930">runtime: make the scavenger more prompt</a></li><li>在<code>unscavenged spans</code>中，优先清除基地址高的。</li></ol><p>上面的<code>Proposal</code>主要提交如下：</p><p><a href="https://go-review.googlesource.com/c/go/+/142960/">runtime: add background scavenger</a></p><p><a href="https://go-review.googlesource.com/c/go/+/143157/">runtime: remove periodic scavenging</a></p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>上面，我们知道了<code>pprof</code>抓的堆内存的大小和<code>RSS</code>不一致，有几种可能：</p><ol><li>是程序申请的内存还没有被<code>GC</code>。</li><li>内存虽然被<code>GO</code>执行了<code>GC</code>，但是可能并没有归还给操作系统（<code>scavenging</code>）。</li></ol><p>为了验证一下上面的结论，我上机器抓了下<code>heap</code>的统计：</p><pre><code>nx-x-x(lark.arch.dts@stock:prod):dts# curl http://ip:port/debug/pprof/heap?debug=1 | grep Heap  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                 Dload  Upload   Total   Spent    Left  Speed  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0# 0xa47ba9        runtime/pprof.writeHeapInternal+0xc9                    /usr/local/go/src/runtime/pprof/pprof.go:566#       0xa47a46        runtime/pprof.writeHeap+0x26                            /usr/local/go/src/runtime/pprof/pprof.go:536100  913M    0  913M    0     0  86.8M      0 --:--:--  0:00:10 --:--:-- 90.6M# 0xa47ba9        runtime/pprof.writeHeapInternal+0xc9                    /usr/local/go/src/runtime/pprof/pprof.go:566#       0xa47a46        runtime/pprof.writeHeap+0x26                            /usr/local/go/src/runtime/pprof/pprof.go:536# HeapAlloc = 11406775960# HeapSys = 13709377536# HeapIdle = 2032746496# HeapInuse = 11676631040# HeapReleased = 167829504# HeapObjects = 49932438</code></pre><p>这里我主要关注几个参数：</p><p><a href="https://github.com/golang/go/blob/master/src/runtime/mstats.go#L160">HeapInuse</a>： 堆上使用中的<code>mspan</code>大小。</p><p><a href="https://github.com/golang/go/blob/master/src/runtime/mstats.go#L173">HeapReleased</a>：归还了多少内存给操作系统。</p><p><a href="https://github.com/golang/go/blob/master/src/runtime/mstats.go#L145">HeapIdle</a>：空闲的<code>mspan</code>大小。<code>HeapIdle - HeapReleased</code> 等于<code>runtime</code>持有了多少个空闲的<code>mspan</code>，这部分还没有释放给操作系统，在<code>pprof</code>的<code>heap</code>火焰图里面是看不到这部分内存的。</p><pre><code>stats.HeapIdle = gcController.heapFree.load() + gcController.heapReleased.load()</code></pre><p>上面我们获取机器的内存信息如下</p><pre><code>HeapInuse = 11676631040 ≈ 10.88G // 堆上使用内存的大小HeapIdle - HeapReleased = 2032746496 - 167829504 ≈ 1.73G // 可以归还但是没有归还的内存</code></pre><p>两个加起来，也差不多<code>12~13G</code>左右，所以容器的内存使用率是<code>80%</code>也是符合预期的。</p><p>还有个问题，为什么我们程序的<code>localcache</code>大小设置的只有了<code>6G</code>，实际<code>heap</code>使用了<code>10.88G</code>，因为<code>HeapInuse</code>除了程序真正使用的内存，还包括：</p><ol><li>程序释放的内存，但是还没有被<code>GC</code>。这部分内存还是算在<code>HeapInuse</code>中（这个应该是大头）。</li><li>上面说的<code>mspan</code>的<code>max waste</code>和<code>tail waste</code>这部分也在<code>HeapInuse</code>（这个应该很少）。</li><li>假设一个<code>8k</code>的<code>mspan</code>上只使用了一个大小为<code>8Byte</code>的<code>obj</code>，这个在<code>HeapInuse</code>会算<code>8K</code>。</li></ol><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p><code>golang</code>堆内存大小不一定跟<code>RSS</code>一致，它跟<code>GC</code>、<code>scavenging</code>时机有关。如果有大量<code>lcoalcache</code>申请释放，很可能导致<code>RSS</code>远远大于<code>heap</code>使用的大小。</p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Golang </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 自增列 Duplicate Error 问题分析</title>
      <link href="/2022/05/25/mysql-insert-auto-incre/"/>
      <url>/2022/05/25/mysql-insert-auto-incre/</url>
      
        <content type="html"><![CDATA[<h1 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h1><p>最近我们在做线上的数据迁移测试（可以理解就是把<code>A</code>数据中心的数据迁移到<code>B</code>数据中心，<code>A</code>和<code>B</code>数据中心的<code>MySQL</code>是同构的，迁移过程中，<code>A</code>、<code>B</code>的<code>MySQL</code>都有正常的业务数据写入，具体背景可以看 <a href="https://fanlv.wiki/2022/02/26/dts/">数据传输系统落地和思考</a> 这篇文章）。每次我们触发迁移的时候，就有业务方反馈他们写入数据的时候就会有<code>Error 1062: Duplicate entry 'xxx' for key 'PRIMARY'</code>这样的错误。业务方同学还反馈他们写数据的时候并没有指定<code>ID</code>，所以他们对这样的报错比较困惑，具体他们的数据写入的伪代码如下：</p><pre><code>type Data struct {    ID           int64     `gorm:"primaryKey;column:id"`    PageID       string    `gorm:"column:page_id`    CreateTime   time.Time `gorm:"column:create_time"`    ModifiedTime time.Time `gorm:"column:modified_time"`}data := &amp;Data{                PageID:       uuid.NewString(),                CreateTime:   now,                ModifiedTime: now,            }err := db.Create(data).Errorif err != nil {    return err}</code></pre><p>再交代一下其他的背景。</p><ol><li>业务上这个表的写入的<code>TPS</code>相对比较高，迁移的数据量也比较大。</li><li>我们做数据迁移的时候，从<code>A</code>数据中心迁移到<code>B</code>数据中心的时候，会抹掉<code>数据</code>中的<code>ID</code>数据，然后用一个中心的发号器<code>IDGenerator</code>生成一个新的<code>ID</code>，然后再插入这个数据。</li></ol><p>由于，每次都是在数据迁移的时候，报这个<code>PK Duplicate Error</code>的错误，基本肯定是我们做数据迁移导致的。引出几个问题：</p><ol><li><code>生成自增ID</code>实现方式？并发生成<code>ID</code>会不会冲突？</li><li><code>生成自增ID</code>加锁机制粒度，锁的释放机制是啥？</li><li><code>生成自增ID</code>和<code>唯一索引冲突检查</code>流程是怎么样的？</li></ol><p>其实已知的问题上看，基本猜想出，具体出现问题的场景如下：</p><table><thead><tr><th align="center">TimeLine</th><th align="center">Session 1</th><th align="center">Session 2</th></tr></thead><tbody><tr><td align="center">时刻1</td><td align="center">生成自增ID</td><td align="center">用IDgen生成ID</td></tr><tr><td align="center">时刻2</td><td align="center"></td><td align="center">唯一索引冲突检查（Pass）</td></tr><tr><td align="center">时刻3</td><td align="center"></td><td align="center">写入成功</td></tr><tr><td align="center">时刻4</td><td align="center">唯一索引冲突检查（报错Duplicate Error）</td><td align="center"></td></tr></tbody></table><p>结论我们知道，但是<code>MySQL</code>的<code>Insert</code>流程到底是如何做的，我并不清楚，问了一圈身边的朋友，好像大家对<code>Insert</code>过程这些细节都不太了解，所以决定自己简单撸下源码验证一下上面的结论。</p><h1 id="二、Auto-Incr-背景知识"><a href="#二、Auto-Incr-背景知识" class="headerlink" title="二、Auto-Incr 背景知识"></a>二、Auto-Incr 背景知识</h1><p><code>MySQL</code>的<a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-auto-increment-handling.html#innodb-auto-increment-lock-modes">《AUTO_INCREMENT Handling in InnoDB》</a> 这篇官方文档，其实把<code>AUTO_INCREMENT</code>相关特性都介绍很清楚了，我们做个简单总结。</p><ol><li><code>InnoDB</code>提供了一种可配置的锁定机制，可以显着提高向具有<code>AUTO_INCREMENT</code>列的表添加行的<code>SQL</code>语句的可伸缩性和性能。</li><li>定义为<code>AUTO_INCREMENT</code>的列，必须是索引的第一列或者是唯一列，因为需要使用<code>SELECT MAX(ai_col)</code>查找以获得最大值列值。不这样定义，<code>Create Table</code>的时候会报<code>1075 - Incorrect table definition; there can be only one auto column and it must be defined as a key</code>错误。</li><li><code>AUTO_INCREMENT</code>的列，可以只定义为普通索引，不一定要是<code>PRIMARY KEY</code>或者<code>UNIQUE</code>，但是为了保证<code>AUTO_INCREMENT</code>的唯一性，建议定义为<code>PK</code>或者<code>UNIQUE</code></li></ol><h2 id="2-1-MySQL插入语句的几种类型"><a href="#2-1-MySQL插入语句的几种类型" class="headerlink" title="2.1 MySQL插入语句的几种类型"></a>2.1 MySQL插入语句的几种类型</h2><p>在介绍<code>AUTO_INCREMENT</code>的锁模式之前，先介绍下，<code>MySQL</code>插入的几种类型：</p><ul><li><p><code>Simple inserts</code>，可以预先确定要插入的行数（当语句被初始处理时）的语句。 这包括没有嵌套子查询的单行和多行<code>INSERT</code>和<code>REPLACE</code>语句。如下：</p><pre><code>  INSERT INTO t1 (c2) VALUES ('xxx');  </code></pre></li><li><p><code>Bulk inserts</code>，事先不知道要插入的行数（和所需自动递增值的数量）的语句。 这包括<code>INSERT ... SELECT</code>，<code>REPLACE ... SELECT</code>和<code>LOAD DATA</code>语句，但不包括纯<code>INSERT</code>。 <code>InnoDB</code>在处理每行时一次为<code>AUTO_INCREMENT</code>列分配一个新值。</p><pre><code>  INSERT INTO t1 (c2) SELECT 1000 rows from another table ...</code></pre></li><li><p><code>Mixed-mode inserts</code>，这些是<code>Simple inserts</code>语句但是指定一些（但不是全部）新行的自动递增值。 示例如下，其中<code>c1</code>是表<code>t1</code>的<code>AUTO_INCREMENT</code>列： </p><pre><code>  INSERT INTO t1 (c1,c2) VALUES (1,'a'), (NULL,'b'), (5,'c'), (NULL,'d');  </code></pre><p>  另一种类型的<code>Mixed-mode inserts</code>是<code>INSERT ... ON DUPLICATE KEY UPDATE</code>，其在最坏的情况下实际上是<code>INSERT</code>语句随后又跟了一个<code>UPDATE</code>，其中<code>AUTO_INCREMENT</code>列的分配值不一定会在<code>UPDATE</code>阶段使用。</p></li><li><p><code>INSERT-like</code> ，以上所有插入语句的统称。</p></li></ul><h2 id="2-2-AUTO-INCREMENT-锁模式"><a href="#2-2-AUTO-INCREMENT-锁模式" class="headerlink" title="2.2 AUTO_INCREMENT 锁模式"></a>2.2 AUTO_INCREMENT 锁模式</h2><p><code>MySQL</code>可以通过设置<code>innodb_autoinc_lock_mode</code> 变量来配置<code>AUTO_INCREMENT</code>列的锁模式，分别可以设置为<code>0</code>、<code>1</code>、<code>2</code> 三种模式。</p><h3 id="0：传统模式（traditional）"><a href="#0：传统模式（traditional）" class="headerlink" title="0：传统模式（traditional）"></a>0：传统模式（traditional）</h3><ol><li>传统的锁定模式提供了与引入<code>innodb_autoinc_lock_mode</code>变量之前相同的行为。由于语义上可能存在差异，提供传统锁定模式选项是为了向后兼容、性能测试和解决“混合模式插入”问题。</li><li>在这一模式下，所有的<code>insert</code>语句(<code>insert like</code>) 都要在语句开始的时候得到一个表级的<code>auto_inc</code>锁，在语句结束的时候才释放这把锁，注意呀，这里说的是语句级而不是事务级的，一个事务可能包涵有一个或多个语句。</li><li>它能保证值分配的可预见性，与连续性，可重复性，这个也就保证了<code>insert</code>语句在复制到<code>slave</code>的时候还能生成和<code>master</code>那边一样的值(它保证了基于语句复制的安全)。</li><li>由于在这种模式下<code>auto_inc</code>锁一直要保持到语句的结束，所以这个就影响到了并发的插入。</li></ol><h3 id="1：连续模式（consecutive）"><a href="#1：连续模式（consecutive）" class="headerlink" title="1：连续模式（consecutive）"></a>1：连续模式（consecutive）</h3><ol><li>在这种模式下，对于<code>simple insert</code>语句，<code>MySQL</code>会在语句执行的初始阶段将一条语句需要的所有自增值会一次性分配出来，并且通过设置一个互斥量来保证自增序列的一致性，一旦自增值生成完毕，这个互斥量会立即释放，不需要等到语句执行结束。所以，在<code>consecutive</code>模式，多事务并发执行<code>simple insert</code>这类语句时， 相对<code>traditional</code>模式，性能会有比较大的提升。</li><li>由于一开始就为语句分配了所有需要的自增值，那么对于像<code>Mixed-mode insert</code>这类语句，就有可能多分配了一些值给它，从而导致自增序列出现<code>空隙</code>。而<code>traditional</code>模式因为每一次只会为一条记录分配自增值，所以不会有这种问题。</li><li>另外，对于Bulk inserts语句，依然会采取AUTO-INC锁。所以，如果有一条Bulk inserts语句正在执行的话，Simple inserts也必须等到该语句执行完毕才能继续执行。</li></ol><h3 id="2：交错模式（interleaved）"><a href="#2：交错模式（interleaved）" class="headerlink" title="2：交错模式（interleaved）"></a>2：交错模式（interleaved）</h3><p>在这种模式下，对于所有的<code>insert-like</code>语句，都不会存在表级别的<code>AUTO-INC</code>锁，意味着同一张表上的多个语句并发时阻塞会大幅减少，这时的效率最高。但是会引入一个新的问题：当<code>binlog_format</code>为<code>statement</code>时，这时的复制没法保证安全，因为批量的<code>insert</code>，比如<code>insert ..select..</code>语句在这个情况下，也可以立马获取到一大批的自增<code>ID</code>值，不必锁整个表，<code>slave</code>在回放这个<code>SQL</code>时必然会产生错乱（<code>binlog</code>使用<code>row</code>格式没有这个问题）。</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ul><li><p>自增值的生成后是不能回滚的，所以自增值生成后，事务回滚了，那么那些已经生成的自增值就丢失了，从而使自增列的数据出现空隙。</p></li><li><p>正常情况下，自增列是不存在<code>0</code>这个值的。所以，如果插入语句中对自增列设置的值为<code>0</code>或者<code>null</code>，就会自动应用自增序列。那么，如果想在自增列中插入为<code>0</code>这个值，怎么办呢？可以通过将<a href="https://dev.mysql.com/doc/refman/5.7/en/sql-mode.html#sqlmode_no_auto_value_on_zero">SQL Mode</a>设置为<code>NO_AUTO_VALUE_ON_ZERO</code>即可。</p></li><li><p>在<code>MySQL 5.7</code>以及更早之前，自增序列的计数器(<code>auto-increment counter</code>)是保存在内存中的。<code>auto-increment counter</code>在每次<code>MySQL</code>重新启动后通过类似下面的这种语句进行初始化：</p><pre><code>  SELECT MAX(AUTO_INC_COLUMN) FROM table_name FOR UPDATE</code></pre></li><li><p>而从<code>MySQL 8</code>开始，<code>auto-increment counter</code>被存储在了<code>redo log</code>中，并且每次变化都会刷新到<code>redo log</code>中。另外，我们可以通过<code>ALTER TABLE … AUTO_INCREMENT = N</code>来主动修改<code>auto-increment counter</code>。</p></li></ul><h3 id="生产环境相关配置"><a href="#生产环境相关配置" class="headerlink" title="生产环境相关配置"></a>生产环境相关配置</h3><p>我们生产环境配置是<code>innodb_autoinc_lock_mode = 2</code>，<code>binlog_format = ROW</code></p><pre><code>mysql&gt; show variables like 'innodb_autoinc_lock_mode';+--------------------------+-------+| Variable_name            | Value |+--------------------------+-------+| innodb_autoinc_lock_mode | 2     |+--------------------------+-------+1 row in set (0.07 sec)mysql&gt; show variables like 'binlog_format';+---------------+-------+| Variable_name | Value |+---------------+-------+| binlog_format | ROW   |+---------------+-------+1 row in set (0.36 sec)</code></pre><h1 id="三、Insert-流程源码分析"><a href="#三、Insert-流程源码分析" class="headerlink" title="三、Insert 流程源码分析"></a>三、Insert 流程源码分析</h1><h2 id="3-1-Insert-执行过程"><a href="#3-1-Insert-执行过程" class="headerlink" title="3.1 Insert 执行过程"></a>3.1 Insert 执行过程</h2><p><a href="https://github.com/mysql/mysql-server/blob/5.7/sql/sql_parse.cc#L5438"><code>mysql_parse</code></a> -&gt; <a href="https://github.com/mysql/mysql-server/blob/5.7/sql/sql_parse.cc#L2456"><code>mysql_execute_command</code></a> -&gt; <a href="https://github.com/mysql/mysql-server/blob/5.7/sql/sql_insert.cc#L3176"><code>Sql_cmd_insert::execute</code></a> -&gt; <a href="https://github.com/mysql/mysql-server/blob/5.7/sql/sql_insert.cc#L428"><code>Sql_cmd_insert::mysql_insert</code></a> -&gt; <a href="https://github.com/mysql/mysql-server/blob/5.7/sql/sql_insert.cc#L1512"><code>write_record</code></a> -&gt; <a href="https://github.com/mysql/mysql-server/blob/5.7/sql/handler.cc#L8153"><code>handler::ha_write_row</code></a> -&gt; <a href="https://github.com/mysql/mysql-server/blob/5.7/storage/innobase/handler/ha_innodb.cc#L7506"><code>ha_innobase::write_row</code></a></p><p>这里我们主要关注<code>innodb</code>层的数据写入函数<a href="https://github.com/mysql/mysql-server/blob/5.7/storage/innobase/handler/ha_innodb.cc#L7506"><code>ha_innobase::write_row</code></a> 相关的代码就好了，<code>生成自增ID</code>和<code>唯一索引冲突检查</code>都是在这个函数里面完成的。</p><h2 id="3-2-innodb-数据插入流程"><a href="#3-2-innodb-数据插入流程" class="headerlink" title="3.2 innodb 数据插入流程"></a>3.2 innodb 数据插入流程</h2><p>通过<a href="https://github.com/mysql/mysql-server/blob/5.7/storage/innobase/handler/ha_innodb.cc#L7506"><code>ha_innobase::write_row</code></a> 代码我们可以知道，在<code>innodb</code>层写入数据主要分为<code>7</code>步：</p><pre><code>1. Validation checks before we commence write_row operation.2. Intermediate commit if original operation involves ALTER table with algorithm = copy. Intermediate commit ease pressure on recovery if server crashes while ALTER is active.3. Handling of Auto-Increment Columns.4. Prepare INSERT graph that will be executed for actual INSERT (This is a one time operation)5. Execute insert graph that will result in actual insert.6. Handling of errors related to auto-increment. 7. Cleanup and exit. </code></pre><p>我们主要关注，自增列相关的<code>步骤三</code>和<code>步骤六</code>，数据写入的<code>步骤五</code>。</p><h3 id="自增ID的相关处理过程"><a href="#自增ID的相关处理过程" class="headerlink" title="自增ID的相关处理过程"></a>自增ID的相关处理过程</h3><p><a href="https://github.com/mysql/mysql-server/blob/5.7/storage/innobase/handler/ha_innodb.cc#L7631">先看第三步代码：Handling of Auto-Increment Columns</a>，主要的函数栈如下：</p><pre><code>-&gt;ha_innobase::write_row    -&gt;handler::update_auto_increment // 调用 update_auto_increment 函数更新auto increment的值        -&gt;ha_innobase::get_auto_increment // 获取 dict_tabel中的当前 auto increment 值，并根据全局参数更新下一个 auto increment 的值到数据字典中            -&gt;ha_innobase::innobase_get_autoinc // 读取 autoinc 值                -&gt;ha_innobase::innobase_lock_autoinc                   -&gt;dict_table_autoinc_lock(m_prebuilt-&gt;table); // lock_mode = 2 的时候                -&gt;dict_table_autoinc_unlock(m_prebuilt-&gt;table); // 解锁        -&gt;set_next_insert_id // 多行插入的时候设置下一个插入的id值</code></pre><p><a href="https://github.com/mysql/mysql-server/blob/5.7/storage/innobase/handler/ha_innodb.cc#L7387">三种模式对应的加锁源码</a>：</p><pre><code>static const long AUTOINC_OLD_STYLE_LOCKING = 0;static const long AUTOINC_NEW_STYLE_LOCKING = 1;static const long AUTOINC_NO_LOCKING = 2;dberr_tha_innobase::innobase_lock_autoinc(void)/*====================================*/{    DBUG_ENTER("ha_innobase::innobase_lock_autoinc");    dberr_t        error = DB_SUCCESS;    long        lock_mode = innobase_autoinc_lock_mode;    ut_ad(!srv_read_only_mode          || dict_table_is_intrinsic(m_prebuilt-&gt;table));    if (dict_table_is_intrinsic(m_prebuilt-&gt;table)) {        /* Intrinsic table are not shared accorss connection        so there is no need to AUTOINC lock the table. */        lock_mode = AUTOINC_NO_LOCKING;    }    switch (lock_mode) {    case AUTOINC_NO_LOCKING: // lock_mode = 2        /* Acquire only the AUTOINC mutex. */        dict_table_autoinc_lock(m_prebuilt-&gt;table);        break;    case AUTOINC_NEW_STYLE_LOCKING:        /* For simple (single/multi) row INSERTs, we fallback to the        old style only if another transaction has already acquired        the AUTOINC lock on behalf of a LOAD FILE or INSERT ... SELECT        etc. type of statement. */        if (thd_sql_command(m_user_thd) == SQLCOM_INSERT            || thd_sql_command(m_user_thd) == SQLCOM_REPLACE) {            dict_table_t*    ib_table = m_prebuilt-&gt;table;            /* Acquire the AUTOINC mutex. */            dict_table_autoinc_lock(ib_table);            /* We need to check that another transaction isn't            already holding the AUTOINC lock on the table. */            if (ib_table-&gt;n_waiting_or_granted_auto_inc_locks) {                /* Release the mutex to avoid deadlocks. */                dict_table_autoinc_unlock(ib_table);            } else {                break;            }        }        /* Fall through to old style locking. */    case AUTOINC_OLD_STYLE_LOCKING:        DBUG_EXECUTE_IF("die_if_autoinc_old_lock_style_used",                ut_ad(0););        error = row_lock_table_autoinc_for_mysql(m_prebuilt);        if (error == DB_SUCCESS) {            /* Acquire the AUTOINC mutex. */            dict_table_autoinc_lock(m_prebuilt-&gt;table);        }        break;    default:        ut_error;    }    DBUG_RETURN(error);}</code></pre><p><a href="https://github.com/mysql/mysql-server/blob/5.7/storage/innobase/handler/ha_innodb.cc#L7719">步骤六：插入成功以后，还需要更新 autoinc 值</a></p><pre><code>            if (auto_inc &gt;= m_prebuilt-&gt;autoinc_last_value) {set_max_autoinc:                /* This should filter out the negative                values set explicitly by the user. */                if (auto_inc &lt;= col_max_value) {                    ut_a(m_prebuilt-&gt;autoinc_increment &gt; 0);                    ulonglong    offset;                    ulonglong    increment;                    dberr_t        err;                    offset = m_prebuilt-&gt;autoinc_offset;                    increment = m_prebuilt-&gt;autoinc_increment;                    auto_inc = innobase_next_autoinc(                        auto_inc,                        1, increment, offset,                        col_max_value);                    err = innobase_set_max_autoinc(                        auto_inc);                    if (err != DB_SUCCESS) {                        error = err;                    }                }            }</code></pre><h3 id="唯一索引冲突检查过程"><a href="#唯一索引冲突检查过程" class="headerlink" title="唯一索引冲突检查过程"></a>唯一索引冲突检查过程</h3><pre><code>  |-Sql_cmd_insert_values::execute_inner() // Insert one or more rows from a VALUES list into a table    |-write_record      |-handler::ha_write_row() // 调用存储引擎的接口        |-ha_innobase::write_row()          |-row_insert_for_mysql            |-row_insert_for_mysql_using_ins_graph              |-trx_start_if_not_started_xa                |-trx_start_low // 激活事务，事务状态由 not_active 变为 active              |-row_get_prebuilt_insert_row // Gets pointer to a prebuilt dtuple used in insertions              |-row_mysql_convert_row_to_innobase // 记录格式从MySQL转换成InnoDB, 不同数据类型处理方式不同，比如整形server端是小端存储，innodb是大端存储              |-row_ins_step                |-trx_write_trx_id(node-&gt;trx_id_buf, trx-&gt;id)                |-lock_table // 给表加IX锁                |-row_ins // 插入记录                  |-while (node-&gt;index != NULL)                    |-row_ins_index_entry_step // 向索引中插入记录,把 innobase format field 的值赋给对应的index entry field                      |-row_ins_index_entry_set_vals // 根据该索引以及原记录，将组成索引的列的值组成一个记录                      |-dtuple_check_typed // 检查组成的记录的有效性                      |-row_ins_index_entry // 插入索引项                        |-row_ins_clust_index_entry // 插入聚集索引                          |-row_ins_clust_index_entry_low // 先尝试乐观插入，修改叶子节点 BTR_MODIFY_LEAF                            |-mtr_t::mtr_t()                            |-mtr_t::start()                            |-btr_pcur_t::open()                              |-btr_cur_search_to_nth_level // 将cursor移动到索引上待插入的位置                                |-buf_page_get_gen //取得本层页面，首次为根页面                                |-page_cur_search_with_match_bytes // 在本层页面进行游标定位                            |-row_ins_duplicate_error_in_clust // 判断插入项是否存在唯一键冲突                              |-row_ins_set_shared_rec_lock // 对cursor 对应的已有记录加S锁（可能会等待）保证记录上的操作，包括：Insert/Update/Delete 已经提交或者回滚                                |-lock_clust_rec_read_check_and_lock // 判断cursor对应的记录上是否存在隐式锁, 若存在，则将隐式锁转化为显示锁                                  |-lock_rec_convert_impl_to_expl // 隐式锁转换                                  |-lock_rec_lock //如果上面的隐式锁转化成功，此处加S锁将会等待，直到活跃事务释放锁。                              |-row_ins_dupl_err_with_rec // S锁加锁完成之后，可以再次做判断，最终决定是否存在唯一键冲突,                                 // 1. 判断insert记录与cursor对应的记录取值是否相同,                                 // 2. 二级唯一键值锁引，可以存在多个 NULL 值,                                 // 3. 最后判断记录的delete flag状态，判断记录是否被删除提交                                |-return !rec_get_deleted_flag();                            |-btr_cur_optimistic_insert // 乐观插入                            |-btr_cur_pessimistic_insert // 乐观插入失败则进行悲观插入                            |-mtr_t::commit() mtr_commit //Commit a mini-transaction.                            |-btr_pcur_t::close()</code></pre><h1 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h1><ol><li><code>innodb_autoinc_lock_mode=2</code>的时候，<code>MySQL</code>是申请到<code>ID</code>以后就会释放锁。并发生成自增<code>ID</code>不会冲突。</li><li><code>MySQL</code>是先生成<code>ID</code>，再去做插入前的<code>唯一索引冲突检查</code>。如果一部分<code>Client</code>用<code>MySQL</code>自增<code>ID</code>，一部分<code>Client</code>用自己生成的<code>ID</code>，是有可能导致自增<code>ID</code>的<code>Client</code>报<code>PK Duplicate Error</code>的。</li></ol><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-auto-increment-handling.html#innodb-auto-increment-lock-modes">AUTO_INCREMENT Handling in InnoDB</a></p><p><a href="https://blog.csdn.net/scientificCommunity/article/details/122846585">Mysql之AUTO_INCREMENT浅析</a></p><p><a href="https://www.bookstack.cn/read/aliyun-rds-core/ea7a43cf992eca56.md">MySQL · 内核分析 · InnoDB主键约束和唯一约束的实现分析</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Go for-range 的奇技淫巧</title>
      <link href="/2022/05/20/go-for-range/"/>
      <url>/2022/05/20/go-for-range/</url>
      
        <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>朋友发了两个代码片段给我看，让我猜输出的内容是啥。具体代码如下：</p><pre><code>// Demo1 // 1. 这个循环是否能停下来？// 2. 如果能停下来，打印的 arr 内容是什么？arr := []int{1, 2, 3}for _, v := range arr {    arr = append(arr, v)}fmt.Println(arr)// Demo2// 1. idx 和 value 输出多少？// 2. 输出几行？str := "你好"for idx, v := range str {    fmt.Printf("idx = %d , value = %c\n", idx, v)}</code></pre><p>不卖关子，先说下第一个<code>Demo</code>输出的是：</p><pre><code>[1 2 3 1 2 3]</code></pre><p>第二个<code>Demo</code>输出的是：</p><pre><code>idx = 0 , value = 你idx = 3 , value = 好</code></pre><p>为什么是这样，我们往下看。</p><h1 id="Demo1分析"><a href="#Demo1分析" class="headerlink" title="Demo1分析"></a>Demo1分析</h1><pre><code>arr := []int{1, 2, 3}for _, v := range arr {    arr = append(arr, v)}</code></pre><p>我们先看下<a href="https://godbolt.org/z/vrzfPz4rz">Demo1生成的汇编代码</a></p><pre><code>main_pc0: ..........................main_pc101:        MOVQ    CX, "".arr+144(SP)        MOVQ    $3, "".arr+152(SP)        MOVQ    $3, "".arr+160(SP)        MOVQ    CX, ""..autotmp_2+192(SP)        MOVQ    $3, ""..autotmp_2+200(SP)        MOVQ    $3, ""..autotmp_2+208(SP)        MOVQ    $0, ""..autotmp_5+80(SP)  // autotmp_5+80 = 0 , 类似 i:=0        MOVQ    ""..autotmp_2+200(SP), DX // 这里设置 DX = 3        MOVQ    DX, ""..autotmp_6+72(SP) // autotmp_6+72(SP) = 3        JMP     main_pc189main_pc189:        MOVQ    ""..autotmp_5+80(SP), DX // DX = 0 (DX = i)        CMPQ    ""..autotmp_6+72(SP), DX // 比较 3 和 DX (i &lt; 3)        JGT     main_pc206  // DX &lt; 3 跳转到 body 模块 , 执行 arr = append(arr, v)        JMP     main_pc358  // DX &gt;= 3 循环结束。执行后续打印代码。main_pc206: .......................... </code></pre><p>从上面汇编代码，我们看出，<code>for range</code>的循环次数是固定的<code>3</code>次，并不是每次都会去读取<code>arr</code>的长度，所以<code>arr</code>只会<code>append</code>三次，也解释了为什么输出是：</p><pre><code>[1 2 3 1 2 3]</code></pre><p>我们再来看下<code>Go编译器</code>是怎么对<code>for range</code>代码翻译转换的。翻了下<a href="https://github.com/golang/go/blob/master/src/cmd/compile/internal/walk/range.go#L85">Go编译器源码</a>，相关代码如下：</p><pre><code>case types.TARRAY, types.TSLICE:    if nn := arrayClear(nrange, v1, v2, a); nn != nil {        base.Pos = lno        return nn    }    // order.stmt arranged for a copy of the array/slice variable if needed.    ha := a    hv1 := typecheck.Temp(types.Types[types.TINT])    hn := typecheck.Temp(types.Types[types.TINT])    init = append(init, ir.NewAssignStmt(base.Pos, hv1, nil))    init = append(init, ir.NewAssignStmt(base.Pos, hn, ir.NewUnaryExpr(base.Pos, ir.OLEN, ha)))    nfor.Cond = ir.NewBinaryExpr(base.Pos, ir.OLT, hv1, hn)    nfor.Post = ir.NewAssignStmt(base.Pos, hv1, ir.NewBinaryExpr(base.Pos, ir.OADD, hv1, ir.NewInt(1)))    // for range ha { body }    if v1 == nil {        break    }    // for v1 := range ha { body }    if v2 == nil {        body = []ir.Node{ir.NewAssignStmt(base.Pos, v1, hv1)}        break    }</code></pre><p>可以看到 <code>ha := a</code> 这句代码，<code>for range</code>的对象是<code>Array</code>或者是<code>Slice</code>的时候，会先<code>Copy</code>一下这个对象。所以在循环的时候<code>Append</code>元素到<code>Slice</code>中去，并不会改变循环的次数。</p><p>编译器会把<code>for-range</code>代码转换成伪代码如下：</p><pre><code>ha := ahv1 := 0hn := len(ha)for ; hv1 &lt; hn; hv1++ {     // v1, v2 = hv1, ha[hv1]    // ...}</code></pre><p>还有一点要指出的是，<code>Golang</code>的<code>Slice</code>是<code>胖指针</code>，所以值复制的时候不会拷贝所有的数据。只会拷贝<code>SliceHeader</code>对应的三个对象</p><pre><code>// SliceHeader is the runtime representation of a slice.// It cannot be used safely or portably and its representation may// change in a later release.// Moreover, the Data field is not sufficient to guarantee the data// it references will not be garbage collected, so programs must keep// a separate, correctly typed pointer to the underlying data.type SliceHeader struct {    Data uintptr    Len  int    Cap  int}</code></pre><h1 id="Demo2分析"><a href="#Demo2分析" class="headerlink" title="Demo2分析"></a>Demo2分析</h1><pre><code>str := "你好"for idx, v := range str {    fmt.Printf("idx = %d , value = %c\n", idx, v)}</code></pre><p>我们也来看下 <a href="https://godbolt.org/z/saEnf3snd">Demo2生成的汇编代码</a></p><p>我先看下关键的循环相关的代码：</p><pre><code>main_pc50:        MOVQ    $6, "".str+104(SP)        MOVQ    DX, ""..autotmp_3+112(SP)        MOVQ    $6, ""..autotmp_3+120(SP)        MOVQ    $0, ""..autotmp_5+64(SP)        JMP     main_pc84main_pc84:        MOVQ    ""..autotmp_5+64(SP), DX  // DX = 0        NOP        CMPQ    ""..autotmp_3+120(SP), DX // 比较 6 和 DX大小        JGT     main_pc108  // 6 &gt; DX 跳转到 108        JMP     main_pc440  // 6 &lt;= DX 跳转到 440</code></pre><p>这里，我们可以看到，循环停止条件是，DX&gt;=<code>6</code>。这个<code>6</code>的值是怎么算出来的？</p><p>因为<code>Golang</code>的源码默认都是用的<code>UTF-8</code>编码。<a href="https://zh.m.wikipedia.org/zh-hans/UTF-8">UTF-8（8-bit Unicode Transformation Format）</a>是一种针对Unicode的可变长度字元编码，也是一种前缀码。它可以用一至四个字节对Unicode字符集中的所有有效编码点进行编码</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9b090629b2e86dfb.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="origin_img_v2_f862efd1-d827-4812-a3f8-9b3e4cd40chu.jpg"></p><p><code>你好</code>两个汉字对应的<code>Unicode</code>编码如下，一共占用<code>6</code>个字节。</p><pre><code>11100100 10111101 10100000 // 你11100101 10100101 10111101 // 好</code></pre><p>再看下<code>for</code>循环的的步长是如何算的，汇编代码如下。</p><pre><code>main_pc172:        MOVQ    ""..autotmp_3+120(SP), BX        PCDATA  $1, $1        CALL    runtime.decoderune(SB)        MOVL    AX, ""..autotmp_7+44(SP)        MOVQ    BX, ""..autotmp_5+64(SP)         NOP        JMP     main_pc194</code></pre><p>我们可以看到，<code>decoderune</code>函数第二个返回值存到了 <code>""..autotmp_5+64(SP)</code> 中，上面会把这个赋值给<code>DX</code>，<code>DX</code>再去跟<code>6</code>比较。</p><p>再来看下<a href="https://github.com/golang/go/blob/master/src/runtime/utf8.go#L60">decoderune</a>这个函数是干什么的，找到<code>runtime</code>代码如下：</p><pre><code>// decoderune returns the non-ASCII rune at the start of// s[k:] and the index after the rune in s.//// decoderune assumes that caller has checked that// the to be decoded rune is a non-ASCII rune.//// If the string appears to be incomplete or decoding problems// are encountered (runeerror, k + 1) is returned to ensure// progress when decoderune is used to iterate over a string.func decoderune(s string, k int) (r rune, pos int) {</code></pre><p>我们可以知道，这个函数会返回当前字符串<code>k</code>之后的<code>rune</code>字符和<code>rune</code>字符对应的位置。所以<code>demo</code>的循环<code>idx</code>是<code>0</code>和<code>3</code>，因为<code>0</code>、<code>3</code>分别是两个字符的起始位置。</p><p><a href="https://github.com/golang/go/blob/master/src/cmd/compile/internal/walk/range.go#L220">在编译器源码里面</a>也可以看到<code>for-range</code>字符串的时候生成的伪代码如下：</p><pre><code>// Transform string range statements like "for v1, v2 = range a" into//// ha := a// for hv1 := 0; hv1 &lt; len(ha); {//   hv1t := hv1//   hv2 := rune(ha[hv1])//   if hv2 &lt; utf8.RuneSelf {//      hv1++//   } else {//      hv2, hv1 = decoderune(ha, hv1)//   }//   v1, v2 = hv1t, hv2//   // original body// }</code></pre><p>也就解释了为什么<code>Demo2</code>输出是</p><pre><code>idx = 0 , value = 你idx = 3 , value = 好</code></pre>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Golang </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Raft-分布式共识算法》</title>
      <link href="/2022/03/08/raft-introduction/"/>
      <url>/2022/03/08/raft-introduction/</url>
      
        <content type="html"><![CDATA[<h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>在分布式系统中，一致性算法至关重要。在所有一致性算法中，<code>Paxos</code>最负盛名，它由莱斯利·兰伯特（Leslie Lamport）于<code>1990</code>年提出，是一种基于消息传递的一致性算法，被认为是类似算法中最有效的。</p><p><code>Paxos</code>算法虽然很有效，但复杂的原理使它实现起来非常困难，截止目前，实现<code>Paxos</code>算法的开源软件很少，比较出名的有<code>Chubby</code>、<code>LibPaxos</code>。此外，<code>Zookeeper</code>采用的 <code>ZAB（Zookeeper Atomic Broadcast）</code>协议也是基于<code>Paxos</code>算法实现的，不过<code>ZAB</code>对<code>Paxos</code>进行了很多改进与优化，两者的设计目标也存在差异——<code>ZAB</code>协议主要用于构建一个高可用的分布式数据主备系统，而<code>Paxos</code> 算法则是用于构建一个分布式的一致性状态机系统。</p><p>由于<code>Paxos</code>算法过于复杂、实现困难，极大地制约了其应用，而分布式系统领域又亟需一种高效而易于实现的分布式一致性算法，在此背景下，<code>Raft</code>算法应运而生。</p><p><code>Raft</code>算法在斯坦福<code>Diego Ongaro</code> 和<code>John Ousterhout</code>于<code>2013</code>年发表的<code>《In Search of an Understandable Consensus Algorithm》</code>中提出。相较于<code>Paxos</code>，<code>Raft</code>通过逻辑分离使其更容易理解和实现，目前，已经有十多种语言的<code>Raft</code>算法实现框架，较为出名的有<code>etcd</code>、<code>Consul</code> 。</p><p><code>Raft</code>是在可信环境的算法，每个节点应该按照“预期”方式运行，非拜占庭，即没有叛徒，有没有欺骗，相互信任。</p><p><code>Raft</code> 主要解决以下几个问题：</p><ol><li>如何在主从上同步数据。  | 日志负责</li><li>如何在异常中选择性的主节点。 | 领导选主</li><li>如何保证异常状态中数据安全。  | 数据安全性</li></ol><p><a href="http://thesecretlivesofdata.com/raft/">Raft算法演示地址</a></p><h2 id="二、Raft-核心算法"><a href="#二、Raft-核心算法" class="headerlink" title="二、Raft 核心算法"></a>二、Raft 核心算法</h2><h3 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h3><p><code>Raft</code>将系统中的角色分为领导者（<code>Leader</code>）、跟从者（<code>Follower</code>）和候选人（<code>Candidate</code>）：</p><ul><li><code>Leader</code>：接受客户端请求，并向<code>Follower</code>同步请求日志，当日志同步到大多数节点上后告诉<code>Follower</code>提交日志。</li><li><code>Follower</code>：接受并持久化<code>Leader</code>同步的日志，在<code>Leader</code>告之日志可以提交之后，提交日志。</li><li><code>Candidate</code>：<code>Leader</code>选举过程中的临时角色。</li></ul><p>任期<code>Term</code></p><ul><li><code>Raft</code>的时间被切分为多个任期</li><li>当切换<code>Leader</code>时，首先会进行选举，同事也开启一个新的任期</li><li><code>Raft</code>每个任期只能产生一名<code>Leader</code></li><li>每一个节点都会保存当前<code>Leader</code>的最大任期</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a0904d409c2e4999.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="2-2-领导人选举"><a href="#2-2-领导人选举" class="headerlink" title="2.2 领导人选举"></a>2.2 领导人选举</h3><p><strong>一次正常请求的处理流程：</strong></p><ol><li>主节点收到请求，追加日志，将数据同步给所有从节点。</li><li>从节点收到数据以后，返回<code>ACK</code>给主节点</li><li>主节点收到了<code>1/2</code>以上的节点<code>ACK</code>后，确认数据安全，提交数据。</li></ol><ul><li><code>Raft</code>保证只要数据提交了，那么半数以上的节点都会有一份数据备份。</li><li><code>Raft</code>保证集群中<strong>只要半数以上的节点有效，则整个集群能提供正常服务</strong>。</li></ul><h4 id="2-2-1-我们如何检查服务是否可用？"><a href="#2-2-1-我们如何检查服务是否可用？" class="headerlink" title="2.2.1 我们如何检查服务是否可用？"></a>2.2.1 我们如何检查服务是否可用？</h4><ul><li>从节点会监控主节点心跳是否超时</li><li>任何节点只要发现主节点心跳超时，就可以认为主节点已经失效</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ce4f99bfb3fb82e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="2-2-2-如何选出新的主节点？"><a href="#2-2-2-如何选出新的主节点？" class="headerlink" title="2.2.2 如何选出新的主节点？"></a>2.2.2 如何选出新的主节点？</h4><ul><li>某个从节点发现心跳超时时，会将自己的任期<code>Term</code>加一，并发起新一轮选举</li><li>任意一个节点收到一轮新任期的选举是，都会进行投票</li><li>当一个候选人收到半数以上的选票时，赢得此次任期</li><li>新的主节点开始向所有节点发送心跳</li></ul><h4 id="2-2-3-从节点如投票？"><a href="#2-2-3-从节点如投票？" class="headerlink" title="2.2.3 从节点如投票？"></a>2.2.3 从节点如投票？</h4><ul><li>选举的任期比当前任期大</li><li>一个任期只会投一次票。</li><li>候选人的数据必须比自己新。 </li></ul><h4 id="2-2-4-如何保证主节点数据是有效的？"><a href="#2-2-4-如何保证主节点数据是有效的？" class="headerlink" title="2.2.4 如何保证主节点数据是有效的？"></a>2.2.4 如何保证主节点数据是有效的？</h4><ul><li>数据被提交前，至少需要超过半数的<code>ACK</code>。即一半以上的节点有已经提交的数据</li><li>如果要赢的选举，要比半数以上的节点数据新。</li></ul><p>结论：赢得选举的节点，必然包含最新已经提交的新数据。</p><h4 id="2-2-5-有个没有可能出现多个主节点？"><a href="#2-2-5-有个没有可能出现多个主节点？" class="headerlink" title="2.2.5 有个没有可能出现多个主节点？"></a>2.2.5 有个没有可能出现多个主节点？</h4><p>不会</p><ul><li>新的任期开始后，所有节点会屏蔽掉比当前任期小的请求和心跳。</li><li>由于超过半数的节点已经进入新一轮任期，旧<code>Leader</code>不再可能获得半数以上的<code>ACK</code>。</li><li>旧<code>Leader</code>一旦收到<code>Term</code>更高的心跳，则直接降级为从节点。</li></ul><h4 id="2-2-6-有没有可能出现无法选出合适的主节点？"><a href="#2-2-6-有没有可能出现无法选出合适的主节点？" class="headerlink" title="2.2.6 有没有可能出现无法选出合适的主节点？"></a>2.2.6 有没有可能出现无法选出合适的主节点？</h4><ul><li>有可能有平票。</li><li>通过随机超时时间，避免下一次选举冲突</li><li>当候选再次超时，会把任期+1 ，发起新一轮选举。</li><li><strong>任何节点收到高任期的心跳，都会退化为从节点</strong>。</li></ul><h4 id="2-2-7-有没有可能出现脑裂？"><a href="#2-2-7-有没有可能出现脑裂？" class="headerlink" title="2.2.7 有没有可能出现脑裂？"></a>2.2.7 有没有可能出现脑裂？</h4><p>由于一个任期需要<strong>半数以上</strong>节点投同意票，因此不会出现脑裂</p><h4 id="2-2-8-预选举"><a href="#2-2-8-预选举" class="headerlink" title="2.2.8 预选举"></a>2.2.8 预选举</h4><ul><li>当网络发生异常, 但是节点没有发生异常时。可能会导致某些节点任期无限增加。</li><li>Raft 采取 “预选举（preVote）”方式避免。</li><li>节点在发起选举前，会先发起一轮预选举，当其发现在预选举中能活的半数的支持时，才会真的发起选举</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-15513b43e2b82cb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="2-2-9-选举总结"><a href="#2-2-9-选举总结" class="headerlink" title="2.2.9 选举总结"></a>2.2.9 选举总结</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3a55cdd489accc61.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="2-3-成员变更"><a href="#2-3-成员变更" class="headerlink" title="2.3 成员变更"></a>2.3 成员变更</h3><ul><li>在生产环境中，有时候需要改变集群配置，比如更换坏掉的节点、增加冗余。</li><li>需要在保证安全性的前提下完成成员变更，不能在同一<code>term</code>有多个<code>leader</code></li><li>同时也喜欢升级不停机，能对外提供服务。</li><li>如果贸然加入多个节点，势必会导致多个<code>Leader</code>节点情况</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c7f83800d50bc8a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="2-3-1-一次只变化一个节点"><a href="#2-3-1-一次只变化一个节点" class="headerlink" title="2.3.1 一次只变化一个节点"></a>2.3.1 一次只变化一个节点</h4><p>加入集群流程（挂了一个节点，集群就不可用了。）</p><ol><li>先<code>Leader</code>申请，<code>Leader</code>同步所有申请信息给所有<code>Follower</code></li><li>超过半数同意后，新节点加入集群。</li><li>之后可以开启新的一轮添加节点。</li><li>新增节点由于没有任何日志，无法直接参与新日志追加，会导致新集群可用性变差。</li><li>可以引入<code>Learner</code>身份，在没有投票权的情况下，先从<code>Leader</code>节点获取一段时间日志。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-41afa1170a64b3cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="2-3-1-一次添加多个节点"><a href="#2-3-1-一次添加多个节点" class="headerlink" title="2.3.1 一次添加多个节点"></a>2.3.1 一次添加多个节点</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9aa937cbe402c949.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e2b0595f11882b92.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="2-4-日志复制"><a href="#2-4-日志复制" class="headerlink" title="2.4 日志复制"></a>2.4 日志复制</h3><h4 id="2-4-1-基本概念"><a href="#2-4-1-基本概念" class="headerlink" title="2.4.1 基本概念"></a>2.4.1 基本概念</h4><ul><li><code>Raft</code>数据包含日志序和数据状态机</li><li>日志本质上就是一个数组，内部存了一条条日志。</li><li>任意一个节点“<code>按序执行</code>”日志里面的操作，都可以还原相同的状态机结果。</li><li><code>Leader</code>产生日志，同步到<code>Follower</code>节点中，<code>Follower</code>按序追加到自己的日志队列中执行</li></ul><p>由于一个<code>Term</code>只会有一个<code>Leader</code>、一个<code>Leader</code>只会在一个位置放一次日志。<br>因此索引+任期，能确认唯一一个数据</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1bb78ec190181fab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="2-4-2-一次正常的日志同步流程"><a href="#2-4-2-一次正常的日志同步流程" class="headerlink" title="2.4.2 一次正常的日志同步流程"></a>2.4.2 一次正常的日志同步流程</h4><ul><li>主节点在收到新的请求以后，先将日志追加到自己的日志中，这个时候日志还未提交（<code>uncommit</code>）</li><li><code>Master</code>将日志提交搞所有从节点，从节点日志也保存到未提交队列中。</li><li>当<code>Master</code>确认半数以上节点获取到日志后，将日志提交</li></ul><h4 id="2-4-3-如何处理日志缺失"><a href="#2-4-3-如何处理日志缺失" class="headerlink" title="2.4.3 如何处理日志缺失?"></a>2.4.3 如何处理日志缺失?</h4><ul><li><code>Master</code>节点中维护了所有冲节点的下一个预期日志(<code>next index</code>)</li><li>知己诶单只会接受当前<code>max index</code>后的下一个日志，其他的日志全部拒绝.</li><li>子节点会先<code>Master</code>汇报自己当前的<code>max index</code></li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-109ee56898ebc158.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="2-4-4-从节点日志冲突"><a href="#2-4-4-从节点日志冲突" class="headerlink" title="2.4.4 从节点日志冲突"></a>2.4.4 从节点日志冲突</h4><ul><li>当主节点宕机时,没有来及吧日志同步给半数以上的节点,就会出现数据冲突</li><li>从节点收到的日志请求时,<strong>会判断未提交的日志是否发生冲突,如果发生冲突则直接截断覆盖</strong>.</li></ul><h4 id="2-4-4-不同节点索引号和任期号是相同，数据一定相同吗？"><a href="#2-4-4-不同节点索引号和任期号是相同，数据一定相同吗？" class="headerlink" title="2.4.4 不同节点索引号和任期号是相同，数据一定相同吗？"></a>2.4.4 不同节点索引号和任期号是相同，数据一定相同吗？</h4><ol><li>获得一个任期，必须得到超过半数的成员投票。</li><li>一个成员永久的只会给一个任期投一次票。</li><li>鸽巢原理。</li></ol><p>可以推出 ： 一个任期只会有一个<code>Leader</code>、<code>Leader</code>只会在一个索引处提交一次日志。新<code>Leader</code>一定有全新的已经<code>commit</code>的日志。</p><p>进而可以退出 <strong>不同节点，两个条目拥有相同的索引号和任期号是相同的，那么他们之前所有的数据都是相同的。</strong></p><h4 id="2-4-5-如何快速确定主从节点的日志是否冲突-相同？"><a href="#2-4-5-如何快速确定主从节点的日志是否冲突-相同？" class="headerlink" title="2.4.5 如何快速确定主从节点的日志是否冲突/相同？"></a>2.4.5 如何快速确定主从节点的日志是否冲突/相同？</h4><p>如果在不同的节点中的两个条目拥有<code>相同</code>的<strong>索引号和任期号</strong>，那么他们之前所有的日志条目也全部相同。</p><p>从节点收到一条新的数据时候，还会收到上一条的<code>Term+Index</code>，只有和自己的上一条数据完全相同才会追加。否则向前追溯，<strong>不符合条件的全部截断</strong>。</p><h4 id="2-4-6-如何判断数据的新旧？"><a href="#2-4-6-如何判断数据的新旧？" class="headerlink" title="2.4.6 如何判断数据的新旧？"></a>2.4.6 如何判断数据的新旧？</h4><p> 比较最新日志的任期，更大的、更长的 新</p><h4 id="2-4-7-新Leader如何处理前任未提交的数据-？"><a href="#2-4-7-新Leader如何处理前任未提交的数据-？" class="headerlink" title="2.4.7  新Leader如何处理前任未提交的数据 ？"></a>2.4.7  新Leader如何处理前任未提交的数据 ？</h4><ol><li>新<code>Leader</code>只会追加自己的日志，不会删除或覆盖自己的日志（无论是否已被<code>Commit</code>）</li><li>不主动提交非自己任期的日志。</li><li>只在新日志请求来到以后顺便提交。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-27751ef85a94560b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="2-4-8-日志压缩-？"><a href="#2-4-8-日志压缩-？" class="headerlink" title="2.4.8 日志压缩 ？"></a>2.4.8 日志压缩 ？</h4><ol><li>不定期将日志合并为一张快照，可以缩短日志长度，节约空间。</li><li>快照保存了当时的状态机，同时也保存了最后一条</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ea90856c6b23b376.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="2-5-安全性"><a href="#2-5-安全性" class="headerlink" title="2.5 安全性"></a>2.5 安全性</h3><ul><li><strong>选举安全性（Election Safety）</strong>：一个任期（<code>term</code>）内最多允许有一个领导人被选上。</li><li><strong>领导人只增加原则（Leader Append-Only）</strong>：领导人永远不会覆盖或者删除自己的日志，他只会增加条目</li><li><strong>日志匹配原则（Log Matching）</strong>：如果两个日志在相同的索引位置上的任期号相同，那么我们就认为这个日志从头到这个索引位置的之间的条目完全相同。</li><li><strong>领导人完全原则（Leader Completeness）</strong>：如果一个日志条目在一个给的任期内被提交，那么这个条目一定会出现在所有任期号更大的领导人中。</li><li><strong>状态机安全原则（State Machine Safety）</strong>：如果一个服务器已经将给定索引位置的日志条目应用到状态机中，则所有其他服务器不会在该索引位置应用不同的条目。</li></ul><h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p><a href="https://web.stanford.edu/~ouster/cgi-bin/papers/OngaroPhD.pdf">https://web.stanford.edu/~ouster/cgi-bin/papers/OngaroPhD.pdf</a></p><p><a href="https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md">https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md</a></p><p><a href="https://github.com/etcd-io/etcd/tree/main/raft">https://github.com/etcd-io/etcd/tree/main/raft</a></p><p><a href="https://ramcloud.atlassian.net/wiki/download/attachments/6586375/raft.pdf">https://ramcloud.atlassian.net/wiki/download/attachments/6586375/raft.pdf</a></p><p><a href="https://raft.github.io/">https://raft.github.io/</a></p><p><a href="https://juejin.cn/post/6844903602918522888">别再怀疑自己的智商了，Raft协议本来就不好理解</a></p><p><a href="https://www.codedump.info/post/20180922-etcd-raft/">etcd Raft库解析</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
            <tag> Distribution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据传输系统落地和思考</title>
      <link href="/2022/02/26/dts/"/>
      <url>/2022/02/26/dts/</url>
      
        <content type="html"><![CDATA[<h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>我们的产品需要支持 <a href="https://docs.microsoft.com/en-us/microsoft-365/enterprise/microsoft-365-multi-geo?view=o365-worldwide">Multi-Geo</a> 功能。 </p><p>什么是<code>Multi-Geo</code>？简单的说就是：“将一个租户下不同用户/设备/组织等数据，分散存储在不同的地理位置的能力”，在同一个租下管理员可以配置任意用户的数据驻留地（<code>Preferred Data Location</code>简称<code>PDL</code>）。</p><p>该功能主要是解决跨国企业，数据合规存放的问题。支持同一个企业下，不用国家的用户，数据存放在不同的国家的机房。</p><p><code>Multi-Geo</code>的功能涉及到几点核心能力。</p><ol><li><strong>数据的路由能力</strong>。比如，我们服务在<code>CN</code>收到一个<code>User</code>数据查询需求，首先我们要知道这个<code>User</code>是归属于<code>CN</code>还是<code>i18n</code>（国外）的<code>Unit</code>，然后再把请求转发给相应的<code>Unit</code>的服务。</li><li><strong>数据的定位能力</strong>，管理员更新用户的<code>PDL</code>时候，我们需要把用户所有的数据（存量和增量）找出来，然后发送到新的数据驻留地。</li><li><strong>数据传输的能力</strong>，数据传输主要包括存量数据和增量数据的传输过程，存量和增量的<code>overlap</code>怎么处理，业务上是否需要停写，什么时间点修改数据的<code>Unit</code>信息，让后续数据的增删改查请求转发到更新以后的<code>Unit</code>。</li></ol><h2 id="二、数据路由"><a href="#二、数据路由" class="headerlink" title="二、数据路由"></a>二、数据路由</h2><p>为了支持数据路由的能力，我们引入了<code>Global Meta</code> 和 <code>Edge Proxy</code>两个组件。这个是之前做<code>Unit</code>互通就已经有了的组件，不是本文讨论重点。简单说下大概流程如下：</p><p>假设一个用户在<code>CN</code>的发起请求，拉取一个<code>i18n</code>的用户的资料，大概链路应该是这样</p><ol><li>客户端<code>Http</code>调用查询<code>GetUserInfo</code>接口查询用户资料。</li><li><code>HttpGateway</code>收到用户的<code>Http</code>请求，转成<code>Thrift</code>请求，<code>RPC</code>调用<code>User</code>服务的<code>GetUserInfo</code>接口。</li><li><code>User</code>服务收到请求以后，会有一个通用的<code>Cross-MiddleWare</code>，它会提出<code>IDL</code>中的打了<code>Cross</code>标记的<code>Tag</code>，当前上下这个<code>Tag</code>是<code>UserID</code>字段，所以<code>Cross-MiddleWare</code> 会提取出 <code>UserID</code>，然后去<code>Global Meta</code>查询这个<code>UserID</code>的归属于哪个<code>Unit</code>。</li><li><code>Cross-MiddleWare</code>查出这个<code>UserID</code>不属于当前<code>Unit</code>以后，它会设置<code>Dst-Unit</code>和<code>Dst-Service</code>，然后当前请求转发给<code>EdgeProxy</code>。</li><li><code>EdgeProxy</code>取出<code>Dst-Unit</code>，然后把这个请求发送<code>Dst-Unit</code>对应的<code>EdgeProxy</code>。</li><li><code>Dst-Unit</code>的<code>EdgeProxy</code> 会取出<code>Dst-Service</code>，然后把请求转发给<code>Dst-Service</code>，这里就是转发给<code>i18n</code>的<code>User</code>服务。</li><li><code>i18n</code>的<code>User</code>服务发现当前请求是<code>Cross</code>过来的，不会再去请求<code>Global Meta</code>，会直接走本地的查询逻辑，查出<code>User</code>的数据返回给<code>EdgeProxy</code></li><li><code>EdgeProxy</code>拿到<code>Response</code>以后，会走原路返回给客户端。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b94e52b2371bb8db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="三、数据定位"><a href="#三、数据定位" class="headerlink" title="三、数据定位"></a>三、数据定位</h2><p>数据定位能力是指，我们能快速在<code>MySQL</code>、<code>Redis</code>、<code>Abase</code>这些存储组件中找到归属于<code>User</code>所有的数据的能力。常用的解决方案有两种：正向查询定位、数据打标定位。</p><h3 id="3-1-正向查询"><a href="#3-1-正向查询" class="headerlink" title="3.1 正向查询"></a>3.1 正向查询</h3><p>一是正向查询，比如我们有一个<code>UserID</code>，我们可以查到<code>User</code>下面所有的<code>Chat</code>，再查到归属于<code>Chat</code>所有的<code>Message</code>，然后查到<code>Message</code>下面的<code>Reaction</code>和<code>File</code>等等资源，依次类推可以查到所有<code>User</code>相关的信息。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b56c83b3c7878d46.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>正向迁移的方案有几个弊端</p><ol><li>并不是所有的场景都能正向查询，就比如文件转发场景，有两个表<code>file_info</code>和<code>file_ref</code>,每次转发的时候，会新产生一个<code>file_key</code>，并把<code>file_key</code>和<code>parent_key</code>关系记录到<code>file_ref</code>中。<code>file_ref</code>表索引只有<code>file_key</code>，如果想通过<code>parent_key</code>拉取改文件所有被转发的记录，是不支持的。</li><li>部分数据还需要解析内容才能得到，比如<code>Message</code>和<code>File</code>关系，<code>Message</code>的<code>Content</code>是加密以后存在数据库中的，如果要拿到<code>Message</code>中的<code>file_key</code>，我们必须把内容取出来，解密，然后反序列化为<code>PB</code>的<code>Struct</code>。这样对业务耦合太重，会对系统的扩展性和可维护性造成一定困难。</li></ol><p>这个方案不够<code>General</code>，站在架构侧我们希望能够提供更<code>General</code>的方案，而不是去过多的关注业务的数据结构、数据层级。</p><h3 id="3-2-数据打标"><a href="#3-2-数据打标" class="headerlink" title="3.2 数据打标"></a>3.2 数据打标</h3><p>数据打标，就是有一个专门的数据打标系统，会对系统里面每条产生的数据打上标记（有个前提需要基础组件支持<code>binlog</code>），可以按自己的需求打上<code>User</code>标、<code>Chat</code>标等等，查询的时候，能够按自己想要的方式快速查询出来。打标方式如下：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-88e519cf13ce5193.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="3-2-1-MySQL-Schema-打标规则描述"><a href="#3-2-1-MySQL-Schema-打标规则描述" class="headerlink" title="3.2.1 MySQL Schema 打标规则描述"></a>3.2.1 MySQL Schema 打标规则描述</h4><table><thead><tr><th>字段</th><th>描述</th></tr></thead><tbody><tr><td>repo_name</td><td>数据库名</td></tr><tr><td>table_name</td><td>数据表名</td></tr><tr><td>index_keys</td><td>唯一索引列，用于存量数据的获取，比如<code>Message</code>表唯一索引是[“chat_id”,”message_id”]，消费到<code>Message</code>表<code>binlog</code>时候，我们会记录下这两个列的值。方便后面需要迁移数据的时候，我们能够快速定位到相关数据</td></tr><tr><td>need-replace-pk</td><td>是否需要替换PK字段，如果表的<code>id</code>字段是用的<code>MySQL</code>的自增<code>id</code>，所以插入时候可能会冲突，需要插入前替换掉</td></tr><tr><td>entity-type</td><td>实体类型，比如<code>Message</code>、<code>Chat</code>、<code>File</code>、<code>Reaction</code>，每个数据都可以归属到一个实体类型，实体类型之间也有层级归属关系。</td></tr><tr><td>entity-id-field</td><td>哪一列是实体数据，比如<code>chat</code>表的<code>id</code>字段表示<code>Chat</code>这个实体数据</td></tr></tbody></table><p><code>MySQL</code>的数据打标匹配比较简单，收到<code>MySQL</code>的<code>Binlog</code>，用<code>DB</code>+<code>Table</code>就能<code>Match</code>到<code>binlog</code>对应的<code>Schema</code>，然后可以找到<code>Schema</code>里面的<code>IndexKeys</code>和<code>Entity</code>数据，就能知道这个<code>binlog</code>表示的<code>DataEntity</code>数据。</p><h4 id="3-2-2-NoSQL-Schema-打标规则描述"><a href="#3-2-2-NoSQL-Schema-打标规则描述" class="headerlink" title="3.2.2 NoSQL Schema 打标规则描述"></a>3.2.2 NoSQL Schema 打标规则描述</h4><table><thead><tr><th>字段</th><th>描述</th></tr></thead><tbody><tr><td>repo_name</td><td>数据库名</td></tr><tr><td>pattern</td><td>key的格式，如 <code>{{env}}:chat_last_msg_id:{{message_id}}</code></td></tr><tr><td>entity-type</td><td>实体类型，比如<code>Message</code></td></tr><tr><td>entity-id-field</td><td>哪一列是实体数据，比如上面<code>pattern</code>中的<code>message_id</code></td></tr></tbody></table><p><code>NoSQL</code>的匹配相对要复杂一点，首先需要根据<code>Repo</code>的纬度构建压缩字典树<code>radix_tree</code>，然后通过<code>key</code>来匹配，找到<code>key</code>对应的<code>Schema</code>。有点类似<code>Http</code>请求的<code>Path</code>匹配，有点不同的是<code>Path</code>都是“<code>/</code>”结束，截取变量的话相对简单一些。</p><p>做<code>NoSQL</code>的<code>Key</code>匹配的时候，可能比<code>Path</code>要复杂一点，比如<code>{{table_id}}_{{rev}}_{{rec_id}}</code>和<code>{{table_id}}_{{rev}}_{{rec_id}}_{{level}</code> 用<code>tableStr_1000_abccc_1</code>key是可以同时匹配上面两个的<code>pattern</code>的。</p><p>这个时候只能通过增加变量的限制条件，比如<code>rec_id</code>必须包含<code>xx</code>字符串，不包含<code>xx</code>字符串，变量必须是数字、变量长度固定是多少位的方式来做匹配。</p><h4 id="3-2-3-打标流程如下"><a href="#3-2-3-打标流程如下" class="headerlink" title="3.2.3 打标流程如下"></a>3.2.3 打标流程如下</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-53278902baada353.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="3-2-4-打标库选型"><a href="#3-2-4-打标库选型" class="headerlink" title="3.2.4 打标库选型"></a>3.2.4 打标库选型</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4893819dee218052.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>存储选型前，需要明确自己的数据量和需求。</p><ol><li>存量数据数在<code>百亿~千亿</code>级别，数据总大小概在<code>几百TB</code>左右。</li><li>增量<code>Binlog</code>的<code>QPS</code>在<code>200W</code>左右， 需要打标的增量数据，预期有<code>10W</code>左右<code>TPS</code>左右。</li><li>写多读少，主要是打标迁移的时候才会查，还会有少量的删除操作。</li><li>对事务没有要求。</li><li>有一定的一致性要求，写入打标数据以后，需要能再预期的时间内查到。</li></ol><table><thead><tr><th>数据库类型</th><th>常见数据库</th></tr></thead><tbody><tr><td>关系型</td><td>MySQL、Oracle、DB2、SQLServer 等。</td></tr><tr><td>非关系型</td><td>Hbase、Redis、MongodDB 等。</td></tr><tr><td>行式存储</td><td>MySQL、Oracle、DB2、SQLServer 等。</td></tr><tr><td>列式存储</td><td>Hbase、ClickHouse 等。</td></tr><tr><td>分布式存储</td><td>Cassandra、Hbase、MongodDB 等。</td></tr><tr><td>键值存储</td><td>Memcached、Redis、MemcacheDB 等。</td></tr><tr><td>图形存储</td><td>Neo4J、TigerGraph 等。</td></tr><tr><td>文档存储</td><td>MongoDB、CouchDB 等</td></tr></tbody></table><p>分别调研了公司内部几个代表性的存储</p><ol><li>关系型，<code>NDB</code>，对标业界最流行的<code>cloud native</code>的<code>RDS AWS Aurora/Alibaba  PolarDB</code>，100%兼容<code>MySQL</code>，计算存储分离，独立扩缩计算/存储，成本较低。 <code>DBA</code>给的数据是单台机器<code>20</code>核 <code>128G</code>内存，最大写入<code>TPS</code>可以到 <code>15~20K</code>左右。</li><li>非关系型，<code>Abase</code>，基于<code>RocksDB</code>的分布式<code>KV</code>存储，支持<code>Redis</code>协议、极致高可用、低延迟、大容量的在线存储 &amp; 缓存服务，一个小集群能支持几十万的写入。单库支持<code>PB</code>级别的数据存储。</li><li>列式存储，<code>ClickHouse</code>，适用于大批量的数据插入、基本无需修改现有数据、拥有许多列的大宽表、在指定的列进行聚合计算操作、快速执行的 <code>SELECT</code> 查询等场景。目前只支持从<code>Kafka</code>导入数据，且导入的数据，有一定的延迟（10分钟以内）才能查到。</li></ol><h4 id="3-2-4-打标方案总结"><a href="#3-2-4-打标方案总结" class="headerlink" title="3.2.4 打标方案总结"></a>3.2.4 打标方案总结</h4><p>打标方案的好处是 方案更<code>General</code>一些，业务方只需要按照我们提供的<code>Schema</code>规则，来我们系统里面注册就好了。</p><p>缺点就是，增加了资源的开销，需要额外的存储。</p><p>还有一个就是，打标方案有一个假设，就是一定能从下往上查，比如可以从<code>Message</code>查到<code>Chat</code>，再从<code>Chat</code>查到归属的<code>User</code>，但是实际中还有少量表的数据是没办法这样往上查的。</p><p>所以这一部分逻辑，我们会在迁移的某个表的数据时候，会正向查出这个数据关联的子表数据。然后一起迁移走。</p><h2 id="四、数据传输"><a href="#四、数据传输" class="headerlink" title="四、数据传输"></a>四、数据传输</h2><h3 id="4-1-业务实体"><a href="#4-1-业务实体" class="headerlink" title="4.1 业务实体"></a>4.1 业务实体</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5bcb732e5d40a5a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li><code>Biz Entity</code> : 业务实体类型，是一个逻辑概念，比如一个用户是一个<code>Biz Entity</code>，或者一个租户是一个<code>Biz Entity</code>，暴露给管理员的最小迁移单元。</li><li><code>Locating Entity</code>: 简称<code>LE</code>，用于判断数据归属<code>Unit</code>以及数据迁移的最小单元。本身可包含子业务实体。比如一个会话、一篇文档都可以认为是一个<code>LE</code></li><li><code>Sub Entity</code>: 简称<code>SE</code>，<code>Sub Enitty</code>的归属<code>Unit</code>继承于<code>Locating Entity</code>并随<code>Locating Entity</code>一起迁移。</li></ul><h3 id="4-2-数据传输方案"><a href="#4-2-数据传输方案" class="headerlink" title="4.2 数据传输方案"></a>4.2 数据传输方案</h3><h4 id="4-2-1-存量-停写-增量"><a href="#4-2-1-存量-停写-增量" class="headerlink" title="4.2.1 存量+停写+增量"></a>4.2.1 存量+停写+增量</h4><p><strong>一、Locating Entity迁移状态修改</strong></p><p>这一步主要是标记<code>Locating Entity</code>状态为迁移中，开始对<code>Locating Entity</code>存量数据扫描，与此同时记录所有<code>Locating Entity</code>相关的增量<code>binlog</code>数据。</p><p><strong>二、存量数据同步</strong></p><p>在打标库中，查出<code>Locating Entity</code>所有的打标数据，然后根据<code>Schema</code>信息，去业务<code>Repo</code>中查出所有数据，生成<code>binlog</code>发送到对端。</p><p><strong>三、增量数据同步</strong></p><p>存量数据同步完成以后，我们再开始同步“<code>步骤一</code>”开始记录的增量数据。</p><p><strong>四、实时同步状态</strong></p><p>发送完增量数据的瞬间，我们需要对这个<code>Locating Entity</code>加锁，然后修改当前<code>Locating Entity</code>的状态为<code>Syncing</code>状态，后续所有的增量<code>binlog</code>可以实时发送到<code>Dst Unit</code>。</p><p><strong>五、停写</strong></p><p>在状态达到<code>Syncing</code>以后，且时间到了我们配置的某个“<code>时间点</code>”，我们会先判断这个<code>LE</code>能否开始停写（检查<code>binlog</code>消费是否有延迟等等），如果可以停写，则设置<code>Locating Entity</code>状态为“<code>停写</code>”。</p><p>停写即是暂停对一个<code>Locating Entity</code>的所有数据的写入和修改，主要为了规避以下问题：</p><ul><li>防止修改<code>Locating Entity</code>的归属<code>Unit</code>时，各服务读取的<code>Locating Entity Unit</code> 有短暂的不一致，这样造成数据会写入地不一致的问题</li><li>保证剩余的增量数据全部同步到目标机房。防止迁移结束后，仍后剩余数据未迁移，亦或是当业务写入数据较快，导致迁移任务无法结束</li></ul><p>停写可以发生在“数据层”和“业务层”。我们这里选取“业务层”停写的方案。</p><p>优点：性能好，可以实现“<code>fail-fast</code>机制”，出错以后可以尽快返回，能减少无用<code>RPC</code>和数据写入请求。</p><p>缺点：需要各业务方识别出所有的“写”接口并引入停写机制，比较难统一处理和维护。</p><p>停写的纬度是 <code>Locating Entity</code>。业务方需要在自己服务中接入停写的<code>MiddleWare</code>，停写的 <code>MiddleWare</code>会根据<code>Locating Entity</code>的停写标记来判断是否要返回停写错误。</p><p><strong>六、发送结束及标记给对端</strong></p><p>设置了<code>Locating Entity</code>状态为停写以后<code>10s</code>，我们可以假设后续不会再产生<code>Locating Entity</code>的数据了，这个时候我们发送一个<code>Last Locating Entity Data</code> 给<code>Dst Unit</code>，告知对端当前的<code>Locating Entity</code>数据已经发送完成。</p><p>PS： <code>Locating Entity</code>下的所有迁移数据是需要有序发送，有序消费。</p><p><strong>七、Dst Unit 回写完所有数据以后 Ack</strong></p><p><code>Dst Unit</code>收到了<code>Last Locating Entity Data</code>消息以后，知道这个<code>Locating Entity</code>之前的所有数据都回写到业务<code>DB</code>成功了，这个时候会回一个<code>Last Data Ack</code>给<code>Source Unit</code></p><p><strong>八、修改 Locating Entity Unit 信息</strong></p><p>收到<code>Last Data Ack</code> 以后表示所有数据对端都回写成功了，我们可以修改<code>Locating Entity</code>的<code>Unit</code>信息为<code>Dst Unit</code>。</p><p><strong>九、关闭停写</strong></p><p>等待所有服务<code>Unit</code>信息同步完成以后，然后关闭<code>Locating Entity</code>的停写标记。至此一个<code>Locating Entity</code>的迁移过程就完成了。</p><p>后续所有<code>Locating Entity</code>的读写操作都会写到新的<code>Unit</code></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5f41d0eb4ed6df22.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9327a6583a577818.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="4-2-1-停写-全量同步"><a href="#4-2-1-停写-全量同步" class="headerlink" title="4.2.1 停写+全量同步"></a>4.2.1 停写+全量同步</h4><p>改方案相当于上面方案的精简版，主要流程就是 <code>停写</code> -&gt; <code>数据迁移</code> -&gt; <code>修改Unit</code> -&gt; <code>回复停写</code> -&gt; <code>结束</code>。</p><p>PS：该方案适用于数据量不大场景，数据量大的场景，会导致停写时间过长，对用户体验不友好。</p><h4 id="4-2-3-双写方案"><a href="#4-2-3-双写方案" class="headerlink" title="4.2.3 双写方案"></a>4.2.3 双写方案</h4><p>双写方案，就是 <code>存量数据迁移</code> -&gt; <code>增量数据迁移</code> -&gt; <code>实时同步 binlog 数据</code> -&gt; <code>修改Unit</code> -&gt; <code>结束</code></p><p>双写的优点是，不用停写，业务对数据迁移过程无感知，存量数据同步完成以后，可以直接修改Locating Entity 的 Unit 信息，实现无缝切换。</p><p>双写的缺点是，数据可能会有一致性的问题。不太适用于对数据有强一致要求的业务。</p><h3 id="4-3-跨Unit数据传输通道"><a href="#4-3-跨Unit数据传输通道" class="headerlink" title="4.3 跨Unit数据传输通道"></a>4.3 跨Unit数据传输通道</h3><p>这个是用的基建提供的一个<code>Mirror</code>服务。</p><p><code>Mirror</code>是一个分布式的消息同步服务，目前支持<code>kafka→kafka</code>、<code>rmq-&gt;rmq</code>之间的数据同步。<code>Mirror</code>集群部署在目标端，跨<code>region</code>消费数据后再写入同<code>region</code>相应的消息队列中。</p><p>简单说，就是在<code>Source Unit</code>写入一个 <code>MQ Message</code>，<code>Mirror</code>会自动把这个数据同步到<code>Dst Unit</code>。可以在<code>Dst Unit</code>直接消费这个消息。</p><h3 id="4-4-Binloger-模块"><a href="#4-4-Binloger-模块" class="headerlink" title="4.4 Binloger 模块"></a>4.4 Binloger 模块</h3><p><code>binloger</code> 模块，主要负责<code>binlog</code>生成和回写。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4fc574f723cc4c4e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="五、架构总览"><a href="#五、架构总览" class="headerlink" title="五、架构总览"></a>五、架构总览</h2><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d61d2a70c79726c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="六、数据迁移中踩的坑"><a href="#六、数据迁移中踩的坑" class="headerlink" title="六、数据迁移中踩的坑"></a>六、数据迁移中踩的坑</h2><h3 id="6-1-MySQL-数据迁移时区问题"><a href="#6-1-MySQL-数据迁移时区问题" class="headerlink" title="6.1 MySQL 数据迁移时区问题"></a>6.1 MySQL 数据迁移时区问题</h3><p>因为国内和国外时区不一样，<code>MySQL</code>库里面时间都是字符串格式存储，所以传输过程中可能有些问题，详见 <a href="https://fanlv.wiki/2021/11/28/mysql-time/">MySQL DateTime和Timestamp时区问题</a>。</p><h3 id="6-2-PK-Duplicate-Error-问题"><a href="#6-2-PK-Duplicate-Error-问题" class="headerlink" title="6.2 PK Duplicate Error 问题"></a>6.2 PK Duplicate Error 问题</h3><p>本质问题，是业务方写入数据的时候<code>ID</code>用的自增<code>ID</code>（或者自己<code>Local</code>方式生成的<code>ID</code>），<code>DTS</code>这边用的<code>ID</code>是一个中心发号器生成的<code>ID</code>。两个<code>ID</code>有一定的冲突概率（概率比我们想象中的要大）。<br>详见 <a href="https://fanlv.wiki/2022/05/25/mysql-insert-auto-incre/">MySQL 自增列 Duplicate Error 问题分析</a>。</p><h3 id="6-3-INSERT-…-ON-DUPLICATE"><a href="#6-3-INSERT-…-ON-DUPLICATE" class="headerlink" title="6.3 INSERT … ON DUPLICATE"></a>6.3 INSERT … ON DUPLICATE</h3><p>这个问题，其实根因就是上面<code>6.2</code>的问题，业务方的生成的<code>ID</code>和<code>DTS</code>生成的<code>ID</code>冲突了。</p><p>然后我们这边最早插入的时候是用<code>INSERT ... ON DUPLICATE</code>，结果由于<code>ID</code>冲突（其实是两个条毫不相干的数据，只是因为<code>ID</code>生成方式没有保持一致，导致<code>PK Duplicate</code>），然后就走了<code>Update</code>逻辑，把业务方的其他数据给写脏了（血的教训）。</p><p>优化后的逻辑：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-981cff5c230fd95c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="6-4-唯一索引列有-Null-值"><a href="#6-4-唯一索引列有-Null-值" class="headerlink" title="6.4 唯一索引列有 Null 值"></a>6.4 唯一索引列有 Null 值</h3><p>我们正常打标流程，是记录数据的唯一索引，但是业务方有些表的唯一索引的列可能为空，这样就退化为普通索引，导致我们数据迁移的时候会有放大读和放大写。</p><p>假设表数据如下，<code>(a,b)</code>是唯一索引</p><table><thead><tr><th>id</th><th>a</th><th>b</th><th>c</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>NULL</td><td>————-</td></tr><tr><td>2</td><td>1</td><td>NULL</td><td>————-</td></tr><tr><td>3</td><td>1</td><td>NULL</td><td>————-</td></tr><tr><td>4</td><td>1</td><td>NULL</td><td>————-</td></tr><tr><td>5</td><td>1</td><td>NULL</td><td>————-</td></tr></tbody></table><p>我们这边对这<code>5</code>条数据，<code>DTS</code>会打<code>5</code>个标，打标的数据如下<code>table=xx， idx_data = 1,NULL</code>, 这样我们用<code>5</code>条打标数据会查出<code>25</code>条数据（因为一个打标数据可以查出<code>5</code>条）。发送到对端以后，对端会直接写入<code>25</code>条数据，这个数据重复很多的时候，会导致放大的很厉害，对<code>DB</code>读写性能有一定影响。</p><p>优化方式，我们迁移过程中会用<code>table+idx_data</code>做个去重。保证相同的打标数据只会查出一次。</p><h2 id="七、名词解释"><a href="#七、名词解释" class="headerlink" title="七、名词解释"></a>七、名词解释</h2><ul><li><code>Unit</code>：一个功能自封闭的部署单元，可以为用户提供完整的产品功能。<code>Unit</code>之间的数据存储是隔离的。一个<code>Unit</code>可以包含多个<code>IDC</code>。在本文上下文中可以假设有<code>CN</code>和<code>i18n</code>（国际化）两个<code>Unit</code>。</li><li><code>Tenant</code>：租户，可以认为一个公司就是一个租户。</li><li><code>Global Meta</code>：记录所有实体（<code>Entity</code>）归属的一个服务吗，还会记录实体一些其他<code>Meta</code>信息，比如<code>是否处于停写状态</code>。</li><li><code>EdgeProxy</code> ：边缘代理，负责转发<code>Unit</code>之间的请求。</li><li><code>Binlog</code>：描述数据实体内容的数据结构，当前上下文中就是指的一个<code>PB</code>的<code>Struct</code>。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> eng-practices </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Golang 编译器优化那些事</title>
      <link href="/2021/12/18/golang-complier-optimize/"/>
      <url>/2021/12/18/golang-complier-optimize/</url>
      
        <content type="html"><![CDATA[<img alt="cover" src="https://upload-images.jianshu.io/upload_images/12321605-7bac65b41e38666d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>去年写了一篇 <a href="https://fanlv.wiki/2020/06/09/golang-memory-model/">Golang Memory Model</a> 文章。当时在文章里面贴了验证一个线程可见性问题<code>Demo</code>，具体代码如下：</p><pre><code>func main() {    running := true    go func() {        println("start thread1")        count := 1        for running {            count++        }        println("end thread1: count =", count) // 这句代码永远执行不到为什么？    }()    go func() {        println("start thread2")        for {            running = false        }    }()    time.Sleep(time.Hour)}</code></pre><p>今年<code>8</code>月份的时候跟一个朋友做技术交流的时候，朋友指出我这个<code>Demo</code>里面<code>thread1</code>不会结束，是因为<code>running = false</code>这句代码被编译器优化掉了，不是因为线程可见性的问题导致不会结束，<a href="https://godbolt.org/z/ba35MsGeY">当时转汇编</a> 看了下，发现的确是被优化的掉了，但是一直没想清楚是为什么，后面因为一直都在忙公司新项目（最近几个月周末自己主动打了<code>8</code>天黑工，不是我卷，只是简单想把负责的项目做好）。总之就是事情比较多，人一直处于超负荷状态，所以也没闲心去研究这些事情了。一拖就是几个月，一眨眼马上<code>2021</code>年都要过去了，想着再不研究下，这个技术债不知道要拖到什么时候去了，所以这两周末花时间看了下。</p><p>先说结论，上面的<code>for</code>中的<code>running = false</code> 的确是被优化掉了，<code>thread2</code>的<code>for</code>循环汇编代码如下（<a href="https://godbolt.org/z/ba35MsGeY">完整汇编代码点我</a>）：</p><pre><code>        CALL    runtime.printlock(SB)        LEAQ    go.string."start thread2\n"(SB), AX        MOVQ    AX, (SP)        MOVQ    $14, 8(SP)        CALL    runtime.printstring(SB)        NOP        CALL    runtime.printunlock(SB)        JMP     main_func2_pc71main_func2_pc71:        PCDATA  $1, $-1 // Golang 垃圾回收器相关的指令，这里可以无视        JMP     main_func2_pc73main_func2_pc73:        JMP     main_func2_pc75main_func2_pc75:        JMP     main_func2_pc71</code></pre><p>由上面汇编代码可以看到，<code>for { running = false }</code>, 直接被优化成了<code>4</code>条<code>JMP</code>的死循环。</p><p>看到编译器这个汇编代码，我第一反应是感觉这个不符合直觉（因为优化后的代码改变了程序含义），是不是<code>Go </code>编译器的<code>Bug</code>？</p><p>为了验证是不是<code>Go</code>编译器<code>BUG</code>，然后我用<code>C</code>写了个类似的 <a href="https://godbolt.org/z/ETrdf8d77">Demo</a>，分别用 <a href="https://godbolt.org/z/ETrdf8d77">GCC</a> 和 <a href="https://godbolt.org/z/Ea758P863">LLVM</a> 去测试，发现在 <code>-O1</code>的优化级别下，编译器都会把<code>for</code>循环里面的变量赋值给优化掉，具体汇编代码如下：</p><pre><code>void thread_test1( void *ptr) {    while (1) {        counter++;    }}// 汇编代码如下thread_test1:.L2:        jmp     .L2                </code></pre><p>从 <a href="https://godbolt.org/z/ETrdf8d77">GCC</a> 和 <a href="https://godbolt.org/z/Ea758P863">LLVM</a> 结果来看，这个优化并不是<code>Go</code>编译器特有的优化，那为什么会这样优化？ 我们接着往下看。</p><h2 id="二、基础知识"><a href="#二、基础知识" class="headerlink" title="二、基础知识"></a>二、基础知识</h2><h3 id="2-1-编译系统"><a href="#2-1-编译系统" class="headerlink" title="2.1 编译系统"></a>2.1 编译系统</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-44302a94fbdcc7a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>一般一个程序从源码翻译成可执行目标文件需要经过“<code>预处理</code>”、“<code>编译</code>”、“<code>汇编</code>”、”<code>链接</code>“四个阶段。</p><p>我们这里主要关注的是“<code>编译器</code>”做的事情。</p><h3 id="2-2-编译器工作流程"><a href="#2-2-编译器工作流程" class="headerlink" title="2.2 编译器工作流程"></a>2.2 编译器工作流程</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-89087d945e04e645.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在我们这个上下文场景中，我们主要关注编译器代码优化流程。</p><h3 id="2-3-中间代码-Intermediate-Representation-IR"><a href="#2-3-中间代码-Intermediate-Representation-IR" class="headerlink" title="2.3 中间代码(Intermediate Representation - IR)"></a>2.3 中间代码(Intermediate Representation - IR)</h3><p>中间代码（<code>Intermediate Representation</code>）也叫<code>IR</code>，是处于源代码和目标代码之间的一种表示形式。我们倾向于使用<code>IR</code>有两个原因。</p><ol><li>是很多解释型的语言，可以直接执行<code>IR</code>，比如<code>Python</code>和<code>Java</code>。这样的话，编译器生成<code>IR</code>以后就完成任务了，没有必要生成最终的汇编代码。</li><li>我们生成代码的时候，需要做大量的优化工作。而很多优化工作没有必要基于汇编代码来做，而是可以基于<code>IR</code>，用统一的算法来完成。</li></ol><p>像<code>GCC</code>、<code>LLVM</code>这种编译器，可以支持<code>N</code>种不同的源语言，并可以生成<code>M</code>个不同机器码，如果没有<code>IR</code>，直接由源语言直接生成真实的机器代码，这个工作量是巨大的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-34f1f5d76e859b3b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>有了<code>IR</code>可以让编译器的工作更好的<strong>模块化</strong>，编译器前端不用再关注机器细节，编译器后端也不用关注编程语言的细节。这种实现会更加合理一些。</p><p><code>IR</code>基于抽象层次划分，可以分为<code>HIR</code>、<code>MIR</code>、<code>LIR</code>。</p><p><code>IR</code>数据结构常见的有几种：类似三地址指令（<code>Three Address Code - TAC</code>）线性结构、树结构、有向无环图（<code>Directed Acyclic Graph - DAG</code>）、程序依赖图（<code>Program Dependence Graph，PDG</code>）</p><h3 id="2-4-静态单赋值形式-Static-Single-Assignment-SSA"><a href="#2-4-静态单赋值形式-Static-Single-Assignment-SSA" class="headerlink" title="2.4 静态单赋值形式(Static Single Assignment - SSA)"></a>2.4 静态单赋值形式(Static Single Assignment - SSA)</h3><p>静态单赋值（<code>SSA</code>），是<code>IR</code>的一种设计范式，它要求一个变量只能被赋值一次。举例来说</p><pre><code> y := 1 y := 2 x := y</code></pre><p>从上面的描述所知，第一行赋值行为是不需要的，因为y在第二行被二度赋值，<code>y</code>的数值在第三行被使用，一个程序通常会进行定义可达性分析（<code>reaching definition analysis</code>）来测定它。在<code>SSA</code>下，将会变成下列的形式：</p><pre><code> y1 := 1 y2 := 2 x1 := y2</code></pre><p>使用SSA的好处：</p><ol><li>当每个变量只有一个定值时，数据流分析和优化算法可以变的更简单。</li><li>如果一个变量有<code>N</code>个使用和<code>M</code>个定值（占了程序中大约<code>N+M</code>条指令），表示定值-使用链所需要的空间（和时间）和<code>N*M</code>成正比，即成平方增大，对于几乎所有的实际程序，<code>SSA</code>形式的大小和原始程序成线性关系。</li><li><code>SSA</code>形式中，变量的使用和定值可以与控制流程图的必经节点结构以一有用的方式联系起来，从而简化诸如冲突图构建这样的算法。</li><li>源程序中同一个变量的不相关使用在<code>SSA</code>形式中变成了不同的变量，从而删除了他们来了之间不必要联系。</li></ol><p>更多可以参考 <a href="https://zh.wikipedia.org/wiki/%E9%9D%99%E6%80%81%E5%8D%95%E8%B5%8B%E5%80%BC%E5%BD%A2%E5%BC%8F">静态单赋值形式 - WIKI</a></p><p>静态单赋值形式引入了一个<code>φ</code>节点(<code>/faɪ/</code>又叫<code>Phi</code>)，这个节点的作用，就是根据控制流提供的信息，来确定选择两个值中的哪一个。<code>Phi</code>运算是<code>SSA</code>格式的<code>IR</code>中必然会采用的一种运算，用来从多个可能的数据流分支中选择一个值。</p><p><strong>现代语言用于优化的 IR，很多都是基于 SSA 的了，比如 Golang 编译器、Java 的 JIT 编译器、JavaScript的 V8 编译器，以及 LLVM 等。</strong></p><h3 id="2-5-编译器常用的循环优化算法"><a href="#2-5-编译器常用的循环优化算法" class="headerlink" title="2.5 编译器常用的循环优化算法"></a>2.5 编译器常用的循环优化算法</h3><p><strong>第一种：归纳变量优化（Induction Variable Optimization`）</strong></p><p>看下面这个循环，其中的变量<code>j</code>是由循环变量派生出来的，这种变量叫做该循环的归纳变量。归纳变量的变化是很有规律的，因此可以尝试做<strong>强度折减优化</strong>。示例代码中的乘法可以由加法替代。</p><pre><code>int j = 0;for (int i = 1; i &lt; 100; i++) {    j = 2*i;  //2*i可以替换成j+2}return j;</code></pre><p><strong>第二种：边界检查消除（Unnecessary Bounds-checking Elimination）</strong></p><p>当引用一个数组成员的时候，通常要检查下标是否越界。在循环里面，如果每次都要检查的话，代价就会相当高（例如做多个数组的向量运算的时候）。如果编译器能够确定，在循环中使用的数组下标（通常是循环变量或者基于循环变量的归纳变量）不会越界，那就可以消除掉边界检查的代码，从而大大提高性能。</p><p><strong>第三种：循环展开（Loop Unrolling）</strong></p><p>把循环次数减少，但在每一次循环里，完成原来多次循环的工作量。比如：</p><pre><code>for (int i = 0; i&lt; 100; i++){  sum = sum + i;}</code></pre><p>优化后可以变成：</p><pre><code>for (int i = 0; i&lt; 100; i+=5){  sum = sum + i;  sum = sum + i + 1;  sum = sum + i + 2;  sum = sum + i + 3;  sum = sum + i + 4;}</code></pre><p>进一步，循环体内的<code>5</code>条语句就可以优化成<code>1</code>条语句：<code>sum = sum + i*5 + 10;</code>。</p><p>减少循环次数，本身就能减少循环条件的执行次数。同时，它还会增加一个基本块中的指令数量，从而为指令排序的优化算法创造机会。指令排序会在下一讲中介绍。</p><p><strong>第四种：循环向量化（Loop Vectorization）</strong></p><p>在循环展开的基础上，我们有机会把多次计算优化成一个向量计算。比如，如果要循环<code>16</code>万次，对一个包含了<code>16</code>万个整数的数组做汇总，就可以变成循环<code>1</code>万次，每次用向量化的指令计算<code>16</code>个整数。</p><p><strong>第五种：重组（Reassociation）</strong></p><p>在循环结构中，使用代数简化和重组，能获得更大的收益。比如，如下对数组的循环操作，其中数组<code>a[i,j]</code>的地址是<code>a+i*N+j</code>。但这个运算每次循环就要计算一次，一共要计算<code>M*N</code>次。但其实，这个地址表达式的前半截<code>a+i*N</code>不需要每次都在内循环里计算，只要在外循环计算就行了。</p><pre><code>for (i = 0; i&lt; M; i++){  for (j = 0; j&lt;N; j++){    a[i,j] = b + a[i,j];  }}</code></pre><p>优化后的代码相当于：</p><pre><code>for (i = 0; i&lt; M; i++){  t=a+i*N;  for (j = 0; j&lt;N; j++){    *(t+j) = b + *(t+j);  }}</code></pre><p><strong>第六种：循环不变代码外提（Loop-Invariant Code Motion，LICM）</strong></p><p>在循环结构中，如果发现有些代码其实跟循环无关，那就应该提到循环外面去，避免一次次重复计算。</p><p><strong>第七种：代码提升（Code Hoisting，或 Expression Hoisting）</strong></p><p>在下面的<code>if</code>结构中，<code>then</code>块和<code>else</code>块都有<code>z=x+y</code>这个语句，它可以提到<code>if</code>语句的外面。</p><pre><code>  if (x &gt; y)    ...    z = x + y    ...  }  else{    z = x + y    ...  }  </code></pre><p>这样变换以后，至少代码量会降低。但是，如果这个<code>if</code>结构是在循环里面，那么可以继续借助<strong>循环不变代码外提优化</strong>，把<code>z=x+y</code>从循环体中提出来，从而降低计算量。</p><pre><code>z = x + yfor(int i = 0; i &lt; 10000; i++){  if (x &gt; y)    ...  }  else{    ...  }}</code></pre><p><strong>第八种：激进的死代码优化</strong>    </p><pre><code>int testFunc()  {    int k =0;    while (k&lt;100) {        k++;    }    return 1;}</code></pre><p>在某些优化场景下，因为循环理解的代码并不影响返回结果，编译器会直接优化掉循环的代码，只剩一个<code>return 1;</code></p><p>还有一些场景，激进的死代码删除算法会删除没有输出的无线循环，从而改变程序的含义。因为在原来的程序不产生任何输出的情况下，删除这种无限循环后，程序会执行该循环之后的语句，而这些语句有可能产生输出。在许多环境下，这被认为是不可接受的（详见<a href="https://book.douban.com/subject/1806974/">《现代编译原理》- 虎书</a><code>19.5</code>章节）。</p><h3 id="2-6-窥孔优化"><a href="#2-6-窥孔优化" class="headerlink" title="2.6 窥孔优化"></a>2.6 窥孔优化</h3><p>窥孔优化，就是通过一个滑动窗口（窥孔）扫描<code>IR</code>、<code>汇编代码</code>或者是<code>机器码</code>，每次扫描<code>n</code>行，然后检查窗口中的指令，看是否可以用更快或者更短的指令来替换窗口中的指令序列。这个窗口可以沿着代码不断的滑动，从而发现所有代码中的优化机会。窺孔优化技术并不要求在窺孔中的代码一定是连续的。窺孔优化的特点是每一次改进又可以产生新的优化机会。</p><p>窺孔优化常用的优化场景有：</p><ol><li>消除冗余的加载和保存指令。</li><li>消除不可达代码。</li><li>控制流优化。</li><li>代数化简和强度消减。</li><li>使用机器特有的指令。</li></ol><h3 id="2-7-寄存器分配和指派"><a href="#2-7-寄存器分配和指派" class="headerlink" title="2.7 寄存器分配和指派"></a>2.7 寄存器分配和指派</h3><p>代码生成的关键问题之一是决定哪个值放在寄存器里面，<strong>寄存器是目标机上运行速度最快的计算单元</strong>，但是我们通常没有足够的寄存器来存放所有的值。没有存放在寄存器中的值必须存放在内存中。只涉及寄存器运算分量的指令比那些涉及内运算分量的指令运行的快，因此，有效利用寄存器非常重要。</p><p>寄存器的使用经常被分解为两个子问题：</p><ol><li>寄存器分配：对于原程序中的每个点，我们选择一组将被存放在寄存器中的变量。</li><li>寄存器指派：我们指定一个变量存放在哪个寄存器中。</li></ol><p>当然，很少有“能够将所有的数据在所有时间内都保存在寄存器”的情况，因此精心的使用好寄存器，控制好对那些不在寄存器中的变量的访问很重要。有个几个需要特别考虑的问题：</p><ol><li><strong>尽可能将程序执行中最频繁的变量分配到寄存器中</strong>。</li><li>尽可能高效的访问那些不在寄存器中的变量。</li><li>使“记账”使用的寄存器（例如，为管理变量对存储器的访问而保留的寄存器）个数尽可能的少，以便能用更多寄存器来容纳变量的值。</li><li>尽可能提高过程调用和相关操作的效率，如进入和退出作用域，以便使他们的开销减至最小。</li></ol><p>参与寄存器分配常见的有以下对象：</p><ul><li>栈指针（frame pointer），指向运行栈当前过程的栈帧开始处。</li><li>动态链（dynamic link）</li><li>静态链 （static link）</li><li>全局偏移表指针（global offset table pointer）</li><li>参数，参数由当前活跃过程传递给被调用过程</li><li>返回值，当前活跃过程调用的过程返回的结果</li><li>频繁使用的变量，最频繁使用的局部（<strong>也可能是非局部的或者全局</strong>）变量</li><li>临时变量，在表达式求值期间和其他较短活跃期内使用和计算出的临时值。</li></ul><h3 id="2-8-Go编译器基础"><a href="#2-8-Go编译器基础" class="headerlink" title="2.8 Go编译器基础"></a>2.8 Go编译器基础</h3><p><code>Go Compiler</code>（下文用<code>gc</code>表示）在<code>1.5</code>版本的时候实现了 <a href="https://en.wikipedia.org/wiki/Bootstrapping_(compilers)">自举</a>。编译器相关源代码主要在 <a href="https://github.com/golang/go/tree/release-branch.go1.16/src/cmd/compile">src/cmd/compile</a> 目录下。</p><p>在<code>gc</code>的 <a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/README.md">README.md</a> 中介绍了<code>gc</code>主要分为<code>4</code>个模块：</p><ol><li><code>Parsing</code>，词法分析和语法分析，词法分析器代码主要在<code>scanner.go</code>中，语法分析器代码主要在<code>parser.go</code>中，<code>Go</code>的<code>AST</code>的节点定义在<code>nodes.go</code>中。</li><li><code>Type-checking and AST transformations</code>，语义分析（类型检查和<code>AST</code>变换），语义分析的代码在<code>typecheck.go</code>中，主要做<code>类型检查</code>、<code>名称消解（Name Resolution）</code>、<code>类型推导</code>、<code>内联优化</code>、<code>逃逸分析</code>等。</li><li><code>Generic SSA</code>，生成<code>SSA</code>格式的<code>IR</code>，<code>gc</code>的<code>IR</code>是基于控制流图<code>CFG</code>的。一个函数会被分成多个<code>基本块</code>，<code>基本块</code>中包含了一行行的<code>指令</code>。<code>Go SSA</code>中有三个比较重要的概念，分别是 <a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/internal/ssa/value.go#L18">Value</a> 、 <a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/internal/ssa/block.go#L12">Block</a>、<a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/internal/ssa/func.go#L26">Func</a>。<ul><li><code>Value</code> 是<code>SSA</code>的最主要构造单元，它可以定义一次、使用多次。在定义一个<code>Value</code>的时候，需要一个标识符<code>ID</code>作为名称、产生该<code>Value</code>的操作码（<a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/internal/ssa/op.go#L19">Op</a>）、一个类型（<a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/internal/types/type.go#L118">Type</a>，就是代码中<code>&lt;&gt;</code>里面的值），以及一些参数。</li><li><code>Block</code>，基本块有三种：简单（<code>Plain</code>）基本块，它只有一个后继基本块；退出（<code>Exit</code>）基本块，它的最后一个指令是一个返回指令；还有<code>if</code>基本块，它有一个<code>控制值</code>，并且它会根据该值是<code>true</code>还是<code>false</code>，跳转到不同的基本块。</li><li><code>Func</code>，函数是由多个基本块构成的。它必须有一个入口基本块（<code>Entry Block</code>），但可以有<code>0</code>到多个退出基本块，就像一个<code>Go</code>函数允许包含多个<code>Return</code>语句一样。</li></ul></li><li><code>Generating machine code</code>，生成机器码，主要代码在 <a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/internal/gc/ssa.go#L6296">cmd/compile/internal/gc/ssa.go</a> 中。需要说的是，<code>gc</code>生成的汇编代码是<code>Plan9</code>汇编，是一种<code>伪汇编</code>，它是一种半抽象的汇编代码。在生成特定<code>CPU</code>的机器码的时候，它还会做一些转换，值得一提的是<code>gc</code>并没有做<code>指令排序</code>的优化工作。</li></ol><p><code>Go compiler SSA</code>更多介绍，可以看<code>gc SSA </code>官方 <a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/internal/ssa/README.md">README.MD</a></p><p><code>gc</code>提供了一个生成可视化<code>SSA</code>的<code>IR</code>的选项，只要我们在执行<code>go build</code>设置环境变量<code>GOSSAFUNC</code>为想打印<code>IR</code>的函数名，<code>gc</code>会生成一个<code>ssa.html</code>文件。<code>ssa.html</code>文件会记录了编译器为了优化我们代码所经过的所有步骤和每个步骤的耗时。如果想在控制台也输出相关代码，可以在函数名后面加上<code>+</code>。<a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/internal/gc/main.go#L525">具体代码点我查看</a></p><p>假如我要看<code>mian</code>包下面函数名为<code>thread1</code>的<code>IR</code>优化过程/代码，可以执行如下命令</p><pre><code>GOSSAFUNC=main.thread1+ go build -gcflags="-N -l" ./demo.go</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-23c3deacc0136f9b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="三、Go编译器For循环优化分析"><a href="#三、Go编译器For循环优化分析" class="headerlink" title="三、Go编译器For循环优化分析"></a>三、Go编译器For循环优化分析</h2><h3 id="3-1-Demo验证"><a href="#3-1-Demo验证" class="headerlink" title="3.1 Demo验证"></a>3.1 Demo验证</h3><p>为了方便验证问题，我简化了一下<a href="https://godbolt.org/z/TTofx3Ma5">Demo</a> 代码如下：</p><pre><code>var counter = 1func main() {    go thread1()    for counter &lt; 2 {}    println("finish , counter = ", counter)}func thread1() {    for {        counter++    }}</code></pre><p>然后<code>go build</code>时候设置<code>GOSSAFUNC</code>环境变量来查看<code>SSA IR</code>输出</p><pre><code>GOSSAFUNC=main.thread1+ go build -gcflags="-N -l" ./demo.go</code></pre><p>我们可以看到控制台输出如下，（完整的<code>SSA</code>结果<a href="./ssa1.html">点我查看</a>）</p><pre><code>thread1 func()   b1:    v1 = InitMem &lt;mem&gt; DEAD // Init函数使用的内存    v2 = SP &lt;uintptr&gt; DEAD // 栈指针    v3 = SB &lt;uintptr&gt; DEAD // 栈底指针    v4 = Addr &lt;*int&gt; {"".counter} v3 DEAD // 全局变量 counter 地址    v7 = Const64 &lt;int&gt; [1] DEAD // 常量"1"    v9 = Addr &lt;*int&gt; {"".counter} v3 DEAD // 全局变量 counter 地址    Plain -&gt; b2  // 跳转到 b2   b2: &lt;- b1 b4    v12 = Phi &lt;mem&gt; v1 v10 DEAD // b1 过来的 v12 = v1 , b4 过来的 v12 = v10    Plain -&gt; b3 // 跳转到 b3  b3: &lt;- b2    v5 = Copy &lt;mem&gt; v12 DEAD  // 把 v12 的值拷贝到 v5    v6 = Load &lt;int&gt; v4 v5 DEAD // 读取 v4 地址中的数据到 v5 中    v8 = Add64 &lt;int&gt; v6 v7 DEAD / v8 = v6 + v7    v10 = Store &lt;mem&gt; {int} v9 v8 v5 DEAD // v9 = v8 使用的内存地址是 v5    Plain -&gt; b4  b4: &lt;- b3    Plain -&gt; b2  b5: DEAD // 这个 Block 不可达，所以直接标记为 Dead 了    v11 = Unknown &lt;mem&gt; DEAD    Ret v11  pass number lines begin</code></pre><p>这个是<code>Go Compiler</code>生成的<code>SSA</code>形式的<code>IR</code>，简单说下这个<code>IR</code>相关含义：</p><ol><li><code>b1~b5</code>就是上文说的<code>Block</code>，表示基本块，不同基本块之间会跳转。</li><li><code>v1~v12</code>就是上文说的<code>Value</code>。<code>Value</code>的<code>=</code> 右边第一个是 <a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/internal/ssa/op.go#L19">Op</a>，<code>&lt;&gt;</code>里面表示的是<code>Value</code>的类型 <a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/internal/types/type.go#L118">Type</a>。所有的<code>Value</code>后面都有<code>DEAD</code>标记，表示这<code>Value</code>已经是<code>死码</code>了。</li><li><code>b2: &lt;- b1 b4</code>表示，数据可以从<code>b1</code>或者<code>b4</code>流转到<code>b2</code></li><li><code>v12 = Phi &lt;mem&gt; v1 v10</code> 表示<code>v12</code>取<code>v1</code>和<code>v10</code>中的一个值（具体需要看是从<code>b1</code>流入的还是<code>b2</code>流入的）</li></ol><p>在 <a href="./ssa1.html">SSA的结果中</a> 我们看到有很多 <a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/internal/ssa/compile.go#L426">Pass</a> ，每一个<code>Pass</code>对应这一个<code>IR</code>代码优化过程，比如<code>deadcode</code>是做<code>死码消除</code>优化、<code>cse</code> 是消除公共子表达式的算法等等。</p><p>可以看到所有<code>Block</code>中的<code>Value</code>在执行<code>Pass</code>优化前就已经标记为<code>Dead</code>了，然后<code>opt deadcode</code>优化的时候，清除掉了所有标记为<code>Dead</code>的<code>Value</code>。在 <a href="./ssa1.html">SSA的结果中</a> 到最后生成汇编的时候，就只剩下<code>4</code>个<code>JUMP</code>指令了，也说就是变成一个空循环了。</p><pre><code>b1 00003 (+13) JMP 4b2 00004 (+14) JMP 5b3 00005 (14) JMP 6b4 00006 (14) JMP 4</code></pre><p>看到这里，我们知道编译器认为<code>for</code>循环里面的代码都是<code>死码</code>，所以把<code>for</code>循环中的代码优化掉了，接着我们要看编译器为什么认为<code>for</code>循环里面的代码是<code>死码</code>。</p><p>由上面 <a href="./ssa1.html">SSA的结果中</a> 输出我们知道，在<code>Pass</code>之前编译器就已经认为所有<code>Value</code>是<code>Dead</code>了，所以后面的<code>Pass</code>优化逻辑，不是我们关注的重点，可以先不用管。我们要看的是编译器如何判断一个<code>Value</code>是否存活。</p><p>在此之前，我们先看下这个<code>4</code>个<code>Block</code>的是如何生成的。我在<code>Go Compiler</code>的<code>ssa.go</code>源码中找到了<code>for</code>转换为<code>Block</code>的<a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/internal/gc/ssa.go#L1449">相关代码</a>，</p><pre><code>// OFOR: for Ninit; Left; Right { Nbody }// cond (Left); body (Nbody); incr (Right)//// OFORUNTIL: for Ninit; Left; Right; List { Nbody }// =&gt; body: { Nbody }; incr: Right; if Left { lateincr: List; goto body }; end:bCond := s.f.NewBlock(ssa.BlockPlain) // b2bBody := s.f.NewBlock(ssa.BlockPlain) // b3bIncr := s.f.NewBlock(ssa.BlockPlain) // b4bEnd := s.f.NewBlock(ssa.BlockPlain)  // b5</code></pre><p>由代码可以知，<code>for</code>循环生成了<code>bCond</code>、<code>bBody</code>、<code>bIncr</code>、<code>bEnd</code>四个<code>Block</code>，转对应的<code>CFG</code>如下图如下：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-61c1541e6ac07077.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>再来看<code>Go Compiler</code>是如何判断一个<code>Block</code>中的<code>Value</code>是<code>Live</code>还是<code>Dead</code>的，我在 <a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/internal/ssa/deadcode.go#L56">compile/internal/ssa/deadcode.go</a> 找到了如下代码：</p><pre><code>// liveValues returns the live values in f and a list of values that are eligible// to be statements in reversed data flow order.// The second result is used to help conserve statement boundaries for debugging.// reachable is a map from block ID to whether the block is reachable.// The caller should call f.retDeadcodeLive(live) and f.retDeadcodeLiveOrderStmts(liveOrderStmts)// when they are done with the return values.func liveValues(f *Func, reachable []bool) (live []bool, liveOrderStmts []*Value) {    // ..... 初始化 live []bool ，默认全部是Dead/false    // ..... 如果是寄存器分配函数，全部设置为存活，我们先无视    // ..... 内联相关代码，我们先无视        // Find all live values    q := f.Cache.deadcode.q[:0]    defer func() { f.Cache.deadcode.q = q }()    // Starting set: all control values of reachable blocks are live.    // Calls are live (because callee can observe the memory state).    for _, b := range f.Blocks {        if !reachable[b.ID] {         // 如果 Block 不可达，则认为这个 Block下面所有的 Value 都是 Dead Value        // 我们这个Demo场景，b5 下面的 Value 都是 Dead Value            continue        }        for _, v := range b.ControlValues() {            if !live[v.ID] {                live[v.ID] = true                q = append(q, v)                if v.Pos.IsStmt() != src.PosNotStmt {                    liveOrderStmts = append(liveOrderStmts, v)                }            }        }        for _, v := range b.Values {            if (opcodeTable[v.Op].call || opcodeTable[v.Op].hasSideEffects) &amp;&amp; !live[v.ID] {                live[v.ID] = true                q = append(q, v)                if v.Pos.IsStmt() != src.PosNotStmt {                    liveOrderStmts = append(liveOrderStmts, v)                }            }            // ... 内联相关代码先忽略        }    }    // Compute transitive closure of live values.    for len(q) &gt; 0 {        // pop a reachable value        v := q[len(q)-1]        q = q[:len(q)-1]        for i, x := range v.Args {             if v.Op == OpPhi &amp;&amp; !reachable[v.Block.Preds[i].b.ID] {                continue            }                        // 存活的 Value 依赖的 Value 也必须是存活状态           // 假设 v11 (15) = StaticLECall &lt;mem&gt; {AuxCall{"".emptyFunc()}} v10           // v11 如果是存活的状态, 那么 v10 也就应该是存活状态，依次类推            if !live[x.ID] {                live[x.ID] = true                q = append(q, x) // push                if x.Pos.IsStmt() != src.PosNotStmt {                    liveOrderStmts = append(liveOrderStmts, x)                }            }        }    }    return}</code></pre><p><code>liveValues</code>函数主要做三件事</p><ol><li>遍历所有<code>reachable</code>（可达）的<code>Block</code>的<code>控制语句（ControlValues）</code>，有的话直接设置该<code>ControlValue</code>为<code>Live=Ture</code>，然后把存活的<code>Value</code>放到到一个队列中去，在第三步会用到。</li><li>遍历所有<code>reachable</code>（可达）的<code>Block</code>的<code>Value</code>，如果<code>Value</code>的<code>Op</code>在 <a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/internal/ssa/opGen.go#L2907">opcodeTable</a> 对象是一个<code>函数调用(call=true)</code>就认为这个<code>Value</code>是存活的，或者如果<code>Value</code>的<code>Op</code>对象的<code>hasSideEffects</code>为<code>true</code>，也认为是存活的。<code>hasSideEffects</code>我翻了下<code>Go</code>的<code>Commit</code>是为了修复<code>Go 1.8</code> <a href="https://github.com/golang/go/issues/19182">sync/atomic loop elided</a> 的<code>Bug</code>加入的，主要是 <code>Atomic</code> 相关的几个方法会设置<code>hasSideEffects</code>为<code>True</code>，具体可以看<a href="https://go-review.googlesource.com/c/go/+/37333/">add opcode flag hasSideEffects for do-not-remove</a> 这个<code>commit</code>。这一步里面存活的<code>Value</code>也会放到队列中去。</li><li>从队列依次取出存活的<code>Value</code>，然后遍历<code>Value</code>依赖的参数，设置参数<code>Value</code>的<code>Live=Ture</code>，依次类推，算出所有存活的<code>Value</code>。</li></ol><p>我们再看下我们上面代码中<code>b1~b4</code>中所有<code>Value</code>的<code>op</code>，依次是<code>InitMem</code>、<code>SP</code>、<code>SB</code>、<code>Addr</code>、<code>Const64</code>、<code>Phi</code>、<code>Copy</code>、<code>Load</code>、<code>Add64</code>、<code>Store</code>，在 <a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/internal/ssa/opGen.go#L2907">opcodeTable</a> 中查了下，<code>call</code>和<code>hasSideEffects</code>都没有设置，默认都是<code>false</code>，所以最后所有<code>Block</code>中的<code>Value</code>都标记为<code>Dead</code>了。</p><h3 id="3-2-添加控制语句、函数调用验证"><a href="#3-2-添加控制语句、函数调用验证" class="headerlink" title="3.2 添加控制语句、函数调用验证"></a>3.2 添加控制语句、函数调用验证</h3><p>上面说了，如果<code>Block</code>中有<code>控制语句（ControlValues）</code>或者<code>函数调用</code>，<code>Go Compiler</code>就会认为这个<code>Value</code>是存活的，我们分别用两个<code>Demo</code>验证下。</p><p>先看<code>控制语句</code>场景，我加了个<code>flag</code>变量，代码如下：（<a href="https://godbolt.org/z/b9sKePKWv">完整代码</a>）</p><pre><code>var flag = truefunc thread1() {    for flag { // 虽然 flag 一直是 true，然而 go 编译器并没有优化掉这个变量        counter++    }}</code></pre><p>我们再执行一下<code>GOSSAFUNC=main.thread1+ go build -gcflags="-N -l" ./demo2.go</code>看一下<code>SSA</code>的<code>IR</code><a href="./ssa2.html">完整代码</a></p><pre><code>thread1 func()  b1:    v1 = InitMem &lt;mem&gt;    v2 = SP &lt;uintptr&gt; DEAD    v3 = SB &lt;uintptr&gt;    v4 = Addr &lt;*bool&gt; {"".flag} v3    v7 = Addr &lt;*int&gt; {"".counter} v3    v10 = Const64 &lt;int&gt; [1]    v12 = Addr &lt;*int&gt; {"".counter} v3    Plain -&gt; b2  b2: &lt;- b1 b4    v5 = Phi &lt;mem&gt; v1 v13    v6 = Load &lt;bool&gt; v4 v5  // v6 依赖了 v4、v5，所以 v4、v5 也是存活的，依次类推    If v6 -&gt; b3 b5 (likely) // v6 是 ControlValue ，所以 v6 是存活的  b3: &lt;- b2    v8 = Copy &lt;mem&gt; v5    v9 = Load &lt;int&gt; v7 v8    v11 = Add64 &lt;int&gt; v9 v10    v13 = Store &lt;mem&gt; {int} v12 v11 v8    Plain -&gt; b4  b4: &lt;- b3    Plain -&gt; b2  b5: &lt;- b2    v14 = Copy &lt;mem&gt; v5    Ret v14</code></pre><p>因为 <a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/internal/gc/ssa.go#L3031">ssa.BlockIf</a> 和 <a href="https://github.com/golang/go/blob/release-branch.go1.16/src/cmd/compile/internal/gc/ssa.go#L1647">ssa.BlockRet</a> 都设置了<code>ControlValue</code>，所以 <code>v6</code>和<code>v14</code>会认为是存活的，然后 <code>v6</code>和<code>v14</code>的<code>Args</code>又间接依赖了其他所有的<code>Value</code>，所以其他所有<code>Value</code>也都标记为存活了（上面这些<code>Value</code>输出的时候末尾没有再标记为<code>Dead</code>了）。</p><p>因为<code>for</code>循环中的的代码没有被优化掉，所以这个时候循环里面的逻辑已经正常，运行函数，也能正常结束了（注意这里并不代表没有可见性问题），具体生成的汇编代码如下。</p><pre><code>        JMP     thread1_pc2thread1_pc2:        CMPB    "".flag(SB), $0 // 判断 flag 是否等于 0        JNE     thread1_pc13 // 不等于 0 跳转到 thread1_pc13        JMP     thread1_pc24 // 等于 0 跳转到 thread1_pc24thread1_pc13:        INCQ    "".counter(SB)  // counter++ （内存操作）        JMP     thread1_pc22 // 跳转到 thread1_pc22thread1_pc22:        JMP     thread1_pc2thread1_pc24:        RET    </code></pre><p>再来看下<strong>函数调用</strong>场景，我在函数中加了个 <a href="https://godbolt.org/z/oE75sT6jo">空函数调用</a> 测试了下一下（注意要加<code>-l</code>禁止内联），结果也一样。<code>for</code>循环的<code>body</code>代码没有再被优化掉。</p><pre><code>func thread1() {    for {        emptyFunc()        counter++    }}func emptyFunc() {}thread1_pc27:        FUNCDATA        $0, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB)        FUNCDATA        $1, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB)        PCDATA  $1, $0        NOP        CALL    "".emptyFunc(SB)        INCQ    "".counter(SB)        JMP     thread1_pc27</code></pre><h3 id="3-3-总结"><a href="#3-3-总结" class="headerlink" title="3.3 总结"></a>3.3 总结</h3><p>基于上面分析，我们可以知道在满足以下两个条件时，<code>Go</code>编译器会才会优化掉<code>for</code>循环的<code>body</code>中相关的<code>Value</code></p><ol><li>循环永远不会结束，或者说没有控制语句。</li><li><code>body</code>中没有函数调用。</li></ol><h2 id="四、进一步思考"><a href="#四、进一步思考" class="headerlink" title="四、进一步思考"></a>四、进一步思考</h2><p>上面，我们分析了<code>gc</code>代码，知道了编译器是怎么一步步把<code>for</code>循环中<code>body</code>的代码优化掉的。但是我们并不知道编译器基于什么的考量来做这个优化的。查阅了下<code>Go</code>编译器的各种资料，也没有找到支撑相关优化算法的资料。</p><p><code>gc</code>这个研究走进死胡同了，这个时候，我把眼光转向了<code>GCC</code>，文章最开始说了，我用 <a href="https://godbolt.org/z/ETrdf8d77">GCC</a> 和 <a href="https://godbolt.org/z/Ea758P863">LLVM</a> 测试相关代码的时候，有一样的优化效果。而且<code>GCC</code>的优化参数，比<code>Go</code>编译器丰富很多（<code>Go</code>编译器相关的只有一个<code>-N</code>参数，<code>GCC</code>可以自己指定各种<a href="https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html">优化算法</a>）。</p><h3 id="4-1-GCC-优化分析"><a href="#4-1-GCC-优化分析" class="headerlink" title="4.1 GCC 优化分析"></a>4.1 GCC 优化分析</h3><p><code>C</code>测试代码如下：</p><pre><code>void thread_test1 (void *ptr);int counter = 1;int main(){    pthread_t thread1;    int res = pthread_create(&amp;thread1, NULL, (void *)&amp;thread_test1, NULL);    if (res != 0) {        printf("pthread_create failed\n");        return 0;    }        while (counter&lt;2) {}    printf("finish, counter = %d\n",counter);}void thread_test1( void *ptr) {    while (1) {        counter++;    }}</code></pre><p>我们先不加优化标记（或者用<code>-O0</code>）看下 <a href="https://godbolt.org/z/6cb9e1TE3">生成的编译代码</a> 如下：</p><pre><code>thread_test1:        pushq   %rbp        movq    %rsp, %rbp        movq    %rdi, -8(%rbp).L7:        movl    counter(%rip), %eax // 读取内存中的值给 eax 寄存器        addl    $1, %eax // eax 寄存器的值 +1        movl    %eax, counter(%rip) // 把寄存器的值回写到内存        jmp     .L7  // 跳转到 .L7</code></pre><p>我们看到，在不指定优化级别的情况下，<code>GCC</code>是不会优化<code>for</code>循环中的代码的。 如果加上了<code>-O1</code>优化级别的话，<code>for</code>循环直接变成一个无限循环的<code>jmp</code>指令，<code>thread_test1</code><a href="https://godbolt.org/z/ETrdf8d77">汇编代码</a> 会变成如下：</p><pre><code>thread_test1:        subq    $8, %rsp        movl    $.LC0, %edi        call    puts.L2:        jmp     .L2</code></pre><p>加个<code>flag</code>控制变量测试再测试一下，<a href="https://godbolt.org/z/Mc3zo1YKM">代码如下</a>：</p><pre><code>int flag = 1;void thread_test1( void *ptr) {    while (flag) {        counter++;    }}thread_test1:    cmpl    $0, flag(%rip) // 判断 flag == 0    je      .L1  // 等于0跳转到.L1.L3:     jmp     .L3 // 不等于 0 .L3 无限循环 .L1:  // 函数返回    ret</code></pre><p>我们看到<code>GCC</code>还是比<code>Go Compiler</code>聪明一点，知道<code>flag</code>变量一直<code>1</code>，所以只执行了一次比较（<code>cmpl</code>）然后依然是一个<code>jmp</code>的死循环。</p><p>我们把<code>flag</code> 换成<code>counter</code>再试下，<a href="https://godbolt.org/z/ah1Wb7KE6">新生成的代码如下</a>：</p><pre><code>void thread_test1( void *ptr) {    while (counter) {        counter++;    }}thread_test1:        movl    counter(%rip), %eax // 读取 counter 内存值给 eax        testl   %eax, %eax // 判断 eax 是否为 0        je      .L1 // eax = 0 跳转到 .L1 函数结束.L3:        addl    $1, %eax // eax = eax+1        jne     .L3 // eax 不等于 0，就跳转到 .L3，一直到 EAX 溢出，才跳出循环        movl    $0, counter(%rip) // 设置 counter = 0.L1:        ret// eax 是 32 位寄存器，CPU 主频按 2GHZ 算，每个时钟信号周期为0.5纳秒// 不考虑流水线和指令拆分，eax 寄存器溢出只要 4294967296*2*0.5ns= 4.2s</code></pre><p>这一次优化结果比较有趣，从内存读取<code>counter</code>的值以后，直接赋值给了<code>eax</code>寄存器，然后<code>for</code>循环里面不停的对<code>eax</code>做加<code>1</code>操作，然后在循环之后（<code>counter</code>溢出）的代码直接设置<code>counter=0</code>（编译器认为只有<code>counter</code>为<code>0</code>的时候<code>for</code>循环才会结束）。</p><p>这里我们发现了一个比较关键的优化，在没有指定优化级别的时候，每次循环的时候，都会从内存中读出<code>counter</code>赋值给<code>eax</code>，然后对<code>eax</code>加<code>1</code>，最后把<code>eax</code>值回写到内存<code>counter</code>地址。但是在<code>-O1</code>优化级别下，如上代码所示，编译器把读取<code>counter</code>赋值给<code>eax</code>的操作提到循环外层来了，循环里面只是简单的对<code>eax</code>做了加<code>1</code>操作。这个优化也符合我们上面说的“<strong>编译器会尽可能将程序执行中最频繁的变量分配到寄存器中</strong>”。</p><p>上面在一个循环内，不停的对寄存器做加<code>1</code>操作，这个算是无意义的操作，编译器没有优化掉我比较意外。</p><pre><code>.L3:        addl    $1, %eax         jne     .L3         movl    $0, counter(%rip) </code></pre><p>所以，我用<code>-O2</code>优化级别测试了下，看下了<a href="https://godbolt.org/z/jPPPace56">输出代码</a>如下：</p><pre><code>thread_test1:        subq    $8, %rsp        movl    $.LC0, %edi        call    puts        movl    counter(%rip), %eax // 读取 counter 内存值给 eax        testl   %eax, %eax // 判断 eax == 0        je      .L1 // eax = 0 的话跳转到 .L1 （函数结束）        movl    $0, counter(%rip) // eax != 0, 执行当前代码，设置 counter = 0 ， 然后继续执行 .L1 （函数结束）.L1:        addq    $8, %rsp        ret</code></pre><p>上面，我们看到整个循环直接被优化掉了（对程序结果没有改变），这个也是符合预期的。</p><h3 id="4-2-问题本质"><a href="#4-2-问题本质" class="headerlink" title="4.2 问题本质"></a>4.2 问题本质</h3><p>再回来 <a href="https://godbolt.org/z/1P3E19dch">看这个代码</a>，在<code>-O1</code>的级别下生成代码如下：</p><pre><code>void thread_test1( void *ptr) {    while (1) {        counter++;    }}thread_test1:.L2:        jmp     .L2</code></pre><p>我们知道，在没有优化的场景下（<code>-O0</code>），每次循环的都会从内存读取<code>counter</code>值给<code>eax</code>，然后对<code>eax+1</code>，最后把<code>eax</code>值回写到内存。</p><p>（以下只是个人猜测）因为<code>counter</code>是在循环里面<code>频繁使用的变量</code>，<code>-O1</code>的场景下，编译器会把<code>counter</code>操作，优化到寄存器，优化以后的伪代码如下：</p><pre><code>thread_test1: movl    counter(%rip), %eax // 读取 counter 内存值给 eax.L2:         addl    $1, %eax        jmp     .L2</code></pre><p>编译器进一步优化的时候比如做<code>窺孔优化</code>的时候，发现循环中做<code>eax+1</code>操作是没有意义的（因为没有读取<code>eax</code>的地方），所以最终就只优化成一条<code> jmp .L2</code>指令了。</p><p><code>C/C++</code>语言中有个<code>volatile</code>关键字，它的作用之一就是禁止编译器使用寄存器优化变量存储。</p><h2 id="五、扩展阅读"><a href="#五、扩展阅读" class="headerlink" title="五、扩展阅读"></a>五、扩展阅读</h2><p>问题一：编译下面这段代码，为什么<code>go build -gcflags "-N" demo.go</code> 和 <code>go build demo.go</code>程序运行的结果不一样</p><pre><code>type Foo struct{}func main() {    fooA()    fooB()}func fooA() {    f1 := &amp;Foo{}    f2 := &amp;Foo{}    fmt.Println("f1 = ", f1, " f2 = ", f2) // f1, f2 逃逸到堆上了    //println("f1 = ", f1, " f2 = ", f2)    fmt.Println(f1 == f2)}func fooB() {    f1 := &amp;Foo{}    f2 := &amp;Foo{}    fmt.Println(f1 == f2)}</code></pre><p>问题二：为什么下面<code>a</code>、<code>b</code> 两个值不相等</p><pre><code>package mainimport "fmt"const s = "123456789"func main() {    var a byte = (1 &lt;&lt; uint(len(s))) / 128           // 编译器计算    var b byte = (1 &lt;&lt; uint(len(s[:]))) / 128        // 运行时计算, 溢出了    var c byte = byte(int(1&lt;&lt;uint(len(s[:]))) / 128) // 运行时计算，用int 所以没有溢出    fmt.Println(a, b, c)}</code></pre><h2 id="六、参考资料"><a href="#六、参考资料" class="headerlink" title="六、参考资料"></a>六、参考资料</h2><p><a href="https://book.douban.com/subject/3296317/">《编译原理》 - 龙书</a></p><p><a href="https://book.douban.com/subject/1806974/">《现代编译原理》-  虎书</a></p><p><a href="https://book.douban.com/subject/1400374/">《高级编译器设计与实现》 - 鲸书</a></p><p><a href="https://time.geekbang.org/column/intro/100052801?tab=intro">《编译原理实战》</a></p><p><a href="https://github.com/golang/go/blob/master/src/cmd/compile/README.md">Introduction to the Go compiler</a></p><p><a href="https://quasilyte.dev/blog/post/go_ssa_rules/">Go compiler: SSA optimization rules description language</a></p><p><a href="https://github.com/golang/go/blob/master/src/cmd/compile/internal/ssa/README.md">Introduction to the Go compiler’s SSA backend</a></p><p><a href="https://sitano.github.io/2018/03/18/howto-read-gossa/">How to read GoLang static single-assignment (SSA)</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Golang </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL DateTime和Timestamp时区问题</title>
      <link href="/2021/11/28/mysql-time/"/>
      <url>/2021/11/28/mysql-time/</url>
      
        <content type="html"><![CDATA[<img alt="cover" src="https://upload-images.jianshu.io/upload_images/12321605-5a4f15ef8cd7c7b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>最近负责一个数据传输的项目，其中一个需求就是能把一个<code>DB</code>里面的数据拉出来 ，然后回放到另外一个同构的<code>DB</code>。两个<code>DB</code>的服务不在一个时区（其实这不是重点），可能配置不同。之前有过类似的项目，当时是基建的同事负责做数据同步，同步过去以后<code>DateTime</code>、<code>Timestamp</code>字段的时区信息都丢了。老板让我调研下问题根因，不要踩之前的坑。</p><p>最早的时候看了下同事写的当时<code>MySQL</code>时区信息丢失的问题总结文档，文档里面当时把<code>DateTime</code>和<code>Timestamp</code>两个时区问题混为一起了，也没分析本质原因，导致我当时没看太明白，然后的武断的认为，之所以时区丢失了，是因为基础组件同步<code>DateTime</code>和<code>Timestamp</code>的时候同步的是字符串，比如<code>2021-11-27 10:49:35.857969</code>这种信息，我们传输的时候，只要转<code>UnixTime</code>然后传过去就行了（这个其实只是问题之一，其实还跟<code>time_zone</code>、<code>loc</code>配置相关，后面会说）。</p><p>先说结论，如果你能保证<code>所有项目</code>连接<code>DB</code>的<code>DSN</code>配置的<code>loc</code>和<code>time_zone</code>（<code>time_zone</code>没有配置的话会用<code>MySQL</code>服务端的默认配置） 都是一样的，那不用看下去了。不管你数据在不同<code>DB</code>之间怎么传输，服务读取的<code>DB</code>的时区都是符合你的预期的。</p><h2 id="二、基础知识"><a href="#二、基础知识" class="headerlink" title="二、基础知识"></a>二、基础知识</h2><h3 id="2-1-Unix时间戳能确定唯一时刻"><a href="#2-1-Unix时间戳能确定唯一时刻" class="headerlink" title="2.1 Unix时间戳能确定唯一时刻"></a>2.1 Unix时间戳能确定唯一时刻</h3><p><a href="https://zh.wikipedia.org/wiki/UNIX%E6%97%B6%E9%97%B4">UNIX时间</a>，是UNIX或类UNIX系统使用的时间表示方式：从<code>UTC 1970年1月1日0时0分0秒</code>起至现在的总秒数<code>('1970-01-01 00:00:00' UTC)</code>。</p><p>时间字符串<code>2021-11-27 02:06:50</code>是不能确定确定唯一时刻的（直白点说就是中国人说的<code>2021-11-27 02:06:50</code>和美国人说的<code>2021-11-27 02:06:50</code>不是同一时刻），简单说就是 <code>UnixTime</code> = <code>2021-11-27 02:06:50</code> + <code>time_zone</code>,<code>UnixTime</code> + <code>time_zone</code> 可以得到不同地区人看到的<code>time_string</code>。</p><p>我们在数据传输和过程中，<strong>是希望这个唯一时刻保持不变，并不是希望时区保持不变</strong>。我发一条消息在中国时间是<code>2021-11-27 02:06:50</code>，在其他地方应该是显示其他地方的当地时间。</p><pre><code>t := time.Unix(1637950010, 0) // 时刻唯一确定，可以打印这个时刻不同时区的时间串fmt.Println(t.UTC().String()) // 2021-11-26 18:06:50 +0000 UTCfmt.Println(t.String()) // 2021-11-27 02:06:50 +0800 CSTnow := time.Now()fmt.Println(now.UTC().String()) // 2021-11-27 18:06:50.981506 +0000 UTCfmt.Println(now.String()) // 2021-11-27 02:06:50.981506 +0800 CST m=+0.000326041</code></pre><br>        <h3 id="2-2-MySQL-DateTime-存储信息不带时区"><a href="#2-2-MySQL-DateTime-存储信息不带时区" class="headerlink" title="2.2 MySQL DateTime 存储信息不带时区"></a>2.2 MySQL DateTime 存储信息不带时区</h3><p>DataTime 表示范围 <code>'1000-01-01 00:00:00' to '9999-12-31 23:59:59'</code>。<code>5.6.4</code> 版本之前，<code>DateTime</code>占用<code>8</code>字节，<code>5.6.4</code>之后默认是<code>5</code>字节（到秒），如果要更高精度可以配置<code>Fractional Seconds Precision</code>， <code>fsp=1~2</code>占用<code>1</code>字节 ，<code>3~4</code>占用 <code>2</code>个字节，<code>5~6</code>占用<code>3</code>个字节， 如<code>DATETIME(6)</code> 精确到秒后<code>6</code>位，一共占用<code>8</code>字节。</p><p>需要注意的是：不论是<code>5.6.4</code>之前，还是<code>5.6.4</code>之后<code>DateTime</code>字段里面都<strong>没有带时区信息，不能确定唯一时刻</strong>，更多可以看 <a href="https://dev.mysql.com/doc/internals/en/date-and-time-data-type-representation.html">MySQL官网文档</a>。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-eeb9a0f6cded28da.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="datetime_type.jpg"></p><br><h3 id="2-3-MySQL-Timestamp-和-time-zone"><a href="#2-3-MySQL-Timestamp-和-time-zone" class="headerlink" title="2.3 MySQL Timestamp 和 time_zone"></a>2.3 MySQL Timestamp 和 time_zone</h3><blockquote><p>Timestamp: A four-byte integer representing seconds UTC since the epoch (‘1970-01-01 00:00:00’ UTC)<br>The Timestamp data type is used for values that contain both date and time parts. Timestamp has a range of ‘1970-01-01 00:00:01’ UTC to ‘2038-01-19 03:14:07’ UTC.</p></blockquote><p><code>Timestamp</code>就是存的<code>Unix</code>时间戳，表示范围是<code>'1970-01-01 00:00:01' UTC to '2038-01-19 03:14:07'</code>，是不是<code>Timestamp</code>就没有时区问题？并不是。<a href="https://dev.mysql.com/doc/refman/8.0/en/datetime.html">MySQL官方文档有如下一段话如下</a>：</p><blockquote><p>MySQL converts Timestamp values from the current time zone to UTC for storage, and back from UTC to the current time zone for retrieval. (This does not occur for other types such as DATETIME.) By default, the current time zone for each connection is the server’s time. The time zone can be set on a per-connection basis. As long as the time zone setting remains constant, you get back the same value you store. If you store a Timestamp value, and then change the time zone and retrieve the value, the retrieved value is different from the value you stored. This occurs because the same time zone was not used for conversion in both directions. The current time zone is available as the value of the time_zone system variable. For more information, see Section 5.1.15, “MySQL Server Time Zone Support”.</p></blockquote><p>简单说，每个<code>session</code>可以设置不同的<code>time_zone</code>，如果你设置<code>session</code>用的<code>time_zone</code>和读取<code>session</code>用的<code>time_zone</code>不一样，那你会得到错误/不同的值。说白了一个<code>Timestamp</code>字段，写入和读取的<code>session</code>必须一样。针对单个<code>DB</code>的场景，建议所有<code>session</code>的<code>dsn</code>都不配置<code>time_zone</code>。</p><p><code>time_zone</code> 有三种设置方法</p><pre><code>set time_zone = '+8:00'; // 设置当前 session 的 time_zone，立即生效set global time_zone = '+8:00'; // 设置MySQL全局默认配置，新的连接才生效dsn里面指定 time_zone='+8:00'user:pwd@tcp(host:port)/db?charset=utf8mb4&amp;parseTime=True&amp;loc=Asia%2FShanghai&amp;time_zone=%27%2B8%3A00%27</code></pre><br><h3 id="2-3-SQL-数据传输时候，DataTime和Timestamp都是字符串传输"><a href="#2-3-SQL-数据传输时候，DataTime和Timestamp都是字符串传输" class="headerlink" title="2.3 SQL 数据传输时候，DataTime和Timestamp都是字符串传输"></a>2.3 SQL 数据传输时候，DataTime和Timestamp都是字符串传输</h3><pre><code>DROP TABLE IF EXISTS `ts_test`;CREATE TABLE ts_test (    `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT 'pk',    `program_insert_time` varchar(100) COMMENT '代码里面获取的时间字符串，insert 语句用的',    `time_zone` INT COMMENT '插入的时候当前 session 的 time_zone 设置的是什么',    `loc` varchar(20) COMMENT '插入这个语句时候，dsn 的 loc',    `ts` Timestamp(6),     PRIMARY KEY (id));</code></pre><p>然后分别执行</p><pre><code>INSERT INTO `dt_test` (`loc`,`program_insert_time`,`dt`) VALUES ('Asia/Shanghai','2021-11-27 14:08:07.3751 +0000 UTC','2021-11-27 14:08:07.3751')SELECT * FORM `dt_test`</code></pre><p>wireshark 抓包可知SQL传输的时候，DataTime和Timestamp都是直接传输不带时区的字符串，如<code>2021-11-27 14:08:07.3751</code>这种。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-84ef5b79baf33dcc.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="insert_1.jpg"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4fc68b0212a26a53.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="insert_2.jpg"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-dc877ee3fc100caa.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="select_req.jpg"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b95c5bbf8a28a707.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="select_resp.jpg"></p><br><h2 id="三、问题分析"><a href="#三、问题分析" class="headerlink" title="三、问题分析"></a>三、问题分析</h2><h3 id="3-1-Datetime-问题分析"><a href="#3-1-Datetime-问题分析" class="headerlink" title="3.1 Datetime 问题分析"></a>3.1 Datetime 问题分析</h3><p>上面我们说过<code>SQL</code>请求和响应的<code>Data</code>里面<code>Datetime</code>和<code>Timestamp</code>字段都是用<strong>时间字符串</strong>，我们用<code>GORM</code>执行<code>SQL</code>的时候，我们传的对<code>Golang</code>的<code>time.Time</code>，这个<code>time</code>类型的时间是怎么最终转换成不带时区的时间字符串呢？翻了下<code>go-sql-driver</code><a href="https://github.com/go-sql-driver/mysql/blob/master/packets.go#L1119">代码</a>，看到有下面这段逻辑。</p><pre><code>case time.Time:    paramTypes[i+i] = byte(fieldTypeString)    paramTypes[i+i+1] = 0x00    var a [64]byte    var b = a[:0]    if v.IsZero() {        b = append(b, "0000-00-00"...)    } else {        b, err = appendDateTime(b, v.In(mc.cfg.Loc)) // v 就是我们传入的 time.Time 对象         if err != nil {            return err        }    }</code></pre><p>看下 <a href="https://github.com/go-sql-driver/mysql/blob/6cf3092b0e12f6e197de3ed6aa2acfeac322a9bb/utils.go#L279">appendDateTime</a> 函数逻辑就是把<code>time.Time</code>转成<code>mc.cfg.Loc</code>时区的字符串。</p><p>举例说明就是，我们插入一个<code>SQL</code>的时候，假设是代码里面 <code>time.Now()</code> 获取了一个时间对象，这个时间对象是有时区信息的（或者说是能确定唯一时刻的），时区是当前系统的时区。传到<code>go-sql-driver</code>里面去以后，<code>driver</code>需要把这个对象转成不带时区的字符串，具体要转成哪个时区的字符串，就是由<code>mc.cfg.Loc</code>决定的。我们再往上跟下看下<code>mc.cfg.Loc</code>是哪里传入的。找到如下代码，由代码可以知道，<code>loc</code>信息是我们配置<code>dns</code>连接串的时候传入的,<code>loc</code>不传的话，默认是<code>UTC 0</code>时间</p><pre><code>https://github.com/go-sql-driver/mysql/blob/master/driver.go#L73// OpenConnector implements driver.DriverContext.func (d MySQLDriver) OpenConnector(dsn string) (driver.Connector, error) {    cfg, err := ParseDSN(dsn) // https://github.com/go-sql-driver/mysql/blob/6cf3092b0e12f6e197de3ed6aa2acfeac322a9bb/dsn.go#L291    if err != nil {        return nil, err    }    return &amp;connector{        cfg: cfg,    }, nil}// https://github.com/go-sql-driver/mysql/blob/6cf3092b0e12f6e197de3ed6aa2acfeac322a9bb/dsn.go#L68// NewConfig creates a new Config and sets default values.func NewConfig() *Config {    return &amp;Config{        Collation:            defaultCollation,        Loc:                  time.UTC, // loc 传的话，默认是UTC时间        MaxAllowedPacket:     defaultMaxAllowedPacket,        AllowNativePasswords: true,        CheckConnLiveness:    true,    }}// Connect implements driver.Connector interface.// Connect returns a connection to the database.func (c *connector) Connect(ctx context.Context) (driver.Conn, error) {    var err error    // New mysqlConn    mc := &amp;mysqlConn{        maxAllowedPacket: maxPacketSize,        maxWriteSize:     maxPacketSize - 1,        closech:          make(chan struct{}),        cfg:              c.cfg,    }    mc.parseTime = mc.cfg.ParseTime    </code></pre><p>再来看查询的时候，时间字符串的转换问题，上面用<code>WireShark</code>抓包的时候，知道我们执行<code>Select</code>查询数据的时候，<code>MySQL</code>给我们返回的也是时间字符串。那客户端代码是如何转成<code>time.Time</code>对象的？我们知道<code>dsn</code>里面有个<code>parseTime</code>字段是来控制，从<code>parseTime</code>相关代码我们可以找到<a href="https://github.com/go-sql-driver/mysql/blob/master/packets.go#L789">如下代码</a>。</p><pre><code>if !mc.parseTime {    continue}// Parse time fieldswitch rows.rs.columns[i].fieldType {case fieldTypeTimestamp,    fieldTypeDateTime,    fieldTypeDate,    fieldTypeNewDate:    if dest[i], err = parseDateTime(dest[i].([]byte), mc.cfg.Loc); err != nil {        return err    }}</code></pre><p>看下 <a href="https://github.com/go-sql-driver/mysql/blob/6cf3092b0e12f6e197de3ed6aa2acfeac322a9bb/utils.go#L109">parseDateTime</a> 函数，就是用<code>mc.cfg.Loc</code>加时间字符串转换成了<code>time.Time</code></p><pre><code>func parseDateTime(b []byte, loc *time.Location) (time.Time, error) {    const base = "0000-00-00 00:00:00.000000"    switch len(b) {    case 10, 19, 21, 22, 23, 24, 25, 26: // up to "YYYY-MM-DD HH:MM:SS.MMMMMM"        if string(b) == base[:len(b)] {            return time.Time{}, nil        }</code></pre><br><h3 id="3-2-Datetime-总结"><a href="#3-2-Datetime-总结" class="headerlink" title="3.2 Datetime 总结"></a>3.2 Datetime 总结</h3><p><code>Datetime</code>在<code>MySQL</code>服务端保存的只是一个字符串，时区信息都是由连接串的<code>loc</code>字符串控制的。如果要想时区保证一致，写入和读取的<code>loc</code>必须保证一致。</p><p>需要注意几点：</p><ol><li><code>loc</code>配置是给插入的时候用<code>time.Time</code>转时间字符串用的。如果你裸写插入<code>SQL</code>（RawSQL），<code>loc</code>怎么配置，都不会影响时间串，数据存的时间，就是你<code>Insert</code>语句里面拼接的时间串。</li><li>如果们插入的是<code>time.Time</code> (能确定唯一时刻)对象，插入客户端所在的系统的时区信息对插入结果没影响，因为客户端是用<code>time.Time</code>+<code>loc</code>来得到时间字符串。</li><li><code>loc</code> 没有配置的话，默认是<code>UTC0</code></li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e3243effcb8ed367.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="datetime.jpg"></p><br><h3 id="3-3-Timestamp"><a href="#3-3-Timestamp" class="headerlink" title="3.3 Timestamp"></a>3.3 Timestamp</h3><p><code>Timestamp</code>在<code>go-sql-driver</code>里面的处理流程跟<code>Datetime</code>一样，区别是是时间字符串到了服务端，服务端会用<code>time_zone</code>加字符串得到<code>UnixTime</code>然后保存（这部分只是个人猜想，并没有去找<code>MySQL</code>源码验证，<a href="./time_span.go">只是通过简单的代码测试</a>和官方文档来验证自己的想法），从结果上来看，读入和写入的<code>session</code>的<code>time_zone</code>必须保持一致读的数据才是对的。</p><p><a href="https://dev.mysql.com/doc/refman/8.0/en/time-zone-support.html">time_zone 相关官方文档</a></p><p><a href="https://dev.mysql.com/doc/internals/en/date-and-time-data-type-representation.html">Timestamp 存的4字节UTC时间</a></p><p><a href="https://dev.mysql.com/doc/refman/8.0/en/datetime.html">Timestamp 和 time_zone 关系 第七段</a></p><br><h3 id="3-4-Timestamp-总结"><a href="#3-4-Timestamp-总结" class="headerlink" title="3.4 Timestamp 总结"></a>3.4 Timestamp 总结</h3><p>如果真的要存时间戳，建议用<code>bigint</code>存，这样不管数据怎么传输，不管<code>loc</code>、<code>time_zone</code> 怎么配置，都没有时区问题。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ffc61b32e15393f2.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Timestamp.jpg"></p><br><h2 id="四、数据传输的时候如何保证数据正确"><a href="#四、数据传输的时候如何保证数据正确" class="headerlink" title="四、数据传输的时候如何保证数据正确"></a>四、数据传输的时候如何保证数据正确</h2><p>知道了上面的基本信息以后，数据传输系统要做的事就很明确了。</p><ol><li>读取和写入的数据的时候，<code>loc</code>和<code>time_zone</code>配置跟业务方保持一致就行了。</li><li><code>DTS</code>数据传输的时候，因为<code>binlog</code>字段都是字符串，需要把<code>时间字符串</code>+<code>loc</code>转成时间戳，然后发送到对端。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ba0159fd7b4faf09.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="dts.png"></p><h2 id="五、问题本质"><a href="#五、问题本质" class="headerlink" title="五、问题本质"></a>五、问题本质</h2><p><code>MySQL</code> 存储、写入读取传输时候都是时间字符。客户端发送和接收的时候需要用<code>loc</code>来标明这个字符串的时区信息，所以读取和写入的<code>loc</code>必须要保证是相同的，所以这个字符串才有相同的语义。</p><p>如果所有业务方，都不设置<code>loc</code>，统一都是默认配置。时间戳，直接用<code>bigint</code>存那就没有任何时区问题。世界美好一点不好吗？何必自己给自己折腾一堆莫名其妙问题。</p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《黑客与画家》</title>
      <link href="/2021/09/21/hackers-and-painters/"/>
      <url>/2021/09/21/hackers-and-painters/</url>
      
        <content type="html"><![CDATA[<h2 id="第一章节：为什么书呆子不受欢迎"><a href="#第一章节：为什么书呆子不受欢迎" class="headerlink" title="第一章节：为什么书呆子不受欢迎"></a>第一章节：为什么书呆子不受欢迎</h2><p>首先这里“<code>书呆子</code>”指的“<code>高智商</code>”的人。</p><p>解开这个谜的关键是把问题换一种提法。为什么聪明的小孩没有让自己变得受欢迎？如果他们真的很聪明，为什么找不到受欢迎的诀窍呢？他们在标准化测试中表现得这么好，为什么就不能在这方面也大获成功呢？</p><p>有一种观点认为，其他小孩妒忌聪明学生，所以聪明的学生不可能受到欢迎。我倒希望这种解释是对的。回想起来，要是初中里真的有人妒忌我，那么他们一定费了很大力气才把这种妒忌隐藏得无法发现。而且，在任何情况下，如果聪明真的令他人妒忌，这反而会招来女生。因为女生喜欢被其他男生妒忌的男生。</p><p>在我就读过的学校，聪明根本就是无足轻重的一样东西。同学们既不看重它，也不唾弃它。如果别的事情都相同，那么大家还是愿意自己变得聪明一点，因为这总比做个笨人好。<strong>但是总的来说，智力在大家心里的分量远远不如相貌、魅力和运动能力的分量重。</strong></p><p><strong><code>书呆子</code>不受欢迎的真正原因，是他们脑子里想着别的事情。他们的注意力都放在读书或者观察世界上面，而不是放在穿衣打扮、开晚会上面</strong>。他们就像头顶一杯水来踢足球，一边踢球，一边拼命保持不让水洒出来。其他人都在一门心思玩足球，遇到这样的对手，自然能够毫不费力地击败，并且心里还奇怪，对方怎么如此无能。</p><p>虽然“<code>书呆子</code>”饱尝不受欢迎之苦，<strong>但是为了解除痛苦而让他们放弃“聪明”，我想大多数人是不会愿意的</strong>。对他们来说，平庸的智力是不可忍受的。不过，要是换了别的孩子，情况就不一样了，大多数人会接受这笔交易。对于很多人来说，这反而是更上一层楼的机会。即使是那些智力排名在前<code>20％</code>的学生（我在这里假设智力可以测量，那时的人们似乎都相信这一点），谁不愿意用<code>30分</code>的成绩换来别人的友爱和钦佩？</p><p>就算<code>书呆子</code>心里想着变得与其他小孩一样受欢迎，做起来却是难上加难。因为那些受欢迎的小孩从小就在琢磨如何受欢迎，打心底里追求这个。但是，<strong>书呆子从小琢磨的却是如何更聪明，心底里也是这样追求的</strong>。这都是受父母的影响，<strong>书呆子被教导追求正确答案，而受欢迎的小孩被教导讨人喜欢</strong>。</p><p><strong>在校园环境下，书呆子为什么会被歧视？</strong></p><p>一部分原因是，青少年在心理上还没有摆脱儿童状态，许多人都会残忍地对待他人；另一方面，一些追求受欢迎的孩子往往会”结盟”,<code>书呆子</code>就是他们一个共同的敌人(这就好比一个政客，他想让选民忘记糟糕的国内局势，方法就是为国家找出一个敌人，哪怕敌人并不真的存在，他也可以创造一个出来),再进一步，由于<code>书呆子</code>是不受欢迎的，处在学校的底层，所以全校学生都把<code>书呆子</code>当作一个可供欺负的安全目标。</p><p><strong>成年人世界如何？</strong></p><p>因为那是成年人的世界，他们都成熟了，不会把<code>书呆子</code>挑出来欺负。更重要的是，真实世界的关键并非在于它是由成年人组成的，而在于它的庞大规模使得你做的每件事都能产生真正意义上的效果。</p><p><strong>学校的作用</strong></p><p>看管监狱的人主要关心的是犯人都待在自己应该待的位置。然后，让犯人有东西吃，尽可能不要发生斗殴和伤害事件，这就可以了；</p><p>校方最重视的事情，就是让学生待在自己应该待的位置。与此同时，让学生有东西吃，避免公然的暴力行为，接下来才是尝试教给学生一些东西。</p><p>公立学校的老师很像监狱的狱卒。表面上，学校的使命是教育儿童。事实上，学校的真正目的是把儿童都关在同一个地方，以便大人们白天可以腾出手来把事情做完</p><p><strong>学校和外面的环境</strong></p><p>有了围墙，成年人与学生并不能相互理解，已然是两个不同的世界，一个扭曲的世界，一个真实的世界。<br>生活在这个扭曲的世界，不仅仅对书呆子，对所有孩子来说，都是充满压力的。就像任何一场战争，胜利方也是要付出代价的。</p><p><strong>校园生活的问题</strong></p><p>校园生活的两大恐怖之处——残忍和无聊</p><p>美国公立学校的平庸并不仅仅是让学生度过了不快乐的六年，还带来了严重后果。这种平庸直接导致学生的叛逆心理，使他们远离那些原本应该要学习的东西。</p><p>校园生活的真正问题是空虚。除非成年人意识到这一点，否则无法解决这个问题。</p><p><strong>作者意见</strong></p><p>对于<code>书呆子</code>来说，意识到学校并非全部的人生，也是很重要的事情。学校是一个很奇怪的、人为设计出来的体系，一半像是无菌室，一半像是野蛮洪荒之地。它就像人生一样，里面无所不包，但又不是事物的真实样子。它只是一个暂时的过程，只要你向前看，你就能超越它，哪怕现在你还是身处其中。</p><p>如果你觉得人生糟透了，那不是因为体内激素分泌失调（你父母相信这种说法），也不是因为人生真的糟透了（你本人相信这种说法）。那是因为你对成年人不再具有经济价值（与工业社会以前的时期相比），所以他们把你扔在学校里，一关就是好几年，根本没有真正的事情可做。任何这种类型的组织都是可怕的生存环境。你根本不需要寻找其他的原因就能解释为什么青少年是不快乐的。</p><p>我在这篇文章中发表了一些刺耳的意见，但是我对未来是乐观的。我们认定无法解决的难题，事实上完全可以解决。青少年并不是洪水猛兽，也并非天生就不快乐。这一点对于青少年和成年人，应该都是令人鼓舞的消息。</p><h2 id="第二章节：黑客与画家"><a href="#第二章节：黑客与画家" class="headerlink" title="第二章节：黑客与画家"></a>第二章节：黑客与画家</h2><p><strong>黑客与画家</strong></p><p>黑客与画家的共同之处，在于他们都是创作者。</p><p>计算机科学就像一个大杂烩，由于某些历史意外，很多不相干的领域被强行拼装在一起。这个学科的一端是纯粹的数学家，中间部分是计算机博物学家，另一端则是黑客。</p><p>对于黑客，只想写出有趣的软件，对于他们来说，计算机只是一种表达的媒介，就像建筑师手里的混凝土，或者画家手里的颜料。</p><p>黑客搞懂“<code>计算理论</code>”（<code>theory of computation</code>）的必要性，与画家搞懂颜料化学成分的必要性差不多大。</p><p><strong>优美的事物</strong></p><p>创造优美事物的方式往往不是从头做起，而是在现有成果的基础上做一些小小的调整，或者将已有的观点用比较新的方式组合起来。这种类型的工作很难用研究性的论文表达。</p><p>唯一有效的外部考核就是时间。经过岁月的洗礼，优美的东西生存发展的机会更大，丑陋的东西往往会被淘汰</p><p><strong>如何赚大钱</strong></p><p>如果大学和实验室不允许黑客做他们想做的事情，那么适合黑客的地方可能就是企业。不幸的是，大多数企业也不允许黑客做他们想做的事情。大学和实验室强迫黑客成为科学家，企业强迫黑客成为工程师。</p><p>如果某一天你想要去赚大钱，那么记住上面这一点，因为这是创业公司能够成功的原因之一。</p><p>大公司只要做到不太烂，就能赢。</p><p>真正竞争软件设计的战场是新兴领域的市场，这里还没有人建立过防御工事。只要你能做出大胆的设计，由一个人或一批人同时负责设计和实现产品，你就能在这里战胜大公司。</p><p>开发优秀软件的方法之一就是自己创业。</p><p><strong>创业的困难</strong></p><p>一个是自己开公司的话，必须处理许许多多与开发软件完全无关的事情；</p><p>另一个问题是赚钱的软件往往不是好玩的软件，两者的重叠度不高。</p><p><strong>黑客如何才能做自己喜欢的事情？</strong></p><p>我认为这个问题的解决方法是一个几乎所有创作者都知道的方法：找一份养家糊口的“<code>白天工作</code>”（<code>day job</code>）。这个词是从音乐家身上来的，他们晚上表演音乐，所以白天可以找一份其他工作。更一般地说，“白天工作”的意思是，你有一份为了赚钱的工作，还有一份为了爱好的工作。</p><p>开源软件的这种工作模式可能就是正确的模式，因为它已经被其他领域的创作者都验证过了。</p><p><strong>如何学习编程</strong></p><p>“画家学习绘画的方法主要是动手去画，黑客学习编程的方法也理应如此 “</p><p>画家的作品都会保留下来，你观察这些作品，就能看出他们是怎么一步步通过实践学习绘画的。</p><p>也许对于黑客来说，采取像画家这样的做法很有好处：应该定期地从头开始，而不要长年累月地在一个项目上不断工作，并且试图把所有的最新想法都以修订版的形式包括进去。</p><p>黑客的出发点是原创，最终得到一个优美的结果；而科学家的出发点是别人优美的结果，最终得到原创性。</p><p><strong>创作者另一个学习的途径是通过范例。</strong></p><p>对画家来说，博物馆就是美术技巧的图书馆。几百年来，临摹大师的作品一直是传统美术教育的一部分，因为临摹迫使你仔细观察一幅画是如何完成的。</p><p>同样地，黑客可以通过观看优秀的程序学会编程，不是看它们的执行结果，而是看它们的源代码。开源运动最鲜为人知的优点之一，就是使得学习编程变得更容易了</p><p>还有一个可以借鉴绘画的地方：一幅画是逐步完成的。通常一开始是一张草图，然后再逐步填入细节。但是，它又不单纯是一个填入细节的过程。有时，原先的构想看来是错的，你就必须动手修改。无数古代油画放在<code>X光</code>下检视，就能看出修改痕迹，四肢的位置被移动过，或者脸部的表情经过了调整。</p><p>绘画的这个创作过程就值得学习。我认为黑客也应该这样工作。你不能盼望先有一个完美的规格设计，然后再动手编程，这样想是不现实的。如果你预先承认规格设计是不完美的，在编程的时候，就可以根据需要当场修改规格，最终会有一个更好的结果。（大公司的内部结构，使得它们很难这样做。这是又一个创业公司占优之处。）</p><p><strong>黑客是一个创作者</strong></p><p>黑客就像画家，工作起来是有心理周期的。有时候，你有了一个令人兴奋的新项目，你会愿意为它一天工作<code>16</code>个小时。等过了这一阵，你又会觉得百无聊赖，对所有事情都提不起兴趣</p><p>对于编程，这实际上意味着你可以把<code>bug</code>留到以后解决。</p><p><strong>对于产品</strong></p><p>大多数创作者都是为人类用户而创作</p><p>就像绘画作品一样，大多数软件是为人类用户准备的。所以，黑客必须像画家一样，时刻考虑到用户的人性需要，这样才能做出伟大的产品。你必须能够站在用户的角度思考问题，也就是说你必须学会“换位思考”。</p><p>普通黑客与优秀黑客的所有区别之中，会不会“<code>换位思考</code>”可能是最重要的单个因素。有些黑客很聪明，但是完全以自我为中心，根本不会设身处地为用户考虑。这样的人很难设计出优秀软件，因为他们不从用户的角度看待问题</p><p><strong>对于源代码</strong></p><p>程序写出来是给人看的，附带能在机器上运行。</p><p>换位思考”不仅是为了你的用户，也是为了你的读者。这对你是有利的，因为你也会读自己写的东西。许多黑客六个月后再读自己的程序，却发现根本看不懂它是怎么运行的。</p><p><strong>我们的时代</strong></p><p>我们能够有把握说的就是，现在正是编程的黄金年代。大多数领域的伟大作品都诞生于很早以前。<code>1430年</code>到<code>1500年</code>之间的绘画杰作，至今仍然是不可超越的。</p><p>我们看到这种模式一再反复出现。一种新的媒介刚刚诞生的时候，人们热情高涨、兴奋不已，短短几代人就探索清楚了这种媒介的大部分可能性，把它的能量发挥到极致。编程目前好像就处在这个阶段。</p><p>在<code>达·芬奇</code>的年代，绘画并不是一件很酷的事情，<code>达·芬奇</code>用自己的工作推动绘画成为一种伟大的表达方式。同样，编程到底能够有多酷，取决于我们能够用这种新媒介做出怎样的工作。</p><p>个人感悟：黑客是创作者，而不是理论研究者(科学家)，也不是大公司中的技工。如果想要赚大钱，在新兴领域进行创作。一个优秀的产品需要等待时间来考证。</p><h2 id="第三章节：不能说的话"><a href="#第三章节：不能说的话" class="headerlink" title="第三章节：不能说的话"></a>第三章节：不能说的话</h2><p><strong>时尚的本质</strong></p><p>所谓“<code>时尚</code>”，本质上就是自己看不见自己的样子。好比我们在地球上，却感觉不到地球在动。<br>流行一时的不仅有衣服，还有道德观念。如果别人都穿流行的衣服，而你不穿，你就会遭到嘲讽；如果别人都遵守流行的道德观念，而你不遵守，结果则要严重得多。<br>书呆子就是那样惹上麻烦的。他们穿着不流行的衣服，讲着不合适的话。他们觉得自己说出了正确的观点，实际上却惹来了麻烦。习俗的力量不足以束缚他们。</p><p><strong>时代的观念</strong></p><p>历史的常态似乎就是，任何一个年代的人们，都会对一些荒谬的东西深信不疑。他们的信念还很坚定，只要有人稍微表示一点怀疑，就会惹来大麻烦。(这一点看，我们这个时代并没有什么不同)</p><p><strong>你是一个随大流的人吗？</strong></p><p>有时候，别人会对你说：“要根据社会需要，改造自己的思想（<code>well-adjusted</code>）。”这种说法隐含的意思似乎是，如果你不认同社会，那么肯定是你自己的问题。你同意这种说法吗？事实上，它不仅不对，而且会让历史倒退。如果你真的相信了它，凡是不认同社会之处，你连想都不敢想，马上就放弃自己的观点，那才会真正出问题。</p><p><strong>哪些话是不能说的？</strong></p><p>到底什么话是我们不能说的？为了找到答案，首先，我们可以看看，周围的人因为说了什么而陷入麻烦。</p><p><strong>1.真话</strong></p><p>第一个条件是，这些话不能说出口；第二个条件是，它们是正确的，或者看起来很可能正确，值得进一步讨论。如果达不到第二个条件，大部分情况下你都不会有麻烦。你说<code>2＋2</code>等于<code>5</code>，或者匹兹堡的市民身高三米，都不会有事的。这些明显错误的言论也许会被当成笑话，或者更糟一点，被当成你发疯的证据，但是肯定不会惹恼任何人。触怒他人的言论是那些可能会有人相信的言论。我猜想，最令人暴跳如雷的言论，就是被认为说出了真相的言论。</p><p>方法：为了找出那些“<code>不能说的话</code>”，让我们问自己，它们会不会是真的？<code>OK</code>，当你发现某些言论很可疑时，你可以这样想，那些话听上去真是大逆不道（或者其他类似的形容词），但是有没有可能是真的？这就是找出“不能说的话”的第一种方法：判断言论的真伪。</p><p><strong>2.异端邪说</strong></p><p>有些想法，纯粹因为非常特别，而不能说出口。比如，某个话题极其富有争议，不管是对是错，没有人敢在公开场合谈论它。我们怎样才能发现这种情况呢？</p><p>方法：我们把这种不一定正确、但是极富争议的言论称为“异端邪说”。关注“<code>异端邪说</code>”，是找出“<code>不能说的话</code>”的第二种方法。比如，有一个标签叫做“<code>性别歧视</code>”，你问自己哪些想法属于“<code>性别歧视</code>”。然后，把头脑中跳出来的那些想法按照先后顺序列出来，再逐个追问，它们真的属于“<code>性别歧视</code>”吗？</p><p><strong>3.时空差异</strong></p><p>(回顾历史，可以发现古人有许多”<code>可笑</code>“的观念)</p><p>如果我们可以通晓未来，那么找出当代的那些表面上正确、实际上可笑的想法是一件很容易的事。</p><p>我们可以自以为是地相信，当代人比古人更聪明、更高尚。但是，了解的历史越多，就越明白事实并非如此。古人与我们是一样的人，他们既不是更勇敢，也不是更野蛮，而是像我们一样通情达理的普通人。不管他们产生怎样的想法，都是正常人产生的想法。</p><p>方法：回顾过去。我们可以去找那些过去被认为理所当然，如今却被认为不可思议的事情，这是用来找出我们自己正在犯下的错误的第三种方法。</p><p><strong>4.道貌岸然</strong></p><p>大多数成年人故意让孩子对世界有一个错误的认识。最鲜明的例子之一就是圣诞老人。我们觉得，小孩子相信圣诞老人，真是太可爱了。我本人其实也是这样想。但是，扪心自问，我们向孩子灌输圣诞老人的神话，到底是为了孩子，还是为了我们自己？</p><p>方法：寻找那些一本正经的卫道者，看看他们到底在捍卫着什么。</p><p><strong>5.机制</strong></p><p>一般来说，流行的时尚产生于某个有影响力的人物，他突发奇想，接着其他人纷纷模仿。但是，流行的道德观念不是这样，它们往往不是偶然产生的，而是被刻意创造出来的。如果有些观点我们不能说出口，原因很可能是某些团体不允许我们说。</p><p>方法：我还想到了第五种方法，可以找出“不能说的话”，那就是去观察禁忌是如何产生的。某种道德观念到底是怎么出现的，又是怎么被其他人接受的？如果我们能够理解它的产生机制，可能就可以应用于我们自己的时代。</p><p>如果你要寻找“不能说的话”，可以观察流行的产生方式，试着预测它会禁止哪些话。哪一个团体势力强大，却又精神高度紧张？这种团体喜欢压制什么样的思想观点？近来有没有什么社会斗争，失败的一方是哪一方，受到他们牵连的是什么样的思想观点？如果一个先锋人物想要挣脱当前的流行（比如上一代人的观点）脱颖而出，他会支持什么样的思想观点？随大流的人对什么样的思想观点抱有恐惧心？</p><p>为什么要这样做？(这样做有哪些好处)</p><p>首先，我这样做与小孩子翻石头是出于同样的原因：纯粹的好奇心。我对任何被禁止的东西都有特别强烈的好奇心。我要亲眼看一下，然后自己做决定。<br>其次，我这样做是因为我不喜欢犯错。如果像其他时代一样，那些我们自以为正确的事情将来会被证明是荒谬可笑的，我希望自己能够知道是哪些事情，这样可以使我不会上当。<br>再次，我这样做，是因为这是很好的脑力训练。想要做出优秀作品，你需要一个什么问题都能思考的大脑。尤其是那些似乎不应该思考的问题，你的大脑也要养成思考它们的习惯。</p><p><strong>然后应该这么做？</strong></p><p><strong>1.守口如瓶</strong></p><p>自由思考比畅所欲言更重要。如果你感到一定要跟那些人辩个明白，绝不咽下这口气，一定要把话说清楚，结果很可能是从此你再也无法自由理性地思考了。</p><p>“守口如瓶”的真正缺点在于，你从此无法享受讨论带来的好处了。讨论一个观点会产生更多的观点，不讨论就什么观点也没有。所以，如果可能的话，你最好找一些信得过的知己，只与他们畅所欲言、无所不谈。这样不仅可以获得新观点，还可以用来选择朋友。能够一起谈论“异端邪说”并且不会因此气急败坏的人，就是你最应该认识的朋友</p><p><strong>2.笑脸相迎</strong></p><p>⑴假设狂热分子试图引诱你说出来真心话，但是你可以不回答。</p><p>你不妨以不变应万变：“我既不反对也不赞成。”，更好的回答是“我还没想好”。</p><p>⑵假设社会上充斥着反对“黄色分子”的人，他们只要看谁不顺眼，就大肆攻击。你看不下去，准备出手反击。</p><p>一种方法就是逐步把辩论提升到一个抽象的层次。</p><p>另一种反击的方法就是使用隐喻（<code>metaphor</code>）。</p><p>所有反击方法之中，最好的一种可能就是幽默。</p><p><strong>3.永远质疑</strong></p><p>如果自己就是潮水的一部分，怎么能看见潮流的方向呢？你只能永远保持质疑。问自己，什么话是我不能说的？为什么？</p><h2 id="第四章节：良好的坏习惯"><a href="#第四章节：良好的坏习惯" class="headerlink" title="第四章节：良好的坏习惯"></a>第四章节：良好的坏习惯</h2><p>“<code>黑</code>”（<code>hack</code>）这个词也有两个意思，既可以用作赞美，也可以用作羞辱。如果你解决问题的方式非常丑陋笨拙，这叫做你很“<code>黑</code>”。如果你解决问题的方式非常聪明高超，将整个系统操纵在股掌之间，这也叫做你很“<code>黑</code>”。</p><p>“<code>黑</code>”的这两个意思也是相关的。丑陋的做法与聪明的做法存在一个共同点，那就是都不符合常规。从“<code>丑陋</code>”到“<code>聪明</code>”，它们之间存在一种连续性渐变。</p><p>公民自由并不仅仅是社会制度的装饰品，或者一种很古老的传统。公民自由使得国家富强。如果将人均国民生产总值与公民自由的关系画成图，你会发现它们是很清楚的正相关关系。公民自由真的是国家富强的原因，而不是结果吗？我认为是的。在我看来，一个人们拥有言论自由和行动自由的社会，往往最有可能采纳最优方案，而不是采纳最有权势的人提出的方案。专制国家会变成腐败国家，腐败国家会变成贫穷国家，贫穷国家会变成弱小国家。经济学里有一条拉弗曲线（<code>Laffer curve</code>），认为随着税率的上升，税收收入会先增加后减少。我认为政府的力量也是如此，随着对公民自由的限制不断上升，政府的力量会先增加后减小。￼至少现在看来，我们的政府很可能蠢到会真的把这个实验付诸实施，亲自验证一下这个观点。但是，税率提高了还能再降下来，而一旦这个实验铸成大错，就悔之晚矣，因为极权主义制度只要形成了，就很难废除。</p><p>这就是为什么黑客感到担忧。政府侵犯公民自由，表面上看，并不会让程序员的代码质量下降。它只是逐渐地导致一个错误观点占上风的世界。黑客对于公民自由是非常敏感的，因为这对他们至关重要。他们远远地就能感到极权主义的威胁，好比动物能够感知即将来临的暴风雨。</p><p>有一种东西，叫做美国精神（<code>American-ness</code>），生活在国外的人最能体会到这一点。如果你想知道哪些事情可以滋养或者削弱这种精神，不妨去问问黑客，他们是最敏感的焦点人群，因为在他们身上，比我知道的其他人群，更能体现出这种精神。真的，他们可能比那些政府里掌管美国的人更懂得什么叫做美国精神。那些政客开口必谈爱国主义，总是让我想起黎塞留￼（<code>Richelieu</code>）或者马萨林￼（<code>Mazarin</code>），而不是杰弗逊或者华盛顿。</p><p>如果读美国开国元勋的自述，你会发现他们听起来很像黑客。“反抗政府的精神，”杰弗逊写道，“在某些场合是如此珍贵，我希望它永远保持活跃。”</p><h2 id="第五章节：另一条路"><a href="#第五章节：另一条路" class="headerlink" title="第五章节：另一条路"></a>第五章节：另一条路</h2><p><strong>互联网软件</strong></p><p>互联网软件运行在服务器上，用户界面就是网页。对于普通用户来说，使用这种新型软件将更容易、更便宜、更机动、更可靠，通常也比桌面软件更强大。</p><p>使用互联网软件，除了软件本身，大多数用户不需要知道别的事情。所有那些乱七八糟、经常变动的东西，都放在服务器端，由精通此道的专业人员维护。</p><p><strong>互联网软件的优点</strong></p><p><strong>对用户</strong></p><ol><li>方便，使用那些纯粹的互联网软件，你只需要一个能够上网的浏览器即可。有了互联网软件，你的数据和软件本身都不保存在终端设备上，可以从任何电脑上获取你的数据。</li><li>不用安装就能使用。不需要用户手动升级</li><li>互联网应用程序能够同时被多人使用，所以非常适合团队协作性的工作。</li><li>如果使用互联网软件，数据会更安全。数据丢失的责任在于公司。</li><li>互联网软件不太容易感染病毒。</li></ol><p><strong>对开发者</strong></p><ol><li>对于开发者来说，互联网软件与桌面软件最显著的区别就是，前者不是一个单独的代码块。它是许多不同种类程序的集合，而不是一个单独的巨大的二进制文件。</li><li>光有软件还不够，我们还花了许多时间琢磨服务器应该如何配置。</li><li>由于互联网应用程序由多种软件而不是单独一个二进制文件构成，所以可以使用多种编程语言开发。</li><li>软件的发布过程可以分解为一系列的渐进式修改，而不是猛地推出一个大幅变动的版本。</li><li>把发现<code>bug</code>的任务交给用户去完成，使得软件可以快速迭代更新。</li><li>开发互联网软件需要的程序员比较少。开发软件需要的程序员人数减少，不仅意味着省下更多的钱，软件开发的效率将指数式增长。</li><li>能够即时发布软件，对开发者是一个巨大的激励。只要想到好的构思，我们就立刻着手实现。</li><li>互联网软件不仅把开发者与他的代码更紧密地联系在了一起，而且把开发者与他的用户也更紧密联系在了一起。因为你能得到用户数据，所以就不用依赖基准测试了。</li></ol><p><strong>目标客户</strong><br>谁是互联网软件的目标客户？<code>Viaweb</code>一开始就把个人和小企业当作目标客户。我认为这是互联网软件的通行规则。这些客户决策比较灵活，又需要低成本的新技术，所以他们更愿意尝试新事物。</p><p><strong>巨无霸公司VS创业公司在于新领域</strong></p><p><strong>微软</strong></p><p>除了微软自己，没有人能让微软遭受严重挫折。随着互联网软件的崛起，微软不仅要面对新的技术问题，还要面对它自己毫无根据、一厢情愿的旧思维。微软需要把它现有的商业模式拆除，建设一个新模式，我看不到它有正视这个问题的任何迹象。它一心一意地坚持桌面软件模式，固然把它带到现在的地位，但是现在开始将成为它继续前进的障碍。<code>IBM</code>曾经有过同样的处境，它没有正确应对。在很晚的阶段，它才进入微机市场，并且三心二意没有倾注全力，因为大型机是<code>IBM</code>的主要利润来源，发展微机就等于扼杀这头金牛，所以它感到很纠结。微软也同样感到纠结，因为它想保住桌面软件。看来金牛也会成为沉重负担。</p><p><strong>创业公司</strong></p><p>典型的创业公司行动快速，看上去不是那么正式，只有很少几个人，资金也有限。</p><p>开发互联网软件的创业公司会把与创业有关的每一件事做到极致。只用更少的人、更少的钱，就可以把软件写出来，并且开始运作。你必须打破常规、快速行动，循规蹈矩不可能成功。</p><p><strong>鼓励黑客创业</strong></p><p>只有懂得设计的黑客，才能设计软件，不能交给对软件一知半解的设计师。如果你不打算自己动手设计和开发，那就不要创业。</p><p>管理企业其实很简单，只要记住两点就可以了：做出用户喜欢的产品，保证开支小于收入。只要做到这两点，你就会超过大多数创业公司。随着事业的发展，你自己就能琢磨出来其他的诀窍。</p><h2 id="第六章节：如何创造财富"><a href="#第六章节：如何创造财富" class="headerlink" title="第六章节：如何创造财富"></a>第六章节：如何创造财富</h2><p>如果你想致富，最好的办法就是自己创业，或者加入创业公司。</p><p>创业公司往往与技术有关，所以“<code>高技术创业公司</code>”这个短语几乎就是同义重复。</p><p><strong>一个命题</strong></p><p>你不再是低强度地工作四十年，而是以极限强度工作四年。在高技术领域，这种压缩的回报尤其丰厚，工作效率越高，额外报酬就越高。</p><p><strong>运气的成分</strong></p><p>比尔·盖茨很聪明，有决断力，工作也很勤奋，但是单单这样还不足以让你成为他。你还需要非同一般的好运气。</p><p><strong>手艺人</strong></p><p>目前还存在的最大的手工艺人群体就是程序员。程序员坐在电脑前就能创造财富。</p><p><strong>可测量性和可放大性</strong></p><p>要致富，你需要两样东西：可测量性和可放大性。你的职位产生的业绩，应该是可测量的，否则你做得再多，也不会得到更多的报酬。此外，你还必须有可放大性，也就是说你做出的决定能够产生巨大的效应。</p><p>小团体＝可测量性</p><p>高科技＝可放大性</p><p><strong>创业潜规则</strong></p><p>一条就是很多事情由不得你，真正创业以后，你的竞争对手决定了你到底要有多辛苦，而他们做出的决定都是一样的：你能吃多少苦，我们就能吃多少苦。</p><p>创业的付出与回报虽然总体上是成比例的，但是在个体上是不成比例的。</p><p>创业公司如同蚊子，往往只有两种结局，要么赢得一切，要么彻底消失。</p><p>保险的做法就是在早期卖掉自己的创业公司，放弃未来发展壮大（但风险也随之增大）的机会，只求数量较少但是更有把握的回报。</p><p><strong>创业</strong></p><p>创造财富不是致富的唯一方法。但是要鼓励大家去创业。只要懂得藏富于民，国家就会变得强大。</p><h2 id="第七章节：关注贫富分化"><a href="#第七章节：关注贫富分化" class="headerlink" title="第七章节：关注贫富分化"></a>第七章节：关注贫富分化</h2><p><strong>为什么人们会有仇富心理？</strong></p><p>第一，我们从小被误导的对财富的看法；第二，历史上积累财富的方式大多名声不好；第三，担心收入差距拉大将对社会产生不利影响。</p><p>就我所知，第一点是错的，第二点已经过时了，第三点通不过现实的检验。</p><p><strong>财富是被创造出来的</strong></p><p>财富与金钱是两个概念。金钱只是用来交易财富的一种手段，财富才是有价值的东西，我们购买的商品和服务都属于财富。<br>孩子没有能力创造财富，他们享有的一切都来自别人无偿的给予。既然得到财富不要求对应的付出，那么它当然应该平均分配。大多数家庭都是这样，如果兄弟姐妹中有人多得到了一份，其他孩子就会喊：“不公平！”</p><p>进入社会，每个人创造财富的能里不同，一个人代表的价值也就不同了。</p><p><strong>公平与平等</strong></p><p>当我们讨论“收入分配不公平”时，我们还要问问收入从何而来，收入背后的财富到底是谁生产出来的。如果收入完全根据个人创造的财富数量而分配，那么结果可能是不平均的，但是很难说是不公平的。</p><p>由于每个人创造财富的能力和欲望强烈程度都不一样，所以每个人创造财富的数量很不平等。</p><p><strong>为什么在过去富人就是不道德的呢？</strong></p><p>在大部分的人类历史中，积累财富最常见的方法其实是偷窃。游牧社会是偷别人的牲口，农业社会是征税（和平时期）和直接掠夺（战争时期）。并且每个人创造财富的能力都很平均，那些勤勤恳恳的个人基本积累不多财富。</p><p><strong>为什么在现在积累财富又是正当了的呢？</strong></p><p>工业革命时代之后，创造财富真正取代掠夺和贪污成为致富的最佳方式。通过技术的方式，可以更快速的积累财富。</p><p><strong>技术的发展是否加剧了贫富分化？</strong></p><p>首先，技术肯定加剧了有技术者与无技术者之间的生产效率差异，毕竟这就是技术进步的目的。一个勤劳的农民使用拖拉机比使用马可以多耕六倍的田。但是，前提条件是他必须掌握如何使用新技术。</p><p>技术应该会引起收入差距的扩大，但是似乎能缩小其他差距。一百年前，富人过着与普通人截然不同的生活。现在，由于技术的发展，富人的生活与普通人的差距缩小了。</p><p>富人日常做的事情也和普通人差不多。无所事事的闲适生活早就成为罕见情况了。</p><p><strong>社会需要有富人(真正创造价值的那一波人)</strong></p><p>一个社会需要有富人，这主要不是因为你需要富人的支出创造就业机会，而是因为他们在致富过程做出的事情。我在这里谈的不是财富从富人流向穷人的那种扩散效应（<code>trickle-down effect</code>），也不是说如果你让亨利·福特致富，他就会在下一场宴会雇用你当服务员，而是说如果你让他致富，他就会造出一台拖拉机，使你不再需要使用马匹耕田了。</p><h2 id="第八章节：防止垃圾邮件的一种方法"><a href="#第八章节：防止垃圾邮件的一种方法" class="headerlink" title="第八章节：防止垃圾邮件的一种方法"></a>第八章节：防止垃圾邮件的一种方法</h2><p>发送垃圾邮件的人形形色色。有的是公司，经营着一个所谓的邮件列表，表面上说你可以选择订阅，但是实际上根本无法退订，他们肆无忌惮地向你发送广告；有的是个人，专门劫持邮件服务器，推广色情网站。如果我们的过滤器迫使他们只能把垃圾邮件写成上面那样，应该会使得垃圾邮件业中合法经营的那部分人退出这个行业。因为他们很乐于遵守各州的法律规定，在邮件中附上正式声明，解释为什么自己不是垃圾邮件以及如何才能取消订阅。这一类文字反而使得识别他们变得更容易了。<br>（我以前曾经认为，那些相信更严格的法律会遏制垃圾邮件的人真是太天真了。我现在认为，更严格的法律或许无法减少我们收到的垃圾邮件的数量，但是肯定有助于减少逃过过滤器拦截的垃圾邮件的数量。）</p><p>在垃圾邮件业中，如果发送销售类垃圾邮件受到限制，那么整个行业将不可避免地受到重创。“行业”这个词是很准确的，发送垃圾邮件的人其实都是商人，他们这么做只是因为这招很有效。虽然垃圾邮件的回应率低到不能再低了（不超过百万分之<code>15</code>，相比之下，传统的邮寄商品目录的回应率是百万分之<code>3000</code>），但是发送垃圾邮件的成本实际上为零，所以它还是有效的。但是对于收到垃圾邮件的人来说，成本却很高昂，假定有<code>100</code>万人分别收到一封垃圾邮件，每人花一秒钟删除，累计起来就相当于一个人<code>5</code>个星期的工作量，而发送人连一分钱也不用付出。</p><p>不过，虽然接近于零，发送垃圾邮件还是有成本的。￼所以，只要我们把垃圾邮件的回应率降得很低（不管手段是直接过滤，还是让垃圾邮件被迫掩盖它们的销售意图），商家就会发现，发送垃圾邮件是一件经济上不值得的事情。</p><p>另一方面，垃圾邮件使用了那么多推销语言就是为了增加回应率。如果有一天推销语言突然不能用了，对他们就是重大打击。为了说明这一点，让我们把自己想象成一个回应垃圾邮件的人，看看这些人到底是怎么想的（这要比把自己想象成垃圾邮件发送者更让人难受）。回应垃圾邮件的人要么是惊人地轻信，要么是表面上完全否认、但是私底下却有着对性的强烈兴趣。不管哪一种情况，也不管垃圾邮件在正常人看来是多么令人反感或愚蠢万分，总是可以让这些人兴奋不已，因为邮件内容写得实在太诱人了，毕竟如果不是这样，商家也就不发送垃圾邮件了。要是邮件内容改成“请点击下面的链接”，对于收信人来说就没有太大的吸引力了，根本比不上现在的效果。结果就是，如果垃圾邮件不能使用诱人的推销语言，它作为推销工具的价值就会大大降低，使用它的商家数量也会减少。</p><p>最终，我们将取得全胜。我开始写垃圾邮件过滤器只是因为不想再让这些东西烦我了。但是，如果我们把过滤器做得足够好，那么垃圾邮件将不再有效，商家最后将不再发送它。</p><p>所有对抗垃圾邮件的方法之中（从软件方法到法律方法），我认为单独来看，“贝叶斯过滤”是最有效的工具。但是，我也认为，我们使用的不同方法越多，综合效果就越好，因为任何对发送人构成限制的方法往往都会使得过滤器工作起来更顺利。即使同样是基于内容的过滤器，我也认为，如果有多种不同的软件可以同时使用会比较好。过滤器的差异越大，垃圾邮件想要逃过拦截就越不可能。</p><h2 id="第九章节：设计者的品味"><a href="#第九章节：设计者的品味" class="headerlink" title="第九章节：设计者的品味"></a>第九章节：设计者的品味</h2><p><strong>品位不是个人偏好，而是共同的认知</strong></p><p>把品味说成个人的偏好可以有效地杜绝争论，防止人们争执哪一种品味更好。但是问题是，这种说法是不正确的。只要你自己开始动手设计东西，就能明白这一点。</p><p><strong>什么才算是优秀的设计</strong></p><p>优秀设计的原则是许多学科的共同原则，一再反复地出现。</p><ul><li>好设计是简单的设计。当你被迫把东西做得很简单时，你就被迫直接面对真正的问题。当你不能用表面的装饰交差时，你就不得不做好真正的本质部分。</li><li>好设计是永不过时的设计。如果你希望自己的作品对未来的人们有吸引力，方法之一就是让你的作品对上几代人有吸引力。</li><li>好设计是解决主要问题的设计。答案可以不断改进，同样，问题本身也可以不断改进。软件的难题通常可以被改成等价的较易解决的形式。</li><li>好设计是启发性的设计。在软件业中，这条原则意味着，你应该为用户提供一些基本模块，使得他们可以随心所欲自由组合，就像玩乐高积木那样。</li><li>好设计通常是有点趣味性的设计。好的设计并非一定要有趣，但是很难想象完全无趣的设计会是好的设计。</li><li>好设计是艰苦的设计。困难的问题需要艰巨的付出才能解决，高难度的数学证明需要结构非常精细的解决方法（它们往往做起来很有趣），工程学也是如此。</li><li>好设计是看似容易的设计。在大多数领域，看上去容易的事情，背后都需要大量的练习。</li><li>好设计是对称的设计。对称也许只是简洁性的一种表现，但是它十分重要，值得单独列为一点。自然界的对称大量存在，这* 就说明了对称的重要性。</li><li>好设计是模仿大自然的设计。模仿大自然也是工程学的有效方法。</li><li>好设计是一种再设计。很少有人一次就把事情做对。专家的做法是先完成一个早期原型，然后提出修改计划，最后把早期原型扔掉。</li><li>好设计是能够复制的设计。我们对待复制的态度经常是一个否定之否定的过程。刚入门的新手不知不觉地模仿他人，逐渐熟练之后才开始创作原创性作品。最后他会意识到，把事情做对比原创更重要。(先模仿，再创作)</li><li>好设计常常是奇特的设计。唯一达到“奇特”的方法，就是追求做出好作品，完成之后再回过头看。</li><li>好设计是成批出现的。你个人最多可以对趋势产生一定的影响，但是你不可能决定趋势，实际上是趋势决定了你。（或许有人办得到，但是米兰的达·芬奇显然没有办到。）</li><li>好设计常常是大胆的设计。在任何一段历史中，人们都会把某些荒谬的东西当作正确的，并且深信不疑，以至于一旦你出言质疑，就有被排挤或者被暴力伤害的危险。</li></ul><p><strong>严格的品位+熟练的技能=优秀的作品</strong></p><p>我们自己的这个时代要是不同以往，当然令人欢欣鼓舞。但是就我所知，它并没有任何不同。</p><p>单单是无法容忍丑陋的东西还不够，只有对这个领域非常熟悉，你才可能发现哪些地方可以动手改进。你必须锻炼自己。只有在成为某个领域的专家之后，你才会听到心里有一个细微的声音说：“这样解决太糟糕了！一定有更好的选择。”不要忽视这种声音，要培育它们。优秀作品的秘诀就是：非常严格的品味，再加上实现这种品味的能力。</p><h2 id="第十章：编程语言解析"><a href="#第十章：编程语言解析" class="headerlink" title="第十章：编程语言解析"></a>第十章：编程语言解析</h2><p>所有机器都有一张操作命令清单，让你可以控制它。有时这个清单非常简短。电水壶就只允许两种操作：打开和关闭。CD播放器稍微复杂点，除了打开和关闭以外，还能调节音量、播放、暂停、快进、快退、随机播放等。</p><p>计算机和其他机器一样，也有一张操作命令清单。比如，可以命令计算机把两个数相加。这种操作命令的总和就是计算机的机器语言（<code>machine language</code>）。</p><p><strong>机器语言</strong></p><p>机器语言和汇编语言的共同问题就是，只能让大多数计算机做一些很简单的事情。比如，假定你想让计算机的蜂鸣器响10次，但是不存在一条直接的机器语言命令让电脑重复进行n次操作，所以只能用机器语言写出下面这样的程序：</p><pre><code>a  将数字10存入内存地址0如果内存地址0的值为负数，跳到b行蜂鸣器发出声音将内存地址0的值减1跳到a行b  ……程序的其他部分……</code></pre><p>如果只是为了让蜂鸣器响<code>10次</code>就不得不写这么多代码，不难想象写出一个文字处理器或电子表格将是一项多么浩大的工程。</p><p><strong>高级语言</strong></p><p>事实上大多数程序员就是这样工作的，不同之处就是，程序员的助手不是一个人，而是编译器。所谓“编译器”，本身就是一个程序，作用是将简便方式书写的程序（就像上面这一行命令）转变为硬件可以理解的语言。</p><p>这种简便方式书写的程序所使用的语言就叫做高级语言。它让你能够使用更强大的命令开发程序，比如现在你就有了“重复n次操作”的命令，不再仅限于只能做简单的“两个数相加”。</p><p>写程序时有了方便的命令，就可以把程序写得更简短。在上面假想的例子中，高级语言写出来的程序的长度只有机器语言的五分之一。所以，要是你犯错了，现在也更容易发现。</p><p>高级语言还有一个优点，它使得程序更具有可移植性。不同计算机的机器语言都不是完全相同的。所以，你无法将为某一种机型写的机器语言程序放到另一种机型上运行，只有彻底重写才能实现。但是，如果你的程序是用高级语言写的，你只需要重写编译器就可以了。</p><p>编译器不是高级语言唯一的实现方法，另一种方法是使用解释器，它的作用是实时地将代码解释为相应的机器语言，然后一行行运行。相比之下，编译器则是先将整个程序全部翻译成机器语言，然后再运行。</p><p><strong>开放源码</strong></p><p>编译器处理的高级语言代码又叫做源码。它经过翻译以后产生的机器码就叫做目标码。顾客购买市场上的商业软件时得到的往往只是目标码。（目标码很难读懂，所以相当于被加密了，可以保护公司的商业秘密。）但是，后来出现另一种潮流：开放源码的软件。你可以得到源码，并且可以不受限制地修改它。</p><p>这两种方式的真正区别在于，开放源码使你对软件有更大的控制权，如果你想理解开源软件如何运行，只要阅读源码就行了。如果愿意，你甚至可以修改软件、重新编译。</p><p><strong>语言战争</strong></p><p><code>Fortran</code>、<code>Lisp</code>、<code>Cobol</code>、<code>Basic</code>、<code>C</code>、<code>Pascal</code>、<code>Smalltalk</code>、<code>C++</code>、<code>Java</code>、<code>Perl</code>和<code>Python</code>，全都是高级语言。它们只是比较出名的几种而已。现在的高级语言大概有几百种之多。不同机器语言的指令集基本相同，但是高级语言就不一样，它们开发程序的模式差别相当大。</p><p>那么，应该使用哪一种语言？嗯，关于这个问题，现在有很多争论。部分原因是，如果你长期使用某种语言，你就会慢慢按照这种语言的思维模式进行思考。所以，后来当你遇到其他任何一种有重大差异的语言，即使那种语言本身并没有任何不对的地方，你也会觉得它极其难用。缺乏经验的程序员对于各种语言优缺点的判断经常被这种心态误导。</p><p><strong>抽象性</strong></p><p>高级语言比汇编语言更接近人类语言，而某些高级语言又比其他语言更进一步。举例来说，<code>C</code> 语言是一种低层次语言，很接近硬件，几乎堪称可移植的汇编语言，而<code>Lisp</code>语言的层次则是相当高。</p><p>如果高层级语言比汇编语言更有利于编程，你也许会认为语言的层次越高越好。一般情况下确实如此，但不是绝对的。编程语言可以变得很抽象，完全脱离硬件，但也有可能走错了方向。比如，我觉得<code>Prolog</code>语言就有这个问题。它的抽象能力强得不可思议，但是只能用来解决<code>2％</code>的问题，其余时间你苦思冥想、运用这些抽象能力写出来的程序实际上就是<code>Pascal</code>语言的程序。</p><p><strong>静态类型和动态类型</strong></p><p>自由语言派的信徒嘲笑另一方是“<code>B&amp;D</code>”（奴役和戒律，<code>Bondage and Discipline</code>）语言，很无礼地暗示用那些语言编程的人是下等人。我不知道对方如何反击这些喜欢Perl的自由派，也许他们不喜欢给别人起绰号，因此我就无从知道。</p><p>由于防止程序员做蠢事有好几种方法，所以上面的争论逐渐分化成几个较小的议题。目前最活跃的议题之一就是静态类型语言与动态类型语言之争。在静态类型语言中，写代码时必须知道每个变量的类型。而在动态类型语言中，随便什么时候，你都可以把变量设为任意类型的值。</p><p>静态类型语言的拥护者认为这样可以防止<code>bug</code>，并且帮助编译器生成更快的代码（这两点理由都成立）。动态类型语言的拥护者认为静态类型对程序构成了限制（这点理由也成立）。我本人更喜欢动态类型，痛恨那些限制我的自由的语言。但是，确实有一些很聪明的人看来喜欢用静态类型语言。所以，这个问题依然值得讨论，并没有固定答案。</p><p><strong>面向对象编程</strong></p><p>面向对象编程的优点在于，如果你需要修改程序，计算另一种图形的面积，比如三角形，你只需要再另外增加一块相应的代码就可以了，甚至可以不修改程序的其他部分。但是，批评者会反驳说，这种方法的缺点是，由于增加代码不用考虑其他部分，结果往往导致写出性能不佳甚至有副作用的代码，就好比造房子不考虑已经完成的部分一样。</p><p>关于面向对象编程优劣的争论并不像静态类型与动态类型之争那样壁垒分明，因为编程的时候你只能在静态类型和动态类型之中选一种。但是，面向对象编程只是程度不同的问题。事实上有两种程度的面向对象编程：某些语言允许你以这种风格编程，另一些语言则强迫你一定要这样编程。</p><p>我觉得后一类语言不可取。允许你做某事的语言肯定不差于强迫你做某事的语言。所以，至少在这方面我们可以得到明确的结论：你应该使用允许你面向对象编程的语言。至于你最后到底用不用则是另外一个问题了。</p><p><strong>编程语言的文艺复兴</strong></p><p>现在好像每隔一段日子就能听到一种新出现的语言。乔纳森·埃里克森把这种现象称为“编程语言的文艺复兴”。人们有时还会用另一个说法，即“编程语言的战争”。这并不矛盾，文艺复兴时期就是存在很多战争的</p><p>实际上，很多历史学家相信战争是文艺复兴的一个副产品。￼当时，欧洲活力旺盛可能就是因为它分成许多互相竞争的小国。它们互相毗邻，所以新思想能够从一个国家传播到另一个国家，但是它们又互相独立，使得单个的统治者无法遏制创新的发展。相比之下，中国古代的封建皇朝禁止民间建造大型的远洋船只，阻止了经济的正常发展。</p><p>所以，程序员活在这个文艺复兴时代可能是一件好事。如果我们所有人都使用同一种编程语言，反而有可能是坏事。</p><h2 id="第十一章：一百年后的编程语言"><a href="#第十一章：一百年后的编程语言" class="headerlink" title="第十一章：一百年后的编程语言"></a>第十一章：一百年后的编程语言</h2><p><strong>一百年以后人们使用什么语言开发软件？</strong></p><p>我认为，编程语言就像生物物种一样，存在一个进化的脉络，许许多多分支最终都会成为进化的死胡同。这种现象已经发生了。<code>Cobol</code> 语言曾经流行一时，但是现在看来没有任何后续语言继承它的思想。它就像尼安德特人￼一样，进化之路已经走到了尽头。</p><p>我预言<code>Java</code>也会如此。有人写信说：“你怎么能说<code>Java</code>不会成功呢？它已经成功了。”我觉得这要看你的成功标准是什么。如果标准是相关书籍的出版量，或者是相信学会<code>Java</code>就能找到工作的大学生数量，那么<code>Java</code>确实已经成功了。当我说<code>Java</code>不会成功时，我的意思是它和<code>Cobol</code>一样，进化之路已经走到了尽头。</p><p>这只是我的猜测，未必正确。这里的重点不是看衰<code>Java</code>，而是提出编程语言存在一个进化的脉络，从而引导读者思考，在整个进化过程中，某一种语言的位置到底在哪里？之所以要问这个问题，不是为了一百年后让后人感叹我们曾经如此英明，而是为了找到进化的主干。它会启发我们去选择那些靠近主干的语言，这样对当前的编程最有利。</p><p><strong>编程语言的进化</strong></p><p>编程语言的进化与生物学进化还是有区别的，因为不同分支的语言会发生聚合。比如，<code>Fortran</code>分支看来正在与<code>Algol</code>￼的继承者聚合。理论上，不同的生物物种也可能发生聚合，但是可能性很低，所以大概从来没有真正出现过。</p><p>编程语言之所以可能出现聚合，一个原因是它的概率空间￼比较小，另一个原因是它的突变不是随机的。语言的设计者们总是有意识地借鉴其他语言的设计思想。</p><p>对于语言设计者来说，认清编程语言的进化路径特别有用，因为这样就可以照着样子设计语言了。这时，认清进化的主干就不仅有助于识别现存的优秀语言，还可以把它当作设计语言的指南</p><p><strong>什么语言在进化主干上</strong></p><p>任何一种编程语言都可以分成两大组成部分：基本运算符的集合（扮演公理的角色）以及除运算符以外的其他部分（原则上，这个部分可以用基本运算符表达出来）。</p><p>我认为，基本运算符是一种语言能否长期存在的最重要因素。其他因素都不是决定性的。这有点像买房子的时候你应该先考虑地理位置。别的地方将来出问题都有办法弥补，但是地理位置是没法变的。</p><p>慎重选择公理还不够，还必须控制它的规模。数学家总是觉得公理越少越好，我觉得他们说到了点子上。</p><p>你仔细审视一种语言的内核，考虑哪些部分可以被摒弃，这至少也是一种很有用的训练。在长期的职业生涯中，我发现冗余的代码会导致更多冗余的代码，不仅软件如此，而且像我这样性格懒散的人，我发现在床底下和房间的角落里这个命题也成立，一件垃圾会产生更多的垃圾。</p><p><strong>我的判断是，那些内核最小、最干净的编程语言才会存在于进化的主干上。一种语言的内核设计得越小、越干净，它的生命力就越顽强。</strong></p><p><strong>硬件的增加的算力会被浪费掉</strong></p><p>浪费可以分成好的浪费和坏的浪费。我感兴趣的是好的浪费，即用更多的钱得到更简单的设计。所以，问题就变成了如何才能充分利用新硬件更强大的性能最有利地“浪费”它们？</p><p>对速度的追求是人类内心深处根深蒂固的欲望。当你看着计算机这个小玩意，就会不由自主地希望程序运行得越快越好，真的要下一番功夫才能把这种欲望克制住。设计编程语言的时候，我们应该有意识地问自己，什么时候可以放弃一些性能，换来一点点便利性的提高。</p><p>很多数据结构存在的原因都与计算机的速度有关。比如，今天的许多语言都同时有字符串和列表。从语义上看，字符串或多或少可以理解成列表的一个子集，其中的每一个元素都是字符。那么，为什么还需要把字符串单列为一种数据类型呢？完全可以不这么做。只是为了提高效率，所以字符串才会存在。但是，这种以加快运行速度为目的、却使得编程语言的语义大大复杂的行为，很不可取。编程语言设置字符串似乎就是一个过早优化的例子。</p><p><strong>我们需要什么样的编程语言</strong></p><p>一百年后的程序员最需要的编程语言就是可以让你毫不费力地写出程序第一版的编程语言，哪怕它的效率低下得惊人（至少按我们今天的眼光来看是如此）。他们会说，他们想要的就是很容易上手的编程语言。</p><p>效率低下的软件并不等于很烂的软件。一种让程序员做无用功的语言才真正称得上很烂。<strong>浪费程序员的时间而不是浪费机器的时间才是真正的无效率</strong>。随着计算机速度越来越快，这会变得越来越明显</p><p><strong>一百年以后还有面向对象语言吗</strong></p><p>顺便说一句，我不认为面向对象编程将来会消亡。我觉得，除了某些特定的领域，这种编程方法其实没有为优秀程序员带来很多好处，但是它对大公司有不可抗拒的吸引力。<strong>面向对象编程使得你有办法对面条式代码进行可持续性开发</strong>。通过不断地打补丁，它让你将软件一步步做大。大公司总是倾向于采用这样的方式开发软件。我预计一百年后也是如此</p><p>如果我们现在就能拥有一百年后的编程语言，那就至少能用来写出优秀的伪码￼。我们会用它开发软件吗？因为一百年后的编程语言需要为某些应用程序生成快速代码，所以很可能它生成的代码能够在我们的硬件上运行，速度也还可以接受。相比一百年后的用户，我们也许不得不对这种语言做更多的优化，但是总的来看，它应该仍然会为我们带来净收益。</p><p>现在，我们的两个观点就是：（1）一百年后的编程语言在理论上今天就能设计出来；（2）如果今天真能设计出这样一种语言，很可能现在就适合编程，并且能够产生更好的结果。如果我们把这两个观点联系起来，那就得出了一些有趣的可能性。为什么不现在就动手尝试写出一百年后的编程语言呢？</p><p>当你设计语言的时候，心里牢牢记住这个目标是有好处的。学习开车的时候，一个需要记住的原则就是要把车开直，不是通过将车身对齐画在地上的分隔线，而是通过瞄准远处的某个点。即使你的目标只在几米开外，这样做也是正确的。我认为，设计编程语言时，我们也应该这样做。</p><h2 id="第十二章：拒绝平庸"><a href="#第十二章：拒绝平庸" class="headerlink" title="第十二章：拒绝平庸"></a>第十二章：拒绝平庸</h2><blockquote><p>主要说创业公司应该选择一个能节省开发效率的语言，作者主要推崇了<code>Lisp</code>，<code>Lisp</code>问世不久，用的人还不多，作者决定使用<code>Lisp</code>原因，是因为它极高的开发效率，可以在很短时间内开发出与竞品一样的功能，是他的秘密武器。</p></blockquote><p>技术的变化速度通常是很快的。但是，编程语言不一样，与其说它是技术，还不如说是程序员的思考模式。编程语言是技术和宗教的混合物。￼所以，一种很普通的编程语言就是很普通的程序员使用的语言，它的变化就像冰山那样缓慢。大概在<code>1960</code>年，<code>Lisp</code>语言引入了垃圾回收机制（<code>Garbage Collection</code>），今天已经被广泛认为是非常好的做法。<code>Lisp</code>的动态类型特点也同样受到越来越多人的认同。闭包是<code>20世纪60</code>年代<code>Lisp</code>语言引入的功能，现在的接受程度还很低。宏也是60年代中期<code>Lisp</code>语言引入的，现在还是一片处女地。</p><p>很显然，那些很普通的编程语言正在主导一切。我不建议你挑战这种强大的习惯势力，相反，我建议你向日本合气道选手学习，利用这种势力削弱你的竞争对手，让他们自食其果。</p><p>如果你为大公司工作，想要改用<code>Lisp</code>语言可能不是一件容易的事。你很难说服自以为是的老板，让他允许你用<code>Lisp</code>语言开发程序。老板受到报纸的影响，认为某些其他语言将主宰世界（就像20年前<code>Ada</code>语言受到的评价）。但是，如果你为创业公司工作，那里没有这样的老板，那么你就能和我们一样，将他人的<code>Blub</code>困境转变为你的优势。你的竞争对手被牢牢粘在那些很普通的语言上面，永远都追不上你使用的技术。</p><p>如果你为创业公司工作，那么这里有一个评估竞争对手的妙招——关注他们的招聘职位。他们网站上的其他内容无非是一些陈腐的照片和夸夸其谈的文字，但是招聘职位却不得不写得很明确，反映出他们到底想干什么，否则就会引来一大批不合适的求职者。</p><h2 id="第十二章：书呆子的复仇"><a href="#第十二章：书呆子的复仇" class="headerlink" title="第十二章：书呆子的复仇"></a>第十二章：书呆子的复仇</h2><p>当你按照<code>Java</code>、<code>Perl</code>、<code>Python</code>、<code>Ruby</code>这样的顺序观察这些语言，你会发现一个有趣的结果。至少，如果你是一个<code>Lisp</code>黑客，你就看得出来，排在越后面的语言越像<code>Lisp</code>。<code>Python</code>语言模仿<code>Lisp</code>，甚至把许多<code>Lisp</code>黑客认为属于设计错误的功能也一起模仿了。至于<code>Ruby</code>语言，如果回到<code>1975年</code>，你声称它是一种有着自己句法的<code>Lisp</code>方言，没有人会提出反对意见。编程语言现在的发展不过刚刚赶上<code>1958</code>年<code>Lisp</code>语言的水平</p><p><strong>朝着数学方向发展</strong></p><p>由此也就得出了<code>20</code>世纪<code>50</code>年代的编程语言到现在还没有过时的原因。简单说，因为这种语言本质上不是一种技术，而是数学。数学是不会过时的。你不应该把<code>Lisp</code>语言与<code>50</code>年代的硬件联系在一起，而是应该把它与快速排序（<code>Quicksort</code>）算法进行类比。这种算法是<code>1960</code>年提出的，至今仍然是最快的通用排序方法。</p><p>Fortran语言也是<code>20</code>世纪<code>50</code>年代出现的，并且一直使用至今。它代表了语言设计的一种完全不同的方向。<code>Lisp</code>语言是无意中从纯理论发展为编程语言的，而<code>Fortran</code>从一开始就是作为编程语言设计出来的。但是，今天我们把<code>Lisp</code>看成高级语言，而把<code>Fortran</code>看成一种相当低层次的语言。</p><p><code>1956</code>年<code>Fortran</code>刚诞生的时候，叫做<code>Fortran I</code>，与今天的<code>Fortran</code>语言差别极大。<code>Fortran I</code> 实际上是汇编语言加上数学，在某些方面还不如今天的汇编语言强大。比如，它没有子例程，只有分支跳转结构（<code>branch</code>）。今天的<code>Fortran</code>语言可以说更接近<code>Lisp</code>而不是<code>Fortran I</code>。</p><p><code>Lisp</code>和<code>Fortran</code>代表了编程语言发展的两大方向。前者的基础是数学，后者的基础是硬件架构。从那时起，这两大方向一直在互相靠拢。<code>Lisp</code>语言刚设计出来的时候就很强大，接下来的二十年它提高了运行速度。而那些所谓的主流语言把更快的运行速度作为设计的出发点，然后再用四十多年的时间一步步变得更强大。直到今天，最高级的主流语言也只是刚刚接近<code>Lisp</code>的水平。虽然已经很接近了，但还是没有<code>Lisp</code>那样强大。</p><p><strong>向心力</strong></p><p>使用一种不常见的语言会出现的问题我想到了三个：你的程序可能无法很好地与使用其他语言写的程序协同工作；你可能找不到很多函数库；你可能不容易雇到程序员。</p><p>它们有多严重？第一个问题取决于你是否控制整个系统。如果你的软件运行在客户的机器上，而客户又使用一个到处都是bug的专有操作系统（我可没提操作系统的名字），那么使用那个操作系统的开发语言可能会给你带来优势。但是，如果你控制整个系统，并且还有各个组成部分的源码（正如我推测ITA就是这种情况），那么你就能使用任何你想用的语言。如果出现不兼容的情况，你自己就能动手解决。</p><p>把软件运行在服务器端就可以没有顾忌地使用最先进的技术。乔纳森·埃里克森说现在是“编程语言的文艺复兴时期”，我想最大的原因就是有了服务器端软件。这也能解释为什么像<code>Perl</code>和<code>Python</code>这样的新语言会流行起来，它们之所以流行不是因为人们使用它们开发<code>Windows</code>应用程序，而是因为人们在服务器上使用它们。随着软件从桌面端向服务器端转移（连微软公司都看出这是未来的趋势），逼迫你使用某一种语言的限制将越来越少。</p><p>至于第二个问题，函数库的重要性也取决于你的应用程序。对于那些条件不苛刻的应用，有没有一个好的函数库比语言本身的能力更重要。那么到底应该怎么选择语言？是根据函数库，还是根据语言本身的能力？很难确切地找出一条清楚的规则，但是无论哪种情况，你都必须考虑到你开发的应用程序的特点。如果你是一家软件公司，你开发的程序打算拿到市场上销售，那么这个程序可能会耗费好几个优秀程序员至少<code>6</code>个月的时间。为一个这样规模的项目选择编程语言，语言本身要有强大的编程能力可能就是最重要的考虑因素，比是否有方便的函数库更重要</p><p>第三个问题是你的经理担忧雇不到程序员，我认为这根本就是混淆视听。说实话，你究竟想雇用多少个黑客？到目前为止，大家公认少于<code>10</code>个人的团队最适合开发软件。雇用这样规模的开发团队，只要使用的不是无人知道的语言，应该都不会遇到很大麻烦。如果你无法找到<code>10</code>个<code>Lisp</code>黑客，那么你可能选错了创立软件公司的城市。</p><p>事实上，选择更强大的编程语言会减少所需要的开发人员数量。因为：（a）如果你使用的语言很强大，可能会减少一些编程的工作量，也就不需要那么多黑客了；（b）使用更高级语言的黑客可能比别的程序员更聪明。</p><p><strong>随大流的代价</strong></p><p>使用一种不强大的语言，你的损失有多大？实际上有一些现成的数据可以说明这个问题。</p><p>衡量语言的编程能力的最简单方法可能就是看代码数量。所谓高级语言，就是能够提供更强大抽象能力的语言，从某种意义上，就像能够提供更大的砖头，所以砌墙的时候用到的砖头数量就变少了。因此，语言的编程能力越强大，写出来的程序就越短（当然不是指字符数量，而是指独立的语法单位）。</p><p>强大的编程语言如何让你写出更短的程序？一个技巧就是（在语言允许的前提下）使用“自下而上”（<code>bottom-up</code>）的编程方法。你不是用基础语言（<code>base language</code>）开发应用程序，而是在基础语言之上先构建一种你自己的语言，然后再用后者开发应用程序。这样写出来的代码会比直接用基础语言开发出来的短得多。实际上，大多数压缩算法也是这样运作的。“自下而上”的编程往往也便于修改，因为许多时候你自己添加的中间层根本不需要变化，你只需要修改前端逻辑就可以了。</p><p><strong>一个诀窍</strong></p><p>在大型组织内部，有一个专门的术语描述这种跟随大多数人的选择的做法，叫做“<code>业界最佳实践</code>”。这个词出现的原因其实就是为了让你的经理可以推卸责任。既然我选择的是“<code>业界最佳实践</code>”，如果不成功，项目失败了，那么你也无法指责我，因为做出选择的人不是我，而是整个“<code>业界</code>”。</p><p>我认为这个词原来是指某种会计方法，大致意思就是不要采用很奇怪的处理方法。在会计方法中，这可能是一个很好的主意。“<code>尖端</code>”和“<code>核算</code>”这两个词听上去就不适合放在一起。但是如果你把这个标准引入技术决策，你就开始要出错了。<br>技术本来就应该是尖端的。正如伊拉恩·加内特所说，编程语言的所谓“<code>业界最佳实践</code>”，实际上不会让你变成最佳，只会让你变得很平常。如果你选择的编程语言使得你开发软件的速度只有（选择更激进技术的）对手的几分之一，那么“<code>最佳实践</code>”真的起错了名字。</p><p>所以，我们就有了两点结论，我认为它们非常有价值。事实上，这是我用自己的经历换来的。第一，不同语言的编程能力不一样。第二，大多数经理故意忽视第一点。你把这两点事实结合起来，其实就得到了赚钱的诀窍。<code>ITA</code> 软件公司是运用这个诀窍的典型例子。如果你想在软件业获得成功，就使用你知道的最强大的语言，用它解决你知道的最难的问题，并且等待竞争对手的经理做出自甘平庸的选择。</p><h2 id="第十四章：梦寐以求的编程语言"><a href="#第十四章：梦寐以求的编程语言" class="headerlink" title="第十四章：梦寐以求的编程语言"></a>第十四章：梦寐以求的编程语言</h2><p>我的朋友曾对一位著名的操作系统专家说他想要设计一种真正优秀的编程语言。那位专家回答，这是浪费时间，优秀的语言不一定会被市场接受，很可能无人使用，因为语言的流行不取决于它本身。至少，那位专家设计的语言就遭遇到了这种情况。<br>那么，语言的流行到底取决于什么因素呢？流行的语言是否真的值得流行呢？还有必要尝试设计一种更好的语言吗？如果有必要的话，怎样才能做到这一点呢？</p><p><strong>流行的秘诀</strong></p><p>专家级黑客的看法不是决定一种语言流行程度的唯一因素，某些古老的软件（<code>Fortran</code>和<code>Cobol</code>的情况）和铺天盖地的广告宣传（<code>Ada</code>和<code>Java</code>的情况）也会起到作用。但是，我认为从长期来看，专家级黑客的看法是最重要的因素。只要有了达到“临界数量”（<code>critical mass</code>）的最初用户和足够长的时间，一种语言可能就会达到应有的流行程度。而流行本身又会使得这种优秀的语言更加优秀，进一步拉大它与平庸语言之间的好坏差异，因为使用者的反馈总是会导致语言的改进。你可以想一下，所有流行的编程语言从诞生至今的变化有多大。<code>Perl</code>和<code>Fortran</code>是极端的例子，除它们两个之外，甚至就连<code>Lisp</code>都发生了很大的变化。</p><p>所以，即使不考虑语言本身的优秀是否能带动流行，我想单单流行本身就肯定会使得这种语言变得更好，只有流行才会让它保持优秀。编程语言的最高境界一直在发展之中。虽然语言的核心功能就像大海的深处，很少有变化，但是函数库和开发环境之类的东西就像大海的表面，一直在汹涌澎湃。</p><p>当然，黑客必须先知道这种语言，才可能去用它。他们怎么才能知道呢？就是从其他黑客那里。所以不管怎样，一开始必须有一群黑客使用这种语言，然后其他人才会知道它。我不知道“一群”的最小数量是多少，多少个黑客才算达到“临界数量”呢？如果让我猜，我会说<code>20</code>人。如果一种语言有<code>20</code>个独立用户，就意味这<code>20</code>个人是自主决定使用这种语言的，我觉得这就说明这种语言真的有优点。</p><p><strong>影响流行的外部因素</strong></p><p>我们得先承认，确实有一个外部因素会影响到语言的流行。一种语言必须是某一个流行的计算机系统的脚本语言（<code>scripting language</code>），才会变得流行。<code>Fortran</code>和<code>Cobol</code>是早期IBM大型机的脚本语言。<code>C</code>是<code>Unix</code>的脚本语言，后来的<code>Perl</code>和<code>Python</code>也是如此。<code>Tcl</code>是<code>Tk</code>的脚本语言，<code>Visual Basic</code>是<code>Windows</code>的脚本语言，（某种形式的）<code>Lisp</code>是<code>Emacs</code>的脚本语言，<code>PHP</code>是网络服务器的脚本语言，<code>Java</code>和<code>JavaScript</code>是浏览器的脚本语言。</p><p>编程语言不是存在于真空之中。“<code>编程</code>”其实是及物动词，黑客一般都是为某个系统编程，在现实中，编程语言总是与它们依附的系统联系在一起的。所以，如果你想设计一种流行的编程语言，就不能只是单纯地设计语言本身，还必须为它找到一个依附的系统，而这个系统也必须流行。除非你只想用自己设计的语言取代那个系统现有的脚本语言。</p><p>这种情况导致的一个结果就是，无法以一种语言本身的优缺点评判这种语言。另一个结果则是，只有当一种语言是某个系统的脚本语言时，它才能真正成为编程语言。如果你对此很吃惊，觉得不公平，那么我会跟你说不必大惊小怪。这就好比大家都认为，如果一种编程语言只有语法规则，没有一个好的实现（<code>implementation</code>），那么它就不能算完整的编程语言。这些都是很正常很合理的事情，编程语言本来就该如此。</p><p><strong>简洁</strong></p><p>假定你的语言已经能够满足上面三项条件——一种免费的实现，一本相关书籍，以及语言所依附的计算机系统——那么还需要做什么才能使得黑客喜欢上你的语言？</p><p>黑客欣赏的一个特点就是简洁。黑客都是懒人，他们同数学家和现代主义建筑师一样，痛恨任何冗余的东西或事情。有一个笑话说，黑客动手写程序之前，至少会在心里盘算一下哪种语言的打字工作量最小，然后就选择使用该语言。这个笑话其实与真实情况相差无几。就算这真的是个笑话，语言的设计者也必须把它当真，按照它的要求设计语言。</p><p><strong>简洁性最重要的方面就是要使得语言更抽象</strong>。为了达到这一点，首先你设计的必须是高级语言，然后把它设计得越抽象越好。语言设计者应该总是看着代码，问自己能不能使用更少的语法单位把它表达出来。如果你有办法让许多不同的程序都能更简短地表达出来，那么这很可能意味着你发现了一种很有用的新抽象方法。</p><p><strong>可编程性（Hackability）</strong></p><p>优秀程序员经常想做一些既危险又令人恼火的事情。所谓“<code>令人恼火</code>”，我指的是他们会突破设计者提供给用户的外部语义层，试着控制某些高级抽象的语言内部接口。比如，黑客喜欢破解，而破解就意味着深入内部，揣测原始设计者的意图。</p><p>你应该敞开胸怀，欢迎这种揣测。对于制造工具的人来说，总是会有用户以违背你本意的方式使用你的工具。如果你制造的是编程语言这样高度组合的系统，那就更是如此了。许多黑客会用你做梦也想不到的方式改动你的语法模型。我的建议就是，让他们这样干吧，而且应该为他们创造便利，尽可能多地把语言的内部暴露在他们面前。</p><p>其实，黑客并不会彻底颠覆你的工具，在一个大型程序中，他可能只是对语言改造一两个地方。但是，改动多少地方并不重要，重要的是他能够对语言进行改动。这可能不仅有助于解决一些特殊的问题，还会让黑客觉得很好玩。黑客改造语言的乐趣就好比外科医生摆弄病人内脏的乐趣，或者青少年喜欢用手挤破青春痘的那种感觉。￼至少对男生来说，某些类型的破坏非常刺激。针对青年男性读者的Maxim杂志每年出版一本特辑，里面一半是美女照片，另一半是各种严重事故的现场照片。这本杂志非常清楚它的读者想看什么。</p><p>一种真正优秀的编程语言应该既整洁又混乱。“整洁”的意思是设计得很清楚，内核由数量不多的运算符构成，这些运算符易于理解，每一个都有很完整的独立用途。“混乱”的意思是它允许黑客以自己的方式使用。<code>C语言</code>就是这样的例子，早期的<code>Lisp</code>语言也是如此。真正的黑客语言总是稍微带一点放纵不羁、不服管教的个性。</p><p>优秀的编程语言所具备的功能，应该会使得言必称“软件工程”的人感到非常不满、频频摇头。与黑客语言形成鲜明对照的就是像<code>Pascal</code>那样的语言，它是井然有序的模范，非常适合教学，但是除此之外就没有很大用处了。</p><p><strong>一次性程序</strong></p><p>为了吸引黑客，一种编程语言必须善于完成黑客想要完成的各种任务。这意味着它必须很适合开发一次性程序。这一点可能出乎很多人的意料。</p><p>所谓一次性程序，就是指为了完成某些很简单的临时性任务而在很短时间内写出来的程序。比如，自动完成某些系统管理任务的程序，或者（为了某项模拟任务）自动生成测试数据的程序，以及在不同格式之间转化数据的程序等。令人吃惊的是，一次性程序往往不是真的只用一次，就像二战期间很多美国大学造的一大批临时建筑后来都成了永久建筑。许多一次性程序后来也都变成了正式的程序，具备了正式的功能和外部用户。</p><p>开发大型程序的另一个方法就是从一次性程序开始，然后不断地改进。这种方法比较不会让人望而生畏，程序在不断的开发之中逐渐进步。一般来说，使用这种方法开发程序，一开始用什么编程语言，就会一直用到最后，因为除非有外部政治因素的干预，程序员很少会中途更换编程语言。所以，我们就有了一个看似矛盾的结论：如果你想设计一种适合开发大型项目的编程语言，就必须使得这种语言也适合开发一次性程序，因为大型项目就是从一次性程序演变而来的。</p><p><strong>函数库</strong></p><p>简洁性的最高形式当然是有人已经帮你把程序写好，你只要运行就可以了。函数库就是别人帮你写好的程序，所以它是编程语言的另一个重要特点，并且我认为正在变得越来越重要。<code>Perl</code> 就赢在它具有操作字符串的巨大函数库。这类函数库对一次性程序特别重要，因为开发一次性程序的原始目的往往就是转化或提取字符串。许多Perl程序的原型可能就是把几个函数库调用放在一起。</p><p><strong>效率</strong></p><p>众所周知，好的编程语言生成的代码有较快的运行速度。但是实际上，我觉得代码的运行速度不是编程语言的设计者能够控制的。高德纳很久以前就指出，运行速度只取决于一些关键的瓶颈。而在编程实践中，许多程序员都已经注意到自己很容易搞错瓶颈到底在哪里。</p><p>所以，<strong>编程时提高代码运行速度的关键是使用好的性能分析器（profiler）</strong>，而不是使用其他方法，比如精心选择一种静态类型的编程语言。为了提高运行速度，并没有必要每个函数的每个参数类型都声明清楚，你只需要在瓶颈处声明清楚参数类型就可以了。所以，更重要的是你需要能够找出瓶颈到底在什么地方。</p><p>人们在使用非常高级的语言（比如<code>Lisp</code>）时，经常抱怨很难知道哪个部分对性能的影响比较大。可能确实如此，如果你使用一种非常抽象的语言，这也许是无法避免的。不管怎样，我认为一个好的性能分析器会解决这个问题，虽然这方面还有很长的路要走，但是未来你可以快速知道程序每个部分的时间开销。</p><p>这个问题一部分源于沟通不畅。<strong>语言设计者喜欢提高编译器的速度，认为这是对自己技术水平的考验，而最多只把性能分析器当作一个附送给使用者的赠品</strong>。但是在现实中，一个好的性能分析器对程序的帮助可能大于编译器的作用。这里又一次反映出语言设计者与用户之间发生了脱节，前者竭尽全力想要解决的问题其实方向不甚正确。</p><p>让性能分析器自动运行可能是一个好主意。它自动告诉程序员每个部分的性能，而不是非要等到程序员手动运行后才能知道。比如，当程序员编辑源码的时候，代码编辑器能够实时用红色显示瓶颈的部分。另一个方法应该是设法显示正在运行的程序的情况，这对互联网软件尤其重要，因为服务器上有很多程序同时运行，它们都需要你密切关注。自动运行的性能分析器用图形实时显示程序运行时的内存状况，甚至可以发出声音，表示出现了问题。</p><p>现在有一些语言先编译成字节码（<code>byte code</code>），然后再由解释器执行。这样做主要是为了让代码容易移植到不同的操作系统，但是这也可以变成一项很有用的功能。让字节码成为语言的正式组成部分，允许程序员在瓶颈处内嵌字节码，这可能是一个不错的主意。然后，针对这部分字节码的优化也就变得可以移植了。</p><p>正如许多最终用户已经意识到的，运行速度的概念正在发生变化。随着互联网软件的兴起，越来越多的程序主要不是受限于计算机的运算速度，而是受限于<code>I/O</code>的速度。加快<code>I/O</code>速度将是很值得做的一件事。在这方面，编程语言也能起到作用，有些措施是显而易见的，比如采用简洁、快速、格式化输出的函数，还有些措施则需要深层次的结构变化，比如采用缓存和持久化对象（<code>persistent object</code>）。</p><p>用户关心的是反应时间（<code>response time</code>），但是软件的另一种效率正在变得越来越重要，那就是每个处理器能够同时支持的用户数量。未来许多有趣的应用程序都将是运行在服务器端的互联网软件，所以每台服务器能够支持的用户数量就成了软件业者的关键问题。互联网软件的成本支出就取决于这个指标。</p><p>许多年以来，大多数面向最终用户的程序都不太关心效率。软件开发者总是假设用户桌面电脑的运算能力会不断增长，所以不用刻意提高软件的效率。帕金森定律￼被证明与摩尔定律一样颠扑不破。<strong>软件不断膨胀，消耗光所有可以得到的资源。这一切将随着互联网软件的出现发生改变，因为硬件和软件现在捆绑在一起供应。对于那些提供互联网软件的公司来说，将每台服务器支持的用户数量最大化会对降低成本产生巨大影响。</strong></p><p>在一些应用程序中，处理器的运算能力是瓶颈，那么最重要的优化对象就是软件的运行速度。但是，一般情况下内存才是瓶颈，你能够同时支持的用户数量取决于用户数据所消耗的内存。编程语言在这方面也能发挥作用，对线程的良好支持将使得所有用户共享同一个内存堆（<code>heap</code>）。持久化对象和语言内核级别的延迟加载（<code>lazy loading</code>）支持也有助于减少内存需求。</p><p><strong>时间</strong></p><p>一种编程语言要想变得流行，最后一关就是要经受住时间的考验。没人想用一种会被淘汰的语言编程，这方面已经有很多前车之鉴了。所以，大多数黑客往往会等上几年，看看某一种新语言的势头，然后才真正考虑使用它。</p><p>新事物的发明者通常对这个发现很震惊，他们没想到人们居然这样对待发明创造。但是，让别人相信一种新事物是需要时间的。我有一个朋友，他的客户第一次提出某种需求时，他很少理会。因为他知道人们有时候会想要自己并不真正需要的东西。为了避免浪费时间，只有当客户第三次或第四次提出同样的需求时，他才认真对待。这个时候客户可能已经很不高兴了，但是这至少保证他们提出的需求应该就是他们真正需要的东西。</p><p>大多数人接触新事物时都学会了使用类似的过滤机制。甚至有时要听到别人提起十遍以上他们才会留意。这样做完全是合理的，因为大多数的热门新商品事后被证明都是浪费时间的噱头，没多久就消失得无影无踪。虚拟现实建模语言<code>VRML</code>刚诞生时曾经轰动一时，但是我决定等到一两年后再去学习它，结果一两年后已经没有学习的必要了，因为市场已经把它遗忘了。</p><p>所以，发明新事物的人必须有耐心，要常年累月不断地做市场推广，直到人们开始接受这种发明。我们就耗费了好几年才使得客户明白<code>Viaweb</code>不需要下载安装就能使用。不过，好消息是，简单重复同一个信息就能解决这个问题。你只需要不停地重复同一句话，最终人们将会开始倾听。人们真正注意到你的时候，不是第一眼看到你站在那里，而是发现过了这么久你居然还在那里。</p><p><strong>再设计</strong></p><p>著名散文家E.B．怀特说过，“<code>最好的文字来自不停的修改</code>”。所有优秀作家都知道这一点，它对软件开发也适用。设计一样东西，最重要的一点就是要经常“再设计”，编程尤其如此，再多的修改都不过分。</p><p>为了写出优秀软件，你必须同时具备两种互相冲突的信念。一方面，你要像初生牛犊一样，对自己的能力信心万丈；另一方面，你又要像历经沧桑的老人一样，对自己的能力抱着怀疑态度。在你的大脑中，有一个声音说“<code>千难万险只等闲</code>”，还有一个声音却说“<code>早岁哪知世事艰</code>”。</p><h2 id="第十五章：设计与研究"><a href="#第十五章：设计与研究" class="headerlink" title="第十五章：设计与研究"></a>第十五章：设计与研究</h2><p>设计与研究的区别看来就在于，前者追求“好”（<code>good</code>），后者追求“新”（<code>new</code>）。优秀的设计不一定很“<code>新</code>”，但必须是“<code>好</code>”的；优秀的研究不一定很“<code>好</code>”，但必须是“<code>新</code>”的。我认为这两条道路最后会发生交叉：只有应用“<code>新</code>”的创意和理论，才会诞生超越前人的最佳设计；只有解决那些值得解决的难题（也就是“<code>好</code>”的难题），才会诞生最佳研究。所以，最终来说，设计和研究都通向同一个地方，只是前进的路线不同罢了</p><p>在软件领域，贴近用户的设计思想被归纳为“弱即是强”（<code>Worse is Better</code>）模式￼。这个模式实际上包含了好几种不同的思想，所以至今人们还在争论它是否真的成立。但是，其中有一点是正确的，那就是如果你正在设计某种新东西，就应该尽快拿出原型，听取用户的意见。</p><p>与之对照，还有另一种软件设计思想，也许可以被称为“<code>万福玛丽亚</code>”模式。它不要求尽快拿出原型，然后再逐步优化，它的观点是你应该等到完整的成品出来以后再一下子隆重地推向市场，就像圣母玛丽亚降临一样，哪怕整个过程漫长得像橄榄球运动员长途奔袭、达阵得分也没有关系。在互联网泡沫时期，无数创业公司因为相信了这种模式而自毁前程。我还没听说过有人采用这种模式而获得成功。</p><p>软件领域以外的人可能没听过“<code>弱即是强</code>”，所以意识不到这种模式在艺术领域普遍存在。以绘画为例，文艺复兴时期就有人发现了这一点。如今，几乎所有的美术老师都会告诉你准确画出一个事物的方法，不是沿着轮廓慢慢一个部分、一个部分地把它画出来，因为这样的话各个部分的错误会累积起来，最终导致整幅画失真。你真正应该采用的方法是快速地用几根线画出一个大致准确的轮廓，然后再逐步地加工草稿。</p><p>软件开发也可以这样做。原型（<code>prototype</code>）并不只是模型（<code>model</code>），不等于将来一定要另起炉灶，你完全能够在原型的基础上直接做出最后的成品。我认为，只要有可能，你就应该这样做。这样的方式使得你可以利用在开发过程中一路产生的新想法。不过更重要的是，这样做有助于鼓舞士气。</p><p>先做出原型，再逐步加工做出成品，这种方式有利于鼓舞士气，因为它使得你随时都可以看到工作的成效。开发软件的时候，我有一条规则：任何时候，代码都必须能够运行。如果你正在写的代码一个小时之后就可以看到运行结果，这好比让你看到不远处就是唾手可得的奖励，你因此会受到激励和鼓舞。其他艺术领域也是如此，尤其是油画。大多数画家都是先画一个草图，然后再逐步加工。如果你采用这种方式，那么从理论上说，你每天收工的时候都可以看到整体的效果，不会对最后的成品一点感觉都没有。跟你说实话吧，画家之间甚至流传着一句谚语：“画作永远没有完工的一天，你只是不再画下去而已。”这种情况对于第一线的程序员真是再熟悉不过了。</p><p>士气也可以解释为什么很难为低端用户设计出优秀产品。因为优秀设计的前提是你自己必须喜欢这种产品，否则你不可能对设计有兴趣，更不要说士气高昂了。为了把产品设计好，你必须对自己说：“哇，这个产品太棒了，我一定要设计好！”而不是心想：“这种垃圾玩意，只有傻瓜才会喜欢，随便设计一下就行了。”</p><p>设计意味着做出符合人类特点和需要的产品。但是，“<code>人类</code>”不仅包括用户，还包括设计师，所以设计工作本身也必须符合设计师的特点和需要</p>]]></content>
      
      
      <categories>
          
          <category> Literature </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《清醒思考的艺术》</title>
      <link href="/2021/09/18/the-art-of-thinking-clearly/"/>
      <url>/2021/09/18/the-art-of-thinking-clearly/</url>
      
        <content type="html"><![CDATA[<h2 id="1-幸存偏误"><a href="#1-幸存偏误" class="headerlink" title="1. 幸存偏误"></a>1. 幸存偏误</h2><p>为什么你该去逛逛墓地</p><p>幸存偏误是指：由于日常生活中更容易看到成功、看不到失败，你会系统性地高估成功的希望。不了解现实的你（与雷托一样）对成功抱有一种幻想，认识不到成功的概率有多微弱。每位成功的作家背后都有100个作品卖不出去的作家，每个作品卖不出去的作家背后又有100个找不到出版社的作者，每个找不到出版社的作者背后又有数百个抽屉里沉睡着刚动笔的手稿的写作爱好者。而我们总是听到成功者的故事，认识不到作家的成功概率有多小。摄影师、企业家、艺术家、运动员、建筑师、诺贝尔奖得主、电视制作人和选美冠军的情况也是一样。媒体没兴趣去刨挖失败者的墓地，这事也不归他们负责。这意味着：要想缓解幸存偏误，你就得了解这些。</p><p>幸存偏误意味着：你系统性地高估了成功概率。解决办法：尽可能常去逛逛曾经大有希望的项目、投资和事业的墓地。这样的散步虽然伤感，但对你是有好处的</p><h2 id="2-游泳选手身材错觉"><a href="#2-游泳选手身材错觉" class="headerlink" title="2. 游泳选手身材错觉"></a>2. 游泳选手身材错觉</h2><p>哈佛是好大学还是烂大学？我们不清楚</p><p>纳西姆·塔勒布是位作家和证券交易商，当他下决心要想办法解决他的顽固体重时，他考虑过各种体育活动。他觉得慢跑者干巴巴的、可怜兮兮的；练健美的看上去肩很宽、傻里傻气的；打网球的人呢，哎呀，他们是高贵的中产阶层！但他喜欢游泳健将，因为他们身材匀称、优美。于是他决定每周两次钻进游泳馆含氯的水里，好好练练。一段时间后他才发觉，他上了一种错觉的当。<strong>职业游泳者体形完美，并不是因为他们锻炼充分。实际情况正好相反：他们之所以成为出色的游泳选手，是因为他们拥有这样的身材</strong>。他们的身躯是一种选择标准，而不是他们运动的结果。</p><p>结论：凡有人讴歌某种东西值得追求——强健肌肉、美貌、高收入、长寿、影响力、快乐，你都要看仔细。在跨入泳池之前，不妨先照照镜子。你要诚实地面对自己。</p><h2 id="3-过度自信效应"><a href="#3-过度自信效应" class="headerlink" title="3. 过度自信效应"></a>3. 过度自信效应</h2><p>你为什么会系统性地高估自己的学识和能力</p><p>几乎没有哪个大项目会比原计划更快、更便宜地竣工。空客A400M运输机、悉尼歌剧院、三条戈特哈德隧道的延期更是令人难以置信。这个清单想列多长就有多长。</p><p>为什么会这样呢？这里有两个效应在共同起作用。一个是传统的过度自信，另一个是项目的直接利益人在激励下低估成本。调研员希望拿到系列订单，建筑企业和供应商亦然，建筑业主感觉得到了乐观数据的支持，政治家们靠这样做来拉选票。我们会在另一章简述这种激励过敏倾向。重要的区别在于：过度自信不是受到了激励，而是自然单纯、生而有之的。</p><p>让我们用三点说明来结束这一章吧：（1）不存在相反的不够自信效应。（2）过度自信效应在男人身上比在女人身上更明显——女人较少高估自己。（3）不仅乐观者会受到过度自信效应的影响，就连自称悲观的人也会高估自己——只不过高估的幅度要少些罢了。</p><p>结论：请对所有预测持怀疑态度，尤其是当这些预测是由所谓的专家们做出的。请你在筹划任何事情时都从悲观的角度出发，作最坏的打算。这样你才会真正有机会，更现实一些地判断形势。</p><h2 id="4-从众心理"><a href="#4-从众心理" class="headerlink" title="4. 从众心理"></a>4. 从众心理</h2><p>就算有数百万人声称某件蠢事是对的，这件蠢事也不会因此成为聪明之举</p><p>简单的所罗门·阿希试验第一次是在1950年进行的，试验显示了团队压力如何压倒健康的人类理性。试验时将不同长度的线条拿给受试者看，要他说出线条比起参照线条是更长、一样长还是更短。如果此人是独自坐在房间里，他会正确估计所有线条，因为这任务确实很简单。现在有7个人走进房间——全是演员，但受试者被蒙在鼓里。那7个人相继说出一个错误答案，虽然线条明显长于参照线条，却说它“更短”。现在轮到受试者回答了。30%的试验者会说出与前面的人一样的错误答案——纯粹是受到了团队压力的影响。</p><p>广告会有计划地充分利用我们拥有从众心理的弱点。当消费者的选择毫无头绪时（汽车品牌、洗涤剂、美容产品等的数量多得无法全面掌握，它们都没有明显的优缺点），在“你我”这样的人们出现的地方，它最有效。</p><p>当一家公司声称它的产品“销量最高”时，请你表示怀疑。这是个荒唐的说法。凭什么一种产品“销量最高”就应该更好呢？英国作家毛姆这样讲道：“就算有5 000万人声称某件蠢事是对的，这件蠢事也不会因此成为聪明之举。”</p><h2 id="5-纠缠于沉没成本"><a href="#5-纠缠于沉没成本" class="headerlink" title="5. 纠缠于沉没成本"></a>5. 纠缠于沉没成本</h2><p>你为什么应该忽视过去</p><blockquote><p>指以往发生的，但与当前决策无关的费用。我们把这些已经发生不可收回的支出，如时间、金钱、精力等称为“沉没成本”</p></blockquote><p>一位朋友被一段问题恋情折磨多年。那女人一次次欺骗他。每当他逮住她时，她都后悔不迭地回来，恳求他的原谅。虽然再跟这个女人维持关系早就没有意义了，他还是一次次接受了她。当我与他谈论此事时，他向我解释道：“我在这段恋情中投入了那么多感情，现在离她而去是错误的。”这是典型的纠缠于沉没成本</p><p>“我们已经行驶了这么远……”“我已经读了这本书的这么多页……”“我已经花了两年时间接受这个培训了……”从这种句子可以看出，你是如此与沉没成本难舍难分。</p><p>有许多好理由支持你继续投资下去，但如果你只是因为舍不得已经做出的投资而决定继续做某件事，这就不是一个好理由了。理性的决定意味着忽视已经投入的成本。你已经投资了什么并不重要，唯一重要的是现在的形势及你对未来的评估</p><h2 id="6-互惠偏误"><a href="#6-互惠偏误" class="headerlink" title="6. 互惠偏误"></a>6. 互惠偏误</h2><p>你为什么不该让别人请你喝饮料</p><p>几十年前，正值嬉皮士文化鼎盛时期，人们经常在火车站和飞机场遇见身裹粉红色长袍的克里希那教派的教徒。他们会给每位匆匆走过的行人赠送一小束花。这些教徒言语不多，只有一句问候、一个微笑，仅此而已。但即使人们觉得一小束花没多大用处，通常也会接受它——人们不想无礼。如果你拒绝接受这份礼物，你会听到他们温和地说：“请收下吧，这是我们给您的礼物。”</p><p>当你在下一条巷子里将花扔进垃圾桶时，你会发现那里已经有几枝了。但事情并非到此结束。当你正在受良心的折磨时，一位克里希那派的教徒会主动上来与你搭话，要求你捐赠。这样做许多时候都成功了。这一募捐方式如此管用，因此许多飞机场都禁止这个教派的教徒进入。科学家罗伯特·西奥迪尼仔细调查了这一现象，发现人们几乎都不能忍受亏欠。</p><p>互惠古来有之。它的基本含义是：“我帮你，你帮我。”我们发现那些食物总量变化很大的动物之间都存在互惠。假定你是猎人，有一天运气好，猎杀了一头鹿。肉很多，你一天吃不完。当时还没有冰箱，于是你就与你的群体成员瓜分了这头鹿。这样，当你有一天运气不好时，你也可以从别人的猎物中获得好处。这是一种出色的生存策略。互惠是风险管理；没有互惠，人类——还有无数种动物——早就灭绝了。</p><p>互惠也有可恶的一面：报复。紧接着报复的是反报复，然后你就会陷入一种恶性循环中。耶稣曾经布道，要求将另一面脸也伸给攻击者，也就是打断恶性循环，但这是很难做到的，因为互惠这一理念已经在我们脑中顽固地存在了一亿多年。</p><h2 id="7-确认偏误"><a href="#7-确认偏误" class="headerlink" title="7. 确认偏误"></a>7. 确认偏误</h2><p>遇到“特殊情况”这个词，你要格外小心</p><p>确认偏误对经济记者的影响之大非其他行业可比。他们常抛出一种廉价理论，再提供两三个“证明”，就算完成了一篇文章。比如：“谷歌如此成功，是因为这家公司拥有一种创造性文化。”于是记者就会去找出两三家同样有创造性文化并成功了（确认证据）的公司。但他不会努力去挖掘反驳证据，也就是找出那些提倡创造性文化却不成功的公司，还有那些成功了但没有创造性文化的公司。两种公司各有很多，但记者故意忽视它们。他若提到其中的一种，他的文章就会被扔进垃圾桶了</p><p>结论：请你与确认偏误做斗争。请你写下你的信条——有关世界观、投资、婚姻、健康预防措施、节食、成功策略的，然后寻找反驳证据。干掉自己最心爱的理论，这是一桩艰苦的工作，但作为聪明人士，你不会躲避的。</p><h2 id="8-权威偏误"><a href="#8-权威偏误" class="headerlink" title="8. 权威偏误"></a>8. 权威偏误</h2><p>你为什么该藐视权威</p><p>关于权威有两个问题。首先是令人警醒的跟踪记录。这个星球上有大约100万受过培训的经济学家，没有一位精确预言了金融危机发生的时间，更别说房地产泡沫的破裂、信用违约互换的瓦解直到通货膨胀引发的经济危机的顺序了。再没有哪个专家群体失灵得比这更惊人了。让我们再举一个医学界的例子：事实可以证明，在1900年之前，病人都不怎么去看医生，因为医生只会使病情恶化（由于当时卫生条件不够，医生只会采用放血和其他不当方法）。</p><p>专家们希望被人认出，为此他们必须用某种信号显示他们的身份。医生和研究人员是通过他们的白大褂，银行行长则是通过西服和领带。领带没有作用，它只是一种信号。国王们头戴王冠，军队中有军衔标志，基督教会里的权威信号更是明显。其他信号还有名人访谈的邀请、图书和其他出版物等。</p><p>任何时候都有不同的权威“流行”。有时是神职人员，有时是国王、武士、罗马教皇、哲学家、诗人、摇滚明星、电视制作人、互联网公司创始人、对冲基金经理、银行总裁。因此存在权威时，社会总是乐于跟从。如果权威们想跨界得到认真对待，将会引起混乱。比如，当一位职业网球运动员推荐咖啡机或一位女演员推荐治疗扁桃腺炎的药物时。有关这方面的更多内容请参见光环效应那一章。</p><p>不管什么时候遇到一位专家，我都会设法向他挑战。请你也这么做。你对权威的批判性越强，你就越自由，就越相信自己有更多的能力。</p><h2 id="9-对比效应"><a href="#9-对比效应" class="headerlink" title="9. 对比效应"></a>9. 对比效应</h2><p>你为什么最好别找模特儿等级的朋友一起出门</p><p>罗伯特·西奥迪尼在他的《影响力》一书里介绍了希德和哈利两兄弟的故事。20世纪30年代，他们在美国经营一家服装店。希德负责销售，哈利负责裁剪。每当希德发现站在镜子前的顾客真的喜欢一套西服时，他就会假装有点耳聋。当顾客询问价格时，希德就对他的兄弟喊：“哈利，这套西服多少钱？”哈利就从他的裁剪台上抬起头，回答说：“这套漂亮的棉质西服42美元。”这价格在当时高得离谱。希德假装他没听懂似的又问：“多少钱？”哈利重复那个价格：“42美元！”希德听完后向他的顾客转过身来说：“他说22美元。”那位顾客听到后就赶紧将22美元放到桌上，抢在可怜的希德发觉“错误”之前，带着昂贵的衣服匆匆离去</p><p>对比效应有时能够毁掉你的整个生活：一个漂亮女人嫁给了一个相当普通的男人。为什么？因为她的父母都很厉害，她觉得普通男人更好，虽然他实际上并没有那么好。最后：在超级名模的广告轰炸下，就连漂亮女人都会觉得自己魅力一般。因此，如果你是女人，想找一个男人，你千万不要让你的模特儿等级的朋友陪你一起出去。男人会因此发觉不到你的真实魅力。你独自去就好。更好的办法是：你带上两个比你难看的朋友去参加舞会。</p><h2 id="10-现成偏误"><a href="#10-现成偏误" class="headerlink" title="10. 现成偏误"></a>10. 现成偏误</h2><p>你为什么宁可用一张错误的地图，也不愿没有地图</p><p>“他终生吸烟，每天3盒，却活到了100多岁。因此吸烟的危险不可能有多大。”“汉堡是安全的。我认识一个人，他住在白沙岛￼中部，他从不锁门，哪怕出去度假都不锁门，他家还从未失窃过。”这些话是想证明某种东西，但它们其实什么也证明不了。这么讲话的人，就是犯了现成偏误</p><p>现成偏误是指：<strong>我们依据现成的例子来想象世界。这当然是愚蠢的，因为外界现实中的某种东西不会因为我们更容易想到而出现得更频繁。</strong></p><p>由于现成偏误，我们的脑子里总有一张错误的风险卡。于是我们系统性地高估了因飞机坠毁、汽车事故或谋杀而遇难的风险。我们低估了因为糖尿病或胃癌这样不太能引起轰动的方式死亡的风险。炸弹袭击要比我们认为的少得多，抑郁症则要常见得多。壮观、华丽或大声的一切，我们都高估了它们出现的概率。而无声、无形的一切，我们都低估了它们出现的概率。因为我们的大脑更容易接受壮观、华丽或大声的东西。我们的大脑是剧本式思维的，而不是量化思维的。</p><p>应对办法：请与跟你想法不同的人合作，跟那些与你拥有截然不同经验的人合作。因为你独自战胜不了现成偏误。</p><h2 id="11-“在好转之前会先恶化”的陷阱"><a href="#11-“在好转之前会先恶化”的陷阱" class="headerlink" title="11. “在好转之前会先恶化”的陷阱"></a>11. “在好转之前会先恶化”的陷阱</h2><p>如果有人建议你选择一条“先经历痛苦的道路”，你应该敲响警钟</p><p>x我们再举个例子，有一位首席执行官，他的公司营业额跌到冰点，营销人员毫无积极性，营销活动屡屡无效。绝望中他聘请来一位顾问，请这位顾问分析公司的情况，费用为每天5 000欧元。顾问检查后得出这样一个结论：“你的销售部门缺乏想象力，你的品牌定位不明确，形势棘手。我可以为你纠正过来，但不可能一蹴而就。问题错综复杂，需要慢慢解决。在好转之前，营业额还会下滑。”首席执行官聘用了这位顾问。一年后营业额果然下滑了。第二年也是。顾问一再强调，公司的发展正好符合他的预测。当三年之后营业额继续不见起色时，首席执行官终于解聘了这位顾问。</p><p>“在好转之前会先恶化”的陷阱是确认偏误的一种变体。使用这个花招大大有利于一个对专业一窍不通或对事情没有把握的专业人员。如果情况继续走下坡路，就证明了他的预言是正确的。如果情况意外地回升了，客户开心，专业人员则可以将好转归功于他的能力。不管怎样——他总是对的</p><p>结论：如果有人说：“在好转之前会先恶化”，你脑子里就应该敲响警钟。不过请小心：确实有那样的情形，先是再次下滑然后回升。事业的转换可能会耗费时间，会造成停发工资。一个企业的重组也需要一定的时间。但所有这些情况，人们很快就能看出措施是否有效。里程碑是明确的，是可以检测的。请你望着里程碑，而不是望着天空。</p><h2 id="12-故事偏误"><a href="#12-故事偏误" class="headerlink" title="12. 故事偏误"></a>12. 故事偏误</h2><p>为什么就连真实的故事也是骗局<br>在媒体里，故事偏误像瘟疫一样肆虐。例如：一辆汽车驶过一座桥，桥突然垮塌了。我们第二天会在报纸上读到什么？我们会读到倒霉蛋的故事，他坐在汽车里，他从哪里来，要驶去哪里。我们会获悉他的生平：生在某处、长在某处、职业是什么。如果他幸存下来，能够接受采访，我们会准确地听到当桥轰然倒塌时，他是什么感觉。荒唐的是，这些故事没有一个是重要的。因为重要的不是那个倒霉蛋，而是桥梁设计。问题具体是出在哪里？是因为材料老化吗？如果是，是哪里的材料？如果不是，是桥受损了吗？那么桥是怎么受损的？会不会是使用了一种从根本上错误的设计原理？所有这些重要问题的麻烦是：不适合将它们编进一则故事里。我们喜欢听故事，不喜欢听抽象事实。这是一种不幸，因为为了不重要的方面，重要方面被忽视了。（这同时也是一种幸运，否则就只有非小说类图书，而没有小说了。）</p><p>下列故事你更容易记起哪一则呢？故事一：“国王死了，然后王后也死了。”故事二：“国王死了，随后王后伤心死去。”如果你与大多数人一样，你会更容易记住第二则故事。因为在这里两次死亡不是简单地先后发生的，而是有着情感联系的。故事一是事实报道，而故事二更有“意义”。按照信息理论，本来故事一应该更容易被记住，因为它更短。可我们的大脑并不是这样理解的。</p><p>讲述一则故事的广告，比理性地罗列产品优点的广告效果更好。事实上，产品的故事只是附带物，可我们的大脑不是这样运转的。它要听故事。谷歌在2010年美国“超级碗”节目中插播的广告出色地证明了这一点。在YouTube视频分享网站上输入“谷歌巴黎之爱”就可以找到这则广告。</p><p>结论：从自传到世界大事——我们将一切炮制成有“意义”的故事。我们这样做是在扭曲真相——这会影响我们决定的质量。应对方法：请你将这些故事拆解开来。请你问问自己：这些故事想隐藏什么？训练方法：请你设法用无关联的眼光看看自己的生平，你会吃惊的。<br>￼</p><h2 id="13-事后诸葛亮偏误"><a href="#13-事后诸葛亮偏误" class="headerlink" title="13. 事后诸葛亮偏误"></a>13. 事后诸葛亮偏误</h2><p>你为什么应该写日记</p><p>今天，任何人重读2007年的经济预测，都会惊讶于当时专家们对2008~2010年前景的展望有多么乐观。一年之后，2008年，金融危机爆发了。问到金融危机的起因，还是那些专家们今天会给出相同的答案：格林斯潘统治下货币的泛滥、资金放贷的宽松、腐败的信贷评级机构、随意的自有资本规定等。回顾之下金融危机的发生似乎完全符合逻辑，是不可避免的。可没有一位经济学家——全世界的经济学家有近百万名——预言到了它的准确发展。相反，很少有哪个专家团体会这么深地落入事后诸葛亮偏误的圈套。</p><p>事后诸葛亮偏误绝对是最顽固的思维错误之一，可以恰如其分地称为“我早知道现象”，即事后回顾时一切都显得是可以理解的、不可避免的。</p><p>但我还有一个建议，这建议更多是来自个人经验而不是科学：请你记日记。请你写下你的预测——有关政治、事业、体重、股市等。请你不时地拿你的记载与实际情况相比较。你会惊讶你是个多么糟糕的预测家。另外，请你也同样读历史。不是事后的、成熟的理论，而是那个时代的日记、剪报、备忘录。这会让你更好地感觉到世界的不可预见性。</p><h2 id="14-司机的知识"><a href="#14-司机的知识" class="headerlink" title="14. 司机的知识"></a>14. 司机的知识</h2><p>你为什么不可以把新闻播音员说的话当真</p><p>马克斯·普朗克于1918年荣获诺贝尔物理学奖，之后他在全德国作巡回报告。不管被邀请到哪里，他都会就新的量子力学演讲一番。他的司机渐渐对他的报告烂熟于心。“普朗克教授，老作同样的报告，你一定觉得无聊。我建议，在慕尼黑由我代你作报告，你坐最前排，戴上我的司机帽。让咱俩换换花样。”普朗克兴致盎然，欣然同意。于是司机为一群专家级听众作了一番有关量子力学的长篇报告。之后，一位物理学教授举手提问。司机回答说：“我压根儿没想到，在慕尼黑这样先进的城市里还会有人提出这么简单的问题。请我的司机来回答这个问题吧。”</p><p>查理·芒格是全球最优秀的投资家之一，普朗克的故事我就是从他那里听来的。他认为知识有两种：一种是真知识，来自那些投入大量时间和思考以获得知识的人们；另一种就是司机的知识，按芒格故事里的意思，司机是指那些装得好像他们知道的人。他们会模仿别人表演，他们也可能拥有动听的声音或具有说服力的形象。但他们传播的知识是空洞的，他们高谈阔论地挥霍着华丽词汇。</p><p>可惜越来越难将真知识与司机的知识区分开来。在新闻播音员身上还算简单，他们是演员，仅此而已。这一点人人都知道。但人们对这些“套话大师”是那么尊敬，这一点让人讶异不已。人们付给他们很多钱，邀请他们主持他们几乎力不从心的调查和访谈节目。</p><p>这种区分在记者身上就困难一些。他们中的有些人确实掌握着扎实的知识，多是学习了很长时间的学生，或是那些长年专门研究某个领域的记者。他们真正努力理解和描绘一个复杂的事实。他们倾向于长篇大论，阐明多个事件和例外。</p><p>可惜多数记者都属于司机范畴。他们变魔术似的会在最短的时间内从帽子里变出任意一篇内容的文章，或者，说得更准确些——从互联网里。他们的文章只有一页，简短、诙谐，经常是作为他们的司机知识的补偿。</p><p>一家企业越大，人们就越期望它的首席执行官具有表演才能——所谓的交际能力。一个安静、固执但可靠的创造者是不行的，至少不能担任最高领导。股东和经济记者们显然相信，一个擅长宣传的人会带来更好的结果——实际上当然不是这么回事。</p><p>查理·芒格的合作伙伴沃伦·巴菲特使用了一个美妙的概念：“能力范围”。在这个范围之内的，人们都像专家一样精通；在这个范围之外的，就不懂或只懂一部分。巴菲特的生活信条是：“请认清你的能力范围，并待在里面。这个范围有多大，并不重要。重要的是知道这个范围的界线在哪里。”查理·芒格重复说：“你必须找出自己的才能在哪里。我几乎可以向你保证，如果你必须在你的能力范围之外碰运气，你的职业生涯将会非常糟糕。”</p><p>结论：请你不要信任司机的知识。请你不要将公司新闻发言人、爱出风头的人、新闻播音员、唠叨鬼、花言巧语者、爱说闲话的人与一个真正有知识的人搞混。你怎么能区分出来呢？这里有个明确的信号。真正有知识的人知道他们知道什么，也知道他们不知道什么。这类人一旦来到他的“能力范围”之外，他要么什么也不说，要么就说“我不知道”。他这么说时不会觉得难为情，甚至还会带着一定的骄傲。而从“司机们”那儿，你别的什么话都能听到，就是听不到这一句</p><h2 id="15-控制错觉"><a href="#15-控制错觉" class="headerlink" title="15. 控制错觉"></a>15. 控制错觉</h2><p>你实际控制的少于你以为</p><p>在赌场里，当人们想要一个大数字时，大多数人会尽可能使劲地掷色子；当人们希望得到一个小数字时，他们会尽可能掷得温柔。这自然与球迷们的手势和脚部动作一样没有意义。球迷们那么做，好像他们真能干预比赛似的。许多人都存有这样的幻想：他们想靠传播正能量影响世界。</p><p>控制错觉是指：相信我们能够控制或影响某种我们客观上无法控制或影响的东西的倾向。这是詹金斯和沃德两位研究人员在1965年发现的。试验规则很简单：两只开关和一盏灯，灯或开或关。詹金斯和沃德可以调节开关和灯光互相制约的强度。即使是在灯纯属偶然地开、关的情况下，受试者也坚信，按开关能够在某种程度上影响灯。</p><p>在曼哈顿横穿街道时，人们会按一个红绿灯按钮。事实上他们按的是个不起作用的按钮。那为什么还要有这个按钮呢？就是为了让行人们相信，他们能影响信号控制器。事实证明，这样他们就能更好地忍受在红绿灯前的等待。许多电梯里的“开门、关门”按钮也是这样的，它们与电梯控制器其实并不相连。科学里称它们为“安慰按钮”。还有大办公室里的空调温度调节开关：有人觉得太热，另一些人觉得太冷。聪明的工程师会利用控制错觉，在每一层楼都安装一个假的温度调节按钮。这样一来，投诉的数量明显减少了。</p><p>因此，请你将注意力集中于你真正能影响的少量东西——坚定不移地只关注其中最重要的那些。其他的，听之任之吧。<br>￼<br>￼</p><h2 id="16-激励过敏倾向"><a href="#16-激励过敏倾向" class="headerlink" title="16. 激励过敏倾向"></a>16. 激励过敏倾向</h2><p>你为什么不该按实际开销付钱给你的律师</p><p>河内的法国殖民政府曾经颁布过一条法令：人们每交出一只死老鼠，政府就给他发钱。政府这样做本是想控制鼠灾，但这条法令却导致人们养殖老鼠。</p><p>这是激励过敏倾向的例子。它先是说明了一个平庸的事实：人们会对激励机制做出反应。这不奇怪。人都会做对自己有益的事情。令人吃惊的是两个次要方面：第一，一旦有激励加入游戏或改变了激励，人们就会迅速而剧烈地改变自己的行为；第二，人们是对激励做出反应，而不是对激励背后的目的做出反应</p><p>结论：请小心激励过敏倾向。如果你对某个人或某个组织的行为感到吃惊，请你想想，那后面隐藏着什么激励机制。我保证，你可以这样解释90%的行为。激情、精神疾病、心理障碍或恶意最多占到10%。</p><h2 id="17-回归均值"><a href="#17-回归均值" class="headerlink" title="17. 回归均值"></a>17. 回归均值</h2><p>医生、顾问、教练及心理治疗师的作用令人怀</p><p>有位男士经常背痛，疼痛时强时弱。有时他觉得自己充满活力，有时他几乎动弹不得。每次碰上这种情况——幸好很少——他妻子就会开车送他去找心理治疗师。每次这样做的第二天，他就明显好多了。于是他逢人就推荐他的心理治疗师。</p><p>另一个年轻人，高尔夫球打得非常好，他逢人便夸他的高尔夫球老师。如果某天他的球打得糟糕，他就去这位老师那儿学上1小时，下回他就又打得很好了。</p><p>第三个人是一家著名银行的投资顾问，他发明了一种“雨舞”。每当他在交易所的表现下滑时，他就在卫生间里跳“雨舞”。不管跳舞时他觉得自己有多荒谬，这支舞都是必须跳的：事实证明，他在交易所的表现好转了。</p><p>将以上三个人联系在一起的其实就是回归均值。</p><p>极端成绩与不太极端的成绩总是来回交替。已经连续3年表现优异的股票几乎不可能在接下来的3年继续走强。因此许多运动员在比赛取得好成绩，并因此登上报刊头版后心中往往会产生恐慌的情绪：潜意识中他们预感到，下回比赛时他们可能再也不会取得这一最高成绩了——这当然与头版毫无关系，而是与他们成绩的自然波动有关。</p><p>结论：当你听到诸如“我病了，去看医生，现在我好了，因此是那位医生帮助了我”或“这一年公司业绩很糟糕，我们请了个顾问回来，现在业绩恢复正常了”的话时，很有可能就是回归均值在起作用</p><h2 id="18-公地悲剧"><a href="#18-公地悲剧" class="headerlink" title="18. 公地悲剧"></a>18. 公地悲剧</h2><p>为什么理性的人不去诉诸理性</p><p>请你设想有一块肥沃的土地，一座村庄的所有农民均可使用。可以预料，每个农民都会将尽量多的奶牛赶到这块土地上去放牧。只要没有人偷猎、没有疾病蔓延，这件事就行得通，简言之：只要奶牛总数不超过一定数量，也就是土地不被剥削殆尽即可。可是，一旦事情不是这样，公地的美丽想法就会突转为悲剧。作为理性的人，每个农民都试图将他的利润最大化。他心想：“我要是再多将一头奶牛赶去公地，我会得到什么好处呢？”他可以得到多出售一头奶牛的额外好处。多一头奶牛造成的过度放牧的害处由所有人承担，单个农民承担的损失极小。所以从他的角度看，将更多的奶牛赶到公地上去放牧是理性的。于是再来一头、再来一头，直至公地被毁掉，这就是公地悲剧。</p><p>简言之，只有上述两种解决方法：将公地私有化或加强管理。对于无法私有化的东西，如臭氧层、海洋、卫星运行轨道等，必须加强管理</p><h2 id="19-结果偏误"><a href="#19-结果偏误" class="headerlink" title="19. 结果偏误"></a>19. 结果偏误</h2><p>切勿以结果判断决定</p><p>来看一个小小的思维试验。我们假设有100万只猴子在股市上投机。它们疯狂地、自然也是纯随机地买卖股票。会发生什么事？一年后，约一半猴子的投资都赚钱了，另一半亏钱了。第二年这帮猴子又是一半赚钱，另一半亏钱。长此以往，10年后大约只剩下1 000只猴子，它们的每次投资都是正确的。20年后就只剩下一只猴子每次投资总是正确的——它成了亿万富翁。我们就叫他“成功猴”吧。</p><p>媒体会怎么反应呢？他们会冲向这只动物，去阐述他的“成功原理”。他们会找到某种原理：也许这只“成功猴”吃的香蕉比其他猴子多，也许它坐在笼子的另一个角落里，也许它是头朝下吊挂在树枝上的，或者也许他捉蚤子时思考的时间很长。它一定拥有某种成功秘诀，不是吗？否则它怎么会有这么出色的表现呢？一只20年来总是做出正确投资决定的猴子，怎么可能只是一只无知的猴子呢？不可能！</p><p>猴子的故事说明了结果偏误：我们倾向于以结果判断决定——而不是当时作决定的过程。这种思维错误又名史学家错误。一个经典例子就是日本人偷袭珍珠港的事件。这座军事基地是不是应该疏散呢？站在今天的角度看：自然应该疏散。因为有大量线索说明，日本即将对其进行袭击。不过这些线索是事后回顾时才显得这么清晰。在当时的1941年，存在无数自相矛盾的线索。有的说明要袭击，有的说明不会袭击。要判断决定（是否疏散）的好坏，必须置身于当时的情境之中，过滤掉我们事后知道的一切信息（尤其是珍珠港果真遭到了袭击的事实）。</p><p>结论：请你切勿以结果判断决定。结果差并不一定意味着当时所做的决定不对，反之亦然。你最好仔细研究一下这样作决定的原因，而不是吐槽一个被证明是错误的决定，或者为一个也许是纯属偶然地获得成功的决定感到庆幸。这个决定是出于理性而做出的吗？如果是，那你下回最好仍然这样做，哪怕上回的结果很糟。</p><h2 id="20-选择的悖论"><a href="#20-选择的悖论" class="headerlink" title="20.  选择的悖论"></a>20.  选择的悖论</h2><p>为什么更多反而是更少</p><p>美国心理学家巴里·施瓦茨在他的《不满指南》一书里说明了为什么会这样。原因有三：第一，选择范围太大会导致无所适从。为了做试验，一家超市摆出了24种果子酱。顾客可以随意品尝并打折购买这些产品。试验第二天，超市只摆出6种。结果如何？第二天卖出的果子酱要比第一天多10倍。为什么？因为品种很多时顾客无法作决定，于是就干脆什么也不买。使用不同的产品重复进行这一试验，结果始终一样。</p><p>第二，选择范围大会导致做出更差的决定。你要是去问年轻人，他们选择生活伴侣的标准是什么，他们会列出所有令人尊敬的特性：智慧、善于沟通和交流、善良、善解人意、风趣和好身材。可在选择时他们果真考虑了这些标准吗？从前，在一座不大不小的村庄里，年轻小伙子约有20个潜在的同龄女性可供选择。她们中的大多数他上学时就认识，因此也很了解。而今天，在网络约会的时代，他有数百万名潜在的女性伴侣可供选择。选择如此之多，男性的大脑干脆就将各种复杂情况浓缩成一个唯一的标准——好身材。你对此可能很熟悉，甚至可能亲身体验过。</p><p>第三，选择范围大会导致不满。你如何能够保证从200个选项中做出完美的选择？答案是：你不能。选择越多，你在选择后就越没有把握，因而也就越不满。</p><p>怎么办？请你在端详面前的选项之前，仔细考虑你想要什么。请你写下你的标准，并务必遵守它们。你要明白，你永远作不出完美的选择。要想做出在事后看来无懈可击的选择是非理性的，因为事情的发展永远有无数种可能性。你就满足于一个适合你的“好答案”吧。是的，在生活伴侣这件事上也是如此。只有最好的才适合你吗？在存在无限选择的年代，情况恰恰相反：适合你的才是最好的。<br>￼</p><h2 id="21-讨喜偏误"><a href="#21-讨喜偏误" class="headerlink" title="21. 讨喜偏误"></a>21. 讨喜偏误</h2><p>你行为不理性，是因为你想讨别人喜欢</p><p>·吉拉德是举世公认的最成功的汽车销售商之一。他的成功秘诀是：“没有什么比让顾客相信你真心喜欢他更管用。”他的撒手锏是，每年给他的所有客户（包括曾经的客户）寄一张小卡片，上面只写一句话：“我喜欢你。”</p><p>讨喜偏误很容易理解，可我们却一再犯这个错误。它是指：某人越讨喜，我们就越倾向于从这个人那儿买东西或者帮助他。那么，什么叫讨喜？科学家列出了一系列因素。我们会觉得一个人讨喜，如果他：（1）外表有吸引力；（2）在出身、个性和兴趣上与我们相似；（3）他觉得我们讨人喜欢。这三个因素是依次排列的。广告模特大多魅力非凡。难看的人显得不讨喜，因此他们不适合做广告模特。但除了超级名模，广告模特也有“你我这样的普通人”——与目标客户相似的长相、方言、背景。简言之，越相似越好。广告很多时候是在奉送恭维——“因为你值得这样。”这就是说：谁发出他觉得我们讨人喜欢的信号，我们就倾向于也觉得他讨喜。恭维具有神奇的效果，哪怕那是谎话连篇。</p><p>结论：你在判断一笔生意时应该坚持不受卖方影响。请你不要考虑他，更好的办法是：请你想象他是个不讨人喜欢的家伙。</p><h2 id="22-禀赋效应"><a href="#22-禀赋效应" class="headerlink" title="22. 禀赋效应"></a>22. 禀赋效应</h2><p>请不要死抱着某种东西不放</p><p>一辆宝马车正在二手车商的停车场上闪闪发光。虽然它已经行驶了一些里程，但看起来完好无损。我只是觉得5万欧元贵得离谱。我对二手车稍有了解，在我眼里它最多值4万欧元。可卖方不让步。当他一星期后打电话给我，说我可以以4万欧元的价格买到那辆车时，我当场同意了。第二天我在一家加油站停车加油。加油站老板与我搭讪，要用53 000欧元买我的车。我谢绝了。直到开车回家时我才认识到，我的行为多么不理智。某种在我眼里最多值4万欧元的东西，转到我手里后，价值一下子就超过了53 000欧元——否则我就会马上卖掉它。这背后的思维错误是：禀赋效应。我们感觉我们拥有的东西比我们没有拥有的更有价值。换句话说：当我们出售某物时，我们要求的钱多于我们自己愿意为它支付的钱。</p><blockquote><p>对一个物品持有了以后，会有持有感情，这部分也会算价值内</p></blockquote><p>你应聘一份工作却没有成功，你会很失望。如果你知道，你一直坚持到了最终环节，然后被拒绝了，你的失望还要大得多——这是没有道理的。因为你要么成功，要么不成功，其他的一切都无关紧要。结论：请不要死抱着某种东西不放，请将你拥有的视作“宇宙”临时留给你的某种东西。要知道你拥有的一切随时又会被拿走</p><h2 id="23-奇迹"><a href="#23-奇迹" class="headerlink" title="23. 奇迹"></a>23. 奇迹</h2><p>不可能事件的必然性</p><p>不知何故，上星期我突然想起了我曾经的一位同学安德雷亚斯，我与他已经很久没联系了。这时电话铃突然响了，打电话的正是安德雷亚斯。“这一定是心灵感应！”我兴奋得叫了起来。这到底是心灵感应还是巧合呢？</p><p>打电话的事也一样。设想一下安德雷亚斯想起你却没有打电话给你的那些情形吧；还有你想起安德雷亚斯而他没有给你打电话、他打电话给你而你没有想起他、你打电话给他而他没有想起你的情形，以及那些你没有想他而他也没有打电话给你的情形。由于人们将时间的90%左右都用在想别人上，从不发生两人彼此想念、其中一个拿起电话听筒的情况是不可能的。再加上，那人不必非得是安德雷亚斯。如果你还有其他100个熟人，这种事情发生的概率就提高了100倍。</p><p>结论：不可思议的意外事件就是虽然少见但完全可能发生的事件。它们的发生没什么可吃惊的，如果它们从不发生，那才令人感到意外。</p><h2 id="24-团体迷思"><a href="#24-团体迷思" class="headerlink" title="24. 团体迷思"></a>24. 团体迷思</h2><p>共识为什么有可能是危险的</p><p>你曾经在某次会议上克制自己，没有说出你的意见吗？肯定有过。你一言不发，点头同意种种动议，毕竟你不想做个（永远的）“捣蛋鬼”。另外你对你的异议也许也没有把握，其他人也不傻啊，他们全都意见一致，那就不吭声吧。如果人人都这样做，就会出现团体迷思：一群智慧的人做出愚蠢的决定，因为他们每个人都误以为自己的意见是正确的共识，从而做出他们每个人在正常情形下都会拒绝的决定。团体迷思是从众心理的一种特殊情况，我们在之前的一章里已经探讨过从众心理这一思维错误。</p><p>结论：如果你是一个智囊团的成员，无论何时，你都要讲出你的看法——哪怕这看法不是很中听。你要仔细考虑没有讲出的意见，必要时要甘冒被隔离在温暖团体之外的风险。如果你领导着一支团队，请你指定某人唱反调。他将不是团队里最受欢迎的人，但也许是最重要的人。</p><h2 id="25-忽视概率偏误"><a href="#25-忽视概率偏误" class="headerlink" title="25. 忽视概率偏误"></a>25. 忽视概率偏误</h2><p>累计奖金为什么会越来越多</p><p>有两种赌博：第一种赌博你有可能赢1 000万欧元，第二种赌博你可能赢1万欧元。你会参加哪一种呢？如果你在第一种赌博里赢了，你的生活将会彻底改变：你可以辞掉工作，从此靠利息生活。如果你在第二种赌博里赢了，你可以美美地前往加勒比海度假，然后一切照旧。第一种赌博赢的概率是亿分之一，第二种赌博赢的概率是万分之一。好吧，你会玩哪一种？我们的感情冲动会将我们拉向第一种，虽然客观地看，第二种赢的概率要大无数倍。因此累计奖金越来越多——数百万、数亿、数万亿美元——无论赢的机会是多么微弱。</p><p>结论：我们很难区分各种风险，除非风险为零。由于我们不能直觉地理解风险，我们必须计算。在概率公开的地方——像彩票——这就很容易。而在普通生活中，风险很难估计，但又是躲也躲不过的</p><h2 id="26-零风险偏误"><a href="#26-零风险偏误" class="headerlink" title="26. 零风险偏误"></a>26. 零风险偏误</h2><p>你为什么会为零风险支付过多</p><p>假设你必须玩俄罗斯轮盘赌。你的左轮手枪的枪膛里可以装进6发子弹。你像转动抽彩轮盘一样转动你的枪膛，拿枪口抵着你的额头，扣下扳机。第一个问题：假如你知道枪膛里有4颗子弹，你愿意付多少钱，来将4颗子弹中的两颗从枪膛里取走？第二个问题：假如你知道枪膛里只有一颗子弹，你愿意为取走这颗子弹支付多少钱？</p><p>对大多数人来说，这是明摆着的：第二种情况下你愿意支付更多，因为那样一来死亡的风险就降到了0。如果单纯看死亡概率降低的幅度，这是没有意义的，因为第一种情况下你将死亡概率降低了1/3，而第二种情况下降低了1/6。因此看起来第一种情况对你应该更有利，但某种东西在驱使我们过高地评价零风险。</p><p>假设你是国家首脑，你想排除恐怖袭击的风险。那你必须派给每个市民一名间谍，每名间谍再派一名间谍。那么很快，90%的人口就都成了间谍。我们知道，这样的社会是不存在的。</p><p>那么在股市中呢？股市中存在零风险吗？很可惜，不存在。即使你卖掉股票，将钱存进你的账户，银行也有可能破产，通货膨胀也会蚕食掉你节约下的钱，或者一次货币改革也可能会毁掉你的财产</p><p>结论：请你告别零风险的想象，学会怀着“没有什么是安全的”想法生活——无论是你的积蓄、你的健康、你的婚姻、你的友谊、你的敌人，还是你的土地。请你满足于至少有东西让你保持相对稳定并体验自身的快乐。研究表明，无论是中了百万彩票还是半身瘫痪都不会长期改变你的满意程度。不管发生什么事，快乐的人照样快乐，不快乐的人依旧不快乐。更多内容请见享乐适应症那一章。</p><h2 id="27-稀少性谬误"><a href="#27-稀少性谬误" class="headerlink" title="27. 稀少性谬误"></a>27. 稀少性谬误</h2><p>为什么饼干越少越好吃</p><p>一位女友请我去她家喝咖啡。我们想聊天，可她家的三个孩子在地板上吵吵闹闹。我想起我带有玻璃球——满满一袋子。我将玻璃球倒在地板上，希望那些淘气鬼会安静地玩玻璃球。我错了，这立即引发了一场激烈的争抢。我不明白是怎么回事，便仔细观看。原来在玻璃球当中刚好有一只蓝色的，孩子们都抢着要它。其实所有玻璃球都一样大、一样漂亮，闪闪发光，可那只蓝色的有个重要优势——稀少。我笑了：孩子们是多么幼稚啊！</p><p>结论：我们对稀少性的典型反应是丧失清晰思考的能力。因此请你仅按价格和作用判断一样东西。你不要在乎它是否稀少，是否有哪位“伦敦来的医生”也想要</p><h2 id="28-忽视基本概率"><a href="#28-忽视基本概率" class="headerlink" title="28. 忽视基本概率"></a>28. 忽视基本概率</h2><p>当你在怀俄明州听到马蹄声、见到黑白条纹时</p><p>马库斯是个瘦瘦的男人，他戴眼镜，爱听莫扎特的音乐。根据以上描述，下列哪种情况的可能性更大呢？（1）马库斯是卡车司机；（2）马库斯是法兰克福的文学教授。大多数人会选（1）。这是错的。德国的卡车司机要比法兰克福的文学教授多得多。因此马库斯是一位卡车司机的可能性更大——即使他爱听莫扎特的音乐。为什么大多数人都会选错呢？精确描述误导了我们，让冷静的目光偏离了统计真相。科学里称这种思维错误为忽视基本概率。忽视基本概率是最常见的思维错误之一。实践中所有记者、经济学家和政治家都常犯这种错误</p><p>有时我会给学习企业管理的大学生们做讲座。当我询问这些年轻人的事业目标时，他们大多数回答，他们的中期计划是成为一家全球化公司的董事。我当年也是一样。幸好这个愿望落空了。我认为我的任务是给大学生们开一门基本概率速成课：“凭这所学校的文凭进入一个集团董事会的概率低于1%。所以不管你们多聪明、多努力，最有可能的情形是，你们会在中层管理部门止步不前。”我收获的是大学生们因惊异而瞪大的眼睛，我想我为缓和未来的中年危机做出了一份贡献。<br>￼<br>￼</p><h2 id="29-赌徒谬误"><a href="#29-赌徒谬误" class="headerlink" title="29. 赌徒谬误"></a>29. 赌徒谬误</h2><p>为什么没有一种平衡命运的力量</p><p>1913年夏天，蒙特卡罗发生了一件令人难以置信的事情。人们挤在赌场的赌台周围，他们不相信自己的眼睛：那只球已经先后20次落在黑色上。许多赌徒趁此机会押红色，可中的又是黑色。更多的人涌来，把他们的钱押在红色上。现在总该换一回了吧！可又是黑色。一而再，再而三，直到第27次时那球才终于落在了红色上。此刻赌徒们已经输掉了他们的数百万美元。他们破产了。</p><p>一枚硬币被连抛3次，每次都是人头朝上。假如有人强迫你，让你自己掏出1 000欧元为下一抛下注。你会押人头还是数字呢？如果你像大多数人那样思考，你会押数字，虽然人头同样是有可能的——这就是著名的赌徒谬误</p><p>结论：请你仔细观看，看你面对的是否是独立的事件——不过这主要存在于赌场、彩票和理论书籍里。现实生活中这些事件大多有着相互联系——已经发生的事情，会影响未来将要发生的事情。因此请你忘记命运的平衡力量（除了回归均值的情形）。</p><h2 id="30-锚定效应"><a href="#30-锚定效应" class="headerlink" title="30. 锚定效应"></a>30. 锚定效应</h2><p>数字轮盘如何搞得我们晕头转向</p><p>再看另一个试验：带领大学生和房地产从业人员参观一套房子，随后请他们估计这套房子值多少钱。之前发给他们一个（偶然生成的）“销售价格表”。不出所料，大学生们，也就是非职业人员，受到了锚定效应的影响。表格上的价格越高，他们给房子的定价就越贵。而房地产从业人员，他们能独立地做出判断吗？不，他们受任意设定的锚的影响和非职业人员一样大。因此，一个对象的价值越无法确定——房地产、公司、艺术品——就越容易受锚定影响，就连职业人员也无法避免。</p><p>现实生活中，锚定效应随处可见。科学证明，如果老师知道一位学生过去的学习成绩，就会影响他给学生的新论文打分。在这里，过去的成绩起了锚定的作用。许多产品在包装袋上刊印的“建议销售价”也是一个锚定。职业销售人员知道，他们必须先设定一个锚——远在他们报价之前。</p><p>我年轻时曾在一家咨询公司工作。我当时的上司是一位真正的锚定效应专家。在首次与客户交谈时，他就会设定一个锚——通常会高出内部价很多：“亲爱的顾客，当您收到报价时，请您不要吃惊：我们为您的一位竞争对手做过一个类似的项目，价格在500万欧元左右。”锚就这样设定了，于是价格谈判就从500万欧元开始。<br>￼</p><h2 id="31-归纳法"><a href="#31-归纳法" class="headerlink" title="31. 归纳法"></a>31. 归纳法</h2><p>如何把别人的钱弄进自己的口袋</p><p>￼<br>一个农民喂食一只鹅。一开始鹅畏畏缩缩，想：“这个人为什么要喂我？这背后一定有什么阴谋。”数星期过去了，农民天天都过来，扔给它谷子。它的疑心渐渐减弱。几个月后这只鹅肯定地想：“这个人很喜欢我！”——这一信念每天都得到证明，于是它越来越坚定。它对农民的善良坚信不疑。鹅没料到，农民在圣诞节会将它从鹅舍里取出并杀掉。这只圣诞鹅成了归纳法思考的牺牲品。大卫·休谟早在18世纪就举过同样的例子，警告人们要小心归纳法。可犯这种错误的不仅是鹅，我们大家都有由观察个体得出普遍适用的结论的倾向。这是危险的。</p><p>归纳法有可能具有诱惑性：“人类一直都是成功的，因此我们也将征服未来的挑战。”听起来不错，但我们没有考虑：只有那些幸存到现在的物种才能这么说。以我们存在的事实来说明将来我们也会存在，这是一个严重的思维错误——估计是最严重的。</p><h2 id="32-规避损失"><a href="#32-规避损失" class="headerlink" title="32. 规避损失"></a>32. 规避损失</h2><p>为什么凶恶的面孔比友善的面孔更容易引起我们注意</p><p>人们害怕失去某种东西的想法要比获得某种同等价值的东西的想法强烈。假定你是为房屋提供隔热层的。假如你告诉顾客，缺少隔热层他们有可能损失多少钱，就要比告诉他们使用好的隔热层他们能够节约多少钱，更能说服他们在房屋里使用隔热层。虽然其实金额是一样的。</p><p>股市上也是一样：投资者们倾向于忽视损失，宁愿继续等待，希望他们的股票重新上涨。被忽视的损失还不算损失。因此他们不卖，即使上涨的希望渺茫、继续回落的概率很大。我曾经认识一个男人，他是一个千万富翁，他因为弄丢了一张100欧元的钞票而生气得要命。这是多大的情感浪费啊！我让他注意这个事实：他的钱包每秒钟都在增减至少100欧元。</p><p>员工（如果他们是独自承担责任，而不是集体作决定的话）都有畏惧风险的倾向。站在他们的立场，这样做是有意义的：如果做成某件事情最多会带给他们一笔奖金，但一旦失败就有可能让他们丢掉工作岗位，那他们干吗要冒这种风险呢？在几乎所有公司的所有情况下，风险都大于可能的收益。如果你作为董事长抱怨你的员工缺少冒险精神，那你现在知道是为什么了——规避损失。</p><p>我们无法改变：恶比善更有影响力。我们对不利东西的反应要比对有利东西的反应敏感。走在大街上，一张凶恶的脸要比一张友善的脸更容易引起我们注意。恶行要比善行更久地留存在我们的记忆里。当然也有例外：在事关我们自己的时候。</p><h2 id="33-社会性懈怠"><a href="#33-社会性懈怠" class="headerlink" title="33. 社会性懈怠"></a>33. 社会性懈怠</h2><p>团队为什么会使人懒</p><p>马克西米利安·林格尔曼是一位法国工程师。1913年，他对马拉车的效率进行调查。他发现：两匹马一起拉一驾马车，效率并非一匹马效率的双倍。这一结果令他意外，遂将他的调查延伸到人类。他让许多人一起拉一根绳子，测量每人释放出的力量。他发现两个人一起拉一根绳子，平均每人只投入其力量的93%，如果是三个人一起拉，每人只投入85%，8个人一起拉时就只剩下49%了</p><p>结论：人们在团队里的行为不同于单独一人的时候（否则就不存在团队了）。可以通过尽可能彰显个人效率，来缓和团队的弊病。</p><h2 id="34-指数增长"><a href="#34-指数增长" class="headerlink" title="34. 指数增长"></a>34. 指数增长</h2><p>一张对折的纸为什么会超出我们的想象</p><p>将一张纸从中间对折，再对折，一直对折下去。对折50次后它会有多厚呢？在你继续阅读之前，请写下你的答案。<br>再看第二个问题。你可以选择：（1）在接下来的30天里我每给你1 000欧元；（2）在接下来的30天里我第一天给你1欧分，第二天2欧分，第三天4欧分，第四天8欧分，以此类推。请你决定：选（1）还是选（2）？</p><p>你做出决定了吗？如果我们假设，一张纸的厚度为0.1毫米，那么对折50次之后它的厚度就是一亿千米。这相当于地球到太阳的距离，使用计算器很容易计算出来。第二个问题选（2）是更合适的，虽然答案（1）听上去更诱人。如果你选（1），30天后你会挣到3万欧元，而（2）是1 000多万欧元。</p><p>我们很容易就能理解线性增长，但我们对指数（或按百分比计算的）增长没有感觉。为什么？因为过去的进化没有让我们为此做好准备。我们祖先的经验大多是线性的。谁花费双倍的时间采摘，谁就会带回家双倍的莓果；谁同时将两只猛犸而不是一只驱逐到地渊上方，谁有肉吃的时间就会是双倍。石器时代几乎没有人遇到过指数增长的例子，但今天不同。</p><p>结论：当事关增长率时，请不要相信你的感觉。你的感觉是没有用的——请你承认这一点。真正有助于你的是计算器，或者，在增长率小时，就使用计算倍增时间的诀窍。<br>￼</p><h2 id="35-赢家的诅咒"><a href="#35-赢家的诅咒" class="headerlink" title="35. 赢家的诅咒"></a>35. 赢家的诅咒</h2><p>你愿意为100欧元支付多少钱？</p><p>20世纪50年代的得克萨斯，有一块地在拍卖，有10家石油公司参与竞拍。每家公司都估计了这块地含有多少石油，价值多少美元。最低估计是1 000万美元，最高为1亿美元。拍卖时出价越高，退出竞价的公司就越多，最终由出价最高的公司成交——看起来它赢了。</p><p>赢家的诅咒是指：拍卖的赢家大多是事实上的输家。行业分析师们发现，常在油田拍卖中胜出的赢家，都系统性地支付过多，并在多年后因此破产。这是可以理解的：如果人们对油田价值的估计在1 000万~1亿美元之间摆动，那么油田的真实价值有可能是中间的某个数。拍卖时的最高报价经常是系统性地过高——除非出价者拥有信息优势。当年在得克萨斯成交的公司并没有信息优势，所以所谓的赢家实际上赢得的是一场皮洛士胜利￼。</p><p>请你记住沃伦·巴菲特的建议：“千万不要参与拍卖。”做不到？你是在一个无法躲开拍卖的行业工作？那你就确定一个最高价，从中扣除20%支付赢家的诅咒效应。请你将这个数字写在一张纸上，然后坚决遵守它。</p><h2 id="36-基本特征谬误"><a href="#36-基本特征谬误" class="headerlink" title="36. 基本特征谬误"></a>36. 基本特征谬误</h2><p>千万别问一位作家他的小说是不是自传</p><p>你打开报纸，看到某位首席执行官因为业绩差不得不辞职。在体育版，你看到你心爱的球队因为某个运动员或某位教练成了冠军。在报刊编辑部有一条规矩：“每个故事都要有人物。”基本特征谬误是指，系统性地高估人的影响，在解释某些东西时低估情境因素。</p><p>结论：不管戏剧多么让我们着迷，舞台上的人绝非孤立的，他们的表演离不开一个个情境。你若真想理解正在表演的戏剧，就请你不要只注重表演者，而是多关注他们的表演或舞蹈。</p><h2 id="37-错误的因果关系"><a href="#37-错误的因果关系" class="headerlink" title="37. 错误的因果关系"></a>37. 错误的因果关系</h2><p>你为什么不该相信仙鹤送子</p><p>错误的因果关系的最好例子是出生率下降和德国的仙鹤配偶数量下降之间的联系。如果画出1965~1987年两条线的发展，它们几乎可以完美地重叠在一起。那么，真的是仙鹤送子吗？不可能。因为这是一个纯属巧合的相互关系，肯定不是因果关系。</p><p>结论：相互关系不等于因果关系。你要看仔细。有时两者之间因果恰恰相反，有时两者之间根本就没有因果关系——就像仙鹤和婴儿一样。</p><h2 id="38-光环效应"><a href="#38-光环效应" class="headerlink" title="38. 光环效应"></a>38. 光环效应</h2><p>长相好的人为什么容易事业有成</p><p>光环效应是指：我们让某一个方面照花了眼睛，并由此推及全貌。“Halo”一词与打招呼毫无关系，而是英语里代表“神圣光环”的词。在思科公司的例子里，光环效应特别明显：记者们让股价照花了眼睛，并由此推及公司的内部质量，而不去进行更仔细的研究。</p><p>光环效应的作用方式总是一样：我们由容易得到的或特别明显的因素，比如一家企业的经济发展形势，自动推论到更难查明的特性，比如管理质量或发展战略。于是，我们倾向于认为一家声誉良好的制造商的产品质量更好，虽然没有客观理由证明此事。而那些在某一个行业成功了的首席执行官们，人们就会认为他们在所有行业都会成功，哪怕是在私生活中他们也必须是模范。</p><p>结论：光环效应挡住了我们的视线，让我们看不到真实的特征。因此你要看仔细。请你排除醒目的特征。世界级交响乐队就是这么选择队员的，他们让选手在一块幕布后面演奏，通过这样做来避免性别、种族或外貌影响他们的评价。我衷心地建议经济记者们，不要靠季度数据来评价一家公司（这已经有股市解决了），而要更深入地挖掘下去。这样挖掘出的东西，并不总是漂亮的，但有时很有教益。</p><h2 id="39-替代途径"><a href="#39-替代途径" class="headerlink" title="39. 替代途径"></a>39. 替代途径</h2><p>恭喜你赢了俄罗斯轮盘赌</p><p>你与一位俄罗斯寡头政治执政者相约在你所在城市郊外的一座森林里见面。那位执政者随身带有：一只箱子和一把左轮手枪。他的箱子里塞得满满的，全是欧元——总共1 000万，齐刷刷的新票子。左轮手枪的枪膛里只有一颗子弹，另外5个弹膛是空的。“有兴趣玩俄罗斯轮盘赌吗？”那位执政者问道，“你只要按一下，这只箱子连同里面的东西就归你所有了。”你开始思考：1 000万欧元会深刻影响你的生活——不再工作！你终于可以收藏赛车而不是邮票了。</p><p>假设你接受挑战，将枪口对着你的太阳穴，按下扳机。你听到轻轻的一声“咔嚓”，感觉肾上腺在你的体内奔涌。没有子弹射出，你活了下来，拿走钱，在法兰克福最漂亮的城区建起一座超大的别墅，你的邻居们非常嫉妒你。</p><p>这些邻居中有一位是个大名鼎鼎的律师，他的房子现在被你的别墅比下去了。他每天工作12小时，一年工作300天。他的小时工资很可观，但也很常见：每小时600欧元。一句话，他每年可以净存50万欧元。你偶尔会站在你的院子里向他打招呼、微笑，你在示意：他必须工作20年，才赶得上你。</p><p>假设20年后你勤快的邻居真的积累了1 000万欧元。他的别墅建在了你的别墅旁边。一位记者经过这里，就居住区“更富裕的”居民作了一篇采访报道。他评论了室内建筑和花园的精致。不过他不知道你俩之间的关键区别：藏在每个1 000万欧元背后的风险。要知道这一点必须了解替代途径——在这一点上，不仅是记者们，我们所有人都不擅长。</p><p>什么是替代途径？替代途径是指，所有同样可能发生但没有发生的事情。在俄罗斯轮盘赌上，我们有4种替代途径可以达到同样的结果（得到1 000万欧元的收益），你死去的可能性占1/5—与前4种达到的结果有着巨大的区别。在律师的例子中，不同替代途径能达到的结果的区别则要小得多。他在一座村庄里也许每小时只挣200欧元，在汉堡市中心和大银行的委托中也许每小时能挣800欧元。可与你不同，没有什么替代途径会让律师丢失他的财产甚至生命。</p><p>替代途径是无形的，因此我们很少会想到它们。任何玩垃圾债券、期权和信用违约互换，挣到数百万欧元的人，都不该忘记，他同时有一堆危险的替代途径，它们会直接将他拖进毁灭。在这么大风险下获得的1 000万欧元，价值远不及通过数年的辛苦劳动挣到的1 000万欧元。不管会计怎样声称1 000万欧元就是1 000万欧元。</p><p>结论：风险从来不是一眼就能看到的。因此，请你时刻考虑你有什么样的替代途径。比起你通过无惊险的平凡途径（比如从事律师、牙医、滑雪教练、飞行员或企业顾问的辛苦工作）获得的成功，别拿通过冒险的替代途径获得的成功太当真。蒙田怎么说来着：“我的生命充满不幸——这些不幸大多没有发生。”</p><h2 id="40-预测的错觉"><a href="#40-预测的错觉" class="headerlink" title="40. 预测的错觉"></a>40. 预测的错觉</h2><p>水晶球如何歪曲了你的目光</p><p>专家们每天都在用他们的预测轰炸我们。这些预测有多可靠呢？直到几年前都没有人花时间验证过它们的准确性。后来菲利普·泰特洛克来了。</p><p>这位伯克利大学的教授分析了总共284位专家在10年内所做的82 361个预言。结果，预测的准确性几乎不及你询问一台随机数字生成器。事实证明，最糟的预测家恰恰是那些媒体关注度最高的专家，尤其是世界灭亡预言家，其中又以瓦解论的代表为最——我们还一直在等待加拿大、尼日利亚、中国、印度、印度尼西亚、南非、比利时和欧盟的瓦解呢（奇怪的是没有专家想到过黎巴嫩）。</p><p>“预言未来的人有两种：一种是一无所知的人，另一种是不知道他们自己一无所知的人。”哈佛经济学家约翰·肯尼思上面的这句话招来了他同行们的憎恨。基金管理人彼得·林奇讲得更加傲慢：“美国有6万多个受过教育的经济学家。他们当中有许多被聘请来预测经济危机的发生时间。他们的预测只要有两次准确，他们就会成为百万富翁。但就我所知，他们中的大多数仍然还只是普通职员。”这是10年前的事。今天美国雇用的经济学家的数量有可能是10年前的3倍，但依然无人能准确地预测出经济危机的发生时间。</p><p>问题是：专家不必为错误预测付出代价——无论是以金钱损失还是以失去好名声的形式。换句话说，社会给了这些人不受约束的预测权利。他们预测错误时没有“坏处”，但如果预测正确，就会获得关注、咨询委托和出版机会等“好处”。由于这种预测的代价为零，我们正经历一场真正的预言膨胀。于是，越来越多的预言纯属巧合地猜中的概率就会上升。理想的情况是，强迫预测家们付钱给一个“预测基金”——比如每个预测1 000欧元，如果预测正确，专家就可以将他的钱连同利息一起拿回；如果预测不正确，这笔钱就交给一个慈善基金会。</p><p>结论：请你对预测持批评态度。我为此训练出了一种条件反射——我会对每个预测报以一笑，以此去除它的装腔作势，随后我会<br>问自己两个问题。第一，这位专家的预测有何约束机制？假如他是雇员，如果他不断出错，他会失去他的工作吗？或者他只是一个自封的预测大师，靠图书和报告增加他的收入？第二，这位专家的预测准确率有多高？他在过去5年里做过多少预测？其中有多少应验了，有多少落空了？我希望媒体在发布所有预测时，一同公布被误以为是大师的人们的成绩证明。<br>最后，我要引用英国前首相托尼·布莱尔的一句话：“我不做预言。我从没做过，我也永远不会做。”</p><h2 id="41-关联谬误"><a href="#41-关联谬误" class="headerlink" title="41. 关联谬误"></a>41. 关联谬误</h2><p>有说服力的故事为什么会误导人</p><p>连专家们也会犯关联谬误。在1982年的一次关于未来石油消耗形势研究大会上，专业人员被分成了两组。丹尼尔·卡尼曼向A组这样预测：“1983年石油消耗会下降30%。”他向B组这样预测：“1983年油价飙升会导致石油消耗下降30%。”受试者必须说出他们认为这种预测实现的可能性有多大。结果一目了然：B组比A组更相信卡尼曼向他们所做的预测。</p><p>卡尼曼由此认为，存在两种思维：一种是直觉、机械、直接的思维；另一种是有意识、理性、缓慢、有逻辑性的思维。只可惜早在有意识的思维开始之前，直觉思维就得出了结论。比如，在2001年9月11日恐怖组织对世贸中心进行袭击之后，我想要购买一份旅行保险。一家聪明的公司利用关联谬误，提供一种专门的“恐怖主义保险”。虽然其他保险公司当时也提供各种意外保障（其中包括恐怖主义袭击），我还是选择了这个专门的保险。最愚蠢的是，我甚至愿意支付比一份普通旅行保险更多的钱来购买这个专门的保险，而普通旅行保险其实也覆盖了这种情况。</p><p>结论：请你忘记“左半脑和右半脑”的说法吧，更重要的是直觉思维和有意识的思维之间的区别。直觉思维偏好可信的故事，作重要决定时不顺从它们对你是有好处的。</p><h2 id="42-框架效应"><a href="#42-框架效应" class="headerlink" title="42. 框架效应"></a>42. 框架效应</h2><p>言为心声</p><p>同样的事情可以有多种表达方式，例如你可以说：“嗨，垃圾桶满了！”或者：“宝贝，你要是能赶紧倒掉垃圾桶，那就太好了。”言为心声，同样的内容，换个不同的说法，效果完全不同。心理学里称之为框架效应。</p><p>框架效应是指：视表达方式的不同，我们会对同样的事情做出不同的反应。丹尼尔·卡尼曼于2002年荣获了诺贝尔奖。20世纪80年代，他与他的同事阿莫斯·特沃斯基一起进行了一次调查。他们拿出了两种消灭瘟疫的方案以供选择。在600人的生命受到威胁的情况下，选择方案A能救200人的性命；选择方案B则有1/3的可能让600人全部获救，有2/3的可能谁也救不了。被问者多数选方案A——自然依据的是这条原则：手中的麻雀胜过屋顶的鸽子。有意思的是，当换了种说法之后：选择方案A会死400人；选择方案B则有1/3的可能无人会死，有2/3的可能600人全都会死，现在选方案A的人只剩下一小部分了，绝大多数人都选方案B。刚好与第一次调查相反。根据表达方式的不同——救命或者死亡——被问者对同样的事实做出了截然不同的选择。</p><p>美化是框架效应特别喜欢采用的方式之一。股价下跌被称为“回调”。支付过高的收购价被称作“善意”。我们在每堂管理学课上都会学到，问题不是“问题”，而是“机会”。一位被开除的经理是某个“开始新生活”的人。一名阵亡的士兵——不管是因为倒霉或愚蠢导致他死去的——是个“战争英雄”。大屠杀是“种族清洗”。成功的迫降，比如迫降在纽约的哈得孙河上，被欢呼为“飞行的胜利”。（如果不迫降就不是胜利吗？）</p><p>结论：你要意识到，没有框架效应，你什么也不能描述，每个事实——不管你是从一位朋友那儿听到的，还是在一份严肃的报刊上读到的——都会受到框架效应的影响。本章也不例外。</p><h2 id="43-行动偏误"><a href="#43-行动偏误" class="headerlink" title="43. 行动偏误"></a>43. 行动偏误</h2><p>为什么不行动光等待是种痛苦</p><p>足球运动员罚点球，有1/3的概率是射向球门中央，1/3的概率射向左边，1/3的概率射向右边。守门员会怎么办？他们或者扑向左，或者扑向右。反正他们很少待在中间——虽然有1/3的球会射向那里。这是为什么？因为扑向错误的一侧要比傻瓜似的呆立原地，看着球从左边或右边飞过去好看得多，看起来也没那么难堪。这就是行动偏误：即使毫无用处，也要采取行动。</p><p>特别是在遇到新情形或不明情形时，就会发生行动偏误。许多投资者表现得都像夜总会门外没有经验的年轻警察那样：他们还不能正确分析股市的活动，就采取过激行动。这当然不值得。沃伦·巴菲特这样表述：“在投资时，行动与成绩没有相互关系。”</p><p>结论：在不明情形下我们会产生要做点什么的冲动，随便什么——不管它有没有帮助。之后我们会感觉好受些，虽然其实什么也没有好转——事实甚至往往正好相反。因此，如果情况不明，请你不要采取任何行动，直到你能更好地分析形势。你要克制自己。“人类的全部不幸就是他们不能安静地待在他们的房间里。”帕斯卡尔就曾经这么写道，在他的书房里</p><h2 id="44-不作为偏误"><a href="#44-不作为偏误" class="headerlink" title="44. 不作为偏误"></a>44. 不作为偏误</h2><p>为什么你不是答案就是问题</p><p>冰川缝隙里，你本来可以帮助他、营救他，但你没有这么做，结果他死了。你将第二个人主动推进冰川缝隙，不久后第二个人也死了。这两种行为哪种更严重呢？理性地看，这两种行为同样应该遭到谴责，无论是放弃不救还是主动谋杀——两者的结果都是死亡。但某种感觉告诉我们，放弃不救不如后者严重。这种思维错误人们称为不作为偏误。不作为偏误总是出现在无论放弃还是行动都会带来损害的地方。在这种情况下大多数人都会选择放弃，因为这样引起的损害主观看来更无害。</p><p>上一章我们了解了行动偏误。行动偏误是不作为偏误的反面吗？不完全是。当形势不明、矛盾、看不透时，行动偏误就会插手了。此时我们倾向于做无用功，哪怕没有这么做的合理理由。而不作为偏误的形势大多是一目了然的：今天的行为可以防止未来的损害，但防止损害对我们的激励并不强。</p><p>不作为偏误很难辨认——放弃行动不像采取行动那么容易看出。我们不得不承认，1968年的欧洲学生运动看穿了不作为偏误，提出了一个醒目的口号来反对它：“如果你不是答案的一部分，你就是问题的一部分。”￼</p><h2 id="45-自利偏误"><a href="#45-自利偏误" class="headerlink" title="45. 自利偏误"></a>45. 自利偏误</h2><p>你为什么从不自责</p><p>我住的公寓里有套合租房，由5名大学生合租。我有时会在电梯里遇到其中的一位。我单独问这些小伙子中的每一位，他多久会将合租房里的垃圾拿出去。一个说：“每两次我会拿一次。”另一个说：“每三次。”还有一个骂骂咧咧，因为我正巧碰见他拎着鼓鼓的垃圾袋：“可以说总是我拿，”90%吧。虽然所有答案加起来应该是100%，但它们加起来却是320%！这些合租者系统性地高估了他们的作用——在这方面他们与我们所有人没有区别。婚姻里也是如此：科学证明，不管是男人还是女人，他们对自己为伴侣关系正常运转所做贡献的评价都高于50%。</p><p>你阅读公司年报——特别是首席执行官的报告吗？不读？很可惜，因为那里泛滥着一种偏误的例子，我们都以某种形式犯过这种偏误。这种思维错误是这样的：如果公司这一年经营出色，首席执行官就会将其归因于他英明的决定、他本人不知疲倦的奉献和他所宣扬的活跃的企业文化。相反，如果公司这一年经营不善，责任就都是欧元走强、政府的政策、中国人的贸易活动、美国人的隐形关税，还有消费者的压抑情绪等。<strong>成功归于自己，失败归于外因。这就是自利偏误</strong>。</p><blockquote><p>其实就是甩锅</p></blockquote><p>如何应对自利偏误呢？你有对你直言不讳的朋友吗？如果有，你很幸运。如果没有，那你至少有个死对头吧？好，那你就挑战一下自己，请他喝咖啡，请他不加掩饰地说出对你个人的意见。你会永远感激他的。</p><h2 id="46-享乐适应症"><a href="#46-享乐适应症" class="headerlink" title="46. 享乐适应症"></a>46. 享乐适应症</h2><p>你为什么应该缩短上班路程</p><p>在之前的一章里我们探讨了政治、经济和社会领域的预测准确性有多低，我们发现专家们的工作并不比一个随机数字生成器的工作强多少。我们在预测我们自己的感觉时有多准确呢？中1 000万欧元的彩票会让你幸福很多年吗？哈佛心理学家丹·吉尔伯特对这些中奖者进行了调查，发现这种幸福感平均三个月后就烟消云散了。在银行汇来大单三个月后，你的感觉会与中奖前一样。</p><p>我有一位朋友是一家银行的经理，靠彩票中奖获得了巨额的收入，他决定搬出城市，在苏黎世城外建座房屋。他最终拥有了一幢有10个房间、外带游泳池的别墅，令人妒忌地可以眺望湖泊和群山。头几个星期他喜形于色，但不久人们就看不出他的兴奋了，6个月后他前所未有地不快乐。发生什么事了？幸福感在三个月后烟消云散了，别墅带来的幸福感也没多到哪儿去。“我工作回到家里，推开门，再也感觉不到这是怎样一幢房子了。我的感觉与我上大学时走进寝室的感觉没什么区别。”而与此同时，这个可怜的家伙现在平均每天必须花50分钟在上班的路上。研究证明，驾驶汽车的往返交通引起的不满最多，人们几乎无法适应。谁都不是天生喜欢来来往往，每天受罪。不管怎样，别墅对我朋友的快乐的净效应，怎么说都加不了什么分。</p><p>其他人的情况也不比他好。事业上迈进了一步的人在平均三个月后的幸福感又与先前一样了。就连那些非要驾驶最新款保时捷的人也一样。科学里称这一效应为享乐适应症：我们工作、升迁，给自己购买更多更漂亮的东西，但我们不会变得更幸福。<br>命运不好的人是怎样的呢——比如半身瘫痪或失去一位朋友，在这种情况下，我们也会系统性地高估负面情绪的持续时间和强度。当一段恋情破裂时，世界就崩溃了。受尽折磨者坚信，他们永远不会再感觉到哪怕一丝幸福了——但平均三个月之后他们就又快活起来了。</p><p>要是我们能够准确地知道，一辆新车、一个新事业、一段新恋情会让我们多么幸福，是不是就好了呢？那样我们做决定时就可以更加明确，也不会不停地暗中摸索了。是的，那样可能就好了。这里有一些科学的建议：（1）请你避免很长时间也不会习惯的负面效应，例如往返交通、噪声、慢性疲累等；（2）请你对物质的东西只期待短期效果，例如汽车、房屋、分红、中彩票、得金奖等；（3）持续的正面效应主要与你如何利用你的时间有关。你要设法让自己得到尽可能多的自由时间和自主权。请你做你最爱做的事情——哪怕你要付出部分收入。请你为友谊投资。对于女人，隆胸具有长期的幸福效应；对于男人，则是升职——不过，只有当男人不与此同时更换对比群体时才会感到幸福。因此，如果你在升为首席执行官之后只跟其他的首席执行官们交谈，幸福感就会消失。</p><h2 id="47-自我选择偏误"><a href="#47-自我选择偏误" class="headerlink" title="47. 自我选择偏误"></a>47. 自我选择偏误</h2><p>请不要惊讶有你存在</p><p>在从巴塞尔通往法兰克福的5号高速公路上我遭遇了塞车。“见鬼，怎么老是我？”我骂骂咧咧，望向相反车道，那里的汽车正以令人妒忌的速度向南行驶。我以蜗牛般的速度在空挡和一挡之间切换了整整一个小时。我问自己，我是否真是这么一个特别可怜的家伙，我真是大多数时候都在几乎不能动弹的地方（银行、邮局、商店）站在柜台前排队吗？或者这只是我的错觉？假设在巴塞尔和法兰克福之间有10%的时间都会出现塞车，我在某一天遭遇塞车的概率不比出现塞车的概率大，也就是10%。但我在行程的某个特定时刻果真陷入塞车的概率要大于10%，因为我在塞车时只能蜗牛爬行似的移动，在塞车中度过的时间多得超过了比例。再加上，如果交通顺利，我就不会浪费念头去想这回事；可一旦被困在里面，塞车就引起了我的注意。</p><p>当我们本身是样品的一部分时，我们必须注意，不要掉进一种以自我选择偏误著称的思维错误的陷阱中。我的男性朋友经常抱怨说他们公司里女性太少，我的女性朋友则经常抱怨在她们公司里工作的男性太少。这与倒霉毫无关系，因为抱怨者是样品的一部分。任何一个男人在一个以男性居多的行业里工作的概率都很高。女人也一样。放大尺度来看：如果你住在一个男性或女性大量过剩的国家（比如中国及俄罗斯），你就有较大可能属于过剩的性别，相应地你会感到恼火。选举时你选择最大的党派候选人的概率最大，表决时你的选票与获胜多数的选票相符的概率最高。</p><h2 id="48-联想偏误"><a href="#48-联想偏误" class="headerlink" title="48. 联想偏误"></a>48. 联想偏误</h2><p>为什么经验有时让人变蠢</p><p>凯文已经第三次向董事会汇报他的主管范围的成就了。每次都很完美，每次他都会穿着他印有绿色圆点的内裤。显然，他想：这是我的幸运内裤。</p><p>珠宝店里的女营业员是那么漂亮，凯文不好意思拒绝，只能买下她随便拿给他看的10 000欧元的订婚戒指。10 000欧元——远远高出他的预算，但潜意识里凯文将这枚戒指与女营业员的美貌联系在了一起。他想，他未来的妻子戴上它同样会光彩照人。</p><p>每年凯文都会去医生那儿进行一次综合性体检。医生大多是向他证明，他，凯文，相对于他的年龄（44岁），“还相当健康”。至今他只有两次是带着令人震惊的诊断离开医院的。一次是必须迅速动手术切除盲肠，另一次是前列腺肿大，幸好后续检查时证明了那不是癌症，而是炎症。当凯文这两次离开医院时，他当然有些失控——两次都是特别热的天气。从此，每当阳光火辣辣的，他就会感觉不舒服。如果约好见医生的那天很热，他就会临时取消。</p><p>我们的大脑是一部联想机器。原则上这样也很好：我们食用一种陌生果子，食后我们感觉不舒服，于是将来我们就会回避相应的植物，认为它的果子有毒或至少是吃不得的。知识就是由此形成的。</p><p>只是，错误的知识也是这么形成的。最早研究此事的是前苏联生理学家巴甫洛夫。他原本只想测量狗过量的唾液分泌。研究程序是这样的：在给狗喂食之前，先摇响一只小铃。不久，光摇铃就足以让狗产生唾液。它们将功能上彼此毫无关系的两样东西联结在了一起——铃响和产生唾液。</p><p>巴甫洛夫的方法也同样适用于人类。广告将产品与积极的情感联系在了一起。因此你永远不会看到可口可乐与一张不满的脸或一个苍老的身体出现在一起。喝可口可乐的人总是年轻、漂亮、无比快乐。</p><p>我们从中可以学到什么？没有谁讲得比马克·吐温更贴切了：“我们应该注意，一个经历里隐藏着多少智慧，我们就只吸取多少——不要多；好让我们不像坐过热灶台的猫一样。被烫过的猫永远不会再坐到热灶台上去——这是对的；但它也永远不会再坐到冷灶台上去了。”</p><h2 id="49-新手的运气"><a href="#49-新手的运气" class="headerlink" title="49. 新手的运气"></a>49. 新手的运气</h2><p>假如开始时一切顺利，请务必多加小心</p><p>下面是联想偏误的一种特殊情况：错误地与从前的成功建立联系。赌场的赌客熟悉这一点，他们称这是新手的运气。在游戏的前几轮就输掉的人会倾向于退出游戏；而赢钱的人，就倾向于继续玩下去。这个幸运儿坚信自己拥有超过平均水平的能力，于是他会加大赌注——后来他一下子就变成了倒霉蛋，也就是在概率“正常化”的时候。</p><p>新手的运气在经济生活里扮演着重要角色：A公司买下了较小的B、C、D公司。由于每次收购都很成功，A公司领导信心倍增，相信自己擅长收购公司。因为受到鼓舞，A公司又买下了比它大得多的E公司。事实证明，这次收购是个灾难。清醒地看，这本是能够预料到的，但A公司被新手的运气照花了眼睛。</p><p>从哪一刻开始就不再是新手的运气，而是天才呢？没有明确的分界，但有两条线索。第一，如果你长期比其他人成功，你可以认为，自己的才华可能起到作用，但你绝不能过于自信、自满。第二，参与的人越多，某人出于纯粹的运气长期成功的概率就越大。也许你就是这个某人。如果你在一个只有10名竞争者的市场上脱颖而出，这说明你有一定的才华。如果你在一个拥有千万名竞争者的市场上成功了，你就不应该太骄傲（比如说在金融市场上）。这种情况下你应该认为，你只是很幸运。</p><p>不管怎样，请你不要急着做出判断。新手的运气有可能是灾难性的。请你像一位科学家那样，武装自己，以防自欺欺人。请测试你的看法，请你试着证明它们是错的。当我写完我的长篇小说处女作《三十五》时，我只将它寄给了一家出版社：Diogenes出版社。它立即就被接受了。有一段时间我感觉自己是个天才，是文坛黑马（一部主动寄去的手稿在Diogenes出版社得到出版的机会为1∶15 000）。在我签好出版合同之后，为了测试，我将手稿又寄给了另外10家大型畅销书出版社。10家出版社通通拒绝了我。我的“天才理论”被证伪了——这又让我回到了现实。</p><h2 id="50-认知失调"><a href="#50-认知失调" class="headerlink" title="50. 认知失调"></a>50. 认知失调</h2><p>你如何撒点小谎，让自己感觉好一些</p><p>一只狐狸偷偷地靠近一棵葡萄树，渴望地盯着树上熟得发紫的大葡萄。它拿前爪撑着树干，伸长脖子，想摘几串葡萄，可葡萄太高了。它恼怒地想再次试试它的运气，它纵身跃起，但扑了个空。第三回它用尽全身的力气一跳——又扑空了，它背朝下摔在地上，而葡萄树连一片叶子都没有动一下。狐狸耸耸鼻子，说：“我觉得它们还没熟透，我不喜欢酸葡萄。”它骄傲地昂首走回了森林里。古希腊寓言家伊索的这则寓言描绘了最常见的思维错误之一，也就是狐狸曾经的打算与结果不符。狐狸有三种方式可以缓和这恼人的矛盾（失调）：（1）它最终还是想办法摘到了葡萄；（2）它承认自己的能力不够，摘不到葡萄；（3）事后做出别的解释。最后一种情况人们称之为认知失调。</p><p>当我不久前必须在两只股票之间作选择时，我的反应十分相似。我买的那只股票不久后就狂跌不止，而另一只暴涨了。我的选择太蠢了，但我不能承认这个错误。相反，我清楚地记得，我一本正经地试图向一位朋友说明，我买的这只股票虽然眼下有点弱，但它比另一只“更有潜力”。这是一种极不理性的自欺欺人，只能用认知失调来解释。因为，如果我耐心等候，不急着购买，一直花时间操作另一只表现好的股票的话，那只股票的“潜力”会更大。我的朋友给我讲了那则伊索寓言。“你还可以尽情地扮演那只狡猾的狐狸——因为你没有吃到葡萄。”</p><h2 id="51-双曲贴现"><a href="#51-双曲贴现" class="headerlink" title="51. 双曲贴现"></a>51. 双曲贴现</h2><p>及时行乐——但请只限于星期天</p><p>你是知道这句话的：“享受每一天，仿佛那是你的末日。”它在每本生活杂志里至少会出现三回，属于那些生活帮助手册的标准保留节目，但这并没有让这句话变得更聪明。你想想，从今天起你不再刷牙、不再洗发、不再打扫房间、放下工作不做、不再支付账单——你很快就会贫穷、生病，甚至会进监狱。但这句话表达了一种深深的向往，对及时行乐的向往。在幸存到今天的所有拉丁文名言中，“及时行乐”恐怕是最受人们喜爱的：享受今天，充分享受，莫管明天。及时行乐对我们很有价值。价值是多少？多得我们无法理性地说明理由。</p><p>你是宁愿在一年后得到1 000欧元还是在一年零一个月后得到1 100欧元呢？如果你像大多数人那样思考，你会选择在13个月后得到1 100欧元。这是有意义的，因为其他地方不会有10%的月息（或120%的年息）。这利息远远弥补了你多等一个月会冒的风险。</p><p>第二个问题：你是宁愿今天拿到1 000欧元还是一个月后拿到1 100欧元呢？如果你像大多数人那样思考，你会决定今天拿到1 000欧元。这令人惊讶。在以上两种情形下你同样都是必须多坚持一个月，为了多得100欧元。在第一种情形下你会对自己说：我既然已经等了一年，那我也可以再多等一个月。在第二种情形下则不是。因此，随着时间长度的不同，我们做出的决定是不一致的。科学里称这一现象为双曲贴现。意思是：一个决定离现在越近，我们的“情感利息”就越多。<br>极少有经济学家能理解，我们主观上在预期不同的利率。经济学家的模式是建立在固定利率的基础上的，是不适用于以上情况的。</p><p>双曲贴现，也就是我们受及时行乐的想法控制的事实，是我们过去的动物性的一种残留。动物不愿意为在将来得到更多奖励而于今天放弃一种奖励。你可以随意训练老鼠，但它们绝不会为了明天得到两块奶酪而放弃今天的一块奶酪。（不过，你说小松鼠会埋起榛子？这纯属本能，事实证明，这种行为与控制冲动毫无关系。）</p><p>孩子们会怎么样呢？沃尔特·米舍尔在20世纪60年代就延迟满足做过一次著名的试验。在YouTube网站上可以找到这段美妙的录像，名为“棉花糖试验”。他将一块棉花糖（甜食）放在一群4岁的小男孩面前，让他们选择要么立即吃掉，要么，如果他们愿意等上几分钟，不吃第一块，就会再得到一块。惊人的是，只有极少数孩子愿意等。更惊人的是，米舍尔发现，是否拥有延迟满足的能力是他们后来事业是否成功的一个可靠的指示器。</p><p>我们年纪越大，建立的自我控制越多，我们就越容易成功地延迟满足。为了得到额外的100欧元，我们乐意在等候了12个月后再多等一个月。可是，如果我们今天能得到一份奖励，要让我们愿意推迟得到它，利息必须很高。这方面的最好证明是信用卡欠款和其他短期消费贷款的高利息。</p><p>结论：及时行乐的诱惑力极大——尽管如此，双曲贴现也是一种思维错误。我们越能控制我们的冲动，我们就越能成功地规避这一错误。我们对我们的冲动控制越小——比如在酒精的影响下——我们就越容易犯这个错误。及时行乐是个好主意——如果每星期一次的话。但天天享受，好像每天都是末日似的，却是不明智的。</p>]]></content>
      
      
      <categories>
          
          <category> Literature </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《被讨厌的勇气》</title>
      <link href="/2021/09/11/the-courage-to-be-disliked/"/>
      <url>/2021/09/11/the-courage-to-be-disliked/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>提出论点：“<code>人可以改变，而且人人都可以获得幸福。</code>”</p><p>青年：世界是简单的，人生也是如此。假若这种命题中含有几分真理，那也是对于孩子的世界而言。孩子的世界没有劳动或纳税之类的现实义务，他们每天都在父母或社会的呵护下自由自在地生活，未来充满无限希望，自己也似乎无所不能。孩子们的眼睛被遮盖了，不必去面对丑恶的现实。的确，孩子眼中的世界呈现出简单的姿态。</p><p>但是，随着年龄的增长，世界便逐渐露出真面目。人们不得不接受“我只不过如此”之类的现实，原以为等候在人生路上的一切“<code>可能</code>”都会变成“<code>不可能</code>”。幸福的浪漫主义季节转瞬即逝，残酷的现实主义时代终将到来。</p><p>人一旦长大，就会被复杂的人际关系所困扰，被诸多的责任所牵绊。工作、家庭或者社会责任，一切都是。当然，孩提时代无法理解的歧视、战争或阶级之类的各种社会问题也会摆在你眼前，不容忽视。这些都没错吧？</p><blockquote><p>青年观点太过悲观，不认同</p></blockquote><p>哲人：人并不是住在客观的世界，<strong>而是住在自己营造的主观世界里</strong>。你所看到的世界不同于我所看到的世界，而且恐怕是不可能与任何人共有的世界。</p><blockquote><p>强烈赞同，井水一年四季都是18°，冬天喝觉得温暖，夏天喝觉得凉爽。</p></blockquote><h2 id="第一夜-我们的不幸是谁的错？"><a href="#第一夜-我们的不幸是谁的错？" class="headerlink" title="第一夜 我们的不幸是谁的错？"></a>第一夜 我们的不幸是谁的错？</h2><p>心理学三巨头 ： 弗洛伊德、阿德勒、荣格</p><h3 id="原因论和目的论"><a href="#原因论和目的论" class="headerlink" title="原因论和目的论"></a>原因论和目的论</h3><p>哲人：如果一味地关注过去的原因，企图仅仅靠原因去解释事物，那就会陷入“<code>决定论</code>”。也就是说，最终会得出这样的结论：我们的现在甚至未来全部都由过去的事情所决定，而且根本无法改变。是这样吧？</p><p>阿德勒心理学考虑的不是过去的“<code>原因</code>”，而是现在的“<code>目的</code>”。</p><p>哲人：当然。否定我们人类的自由意志、把人看作机器一样的存在，这是弗洛伊德式的原因论。</p><p>引出“<code>原因论</code>”和“<code>目的论</code>”</p><p>原因论/决定论：因为过去在父母虐待中长大的，所以有不安的情绪，然后朋友闭门不出。因为过去的事情决定了现在的一切。<br>目的论：你朋友的因为不想出去，所以才给自己制造了不安的情绪。</p><blockquote><p>第一次看到这个观点，的确比较新奇。个人觉得原因论和和目的论都有一定的道理，但是目的论比原因论更正向一些，能人改变的更好</p><p>如果一味地关注过去的原因，企图仅仅靠原因去解释事物，那就会陷入“决定论”。也就是说，最终会得出这样的结论：我们的现在甚至未来全部都由过去的事情所决定，而且根本无法改变。这就是 “原因论”让人参数负面情绪的原因</p><p>相反，目的论告诉我们，我们现在的状态都是我们自己选择的，只要我们想去改变，不过过去多么不堪，我们还是有机会改变让自己的活的更好。 </p></blockquote><h3 id="心理创伤并不存在"><a href="#心理创伤并不存在" class="headerlink" title="心理创伤并不存在"></a>心理创伤并不存在</h3><p>阿德勒在否定心理创伤学说的时候说了下面这段话：“<strong>任何经历本身并不是成功或者失败的原因</strong>。我们并非因为自身经历中的刺激——所谓的心理创伤——而痛苦，事实上我们会从经历中发现符合自己目的的因素。决定我们自身的不是过去的经历，而是我们自己赋予经历的意义。”</p><h3 id="愤怒都是捏造出来的"><a href="#愤怒都是捏造出来的" class="headerlink" title="愤怒都是捏造出来的"></a>愤怒都是捏造出来的</h3><p>哲人：这很简单。你并不是“<code>受怒气支配而大发雷霆</code>”，完全是“<code>为了大发雷霆而制造怒气</code>”。也就是说，为了达到大发雷霆这个目的而制造出来愤怒的感情</p><h3 id="你想“变成别人”吗？"><a href="#你想“变成别人”吗？" class="headerlink" title="你想“变成别人”吗？"></a>你想“变成别人”吗？</h3><p>哲人：我还要再次引用阿德勒的话。他这么说：“<strong>重要的不是被给予了什么，而是如何去利用被给予的东西。</strong>”你之所以想要变成<code>Y</code>或者其他什么人，就是因为你只一味关注着“<code>被给予了什么</code>”。其实，你不应该这样，而是应该把注意力放在“<code>如何利用被给予的东西</code>”这一点上。</p><blockquote><p>这与“ <code>决定我们自身的不是过去的经历，而是我们自己赋予经历的意义。</code>”的观点是一致的。我们不能沉溺于过去的幸福或不幸，人都是要往前看的对吧，应该想想怎么把资源整合了最大化～</p><p>有多少人出生就拥有一手好牌却打成稀巴烂，大部分人的人生成就，还在于如何进行有限的资源利用最大化。</p></blockquote><h3 id="你的不幸，皆是自己“选择”的"><a href="#你的不幸，皆是自己“选择”的" class="headerlink" title="你的不幸，皆是自己“选择”的"></a>你的不幸，皆是自己“选择”的</h3><p>哲人：<strong>在希腊语中，“善”这一词语并不包含道德含义，仅仅有“有好处”这一层含义；另一方面，“恶”这一词语也有“无好处”的意义。这个世界上充斥着违法或犯罪之类的种种恶行。但是，纯粹意义上想要做“恶=没好处的事”的人根本没有</strong>。</p><blockquote><p>“<code>善</code>”这个词并不包含道德的含义，人很多“<code>善行</code>”只是为了自己心里上更好受一些</p></blockquote><p>哲人：你在人生的某个阶段里选择了“<code>不幸</code>”。这既不是因为你生在了不幸的环境中，也不是因为你陷入了不幸的境地中，而是因为你认为“<code>不幸</code>”对你自身而言是一种“<code>善</code>”。</p><p>哲人：某人如何看“<code>世界</code>”，又如何看“<code>自己</code>”，把这些“<code>赋予意义的方式</code>”汇集起来的概念就可以理解为<strong>生活方式</strong>。从狭义上来讲可以理解为性格；从广义上来说，这个词甚至包含了某人的世界观或人生观。</p><p>哲人：当然，并不是有意地选择了“<code>这样的我</code>”，最初的选择也许是无意识的行为。并且，在选择的时候，你再三提到的外部因素，也就是人种、国籍、文化或者家庭环境之类的因素也会产生很大的影响。即便如此，选择了“<code>这样的我</code>”的还是你自己</p><p>哲人：人时常在选择着自己的生活方式，即使像现在这样促膝而谈的瞬间也在进行着选择。你把自己说成不幸的人，还说想要马上改变，甚至说想要变成别人。尽管如此还是没能改变，这是为什么呢？那是因为你在不断地下着不改变自己生活方式的决心。</p><p>哲人：是的，阿德勒心理学就是勇气心理学。<strong>你之所以不幸并不是因为过去或者环境，更不是因为能力不足，你只不过是缺乏“勇气”，可以说是缺乏“获得幸福的勇气</strong>”。</p><blockquote><p>这种勇气绝不是一时兴起，三分热度，改变我们的生活看起来很简单，实则跳出舒适圈很难，我们要时常给自己鼓起并充满对未来的美好向往，相信自己无论处于什么情况下都能游刃有余，而不是随波逐流</p></blockquote><h3 id="你的人生取决于“当下”"><a href="#你的人生取决于“当下”" class="headerlink" title="你的人生取决于“当下”"></a>你的人生取决于“当下”</h3><p>哲人：不，不是定罪。阿德勒的目的论是说：<strong>“无论之前的人生发生过什么，都对今后的人生如何度过没有影响。”决定自己人生的是活在“此时此刻”的你自己。</strong></p><blockquote><p>【转】三个层次：<br>第一层次的人不认为当下的我有选择的自由，因此未来的一切都是过去剧情的延续；</p><p>第二层次的人认为当下的我有选择的自由，但是面对的选项却是由过去造成的，因此我只能在受限于过去的有限可能性中做出选择；</p><p>第三层次的人认为当下的我面对的是无穷多的选项，因为我有做出“<code>改变</code>”的自由，而不仅仅是做出“<code>选择</code>”的自由。这种立足当下绝对的自由观是阿德勒哲学的精髓。</p></blockquote><h2 id="第二夜-一切烦恼都来自人际关系"><a href="#第二夜-一切烦恼都来自人际关系" class="headerlink" title="第二夜 一切烦恼都来自人际关系"></a>第二夜 一切烦恼都来自人际关系</h2><h3 id="为什么讨厌自己？"><a href="#为什么讨厌自己？" class="headerlink" title="为什么讨厌自己？"></a>为什么讨厌自己？</h3><blockquote><p>有时候会讨厌没用的自己，不争气的自己，喜欢理想的自己。</p></blockquote><p>青年：啊！这个恶魔！你简直是一个恶魔！是的，就是这样！我很害怕，不想在人际关系中受伤，非常害怕自己被人拒绝和否定！我承认的确如此</p><p>哲人：承认就是很了不起的态度。但是，请你不要忘记，在人际关系中根本不可能不受伤。只要涉入人际关系就会或大或小地受伤，也会伤害别人。阿德勒曾说“<code>要想消除烦恼，只有一个人在宇宙中生存</code>”。但是，那种事情根本就无法做到。</p><h3 id="一切烦恼都是人际关系的烦恼"><a href="#一切烦恼都是人际关系的烦恼" class="headerlink" title="一切烦恼都是人际关系的烦恼"></a>一切烦恼都是人际关系的烦恼</h3><blockquote><p>其实不太认同这个观点，作为程序员，有时候其实只要简单写好代码做好自己做的事情就行了。</p></blockquote><p>青年：我说的不是这个问题！人际关系的确是一个很大的问题，这一点我也认可。但是，一切烦恼皆源于人际关系这种论调也太极端了！独立于人际关系之外的烦恼、个体内心的苦闷、自我难解的苦恼等，难道您要否定这一切烦恼吗？！</p><p>哲人：仅止于个人的烦恼，即所谓的“<code>内部烦恼</code>”根本不存在。任何烦恼中都会有他人的因素。</p><h3 id="自卑感来自主观的臆造"><a href="#自卑感来自主观的臆造" class="headerlink" title="自卑感来自主观的臆造"></a>自卑感来自主观的臆造</h3><p>哲人：我对自己身高的感觉终究还是在与他人的比较——也就是人际关系——中产生的一种主观上的“<code>自卑感</code>”。如果没有可以比较的他人存在，我也就不会认为自己太矮。你现在也有各种“<code>自卑感</code>”并深受其苦吧？但是，那并不是客观上的“<code>劣等性</code>”，而是主观上的“<code>自卑感</code>”。即使像身高这样的问题也可以进行主观性的还原。</p><p>青年：也就是说，困扰我们的自卑感不是“<code>客观性的事实</code>”而是“<code>主观性的解释</code>”？</p><p>哲人：价值必须建立在社会意义之上。即使1美元纸币所承载的价值是一种常识（共通感觉），那它也不是客观意义上的价值。如果从印刷成本考虑的话，它根本不等于1美元。</p><p>如果这个世界上只有我一个人存在，那我也许会把这<code>1</code>美元的纸币放入壁炉当燃料或者当卫生纸用。同样的道理，我自然也就不会再为自己的身高而苦恼。</p><h3 id="自卑情结只是一种借口"><a href="#自卑情结只是一种借口" class="headerlink" title="自卑情结只是一种借口"></a>自卑情结只是一种借口</h3><p>哲人：阿德勒也承认自卑感人人都有。自卑感本身并不是什么坏事。</p><p>哲人：这需要从头说起。首先，人是作为一种无力的存在活在这个世界上。并且，人希望摆脱这种无力状态，继而就有了普遍欲求。阿德勒称其为“<code>追求优越性</code>”。</p><p>哲人：在这里，你可以简单将其理解为“<code>希望进步</code>”或者“<code>追求理想状态</code>”。例如，蹒跚学步的孩子学会独自站立；他们学会语言，可以与周围的人自由沟通。我们都有想要摆脱无力状态、追求进步的普遍欲求。人类史上的科学进步也是“<code>追求优越性</code>”的结果。</p><p>哲人：与此相对应的就是自卑感。人都处于追求优越性这一“<code>希望进步的状态</code>”之中，树立某些理想或目标并努力为之奋斗。同时，对于无法达成理想的自己就会产生一种自卑感。例如，越是有远大志向的厨师也许就越会产生“<code>还很不熟练</code>”或者“<code>必须做出更好的料理</code>”之类的自卑感。</p><p>哲人：这一点请注意。目前“<code>自卑情结</code>”这个词似乎在使用的时候与自卑感是一样的意思。就像“<code>我为自己的单眼皮感到自卑</code>”或者“<code>他对自己的学历有自卑感</code>”之类的描述中全都用“<code>自卑情结</code>”这个词来表示自卑感。其实，这完全是一种误用。自卑情结一词原本表示的是一种复杂而反常的心理状态，跟自卑感没有关系。例如，即使弗洛伊德提出的“<code>俄狄浦斯情结</code>”原本也是指一种对同性父母亲的反常对抗心理。</p><p>哲人：<strong>自卑感本身并不是坏事。这一点你能够理解吧？就像阿德勒说过的那样，自卑感也可以成为促成努力和进步的契机。例如，虽然对学历抱有自卑感，但若是正因为如此，才下定“我学历低所以更要付出加倍的努力”之类的决心，那反而成了好事。<br>而另一方面，自卑情结是指把自己的自卑感当作某种借口使用的状态。具体就像“我因为学历低所以无法成功”或者“我因为长得不漂亮所以结不了婚”之类的想法。像这样在日常生活中大肆宣扬“因为有A所以才做不到B”这样的理论，这已经超出了自卑感的范畴，它是一种自卑情结</strong>。</p><blockquote><p>要区分“<code>自卑感</code>”和“<code>自卑情节</code>”</p><p>自卑感是一种超越自我的动力</p><p>自卑情结则是用来逃避失败的借口</p></blockquote><h3 id="越自负的人越自卑"><a href="#越自负的人越自卑" class="headerlink" title="越自负的人越自卑"></a>越自负的人越自卑</h3><p>哲人：是的。关于自卑感，阿德勒指出“<code>没有人能够长期忍受自卑感</code>”。也就是说，自卑感虽然人人都有，但它沉重得没人能够一直忍受这种状态。</p><p>青年：嗯？这好像有点乱啊？！</p><p>哲人：请你慢慢去理解。拥有自卑感即感觉目前的“<code>我</code>”有所欠缺的状态。如此一来问题就在于……</p><p>青年：如何去弥补欠缺的部分，对吧？</p><p>哲人：正是如此。如何去弥补自己欠缺的部分呢？最健全的姿态应该是想要通过努力和成长去弥补欠缺部分，例如刻苦学习、勤奋练习、努力工作等。</p><p>但是，没有这种勇气的人就会陷入自卑情结。拿刚才的例子来讲，就会产生“<code>我因为学历低所以无法成功</code>”之类的想法，并且还会进一步通过“<code>如果有高学历自己也很容易成功</code>”之类的话来暗示自己的能力。意思就是“<code>现在只不过是被学历低这个因素所埋没，‘真正的我’其实非常优秀</code>”。</p><p>青年：不不，第二种说法已经不属于自卑感了。那应该是自吹自擂吧。</p><p>哲人：正是如此。自卑情结有时会发展成另外一种特殊的心理状态。</p><p>哲人：这也许是你没听说过的词语，是“<code>优越情结</code>”。</p><p>青年：优越情结？</p><p>哲人：虽然苦于强烈的自卑感，但却没有勇气通过努力或成长之类的健全手段去进行改变。即便如此，又没法忍受“<code>因为有A所以才做不到B</code>”之类的自卑情结，无法接受“<code>无能的自己</code>”。如此一来，人就会想要用更加简便的方法来进行补偿。</p><p>青年：怎么做呢？</p><p>哲人：表现得好像自己很优秀，继而沉浸在一种虚假的优越感之中。</p><p>哲人：一个很常见的例子就是“<code>权势张扬</code>”。</p><p>青年：那是什么呢？</p><p>哲人：例如大力宣扬自己是权力者——可以是班组领导，也可以是知名人士，其实就是在通过此种方式来显示自己是一种特别的存在。虚报履历或者过度追逐名牌服饰等也属于一种权势张扬、具有优越情结的特点。这些情况都属于“<code>我</code>”原本并不优秀或者并不特别。而通过把“我”和权势相结合，似乎显得“<code>我</code>”很优秀。这也就是“<code>虚假优越感</code>”。<br>青年：其根源在于怀有强烈的自卑感吧？</p><p>哲人：当然。我虽然对时尚不太了解，但10根手指全都戴着红宝石或者绿宝石戒指的人与其说是有审美意识的问题，倒不如说是自卑感的问题，也就是一种优越情结的表现。</p><blockquote><p>我们生活当中很容易沉迷在“<code>虚假的优越感</code>”当中。</p><p>比如，学生时期大家喜欢买名牌衣服。成年时间喜欢比较谁开的车好，谁买的房子大。</p><p>街头的混混喜欢买个大金链子挂在脖子上，仿佛想告诉全世界说，我很有钱。</p></blockquote><p>青年：的确如此。</p><p>哲人：不过，借助权势的力量来抬高自己的人终究是活在他人的价值观和人生之中。这是必须重点强调的地方。</p><p>青年：哦，是优越情结吗？这是一种很有意思的心理。您能再举一些例子吗？</p><p>哲人：例如，那些想要骄傲于自我功绩的人，那些沉迷于过去的荣光整天只谈自己曾经的辉煌业绩的人，这样的人恐怕你身边也有。这些都可以称之为优越情结。</p><p>青年：骄傲于自我功绩也算吗？那虽然是一种骄傲自大的态度，但也是因为实际上就很优秀才骄傲的吧。这可不能叫作虚假优越感。</p><p>哲人：不是这样。特意自吹自擂的人其实是对自己没有自信。阿德勒明确指出“如果有人骄傲自大，那一定是因为他有自卑感”。</p><p>青年：您是说自大是自卑感的另一种表现。</p><p>哲人：是的。如果真正地拥有自信，就不会自大。正因为有强烈的自卑感才会骄傲自大，那其实是想要故意炫耀自己很优秀。担心如果不那么做的话，就会得不到周围的认可。这完全是一种优越情结。</p><p>青年：……也就是说，自卑情结和优越情结从名称上来看似乎是正相反的，但实际上却有着密切的联系？</p><h3 id="人生不是与他人的比赛"><a href="#人生不是与他人的比赛" class="headerlink" title="人生不是与他人的比赛"></a>人生不是与他人的比赛</h3><p>青年：您是说人生不是竞争？</p><p>哲人：是的。不与任何人竞争，只要自己不断前进即可。当然，也没有必要把自己和别人相比较。</p><h3 id="在意你长相的，只有你自己"><a href="#在意你长相的，只有你自己" class="headerlink" title="在意你长相的，只有你自己"></a>在意你长相的，只有你自己</h3><p>哲人：请从你自己的角度来具体考虑一下。假设你对周围的人都抱有“竞争”意识。但是，竞争就会有胜者和败者。因为他们之间的关系，所以必然会意识到胜负，会产生“<code>A君上了名牌大学，B君进了那家大企业，C君找了一位那么漂亮的女朋友，而自己却是这样</code>”之类的想法。</p><p>青年：哈哈，可真具体啊。</p><p>哲人：如果意识到竞争或胜负，那么势必就会产生自卑感。因为常常拿自己和别人相比就会产生“<code>优于这个、输于那个</code>”之类的想法，而自卑情结或优越情结就会随之而生。那么，对此时的你来说，他人又会是什么样的存在呢？</p><p>青年：呀，是竞争对手吗？</p><p>哲人：不，不是单纯的的竞争对手。不知不觉就会把他人乃至整个世界都看成“<code>敌人</code>”。</p><p>青年：敌人？</p><p>哲人：也就是会认为人人都是随时会愚弄、嘲讽、攻击甚至陷害自己、绝不可掉以轻心的敌人，而世界则是一个恐怖的地方。</p><p>哲人：关键在于下面这一点。如果能够体会到“<code>人人都是我的伙伴</code>”，那么对世界的看法也会截然不同。不再把世界当成危险的所在，也不再活在不必要的猜忌之中，你眼中的世界就会成为一个安全舒适的地方。人际关系的烦恼也会大大减少。</p><h3 id="人际关系中的“权力斗争”与复仇"><a href="#人际关系中的“权力斗争”与复仇" class="headerlink" title="人际关系中的“权力斗争”与复仇"></a>人际关系中的“权力斗争”与复仇</h3><p>哲人：比如，假设你和朋友正在谈论时下的政治形势，谈着谈着你们之间的争论越来越激烈，彼此都各不相让，于是对方很快就上升到了人格攻击，骂你说：“<code>所以说你是个大傻瓜，正因为有你这种人存在我们国家才不能发展。</code>”</p><p>青年：如果被这样说的话，我肯定会忍无可忍。</p><p>哲人：这种情况下，对方的目的是什么呢？是纯粹想要讨论政治吗？不是。对方只是想要责难挑衅你，通过权力之争来达到让不顺眼的你屈服的目的。这个时候你如果发怒的话，那就是正中其下怀，关系会急剧转入权力之争。所以，我们不能上任何挑衅的当。</p><p>青年：不不，没必要逃避。对于挑衅就应该进行回击。因为错在对方。对那种无礼的混球就应该直接挫挫其锐气，用语言的拳头！</p><p>哲人：那么，假设你压制住了争论，而且彻底认输的对方爽快地退出。但是，权力之争并没有就此结束。败下阵来的对方会很快转入下一个阶段。</p><p>青年：下一个阶段？</p><p>哲人：是的，“<code>复仇</code>”阶段。尽管暂时败下阵来，但对方会在别的地方以别的形式策划着复仇、等待着进行报复。</p><p>青年：比如说？</p><p>哲人：遭受过父母虐待的孩子有些会误入歧途、逃学，甚至会出现割腕等自残行为。如果按照弗洛伊德的原因论，肯定会从简单的因果律角度归结为：“<code>因为父母用这样的方法教育，所以孩子才变成这样。</code>”就像因为不给植物浇水，所以它们才会干枯一样。这的确是简单易懂的解释。</p><p>但是，阿德勒式的目的论不会忽视孩子隐藏的目的——也就是“<code>报复父母</code>”。如果自己出现不良行为、逃学，甚至是割腕，那么父母就会烦恼不已，父母还会惊慌失措、痛不欲生。孩子正是因为知道这一点，所以才会出现问题行为。孩子并不是受过去原因（家庭环境）的影响，而是为了达到现在的目的（报复父母）。</p><h3 id="承认错误，不代表你失败了"><a href="#承认错误，不代表你失败了" class="headerlink" title="承认错误，不代表你失败了"></a>承认错误，不代表你失败了</h3><p>哲人：首先希望你能够理解这样一个事实，那就是发怒是交流的一种形态，而且不使用发怒这种方式也可以交流。我们即使不使用怒气，也可以进行沟通以及取得别人的认同。如果能够从经验中明白这一点，那自然就不会再有怒气产生了。</p><p>青年：但是，即使对方明显找碴儿挑衅，恶意说一些侮辱性的语言，也不能发怒吗？</p><p>哲人：你似乎还没有真正理解。不是不能发怒，而是“<code>没必要依赖发怒这一工具</code>”。易怒的人并不是性情急躁，而是不了解发怒以外的有效交流工具。所以才会说“<code>不由得发火</code>”之类的话。这其实是在借助发怒来进行交流。</p><p>青年：发怒之外的有效交流……</p><p>哲人：我们有语言，可以通过语言进行交流；要相信语言的力量，相信具有逻辑性的语言。</p><p>青年：……的确，如果不相信这一点的话，我们的这种对话也就不会成立了。</p><p>哲人：关于权力之争，还有一点需要注意。那就是无论认为自己多么正确，也不要以此为理由去责难对方。这是很多人都容易陷落进去的人际关系圈套。</p><p>青年：为什么？</p><p>哲人：人在人际关系中一旦确信“<code>我是正确的</code>”，那就已经步入了权力之争。</p><p>青年：仅仅是认为自己正确就会那样吗？不不，这也太夸张了吧？</p><p>哲人：<strong>我是正确的，也就是说对方是错误的。一旦这样想，辩论的焦点便会从“主张的正确性”变成了“人际关系的方式”。也就是说，“我是正确的”这种坚信意味着坚持“对方是错误的”，最终就会演变成“所以我必须获胜”之类的胜负之争。这就是完完全全的权力之争吧？</strong></p><blockquote><p>技术交流的时候一定要注意，双方进行观点辩论的时候，如果对方提的观点在你看来很荒谬，也不能觉得我是对的你是错的我跟你解释不清楚，懒得跟你扯的想法，这样到最后很容易上升到人生攻击。</p></blockquote><p>青年：嗯。</p><p>哲人：原本主张的对错与胜负毫无关系。如果你认为自己正确的话，那么无论对方持什么意见都应该无所谓。但是，很多人都会陷入权力之争，试图让对方屈服。正因为如此，才会认为“<code>承认自己的错误</code>”就等于“<code>承认失败</code>”。</p><p>青年：的确有这么一方面。</p><p>哲人：因为不想失败，所以就不愿承认自己的错误，结果就会选择错误的道路。承认错误、赔礼道歉、退出权力之争，这些都不是“<code>失败</code>”。</p><p>追求优越性并不是通过与他人的竞争来完成的。</p><h3 id="人生的三大课题：交友课题、工作课题以及爱的课题"><a href="#人生的三大课题：交友课题、工作课题以及爱的课题" class="headerlink" title="人生的三大课题：交友课题、工作课题以及爱的课题"></a>人生的三大课题：交友课题、工作课题以及爱的课题</h3><p>青年：人生的课题？</p><p>哲人：是的。这非常重要。阿德勒心理学关于人的行为方面和心理方面都提出了相当明确的目标。</p><p>青年：哦。那是什么样的目标呢</p><p>哲人：首先，行为方面的目标有<strong>“自立”和“与社会和谐共处”</strong>这两点。而且，支撑这种行为的心理方面的目标是<strong>“我有能力”以及“人人都是我的伙伴”</strong>这两种意识。</p><p>哲人：而且，这些目标可以通过阿德勒所说的直面“人生课题”来实现。</p><p>青年：那么，“<code>人生课题</code>”又指什么呢？</p><p>哲人：请从孩提时代开始考虑人生这个词。孩提时代，我们在父母的守护下生活，即使不怎么劳动也可以生存下去。但是，很快就到了“自立”之时，不能继续依赖父母而必须争取精神性的自立这一点自不必说，即使在社会意义上也要自立，必须从事某些工作——这里不是指在企业上班之类狭义上的工作。</p><p>此外，在成长过程中会遇到各种各样的朋友关系。当然，也会与某人结成恋爱关系甚至还有可能发展到结婚。如果是那样的话，就又会产生夫妻关系，一旦有了孩子还会出现亲子关系。</p><p>阿德勒把这些过程中产生的人际关系分为<strong>“工作课题”“交友课题”和“爱的课题”</strong>这三类，又统称为“人生课题”。</p><h3 id="“人生谎言”教我们学会逃避"><a href="#“人生谎言”教我们学会逃避" class="headerlink" title="“人生谎言”教我们学会逃避"></a>“人生谎言”教我们学会逃避</h3><p>青年：啊，我的头又乱了。先生也说过吧，我之所以把别人看成是“敌人”<code>而不能看成是</code>“伙伴”，是因为在逃避人生的课题。那究竟是什么意思呢？</p><p>哲人：假设你讨厌A这个人，说是因为A身上有让人无法容忍的缺点。</p><p>青年：是啊，如果是讨厌的人，那还真不少。</p><p>哲人：但是，那并不是因为无法容忍A的缺点才讨厌他，而是你先有“<code>要讨厌A</code>”这个目的，之后才找出了符合这个目的的缺点。</p><p>青年：怎么可能？！那我这么做又是为了什么呢？</p><p>哲人：为了逃避与A之间的人际关系。</p><p>青年：哎呀，这绝对不可能！即使再怎么想，顺序也是反的。是因为他做了惹人讨厌的事，所以大家才会讨厌他，否则也没有理由讨厌他！</p><p>哲人：不，不是这样的。如果想一想与处于恋爱关系的人分手时候的情况就会容易理解了。<br>在恋爱或夫妻关系中，过了某个时期之后，有时候对方的任何言行都会让你生气。吃饭的方式让你不满意，在房间里的散漫姿态令你生厌，甚至就连对方睡眠时的呼吸声都让你生气，尽管几个月前还不是这样。</p><p>青年：……是的，这个能够想象得到。</p><p>哲人：这是因为那个人已经下定决心要找机会“结束这种关系”，继而正在搜集结束关系的材料，所以才会那样感觉。对方其实没有任何改变，只是自己的“目的”变了而已。<br>人就是这么任性而自私的生物，一旦产生这种想法，无论怎样都能发现对方的缺点。即使对方是圣人君子一样的人物，也能够轻而易举地找到对方值得讨厌的理由。正因为如此，世界才随时可能变成危险的所在，人们也就有可能把所有他人都看成“敌人”。</p><blockquote><p>这个观点有点不敢苟同</p></blockquote><p>青年：那么，您是说我为了逃避人生课题或者进一步说是为了逃避人际关系，仅仅为了这些我就去捏造别人的缺点？</p><p>哲人：是这样的。阿德勒把这种企图设立种种借口来回避人生课题的情况叫作“<code>人生谎言</code>”。</p><p>青年：……</p><p>哲人：这词很犀利吧。对于自己目前所处的状态，把责任转嫁给别人，通过归咎于他人或者环境来回避人生课题。前面我提到的患脸红恐惧症的那个女学生也是一样——对自己撒谎，也对周围的人撒谎。仔细考虑一下，这的确是一个相当犀利的词语。</p><p>青年：但是，为什么要把那判定为撒谎呢？我周围都有什么样的人，之前又经历过怎样的人生，先生您根本一无所知吧！</p><p>哲人：是的，我对你的过去一无所知，有关你父母和你哥哥的事情我也一无所知。不过，我只知道一点。</p><p>青年：是什么？</p><p>哲人：那就是，决定你的生活方式（人生状态）的不是其他任何人，而是你自己这一事实。</p><p>青年：啊……！！</p><p>哲人：假若你的生活方式是由他人或者环境所决定的，那还有可能转嫁责任。但是，我们是自己选择自己的生活方式，责任之所在就非常明确了。</p><h2 id="第三夜-让干涉你生活的人见鬼去"><a href="#第三夜-让干涉你生活的人见鬼去" class="headerlink" title="第三夜 让干涉你生活的人见鬼去"></a>第三夜 让干涉你生活的人见鬼去</h2><h3 id="自由就是不再寻求认可？"><a href="#自由就是不再寻求认可？" class="headerlink" title="自由就是不再寻求认可？"></a>自由就是不再寻求认可？</h3><p>哲人：是的，你考虑过自由是什么了吗？</p><p>青年：这我已经仔细考虑过了。</p><p>哲人：得出结论了吗？</p><p>青年：哎呀，没得出结论。但是，有一个不是我自己的想法，而是从图书馆发现的这么一句话，就是：货“<code>币是被铸造的自由。</code>”它是陀思妥耶夫斯基的小说中出现的一句话。“<code>被铸造的自由</code>”这种说法是何等的痛快啊！我认为这是一句非常精辟的话，它一语道破了货币的本质。</p><p>哲人：的确如此。如果要坦率地说出货币所带来的东西的本质的话，那或许就是自由。这大概也可以被称为名言。不过，也不可以据此就说“<code>自由就是货币</code>”吧？</p><blockquote><p>也就是大家追求的财务自由吧</p></blockquote><h3 id="要不要活在别人的期待中？"><a href="#要不要活在别人的期待中？" class="headerlink" title="要不要活在别人的期待中？"></a>要不要活在别人的期待中？</h3><p>哲人：得到别人的认可的确很让人高兴。但是，要说是否真的需要被人认可，那绝对不是。人原本为什么要寻求认可呢？说得再直接一些，人为什么想要得到别人的表扬呢？</p><blockquote><p>被认可，可以认为是自身价值的体现，还有一方面，工作面临绩效的压力，被认可也是绩效的提现。</p></blockquote><p>青年：答案很简单。只有得到了别人的认可，我们才能体会到“<code>自己有价值</code>”。通过别人的认可，我们能够消除自卑感，可以增加自信心。对，这就是“<code>价值</code>”的问题。先生您上次不也说过吗？自卑感就是价值判断的问题。我正是因为得不到父母的认可所以才一直活在自卑之中！</p><p>哲人：在犹太教教义中有这么一句话：“<code>倘若自己都不为自己活出自己的人生，那还有谁会为自己而活呢？</code>”你就活在自己的人生中。要说为谁活着，那当然是为你自己。假如你不为自己而活的话，那谁会为你而活呢？我们最终还是为自己活着。没理由不可以这样想。</p><p>哲人：过于希望得到别人的认可，就会按照别人的期待去生活。也就是舍弃真正的自我，活在别人的人生之中。<br>而且，请你记住，假如说你“<code>不是为了满足他人的期待而活</code>”，那他人也“<code>不是为了满足你的期待而活</code>”。当别人的行为不符合自己的想法的时候也不可以发怒。这也是理所当然的事情。</p><blockquote><p>这一点不太认同，并不是为别人而活，是我们能针对不同环境改变自己生活方式/工作方式，迎合当前工作环境，做到自己和团队的利益最大化。</p></blockquote><h3 id="自己和别人的“人生课题”分开来"><a href="#自己和别人的“人生课题”分开来" class="headerlink" title="自己和别人的“人生课题”分开来"></a>自己和别人的“人生课题”分开来</h3><p>哲人：明白了。那么，我就从阿德勒心理学的基本原理开始说起。例如，当眼前有“<code>学习</code>”这个课题的时候，阿德勒心理学会首先考虑“<code>这是谁的课题</code>”。</p><p>青年：谁的课题？</p><p>哲人：<strong>孩子学不学习或者跟不跟朋友玩，这原本是“<code>孩子的课题</code>”，而不是父母的课题</strong>。</p><p>青年：您是说这是孩子应该做的事吗？</p><p>哲人：坦率说的话，就是如此。即使父母代替孩子学习也没有任何意义吧？</p><p>青年：哎呀，那倒是。</p><p>哲人：学习是孩子的课题。与此相对，父母命令孩子学习就是对别人的课题妄加干涉。如果这样的话，那肯定就避免不了冲突。因此，我们必须从“<code>这是谁的课题</code>”这一观点出发，把自己的课题与别人的课题分离开来。</p><p>青年：分离之后再怎么做呢？</p><p>哲人：不干涉他人的课题。仅此而已。</p><p>哲人：辨别究竟是谁的课题的方法非常简单，只需要考虑一下“<code>某种选择所带来的结果最终要由谁来承担？</code>”<br>如果孩子选择“<code>不学习</code>”这个选项，那么由这种决断带来的后果——例如成绩不好、无法上好学校等——最终的承担者不是父母，而是孩子。也就是说，学习是孩子的课题。</p><p>青年：不不，根本不对！为了不让这种事态发生，既是人生前辈又是保护人的父母有责任告诫孩子“<code>必须好好学习！</code>”。这是为孩子着想，而不是妄加干涉。“<code>学习</code>”或许是孩子的课题，但“<code>让孩子学习</code>”却是父母的课题。</p><p>哲人：的确，世上的父母总是说“<code>为你着想</code>”之类的话。但是，父母们的行为有时候很明显是为了满足自己的目的——面子和虚荣又或者是支配欲。也就是说，不是“<code>为了你</code>”而是“<code>为了我</code>”，正因为察觉到了这种欺骗行为，孩子才会反抗。</p><p>哲人：苦恼于与孩子之间的关系的父母往往容易认为：孩子就是我的人生。总之就是把孩子的课题也看成是自己的课题，总是只考虑孩子，而当意识到的时候，他们已经失去了自我。</p><p><strong>但即使父母再怎么背负孩子的课题，孩子依然是独立的个人，不会完全按照父母的想法去生活</strong>。孩子的学习、工作、结婚对象或者哪怕是日常行为举止都不会完全按照父母所想。当然，我也会担心甚至会想要去干涉。但是，刚才我也说过：“<code>别人不是为了满足你的期待而活。</code>”即使是自己的孩子也不是为了满足父母的期待而活。</p><p>哲人：当然。但请你这样想，干涉甚至担负起别人的课题这会让自己的人生沉重而痛苦。如果你正在为自己的人生而苦恼——这种苦恼源于人际关系——那首先请弄清楚“<code>这不是自己的课题</code>”这一界限；然后，请丢开别人的课题。这是减轻人生负担，使其变得简单的第一步。</p><p>哲人：分离就是这么回事。你太在意别人的视线和评价，所以才会不断寻求别人的认可。那么，人为什么会如此在意别人的视线呢？阿德勒心理学给出的答案非常简单，那就是因为你还不会进行课题分离。把原本应该是别人的课题也看成是自己的课题。</p><p><strong>请你想想前面那位老婆婆说的“<code>在意你的脸的只有你自己</code>”那句话。她的话一语道破了课题分离的核心。看到你的脸的别人怎么想，那是别人的课题，你根本无法左右。</strong></p><p>青年：课题分离作为道理来讲完全正确。别人怎么看我怎么评价我，这是别人的课题，我无法左右。我只需要诚实面对自己的人生，做自己应该做的事情。这简直可以称为“<code>人生的真理</code>”。</p><h3 id="人与人之间的距离"><a href="#人与人之间的距离" class="headerlink" title="人与人之间的距离"></a>人与人之间的距离</h3><p>哲人：例如，读书的时候如果离得太近就会什么都看不见。同样，要想构筑良好的人际关系也需要保持一定的距离。如果距离太近，贴在一起，那就无法与对方正面对话。</p><p>虽说如此，但距离也不可以太远。父母如果一味训斥孩子，心就会疏远。如果这样的话，孩子甚至都不愿与父母商量，父母也不能提供适当的援助。伸伸手即可触及，但又不踏入对方领域，保持这种适度距离非常重要。</p><blockquote><p>同意这个观点，有句话说君子之交淡如水，就是说再好的朋友，不能过度亲密，过度亲密就会变成你跟我好，你必须怎么怎么样，这样到最后关系很容易破裂。</p></blockquote><p>青年：即使亲子关系也需要保持距离吗？</p><p>哲人：当然。你刚才说课题分离是肆意践踏对方好意。这其实是一种受“<code>回报</code>”思想束缚的想法。也就是说，如果对方为自己做了什么——即使那不是自己所期望的事情——自己也必须给予报答。</p><p>这其实并非是不辜负好意，而仅仅是受回报思想的束缚。无论对方做什么，决定自己应该如何做的都应该是自己。</p><p>青年：您是说，我所说的羁绊的本质其实是回报思想？</p><p>哲人：是的。如果人际关系中有“<code>回报思想</code>”存在，那就会产生“<code>因为我为你做了这些，所以你就应该给予相应回报</code>”这样的想法。当然，这是一种与课题分离相悖的思想。我们既不可以寻求回报，也不可以受其束缚。<br>青年：嗯。</p><p>哲人：但是，有些情况下不进行课题分离而是干涉别人的课题会更加容易。例如孩子总是系不上鞋带，对繁忙的母亲而言，直接帮孩子系上要比等着孩子自己系上更快。但是，这种行为是一种干涉，是在剥夺孩子的课题。而且，反复干涉的结果会是孩子什么也学不到，最终还会失去面对人生课题的勇气。阿德勒说：“<code>没有学会直面困难的孩子最终会想要逃避一切困难。</code>”</p><h3 id="自由就是被人讨厌"><a href="#自由就是被人讨厌" class="headerlink" title="自由就是被人讨厌"></a>自由就是被人讨厌</h3><p>哲人：就像我前面反复提到的那样，阿德勒心理学认为“<code>一切烦恼皆源于人际关系</code>”。也就是说，我们都在追求从人际关系中解放出来的自由。但是，一个人在宇宙中生存之类的事情根本不可能。想到这里自然就能明白何谓自由了吧。</p><p>青年：是什么？</p><p>哲人：也就是说“<code>自由就是被别人讨厌</code>”。</p><blockquote><p>不太认可这种观点，我自由了，我远离人群，过着自己的生活，为什么一定会被人讨厌。</p></blockquote><p>青年：什、什么？！</p><p>哲人：是你被某人讨厌。这是你行使自由以及活得自由的证据，也是你按照自我方针生活的表现。</p><p>青年：哎、哎呀，但是……</p><p>哲人：的确，招人讨厌是件痛苦的事情。如果可能的话，我们都想毫不讨人嫌地活着，想要尽力满足自己的认可欲求。但是，八面玲珑地讨好所有人的生活方式是一种极其不自由的生活方式，同时也是不可能实现的事情。<br>如果想要行使自由，那就需要付出代价。而在人际关系中，自由的代价就是被别人讨厌。</p><p>青年：虽然不想被人讨厌，但即使被人讨厌也没有关系？</p><p>哲人：是啊。“<code>不想被人讨厌</code>”也许是我的课题，但“<code>是否讨厌我</code>”却是别人的课题。即使有人不喜欢我，我也不能去干涉。如果用刚才介绍过的那个谚语说的话，那就是只做“<code>把马带到水边</code>”的努力，是否喝水是那个人的课题。</p><p>哲人：<strong>获得幸福的勇气也包括“<code>被讨厌的勇气</code>”。一旦拥有了这种勇气，你的人际关系也会一下子变得轻松起来</strong>。</p><blockquote><p>从来就不缺“<code>被讨厌的勇气</code>”</p></blockquote><h2 id="第四夜-要有被讨厌的勇气"><a href="#第四夜-要有被讨厌的勇气" class="headerlink" title="第四夜 要有被讨厌的勇气"></a>第四夜 要有被讨厌的勇气</h2><h3 id="个体心理学和整体论"><a href="#个体心理学和整体论" class="headerlink" title="个体心理学和整体论"></a>个体心理学和整体论</h3><blockquote><p>首先论述了阿德勒心理学（个体心理学）针对个体的理解：即身体与心灵、理性与感情、意识与无意识的结合体。</p><p>其次，处理人际关系的“<code>课题分离</code>”思想，其目的并不是孤立主义，而是通过保持一定得距离以解开人际关系的矛盾和纠缠点。</p><p>并且“<code>课题分离</code>”仅仅是处理人际关系的起点，后续的方法将逐步揭晓。</p></blockquote><p>哲人：的确，阿德勒所命名的“<code>个体心理学</code>”这一名称也许很容易招人误解。在这里我要简单做一下说明。首先，在英语中，个体心理学叫作“<code>individual psychology</code>”。而且，这里的个人（individual）一词在语源上有“<code>不可分割</code>”的意思。<br>青年：不可分割？</p><p>哲人：总之就是不可再分的最小单位的意思。那么具体来讲，什么不可以分割呢？阿德勒反对把精神和身体、理性和感情以及意识和无意识等分开考虑的一切二元论的价值观。</p><p>青年：什么意思？</p><p>哲人：比如，请你想一想那位因为脸红恐惧症而来咨询的女学生的话。她为什么会得脸红恐惧症呢？阿德勒心理学不把身体症状与心灵（精神）分离开来考虑，而是认为心灵和身体是不可分割的一个“<code>整体</code>”，就好比由于内心的紧张手脚会发抖、脸颊会变红或者由于恐惧而脸色苍白等。</p><p>青年：心灵和身体会有联系部分吧。</p><p>哲人：理性和感情、意识和无意识也是一样。一般情况下，冷静的人不会因被冲动驱使而大发雷霆。我们并不是受感情这一独立存在所左右，而是一个统一的整体。</p><p>青年：不，这不对。只有把心灵和身体、理性和感情、意识和无意识这些因素明确区分开来进行考虑，才能正确理解人的本质。这不是理所当然的道理吗？</p><p>哲人：当然，心灵和身体是不一样的存在，理性和感情也各有不同，而且还有有意识和无意识之分，这些都是事实。<br>但是，当对他人大发雷霆的时候，那是“<code>作为整体的我</code>”选择了勃然大怒，绝对不是感情这一独立存在——可以说与我的意志无关——发出了怒吼。在这里，如果把“<code>我</code>”和“<code>感情</code>”分离开来认为“<code>感情让我那么做或者受感情驱使</code>”，那就容易陷入人生谎言。</p><p>青年：您是说我对服务员发火那件事吧？</p><p>哲人：是的。像这样把人看作不可分割的存在和作为“<code>整体的我</code>”来考虑的方式叫作“<code>整体论</code>”。</p><p>青年：那倒是可以。但是先生，我并不想听您空谈“<code>个人</code>”的定义。如果彻底探讨阿德勒心理学会发现它最终将把人导向“<code>我是我、你是你</code>”的孤立境地。也就是我不干涉你，你也别干涉我，彼此都任性地活着。请您坦率地分析一下这一点。</p><p>哲人：明白了。关于一切烦恼皆源于人际关系这一阿德勒心理学的基本思想，你已经理解了吧？</p><p>青年：是的。作为解决这种烦恼的手段，出现了人际关系方面的不干涉，即课题分离这一观点。</p><p>哲人：我上次应该说过这样的话——“<code>要想缔结良好的人际关系，需要保持一定距离；太过亲密就无法正面对话。但是，距离也不可以太远。</code>”<strong>课题分离不是为了疏远他人，而是为了解开错综复杂的人际关系之线</strong>。</p><h3 id="人际关系的终极目标：“共同体感觉”"><a href="#人际关系的终极目标：“共同体感觉”" class="headerlink" title="人际关系的终极目标：“共同体感觉”"></a>人际关系的终极目标：“共同体感觉”</h3><p>哲人：就像我一直说的那样，阿德勒心理学认为“<code>一切烦恼皆源于人际关系</code>”。不幸之源也在于人际关系。反过来说就是，幸福之源也在于人际关系。</p><p>青年：的确。</p><p>哲人：共同体感觉是幸福的人际关系的最重要的指标。</p><p>青年：愿闻其详。</p><p>哲人：在英语中，共同体感觉叫作“<code>social interest</code>”，也就是“<code>对社会的关心</code>”。这里我要问问你，你知道社会学上所讲的社会的最小单位是什么吗？</p><p>青年：社会的最小单位？哎呀，是家庭吧。</p><p>哲人：不对，是“<code>我和你</code>”。只要有两个人存在，就会产生社会、产生共同体。要想理解阿德勒所说的共同体感觉，首先可以以“<code>我和你</code>”为起点。</p><p>青年：以此为起点怎么做呢？</p><p>哲人：把对自己的执著（self interest）变成对他人的关心（social interest）。</p><p>青年：对自己的执著？对他人的关心？这又是什么呢？</p><p>本章概要：本章引出人际关系的最终目标：共同体感觉。阿德勒心理学中的共同体，是指跨越过去与未来的时间维度，包含宇宙万物的整体。而共同体感觉，是指每个人进行课题拆分后，在共同体中“<code>拥有自己的位置，扮演自己的角色，做出自己的贡献</code>”的一种状态。</p><p>共同体感觉可以将对个人的执着变为对他人的关心。</p><p>个人理解，上面👆这句话的含义是指，每个人都能各司其职，扮演自己的角色，尊重他人的想法，会在共同体中形成一股自由的氛围，在这个氛围里，每个人都能独立自主的发展自己，不受他人的影响，这就是所谓的“<code>对他人的关心</code>”。</p><h3 id="“拼命寻求认可”反而是以自我为中心？"><a href="#“拼命寻求认可”反而是以自我为中心？" class="headerlink" title="“拼命寻求认可”反而是以自我为中心？"></a>“拼命寻求认可”反而是以自我为中心？</h3><p>哲人：请你考虑一下认可欲求的实质——他人如何关注自己、如何评价自己？又在多大程度上满足自己的欲求？受这种认可欲求束缚的人看似在看着他人，但实际上眼里却只有自己。失去了对他人的关心而只关心“<code>我</code>”，也就是以自我为中心。</p><p>青年：那么，也就是说像我这样非常在意别人评价的人也是以自我为中心吗？虽然如此竭尽全力地在迎合他人？！</p><p>哲人：是的。在只关心“<code>我</code>”这个意义上来讲，是以自我为中心。你正因为不想被他人认为自己不好，所以才在意他人的视线。这不是对他人的关心，而是对自己的执著。</p><h3 id="你不是世界的中心，只是世界地图的中心"><a href="#你不是世界的中心，只是世界地图的中心" class="headerlink" title="你不是世界的中心，只是世界地图的中心"></a>你不是世界的中心，只是世界地图的中心</h3><p>哲人：但是，在地球仪上看世界的时候又会如何呢？如果是地球仪，既可以把法国看作中心，也可以把中国看作中心，还可以把巴西看作中心。一切地方都是中心，同时一切地方又都不是中心。根据看的人所处的位置或角度可以产生无数个中心。这就是地球仪。</p><p>青年：嗯，的确如此。</p><p>哲人：刚才所说的“<code>你并不是世界的中心</code>”也是一样，你是共同体的一部分，而不是中心。</p><p>青年：我并不是世界的中心。世界不是被切割成平面的地图而是像地球仪一样的球体。哎呀，作为道理大体能明白，但为什么一定要特别意识到“<code>不是世界的中心</code>”呢？</p><p>哲人：这应该再回到最初的话题。我们都在寻求“<code>可以在这里</code>”的归属感。但是，阿德勒心理学认为归属感不是仅仅靠在那里就可以得到的，它必须靠积极地参与到共同体中去才能够得到。</p><p>青年：积极地参与？具体是什么意思呢？</p><p>哲人：就是直面“<code>人生课题</code>”。也就是不回避工作、交友、爱之类的人际关系课题，要积极主动地去面对。如果你认为自己就是世界的中心，那就丝毫不会主动融入共同体中，因为一切他人都是“<code>为我服务的人</code>”，根本没必要由自己采取行动。<br>但是，无论是你还是我，我们都不是世界的中心，必须用自己的脚主动迈出一步去面对人际关系课题；不是考虑“<code>这个人会给我什么</code>”，而是要必须思考一下“<code>我能给这个人什么</code>”。这就是对共同体的参与和融入。</p><p>青年：您是说只有付出了才能够找到自己的位置？</p><p>哲人：是的。归属感不是生来就有的东西，要靠自己的手去获得。</p><p>共同体感觉是阿德勒心理学的关键概念，也是最具争议的观点。的确，这种观点对青年来说很难马上接受。而且，对于被指出“<code>你是以自我为中心</code>”这件事，他也是心怀不满。但是，最接受不了的还是甚至包括宇宙或非生物在内的共同体范围问题。</p><h3 id="在更广阔的天地寻找自己的位置"><a href="#在更广阔的天地寻找自己的位置" class="headerlink" title="在更广阔的天地寻找自己的位置"></a>在更广阔的天地寻找自己的位置</h3><p>哲人：例如，有人一旦退休便立即没了精神。被从公司这个共同体中分离出来，失去了头衔、失去了名片，成了无名的“<code>平凡人</code>”，也就是变得普通了，有人接受不了这一变化就会一下子衰老。<br>但是，这只不过是从公司这个小的共同体中被分离出来而已，任何人都还属于别的共同体。因为，无论怎样，我们的一切都属于地球这个共同体，属于宇宙这个共同体。</p><p>青年：这只不过是诡辩而已！突然听到有人告诉自己“<code>你属于宇宙</code>”，这到底能带来什么归属感呢？！</p><p>哲人：的确，宇宙很难立刻想象出来。但是，希望你不要只拘泥于眼前的共同体，而要意识到自己还属于别的共同体，属于更大的共同体，例如国家或地域社会等，而且在哪里都可以作出某些贡献。</p><p>青年：那么，这种情况怎么样呢？假设有一个人既没有结婚，也还没有工作、没有朋友，且不与任何人交往，仅靠父母的遗产生活。他逃避“<code>工作课题</code>”“<code>交友课题</code>”和“<code>爱的课题</code>”等一切人生课题。可以说这样的人也属于某种共同体吗？</p><p>哲人：当然。假如他要买一片面包，相应地要支付一枚硬币。这枚被支付的硬币不仅可以联系到面包店的工作人员，还可以联系到小麦或黄油的生产者，抑或是运输这些物品的流通行业的工作人员、销售汽油的从业人员，还有产油国的人们等，这一切都可以说环环相扣紧密相连。人绝不会，也不可能离开共同体“<code>独自</code>”生活。</p><h3 id="批评不好……表扬也不行？横向关系和纵向关系"><a href="#批评不好……表扬也不行？横向关系和纵向关系" class="headerlink" title="批评不好……表扬也不行？横向关系和纵向关系"></a>批评不好……表扬也不行？横向关系和纵向关系</h3><p>哲人：是的，重要的就是这里——分离课题如何带来良好的关系。也就是，如何才能形成相互协调与合作的关系？这里就需要提到“<code>横向关系</code>”这个概念。</p><p>青年：横向关系？</p><p>哲人：举一个容易明白的亲子关系的例子。在教育孩子或是培养部下的时候，一般都认为有两个方法：批评教育法和表扬教育法。</p><p>青年：啊，这是经常被拿来讨论的问题。</p><p>哲人：你认为批评和表扬应该选择哪一种呢？</p><p>青年：当然应该是表扬教育法。</p><p>哲人：为什么？</p><p>青年：想想动物训练就能够明白。训练动物耍技艺的时候可以挥舞着鞭子让其顺从，这是典型的“<code>批评教育</code>”的做法。另一方面，也可以一个手拿着食物并通过语言的赞美让其记住所教技艺。这就是“<code>表扬教育</code>”。<br>这两种方法在“<code>掌握技艺</code>”这个结果上也许一样。但是，在“<code>因为被批评而做</code>”和“<code>想要被表扬而做</code>”这两种情况中，行动对象的动机完全不同，后者中含有喜悦的成分。因为批评会让对方萎缩，所以只有在表扬教育下才能茁壮成长。这是理所当然的结论吧。</p><p>哲人：的确如此。动物训练是很有意思的观点。那么，我要说明一下阿德勒心理学的立场。关于以育儿活动为代表的一切与他人的交流，阿德勒心理学都采取“<code>不可以表扬</code>”的立场。</p><p>青年：不可以表扬？</p><p>哲人：当然，同时也反对体罚、不认可批评。不可以批评也不可以表扬，这就是阿德勒心理学的立场。</p><p>青年：究竟为什么呢？</p><p>哲人：请你考虑一下表扬这种行为的实质。例如，假设我赞美你，说“<code>不错嘛，你做得很好</code>”。你不觉得这种话有些别扭吗？</p><p>哲人：为什么感觉不愉快呢？你能说明一下理由吗？</p><p>青年：那句“<code>不错嘛，你做得很好</code>”中所包含的俯视般的语感让人不愉快。</p><p>哲人：是的。表扬这种行为含有“<code>有能力者对没能力者所做的评价</code>”这方面的特点。有的母亲会赞美帮忙准备晚饭的孩子说“<code>你真了不起</code>”。但是，如果是丈夫做了同样的事情则一般不会表扬说“<code>你真了不起</code>”吧。<br>青年：哈哈，的确如此。</p><p>哲人：也就是说，用“<code>你真了不起</code>”“<code>做得很好</code>”或者“<code>真能干</code>”之类的话表扬孩子的母亲无意之中就营造了一种上下级关系——把孩子看得比自己低。你刚才提到的训练的事情正好象征了一种“<code>表扬</code>”背后的上下级关系和纵向关系。人表扬他人的目的就在于“<code>操纵比自己能力低的对方</code>”，其中既没有感谢也没有尊敬。</p><p>青年：为了操纵而表扬？</p><p>哲人：是的。我们表扬或者批评他人只有“<code>用糖还是用鞭子</code>”的区别，其背后的目的都是操纵。阿德勒心理学之所以强烈否定赏<br>罚教育，就因为它是为了操纵孩子。</p><p>青年：不不，这不对。请您站在孩子的立场考虑一下。对孩子来说，被父母表扬是无上的喜悦吧？正因为希望得到表扬才努力学习、才好好表现。实际上，我在小时候就非常希望得到父母的表扬。长大之后也是一样，如果得到了上司的表扬就会很高兴。这是一种不关乎理论的本能的感情。</p><p>哲人：希望被别人表扬或者反过来想要去表扬别人，这是一种把一切人际关系都理解为“<code>纵向关系</code>”的证明。你也是因为生活在纵向关系中，所以才希望得到表扬。阿德勒心理学反对一切“<code>纵向关系</code>”，提倡把所有的人际关系都看作“<code>横向关系</code>”。在某种意义上，这可以说是阿德勒心理学的基本原理。</p><p>青年：这可以表达为“<code>虽不同但平等</code>”吗？</p><p>哲人：是的，是平等即“<code>横向</code>”关系。例如，有些男人会骂家庭主妇“<code>又不挣钱！</code>”或者“<code>是谁养着你呀？</code>”之类的话，也听到过有人说“<code>钱随便你花，还有什么不满的呀？</code>”之类的话，这都是多么无情的话呀！经济地位跟人的价值毫无关系。公司职员和家庭主妇只是劳动场所和任务不同，完全是“<code>虽不同但平等</code>”。</p><p>青年：的确如此。<br>哲人：他们恐怕是非常害怕女性变得聪明、比自己挣钱多或者是跟自己顶嘴之类的事情。他们把人际关系都看成是“<code>纵向关系</code>”，害怕被女性瞧不起，也就是在掩饰自己强烈的自卑感。</p><p>青年：是不是在某种意义上已经陷入了想要尽力夸耀自己能力的优越情结呢？</p><p>哲人：是这样的。自卑感原本就是从纵向关系中产生的一种意识。只要能够对所有人都建立起“<code>虽不同但平等</code>”的横向关系，那就根本不会产生自卑情结。</p><p>青年：嗯，的确。我在想要表扬他人的时候，心中多少也会有些“<code>操纵</code>”意识。企图通过说一些恭维的话来讨好上司，这也完全是一种操纵吧。反过来说，我自己也因为被某人表扬而被操纵着。呵呵呵，人就是这么回事吧！</p><p>哲人：在无法摆脱纵向关系这个意义上的确如此。</p><p>哲人：在说明课题分离的时候我说过“<code>干涉</code>”这个词。也就是一种对他人的课题妄加干涉的行为。<br>那么，人为什么会去干涉别人呢？其背后实际上也是一种纵向关系。正因为把人际关系看成纵向关系、把对方看得比自己低，所以才会去干涉。希望通过干涉行为把对方导向自己希望的方向。这是坚信自己正确而对方错误。<br>当然，这里的干涉就是操纵。命令孩子“<code>好好学习</code>”的父母就是一个典型例子。也许本人是出于善意，但结果却是妄加干涉，因为这是想按照自己的意思去操纵对方。</p><p>青年：如果能够建立起横向关系，那也就不会再有干涉吗？</p><p>哲人：不会再有。</p><p>青年：但是，学习的例子暂且不谈，如果眼前有一个非常苦恼的人，那总不能置之不理吧？这种情况也可以说一句“<code>我若插手那就是干涉</code>”而什么也不做吗？</p><p>哲人：不可以置之不问。需要做一些不是干涉的“<code>援助</code>”。</p><p>青年：干涉和援助有什么不同呢？</p><p>哲人：请你想一下关于课题分离的讨论。孩子学习的事情，这是应该由孩子自己解决的课题，父母或老师无法代替。而干涉就是对别人的课题妄加干预，做出“<code>要好好学习</code>”或者“<code>得上那个大学</code>”之类的指示。<br>另一方面，援助的大前提是课题分离和横向关系。在理解了学习是孩子的课题这个基础上再去考虑能做的事情，具体就是不去居高临下地命令其学习，而是努力地帮助他本人建立“<code>自己能够学习</code>”的自信以及提高其独立应对课题的能力。</p><p>青年：这种作用并不是强制的吧？</p><p>哲人：是的，不是强制的，而是在课题分离的前提下帮助他用自己的力量去解决，也就是“<code>可以把马带到水边，但不能强迫其喝水</code>”。直面课题的是其本人，下定决心的也是其本人。</p><p>青年：既不表扬也不批评？</p><p>哲人：是的，既不表扬也不批评。阿德勒心理学把这种基于横向关系的援助称为“<code>鼓励</code>”。</p><p>青年：鼓励？……啊，以前您说过日后要对其进行说明的一个词。</p><p>哲人：人害怕面对课题并不是因为没有能力。阿德勒心理学认为这不是能力问题，纯粹是“<code>缺乏直面课题的‘勇气’</code>”。如果是这样的话，那就首先应该找回受挫的勇气。</p><p>青年：哎呀，这不是又绕回来了吗？结果不还得是表扬吗？人在得到别人表扬的时候就能体会到自己有能力，继而找回勇气。这一点就不要固执了，请您承认表扬的必要性吧！</p><p>哲人：不承认。</p><p>青年：为什么？</p><p>哲人：答案很清楚。因为人会因为被表扬而形成“<code>自己没能力</code>”的信念。</p><p>青年：您说什么呀？！</p><p>哲人：<strong>还要我再重复一遍吗？人越得到别人的表扬就越会形成“<code>自己没能力</code>”的信念。请你好好记住这一点</strong>。</p><p>青年：哪里有那种傻瓜呀？！正相反吧？只有得到了表扬才会感觉自己有能力。这不是理所当然的吗？</p><p>哲人：不对。<strong>假如你会因为得到表扬而感到喜悦，那就等于是从属于纵向关系和承认“<code>自己没能力</code>”</strong>。因为表扬是“<code>有能力的人对没能力的人所作出的评价</code>”。</p><p>青年：但是……但是，这还是难以接受！</p><p>哲人：如果以获得表扬为目的，那最终就会选择迎合他人价值观的生活方式。你不就一直因为按照父母的期待生活而感到厌烦吗？</p><p>青年：哎……哎呀，这个嘛。</p><p>哲人：首先应该进行课题分离，然后应该在接受双方差异的同时建立平等的横向关系。“<code>鼓励</code>”则是这种基础之上的一种方法。</p><blockquote><p> 横向关系和纵向关系表达不太认同，哲人觉得“<code>你干的不错</code>”就是纵向关系，“<code>谢谢</code>”就是横向关系，就是承认自己没有能力，这点我不太认可。</p></blockquote><h3 id="无论在哪里，都可以有平等的关系"><a href="#无论在哪里，都可以有平等的关系" class="headerlink" title="无论在哪里，都可以有平等的关系"></a>无论在哪里，都可以有平等的关系</h3><p>哲人：这是非常重要的一点。是建立纵向关系？还是建立横向关系？这是生活方式问题，人还没有灵活到可以随机应变地分别使用自己的生活方式，主要是“<code>不可能与这个人平等，因为与这个人是上下级关系</code>”。</p><p>青年：您是说在纵向关系和横向关系中只能选择一种？</p><p>哲人：是的。如果你与某人建立起了纵向关系，那你就会不自觉地从“<code>纵向</code>”去把握所有的人际关系。</p><p>青年：您是说我甚至对朋友也用纵向关系去理解？</p><p>哲人：没错。即使不按照上司或部下的关系去理解，也会产生诸如“<code>A君比我强，B君不如我</code>”“<code>要听从A君的意见，但不听B君的</code>”或者“<code>与C君的约定可以作废</code>”之类的想法。</p><p>青年：嗯……</p><p>哲人：反过来讲，如果能够与某个人建立起横向关系，也就是建立起真正意义上的平等关系的话，那就是生活方式的重大转变。以此为突破口，所有人际关系都会朝着“<code>横向</code>”发展。</p><h2 id="第五夜-认真的人生“活在当下”"><a href="#第五夜-认真的人生“活在当下”" class="headerlink" title="第五夜 认真的人生“活在当下”"></a>第五夜 认真的人生“活在当下”</h2><h3 id="不是肯定自我，而是接纳自我"><a href="#不是肯定自我，而是接纳自我" class="headerlink" title="不是肯定自我，而是接纳自我"></a>不是肯定自我，而是接纳自我</h3><p>哲人：还是共同体感觉。具体来说就是，把对自己的执著（self interest）转换成对他人的关心（social interest），建立起共同体感觉。这需要从以下三点做起：<strong>“<code>自我接纳</code>”“<code>他者信赖</code>”和“<code>他者贡献</code>”</strong>。</p><p>青年：噢，是新的关键词呀。都是什么呢？</p><p>哲人：首先从“<code>自我接纳</code>”开始说明。第一夜的时候，我曾经介绍了阿德勒“<code>重要的不是被给予了什么，而是如何去利用被给予的东西</code>”这句话，你还记得吧？</p><p>青年：当然。</p><p>哲人：我们既不能丢弃也不能更换“<code>我</code>”这个容器。但是，重要的是“<code>如何利用被给予的东西</code>”来改变对“<code>我</code>”的看法和利用方法。</p><p>青年：这是指更加积极、获得更强的自我肯定感、凡事都朝前看吗？</p><p>哲人：没必要特别积极地肯定自己，不是自我肯定而是自我接纳。</p><p>青年：不是自我肯定而是自我接纳？</p><p>哲人：是的，这两者有明显差异。自我肯定是明明做不到但还是暗示自己说“<code>我能行</code>”或者“<code>我很强</code>”，也可以说是一种容易导致优越情结的想法，是对自己撒谎的生活方式。</p><p>而另一方面，自我接纳是指假如做不到就诚实地接受这个“<code>做不到的自己</code>”，然后尽量朝着能够做到的方向去努力，不对自己撒谎。</p><p><strong>说得更明白一些就是，对得了60分的自己说“<code>这次只是运气不好，真正的自己能得100分</code>”，这就是自我肯定；与此相对，在诚实地接受60分的自己的基础上努力思考“<code>如何才能接近100分</code>”，这就是自我接纳。</strong></p><blockquote><p>自我接纳，就是认清自己的优缺点，接受现在的优缺点，利用现有的被给予的东西</p></blockquote><p>青年：您是说即使得了60分也不必悲观？</p><p>哲人：当然，毫无缺点的人根本没有，这在说明优越性追求的时候已经说过了吧？人都处于“<code>想要进步的状态</code>”。<br>反过来说也就是，根本没有满分的人。这一点必须积极地承认。</p><p>青年：嗯，这话听起来似乎很积极，但同时又有消极的因素。</p><p>哲人：所以我要使用“<code>肯定性的达观</code>”这个词。</p><p>青年：肯定性的达观？</p><p>哲人：课题分离也是如此，要分清“<code>能够改变的</code>”和“<code>不能改变的</code>”。我们无法改变“<code>被给予了什么</code>”。但是，关于“<code>如何去利用被给予的东西</code>”，我们却可以用自己的力量去改变。这就是不去关注“<code>无法改变的</code>”，而是去关注“<code>可以改变的</code>”。这就是我所说的自我接纳。</p><p>青年：……可以改变的和无法改变的。<br>哲人：是的。接受不能更换的事物，接受现实的“<code>这个我</code>”，然后，关于那些可以改变的事情，拿出改变的“<code>勇气</code>”。这就是自我接纳。</p><p>青年：哦，这么一说……以前有位作家曾引用过这样的话，“<code>上帝，请赐予我平静，去接受我无法改变的；给予我勇气，去改变我能改变的；赐我智慧，分辨这两者的区别。</code>”来自一部小说。</p><p>哲人：是的，我知道，这是广为流传的“<code>尼布尔的祈祷文</code>”，是一段非常有名的话。</p><p>青年：而且，这里也使用了“<code>勇气</code>”这个词。我本以为已经烂熟于心了，但现在才察觉到它的意思。</p><p>哲人：是的，我们并不缺乏能力，只是缺乏“<code>勇气</code>”。一切都是“<code>勇气</code>”的问题</p><p>建立共同体感觉需要从三点做起：自我接纳，他者信赖，他者贡献。</p><ol><li><p>自我接纳：肯定性的达观。接受自己无法改变的；有勇气去改变自己能改变的；有智慧去分辨这两者的区别。</p></li><li><p>他者信赖：在相信他人的时候不附加任何条件。即使没有足以构成信用的客观依据也依然相信，不考虑抵押之类的事情，无条件地相信。这就是信赖。<br>信赖的反义是怀疑，不是背叛。</p></li><li><p>他者贡献：他者贡献并不是牺牲自我为他人，相反，它是为了能够体会到“<code>我</code>”的价值而采取的一种手段。因为我们只有在感觉到自己的存在或行为对共同体有益的时候，也就是体会到“<code>我对他人有用</code>”的时候，才能切实感受到自己的价值。</p></li></ol><h3 id="信用和信赖有何区别？"><a href="#信用和信赖有何区别？" class="headerlink" title="信用和信赖有何区别？"></a>信用和信赖有何区别？</h3><p>哲人：在这里需要把“<code>相信</code>”这个词分成信用和信赖来区别考虑。首先，信用有附加条件，用英语讲就是“<code>credit</code>”。例如，想要从银行贷款，就必须提供某些抵押。银行会估算抵押价值然后贷给你相应的金额。“<code>如果你还的话我就借给你</code>”或是“<code>只借给你能够偿还的份额</code>”，这种态度并不是信赖，而是信用。</p><p>哲人：与此相对，阿德勒心理学认为人际关系的基础不应该是“<code>信用</code>”，而应该是“<code>信赖</code>”。</p><p>哲人：阿德勒心理学的观点很简单。你现在认为“<code>无条件地信赖别人只会遭到背叛</code>”。但是，决定背不背叛的不是你，那是他人的课题。你只需要考虑“<code>我该怎么做</code>”。“<code>如果对方讲信用我也给予信任</code>”，这只不过是一种基于抵押或条件的信用关系。<br>青年：您是说这也是课题分离？</p><p>哲人：是的。就像我反复提到的一样，如果能够进行课题分离，那么人生就会简单得令你吃惊。但是，即使理解课题分离的原理和原则比较容易，实践起来也非常困难。这一点我也承认。</p><p>青年：那么，难道我们就应该信赖所有人，即使遭到欺骗依然继续相信，一直做个傻瓜式的老好人吗？这种论调既不是哲学也不是心理学，这简直是宗教家的说教！</p><p>哲人：这一点我要明确否定。阿德勒心理学并没有基于道德价值观去主张“<code>要无条件地信赖他人</code>”。<strong>无条件的信赖是搞好人际关系和构建横向关系的一种“<code>手段</code>”</strong>。<br>如果你并不想与那个人搞好关系的话，也可以用手中的剪刀彻底剪断关系，<strong>因为剪断关系是你自己的课题</strong>。</p><blockquote><p>这个观点好像没太大意义，无非告诉我需要无条件信任朋友，但是如果朋友背叛了你，你可以剪短关系？</p></blockquote><h3 id="工作的本质是对他人的贡献"><a href="#工作的本质是对他人的贡献" class="headerlink" title="工作的本质是对他人的贡献"></a>工作的本质是对他人的贡献</h3><blockquote><p>他者贡献是指在自我接纳和他者信任的基础上，无私的为他人做出贡献。</p></blockquote><p>哲人：首先，真诚地接受不能交换的“<code>这个我</code>”，这就是自我接纳。同时，对他人寄予无条件的信赖即他者信赖。<br>既能接纳自己又能信赖他人，这种情况下，对你来说的他人会是怎样的存在呢？</p><p>青年：……是伙伴吗？</p><p>哲人：正是如此。对他人寄予信赖也就是把他人看成伙伴。正因为是伙伴，所以才能够信赖。如果不是伙伴，也就做不到信赖。</p><p>并且，如果把他人看作伙伴，那你也就能够在所属的共同体中找到自己的位置，继而也就能够获得“<code>可以在这里</code>”的归属感。</p><p>青年：也就是说，要想获得归属感就必须把他人看作伙伴，而要做到视他人为伙伴就需要自我接纳和他者信赖。</p><p>哲人：是的，你理解得越来越快啦！并且，视他人为敌的人既做不到自我接纳，也无法充分做到他者信赖。</p><p>青年：好吧。人的确都在寻找一种“<code>可以在这里</code>”的归属感，因此就需要自我接纳和他者信赖。这一点我没有异议。<br>但是，仅凭把他人看作伙伴并给予信赖就可以获得归属感吗？</p><p>哲人：当然，共同体感觉并不是仅凭自我接纳和他者信赖就可以获得的。这里还需要第三个关键词——“<code>他者贡献</code>”。</p><p>青年：他者贡献？</p><p>哲人：对作为伙伴的他人给予影响、作出贡献，这就是他者贡献。</p><p>青年：贡献也就是发扬自我牺牲精神为周围人效劳吧？</p><p>哲人：他者贡献的意思并不是自我牺牲。相反，阿德勒把为他人牺牲自己人生的人称作“<code>过度适应社会的人</code>”，并对此给予警示。<br>并且，请你想一想。我们只有在感觉到自己的存在或行为对共同体有益的时候，也就是体会到“<code>我对他人有用</code>”的时候，才能切实感受到自己的价值。是这样吧？<br>也就是说，他者贡献并不是舍弃“<code>我</code>”而为他人效劳，它反而是为了能够体会到“<code>我</code>”的价值而采取的一种手段。</p><p>青年：贡献他人是为了自己？</p><p>哲人：是的，不需要自我牺牲。</p><p>青年：哎呀哎呀，您的论调越来越危险了吧？这可真是自掘坟墓啊！为了满足“<code>我</code>”而去为他人效劳，这不正是伪善的定义吗？！所以我才说您的主张全都是伪善！您的论调全都不可信！算了吧，先生！比起满口道德谎言的善人，我宁愿相信那些忠实于自己欲望的恶徒！</p><p>哲人：言之过早了。你还没有真正理解共同体感觉。</p><p>青年：那么，关于先生主张的他者贡献，请您举个具体例子吧。</p><p>哲人：最容易理解的他者贡献就是工作——到社会上去工作或者做家务。劳动并不是赚取金钱的手段，我们通过劳动来实现他者<br>贡献、参与共同体、体会“<code>我对他人有用</code>”，进而获得自己的存在价值。</p><p>哲人：当然，赚钱也是一个重大要素。正如你之前查到的陀思妥耶夫斯基所说的“<code>被铸造的自由</code>”一样。但是，有些富豪已经拥有了一生也花不完的巨额财产，但他们中的多数人至今依然继续忙碌工作着。为什么要工作呢？是因为无底的欲望吗？不是。这是为了他者贡献继而获得“<code>可以在这里的</code>”归属感。获得巨额财富之后便致力于参加慈善活动的富豪们，也为了能够体会自我价值、确认“<code>可以在这里</code>”的归属感而进行着各种各样的活动。</p><h3 id="“工作狂”是人生谎言"><a href="#“工作狂”是人生谎言" class="headerlink" title="“工作狂”是人生谎言"></a>“工作狂”是人生谎言</h3><p>哲人：阿德勒心理学认为这种生活方式是缺乏“<code>人生和谐</code>”的生活方式，是一种只凭事物的一部分就来判断整体的生活方式。</p><p>青年：人生和谐？</p><p>哲人：犹太教教义中有这么一段话：“<code>假如有10个人，其中势必会有1个人无论遇到什么事都会批判你。他讨厌你，你也不喜欢他。而且，10个人中也会有2个人能够成为与你互相接纳一切的好朋友。剩下的7个人则两者都不是。</code>”<br>这种时候，是关注讨厌你的那个人呢？还是聚焦于非常喜欢你的那2个人？抑或是关注其他作为大多数的7个人？缺乏人生和谐的人就会只关注讨厌自己的那个人来判断“<code>世界</code>”。</p><p>哲人：例如，那些是“<code>工作狂</code>”的人。这些人也缺乏人生和谐。</p><p>青年：工作狂也是？为什么？</p><p>哲人：口吃者是只看事物的一部分便来判断其整体。与此相对，工作狂则是只关注人生特定的侧面。<br>也许他们会辩解说：“<code>因为工作忙，所以无暇顾及家庭。</code>”但是，这其实是人生的谎言。只不过是以工作为借口来逃避其他责任。本来家务、育儿、交友或兴趣应该全都给予关心，阿德勒不认可任何一方面突出的生活方式。</p><p>青年：啊……我父亲就是这样的人。他是个工作狂，一心只想着在工作上出成绩；并且，还以自己挣钱为理由来支配家人；是个非常封建的人。</p><p>哲人：在某种意义上来说，这是一种不敢正视人生课题的生活方式。“<code>工作</code>”并不仅仅是指在公司上班。家庭里的工作、育儿、对地域社会的贡献、兴趣等，这一切都是“<code>工作</code>”，公司等只不过是一小部分而已。只考虑公司的工作，那是一种缺乏人生和谐的生活方式。</p><blockquote><p>说的我就是这样的人。需要反思下自己。的确是在逃避其他责任。缺乏人生和谐。</p></blockquote><p>青年：哎呀，正是如此！而且，被抚养的家人还根本不能反驳。对于父亲“<code>想想你是靠谁才吃上饭的吧！</code>”这种近似暴力的语言也不能反驳。</p><p>哲人：也许这样的父亲只能靠“<code>行为标准</code>”来认可自己的价值。认为自己工作了这些时间、挣了足以养活家人的钱、也得到了社会的认可，所以自己就是家里最有价值的人。<br>但是，任何人都有自己不再是生产者的时候。例如，上了年纪退休之后不得不靠退休金或孩子们的赡养生活；或者虽然年轻但因为受伤或生病而无法劳动。这种时候，只能用“<code>行为标准</code>”来接受自己的人总会受到非常严重的打击。</p><p>青年：也就是那些拥有“<code>工作就是一切</code>”这种生活方式的人吧？</p><p>哲人：是的。是缺乏人生和谐的人。</p><p>青年：……如此想来，我似乎能够理解先生上次所说的“<code>存在标准</code>”的意思了。我的确没有认真想过自己无法劳动、在“<code>行为标准</code>”上做不了任何事时候的情况。</p><p>哲人：是按照“<code>行为标准</code>”来接受自己还是按照“<code>存在标准</code>”来接受自己，这正是一个有关“<code>获得幸福的勇气</code>”的问题。</p><h3 id="从这一刻起，就能变的幸福"><a href="#从这一刻起，就能变的幸福" class="headerlink" title="从这一刻起，就能变的幸福"></a>从这一刻起，就能变的幸福</h3><p>哲人：对人而言，最大的不幸就是不喜欢自己。对于这种现实，阿德勒准备了极其简单的回答——“<code>我对共同体有益</code>”或者“<code>我对他人有用</code>”这种想法就足以让人体会到自己的价值。</p><p>青年：也就是您刚才提到的他者贡献吧？</p><p>哲人：是的。并且，还有非常重要的一点，那就是这里所说的他者贡献也可以是看不见的贡献。</p><p>青年：可以是看不见的贡献？</p><p>哲人：判断你的贡献是否起作用的不是你，那是他人的课题，是你无法干涉的问题。是否真正作出了贡献，从原理上根本无从了解。也就是说，进行他者贡献时候的我们即使作出看不见的贡献，只要能够产生“<code>我对他人有用</code>”的主观感觉即“<code>贡献感</code>”也可以。</p><p>青年：请等一下！这么说来，先生认为的幸福就是……</p><p>哲人：你已经察觉到了吧？也就是“<code>幸福即贡献感</code>”。这就是幸福的定义。</p><blockquote><p>作者认为一切烦恼来自人际关系，所以幸福也来自人际关系。所以“<code>共享感(自身价值的提醒)</code>”就是幸福。</p><p>这个观点我只认可一般。而且我也不认为一切烦恼都来自人际关系</p></blockquote><p>青年：但、但是，这……</p><p>哲人：怎么啦？</p><p>青年：我不能认可这么简单的定义！先生的话我还记得，就是您以前说过的“<code>即使在行为标准上对谁都没有用，但从存在标准上考虑人人都有用</code>”那句话。如果是这样的话，那岂不是成了所有的人都幸福吗？！</p><h3 id="人生是一连串的刹那-活在当下、活好当下"><a href="#人生是一连串的刹那-活在当下、活好当下" class="headerlink" title="人生是一连串的刹那 : 活在当下、活好当下"></a>人生是一连串的刹那 : 活在当下、活好当下</h3><p>哲人：请不要把人生理解为一条线，而要理解成点的连续。<br>如果拿放大镜去看用粉笔画的实线，你会发现原本以为的线其实也是一些连续的小点。看似像线一样的人生其实也是点的连续，也就是说人生是连续的刹那。</p><p>青年：连续的刹那？</p><p>哲人：是的，是“<code>现在</code>”这一刹那的连续。我们只能活在“<code>此时此刻</code>”，我们的人生只存在于刹那之中。<br>不了解这一点的大人们总是想要强迫年轻人过“<code>线</code>”一样的人生。在他们看来，上好大学、进好企业、拥有稳定的家庭，这样的轨道才是幸福的人生。但是，人生不可能是一条线。</p><p>青年：您是说没必要进行人生规划或者职业规划？</p><p>哲人：如果人生是一条线，那么人生规划就有可能。但是，我们的人生只是点的连续。计划式的人生不是有没有必要，而是根本不可能</p><ol><li>过去不可逆，未来即注定。人类活在惯性中，从小看老，现在游手好闲，未来也不会有成就。</li><li>想种一棵树最好的时间是十年前，其次是现在。<br>我们活在当下，拥有现在。因为我们掌控着名为“<code>此时此刻</code>”的刹那，现在做的每一件事，在未来都有相应的结果。<br>想健身，趁现在；想旅行，趁现在；想学习，趁现在。</li><li>不要为失去的奶酪哭泣。<br>情绪是可以通过练习被理智掌控。<br>现在的焦虑、痛苦和不幸，其实是你不愿意走出的“<code>舒适圈</code>”，因为你可以用这些情绪掩饰你的好吃懒做、吐嘈抱怨。<br>但是，这些负面情绪真的好吗，与其怀着焦虑的心情玩一天手机，不如开开心心没心没肺的追一天剧。<br>可以不努力，选择随波逐流的活着，但是垃圾的情绪留着也没用，不是吗？</li></ol><h3 id="舞动人生"><a href="#舞动人生" class="headerlink" title="舞动人生"></a>舞动人生</h3><p>哲人：请你这样想。人生就像是在每一个瞬间不停旋转起舞的连续的刹那。并且，暮然四顾时常常会惊觉：“<code>已经来到这里了吗？</code>”<br>在跳着小提琴之舞的人中可能有人成了专业小提琴手，在跳着司法考试之舞的人中也许有人成为律师，或许还有人跳着写作之舞成了作家。当然，也有可能有着截然不同的结果。但是，所有的人生都不是终结“<code>在路上</code>”，只要跳着舞的“<code>此时此刻</code>”充实就已经足够。</p><p>青年：只要跳好当下就可以？</p><p>哲人：是的。在舞蹈中，跳舞本身就是目的，最终会跳到哪里谁都不知道。当然，作为跳的结果最终会到达某个地方。因为一直在跳动所以不会停在原地。但是，并不存在目的地。</p><p>青年：怎么能有不存在目的地的人生呢？！谁会承认这种游移不定、随风飘摇的人生呢？！</p><p>哲人：你所说的想要到达目的地的人生可以称为“<code>潜在性的人生</code>”。与此相对，我所说的像跳舞一样的人生则可以称为“<code>现实性的人生</code>”。</p><p>青年：潜在性和现实性？</p><p>哲人：我们可以引用亚里士多德的说明。一般性的运动——我们把这叫作移动——有起点和终点。从起点到终点的运动最好是尽可能地高效而快速。如果能够搭乘特快列车的话，那就没有必要乘坐各站都停的普通列车。</p><p>青年：也就是说，如果有了想要成为律师这个目的地，那就最好是尽早尽快地到达。</p><p>哲人：是的。并且，到达目的地之前的路程在还没有到达目的地这个意义上来讲并不完整。这就是潜在性的人生。</p><h3 id="最重要的是“此时此刻”"><a href="#最重要的是“此时此刻”" class="headerlink" title="最重要的是“此时此刻”"></a>最重要的是“此时此刻”</h3><p>哲人：请你想象一下自己站在剧场舞台上的样子。此时，如果整个会场都开着灯，那就可以看到观众席的最里边。但是，如果强烈的聚光灯打向自己，那就连最前排也看不见。<br>我们的人生也完全一样。正因为把模糊而微弱的光打向人生整体，所以才能够看到过去和未来；不，是感觉能够看得到。但是，如果把强烈的聚光灯对准“<code>此时此刻</code>”，那就会既看不到过去也看不到未来。</p><p>青年：强烈的聚光灯？</p><p>哲人：是的。我们应该更加认真地过好“<code>此时此刻</code>”。如果感觉能够看得到过去也能预测到未来，那就证明你没有认真地活在“<code>此时此刻</code>”，而是生活在模糊而微弱的光中。<br>人生是连续的刹那，根本不存在过去和未来。你是想要通过关注过去或未来为自己寻找免罪符。过去发生了什么与你的“<code>此时此刻</code>”没有任何关系，未来会如何也不是“<code>此时此刻</code>”要考虑的问题。假如认真地活在“<code>此时此刻</code>”，那就根本不会说出那样的话。</p><p>青年：但、但是……</p><p>哲人：如果站在弗洛伊德式原因论的立场上，那就会把人生理解为基于因果律的一个长故事。何时何地出生、度过了什么样的童年时代、从什么样的学校毕业、进了什么样的公司，正是这些因素决定了现在的我和将来的我。<br>的确，把人生当作故事是很有趣的事情。但是，在故事的前面部分就能看到“<code>模糊的将来</code>”；并且，人们还会想要按照这个故事去生活。我的人生就是这样，所以我只能照此生活，错不在我而在于过去和环境。这里搬出来的过去无非是一种免罪符，是人生的谎言。<br>但是，人生是点的连续、是连续的刹那。如果能够理解这一点，那就不再需要故事。</p><p>青年：如果这么说的话，阿德勒所说的生活方式不也是一种故事吗？！</p><p>哲人：生活方式说的是“<code>此时此刻</code>”，是可以按照自己意志改变的事情。像直线一样的过去的生活只不过是在你反复下定决心“<code>不做改变</code>”的基础上才貌似成了直线而已。并且，将来的人生也完全是一张白纸，并未铺好行进的轨道。这里没有故事。<br>青年：但是，这是一种逍遥主义！不，应该说是更加恶劣的享乐主义！</p><p>哲人：不！聚焦“<code>此时此刻</code>”是认真而谨慎地做好现在能做的事情</p><p>哲人：因为过去和未来根本不存在，所以才要谈现在。起决定作用的既不是昨天也不是明天，而是“<code>此时此刻</code>”</p>]]></content>
      
      
      <categories>
          
          <category> Literature </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MIT - 线性代数</title>
      <link href="/2021/08/18/mit-linear-algebra-1-3/"/>
      <url>/2021/08/18/mit-linear-algebra-1-3/</url>
      
        <content type="html"><![CDATA[<h2 id="一、Lesson-1"><a href="#一、Lesson-1" class="headerlink" title="一、Lesson 1"></a>一、Lesson 1</h2><h3 id="1-1-方程组的几何解释"><a href="#1-1-方程组的几何解释" class="headerlink" title="1.1 方程组的几何解释"></a>1.1 方程组的几何解释</h3><p><img src="/2021/08/18/mit-linear-algebra-1-3/math.svg"></p><p>上面方程组我们可以写成矩阵形式</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-1.svg"></p><p>上面的矩阵可以看成 <code>Ax = b</code>的形式 :</p><ol><li>系数矩阵(A)：将方程系数按行提取出来，构成一个矩阵</li><li>未知向量(x)：将方程未知数提取出来，按列构成一个向量。</li><li>向量(b) ：将等号右侧结果按列提取，构成一个向量</li></ol><h4 id="1-1-1-行图像"><a href="#1-1-1-行图像" class="headerlink" title="1.1.1 行图像"></a>1.1.1 行图像</h4><p>在坐标系上画出“行图像”，可以知两个线交点就是我们要求的解</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-daf0b19e00a4a95f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="1-1-2-列图像"><a href="#1-1-2-列图像" class="headerlink" title="1.1.2 列图像"></a>1.1.2 列图像</h4><p>从列图像的角度，我们再求这个方程可以看成矩阵：</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-2.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-3.svg"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f9002c2daa374179.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="1-2-方程组的几何形式推广"><a href="#1-2-方程组的几何形式推广" class="headerlink" title="1.2 方程组的几何形式推广"></a>1.2 方程组的几何形式推广</h3><h4 id="1-2-1-高维行图像"><a href="#1-2-1-高维行图像" class="headerlink" title="1.2.1 高维行图像"></a>1.2.1 高维行图像</h4><p>我们将方程维数推广，从三维开始，如果我们继续做行图像求解，那么会的到一个很复杂的图像。</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-4.svg"></p><p>矩阵如下：</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-5.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-6.svg"></p><p>如果绘制行图像，很明显这是一个三个平面相交得到一点，我们想直接看出 这个点的性质可谓是难上加难，比较靠谱的思路是先联立其中两个平面，使其相 交于一条直线，在研究这条直线与平面相交于哪个点，最后得到点坐标即为方程 的解。</p><p><strong>这个求解过程对于三维来说或许还算合理，那四维呢？五维甚至更高维数 呢？直观上很难直接绘制更高维数的图像，这种行图像受到的限制也越来越多。</strong></p><h4 id="1-2-2-高维列图像"><a href="#1-2-2-高维列图像" class="headerlink" title="1.2.2 高维列图像"></a>1.2.2 高维列图像</h4><p>我们使用列图像的思路进行计算，那矩阵形式就变为：</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-7.svg"></p><p>左侧是线性组合，右侧是合适的线性组合组成的结果，这样一来思路就清晰多 了，“寻找线性组合”成为了解题关键。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8d2264df832779b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="1-2-3-不能求解的场景"><a href="#1-2-3-不能求解的场景" class="headerlink" title="1.2.3 不能求解的场景"></a>1.2.3 不能求解的场景</h4><p>另外，还要注意的一点是对任意的 b 是不是都能求解 Ax = b 这个矩阵方程呢？ 也就是对 3<em>3 的系数矩阵 A，其列的线性组合是不是都可以覆盖整个三维空间呢？ 对于我们举的这个例子来说，一定可以，还有我们上面 2</em>2 的那个例子，也可以 覆盖整个平面，但是有一些矩阵就是不行的，比如三个列向量本身就构成了一个 平面，那么这样的三个向量组合成的向量只能活动在这个平面上，肯定无法覆盖 2 −1 1 一个三维空间，</p><p>比如三个列向量分别为</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-8.svg"></p><p>由于第一列+第二列=第三列，第三列向量并没有任何作用。</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-9.svg"></p><p>上面的矩阵，只能构成一个平面，这样的矩阵构成的方程<code>Ax = b</code>,其中的b就无法覆盖整个三维空间，也就无法实现：对任意的b，都能求解 <code>Ax = b</code>这个方程。</p><h3 id="1-3-矩阵乘法"><a href="#1-3-矩阵乘法" class="headerlink" title="1.3 矩阵乘法"></a>1.3 矩阵乘法</h3><p>行列式乘法，C_1_1 = A(Row 1) * B(Col 1)</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-10.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-11.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-12.svg"></p><h2 id="Lesson-2"><a href="#Lesson-2" class="headerlink" title="Lesson 2"></a>Lesson 2</h2><p>本节主要内容是矩阵消元和逆矩阵。</p><h3 id="2-1-消元矩阵"><a href="#2-1-消元矩阵" class="headerlink" title="2.1 消元矩阵"></a>2.1 消元矩阵</h3><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-13.svg"></p><p>写成矩阵形式 <code>Ax = b</code></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-14.svg"></p><p>矩阵消元其实跟方程消元差不多，不过矩阵消元得到的结果是最终是一个下三角都是0的矩阵（上三角矩阵）。</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-15.svg"></p><p>主元：U(1,1),U(2,2),U(3,3)  我们视为主元(pivot)</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-16.svg"></p><p>注： 并不是所有的 A 矩阵都可消元处理，需要注意在我们消元过程中，如果 主元位置（左上角）为 0，那么意味着这个主元不可取，需要进行 “换行”处理， 首先看它的下一行对应位置是不是 0，如果不是，就将这两行位置互换，将非零数视为主元。</p><p>如果是，就再看下下行，以此类推。若其下面每一行都看到了，仍然没有非零数的话，那就意味着这个矩阵不可逆，消元法求出的解不唯一(其实就是少了一个变量，求解不了方程)。下面是三个例子：</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-17.svg"></p><p>我们把上面的U 带回方程<code>Ax = b</code></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-18.svg"></p><h3 id="2-2-单位阵"><a href="#2-2-单位阵" class="headerlink" title="2.2 单位阵"></a>2.2 单位阵</h3><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-19.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-20.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-21.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-22.svg"></p><p>我们很显然验证单位阵与任意矩阵相乘，不改变矩阵。例如：</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-23.svg"></p><p>再看下上面的消元过程</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-24.svg"></p><p>第一步是把第一行乘以 -3 然后加上第二行。所以有</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-26.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-27.svg"></p><p>所以第一步消元矩阵就是</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-28.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-29.svg"></p><p>结论： 求消元矩阵，其实就是从“单位阵”开始，按照A每次变化消元的步骤操作 I 矩阵，分别能得到E(row,clo),最后累积得到E即可。</p><h3 id="2-3-行列变换"><a href="#2-3-行列变换" class="headerlink" title="2.3 行列变换"></a>2.3 行列变换</h3><p>由上面的“单位阵”起发，不难得到交换2 x 2矩阵行列矩阵为：</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-30.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-31.svg"></p><p><strong>所以左乘同行交换，右乘同列交换</strong></p><h3 id="2-4-逆矩阵"><a href="#2-4-逆矩阵" class="headerlink" title="2.4 逆矩阵"></a>2.4 逆矩阵</h3><p>E(2,1) 是基于“单位阵 I” 第一行*(-3)加第二行得到：</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-32.svg"></p><p>反之，我们在第二行上加上第一行乘以3可以复原这一运算过程：</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-33.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-34.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-35.svg"></p><h2 id="Lesson-3"><a href="#Lesson-3" class="headerlink" title="Lesson 3"></a>Lesson 3</h2><h3 id="3-1-矩阵乘法"><a href="#3-1-矩阵乘法" class="headerlink" title="3.1 矩阵乘法"></a>3.1 矩阵乘法</h3><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-36.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-37.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-38.svg"></p><h3 id="3-2-列组合"><a href="#3-2-列组合" class="headerlink" title="3.2 列组合"></a>3.2 列组合</h3><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-39.svg"></p><p>矩阵乘法也可以拆解成矩阵和向量乘法</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-40.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-41.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-42.svg"></p><p>这种方法的关键就是将右侧矩阵 B 看做列向量组合，将问题转化为矩阵与向量的乘法问题。也表明了矩阵 C 就是矩阵 A 中各列向量的线性组合，而 B 其实是在告诉我们，要以什么样的方式组合 A 中的列向量。</p><h3 id="3-3-行组合"><a href="#3-3-行组合" class="headerlink" title="3.3 行组合"></a>3.3 行组合</h3><p>与上面列组合有点相似</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-43.svg"></p><h3 id="3-4-逆矩阵"><a href="#3-4-逆矩阵" class="headerlink" title="3.4 逆矩阵"></a>3.4 逆矩阵</h3><p>对于一个方阵A，如果A可逆，就有这样一个A<sup>-1</sup>使得<br><img src="/2021/08/18/mit-linear-algebra-1-3/math-44.svg"></p><p>如果A不是方阵，左侧的A<sup>-1</sup>和右侧的A<sup>-1</sup>肯定不相等。违背了我们说的有唯一的一个A<sup>-1</sup></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-45.svg"></p><p>或者换个看法，我们看到这个矩阵中两个列向量[1,2] ,[3,6],他们是线性相关的，他们之前互为倍数，也就是说这两个向量之一对其线性组合无意义，那么A不可能有逆。所有推出：</p><p><strong>若存在非零向量x，使得 Ax = 0， 那么A就不可能有逆矩阵。</strong></p><p>因为如果 A 有逆，在 <code>Ax=0</code>这个等式两端同时乘上A<sup>-1</sup>就有：</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-47.svg"></p><p>而 I 是单位矩阵，x 又是一个非零0的向量所以不可能是零向量。自相矛盾，所以此时A没有逆矩阵。</p><h3 id="3-5-高斯消元求逆矩阵"><a href="#3-5-高斯消元求逆矩阵" class="headerlink" title="3.5 高斯消元求逆矩阵"></a>3.5 高斯消元求逆矩阵</h3><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-48.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-49.svg"></p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-50.svg"></p><p>高斯消元来求逆矩阵</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-51.svg"></p><p>所以逆矩阵为</p><p><img src="/2021/08/18/mit-linear-algebra-1-3/math-52.svg"></p>]]></content>
      
      
      <categories>
          
          <category> Maths </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LinearAlgebra </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>遍历二叉树的几种思路</title>
      <link href="/2021/08/12/post-order/"/>
      <url>/2021/08/12/post-order/</url>
      
        <content type="html"><![CDATA[<h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>最近在公司面试（一面、二面）候选人的时候，大多数候选人基本都能正确的写出非递归版的<code>前序遍历</code>和<code>中序遍历</code>二叉树，但是大多数人都不能正确的写出非递归版的<code>后续遍历</code>。跟一个曾经拿过<code>NOI银牌</code>同事私下讨论了下<code>后续遍历</code>算法到底难不难。结论是，说难也难说不难也不难，说不难是因为，如果你看过相关解法，你可以很快就就理解解法的思路。说难，是如果你没看过，或者看了过了很久又忘了，要在15分钟左右写个<code>Bug free</code>的版本还是有点难的。</p><p>跟同事讨论下二叉树遍历的几种写法，所以就有了这篇文章。</p><h2 id="二、二叉树几种解法的思考"><a href="#二、二叉树几种解法的思考" class="headerlink" title="二、二叉树几种解法的思考"></a>二、二叉树几种解法的思考</h2><h3 id="2-1-递归版"><a href="#2-1-递归版" class="headerlink" title="2.1 递归版"></a>2.1 递归版</h3><p>前序遍历递归</p><pre><code>func preOrderRecursion(node *TreeNode, ans *[]int) {    if node == nil {        return    }    *ans = append(*ans, node.Val)    postorderTraversal1(node.Left, ans)    postorderTraversal1(node.Right, ans)    return}</code></pre><p>中序遍历递归</p><pre><code>func inOrderRecursion(node *TreeNode, ans *[]int) {    if node == nil {        return    }    postorderTraversal1(node.Left, ans)    *ans = append(*ans, node.Val)    postorderTraversal1(node.Right, ans)    return}</code></pre><p>后序遍历递归</p><pre><code>func postOrderRecursion(node *TreeNode, ans *[]int) {    if node == nil {        return    }    postorderTraversal1(node.Left, ans)    postorderTraversal1(node.Right, ans)    *ans = append(*ans, node.Val)    return}</code></pre><h3 id="2-2-迭代-栈"><a href="#2-2-迭代-栈" class="headerlink" title="2.2 迭代 - 栈"></a>2.2 迭代 - 栈</h3><p>前序遍历 - 栈</p><pre><code>func preOrder(root *TreeNode) []int {    res := make([]int, 0)    stack := []*TreeNode{root}    for len(stack) != 0 {        node := stack[len(stack)-1]        stack = stack[:len(stack)-1]        res = append(res, node.Val)        if node.Right != nil {            stack = append(stack, node.Right)        }        if node.Left != nil {            stack = append(stack, node.Left)        }    }    return res}</code></pre><p>中序遍历 - 栈</p><pre><code>func inOrder(root *TreeNode) []int {    res := make([]int, 0)    stack := make([]*TreeNode, 0)    node := root    for node != nil || len(stack) &gt; 0 {        if node != nil {            stack = append(stack, node)            node = node.Left            continue        }        node = stack[len(stack)-1]        stack = stack[:len(stack)-1]        res = append(res, node.Val)        node = node.Right    }    return res}</code></pre><p>后续遍历</p><pre><code>func postOrder(root *TreeNode) []int {    res := make([]int, 0)    node := root    stack := make([]*TreeNode, 0)    var prev *TreeNode    for node != nil || len(stack) &gt; 0 {        if node != nil {            stack = append(stack, node)            node = node.Left            continue        }        node = stack[len(stack)-1]        stack = stack[:len(stack)-1]        if node.Right == nil || node.Right == prev {            res = append(res, node.Val)            prev = node            node = nil        } else {            stack = append(stack, node)            node = node.Right        }    }    return res}</code></pre><h3 id="2-3-Morris-遍历"><a href="#2-3-Morris-遍历" class="headerlink" title="2.3 Morris 遍历"></a>2.3 Morris 遍历</h3><p>前序遍历 - Morris</p><pre><code>func preorderTraversal(root *TreeNode) (vals []int) {    var p1, p2 *TreeNode = root, nil    for p1 != nil {        p2 = p1.Left        if p2 != nil {            for p2.Right != nil &amp;&amp; p2.Right != p1 {                p2 = p2.Right            }            if p2.Right == nil {                vals = append(vals, p1.Val)                p2.Right = p1                p1 = p1.Left                continue            }            p2.Right = nil        } else {            vals = append(vals, p1.Val)        }        p1 = p1.Right    }    return}</code></pre><p>中序遍历 - Morris</p><pre><code>func inorderTraversal(root *TreeNode) (res []int) {    for root != nil {        if root.Left != nil {            // predecessor 节点表示当前 root 节点向左走一步，然后一直向右走至无法走为止的节点            predecessor := root.Left            for predecessor.Right != nil &amp;&amp; predecessor.Right != root {                // 有右子树且没有设置过指向 root，则继续向右走                predecessor = predecessor.Right            }            if predecessor.Right == nil {                // 将 predecessor 的右指针指向 root，这样后面遍历完左子树 root.Left 后，就能通过这个指向回到 root                predecessor.Right = root                // 遍历左子树                root = root.Left            } else { // predecessor 的右指针已经指向了 root，则表示左子树 root.Left 已经访问完了                res = append(res, root.Val)                // 恢复原样                predecessor.Right = nil                // 遍历右子树                root = root.Right            }        } else { // 没有左子树            res = append(res, root.Val)            // 若有右子树，则遍历右子树            // 若没有右子树，则整颗左子树已遍历完，root 会通过之前设置的指向回到这颗子树的父节点            root = root.Right        }    }    return}</code></pre><p>后序遍历 - Morris</p><pre><code>func reverse(a []int) {    for i, n := 0, len(a); i &lt; n/2; i++ {        a[i], a[n-1-i] = a[n-1-i], a[i]    }}func postorderTraversal(root *TreeNode) (res []int) {    addPath := func(node *TreeNode) {        resSize := len(res)        for ; node != nil; node = node.Right {            res = append(res, node.Val)        }        reverse(res[resSize:])    }    p1 := root    for p1 != nil {        if p2 := p1.Left; p2 != nil {            for p2.Right != nil &amp;&amp; p2.Right != p1 {                p2 = p2.Right            }            if p2.Right == nil {                p2.Right = p1                p1 = p1.Left                continue            }            p2.Right = nil            addPath(p1.Left)        }        p1 = p1.Right    }    addPath(root)    return}</code></pre><h3 id="2-4-基于栈帧的思想把递归转成for循环"><a href="#2-4-基于栈帧的思想把递归转成for循环" class="headerlink" title="2.4 基于栈帧的思想把递归转成for循环"></a>2.4 基于栈帧的思想把递归转成for循环</h3><p>我们可以把递归版本的迭代，基于函数调用的栈帧思想，转成<code>for</code>循环，如下代码，我们知道递归调用对应了<code>4</code>行代码：</p><pre><code>func postOrder(node *TreeNode, ans *[]int) {    if node == nil {return}       // line == 0    postOrder(node.Left, ans)     // line == 1    postOrder(node.Right, ans)    // line == 2    *ans = append(*ans, node.Val) // line == 3}</code></pre><p>转成<code>for</code>循环如下，我们在每个<code>line</code>执行上面不同的逻辑操作。这种方法的好处是，无聊是前序、中序、后续算法，我们只要调整下面 <code>if line == xx</code>的逻辑就行了。理论上所有的递归转非递归都可以基于这个思想去做。</p><pre><code>type DFSNode struct {    line int   // 表示代码行数    v    *TreeNode // 表示当前 node }func postorderTraversal(root *TreeNode) []int {    stack := []*DFSNode{}    stack = append(stack, &amp;DFSNode{        v:    root,        line: 0,    })        ans := []int{}        for len(stack) &gt; 0 {        cur := stack[len(stack)-1]        if cur.line == 0 {  // 对应上面的 if node == nil {return}            if cur.v == nil { // 如果 node 为 nil ，出栈                stack = stack[0 : len(stack)-1]                continue            }        } else if cur.line == 1 {  // 对应上面 postOrder(node.Left, ans)            stack = append(stack, &amp;DFSNode{ // 函数调用，压栈                v:    cur.v.Left,                line: 0, // 从第 0 行 开始            })        } else if cur.line == 2 { // postOrder(node.Right, ans)              stack = append(stack, &amp;DFSNode{ // 函数调用，压栈                v:    cur.v.Right,                line: 0, // 从第 0 行 开始            })        } else if cur.line == 3 { //             ans = append(ans, cur.v.Val)            stack = stack[0 : len(stack)-1]        }                cur.line++ // 执行下一行代码    }    return ans}</code></pre>]]></content>
      
      
      <categories>
          
          <category> Arithmetic </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataStructure </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《操作系统45讲》</title>
      <link href="/2021/08/10/note/linux-45-lesson/"/>
      <url>/2021/08/10/note/linux-45-lesson/</url>
      
        <content type="html"><![CDATA[<h1 id="操作系统45讲"><a href="#操作系统45讲" class="headerlink" title="操作系统45讲"></a>操作系统45讲</h1><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5fd742af2782769a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-679715f333a27afc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-74355884ea399e7a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-680eb04404fa648d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f7c602ebd04540dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-11f0e5ab00983c4d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8927b690afc9a45f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6d55610b87cc5ee0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="宏内核"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0cc4c1f1b3b9fed9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="微内核结构图"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e29c558bca7f1c11.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Linux内部的全景图"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-265c3bc2d1aa569a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ed0ff1c9f3820e4d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Darwin架构图"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9b57316ac8980956.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="NT内核架构图"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-794636d7d1ac12c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="实模式下访问内存"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e49db2cd563ea3f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="保护模式段描述符"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-179be6788bfa20e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="全局段描述符表"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c0287e4ca75985a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="32位 - 保护模式下的4MB分页"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0e85d7277d4458a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="64位下的2MB分页"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e411e911d280f102.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-db20b7be1750848a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c6acdef8ee704577.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-128884cf01960b67.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c9c45c6cfddf4566.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Linux 初始化要点示意图"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9ec989cc13c42ca8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Linux内存数据结构关系"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-19dad8dd9a73a756.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-63195876bbc05c4e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="SLAB对象示意图"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fc8a5e167e72c590.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="kmem_cache结构图解"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-127d0e464b0d8653.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="SLAB全局结构示意图"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-67ff1c93ccdcd6b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="SLAB分配对象的过程图解"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e78db2cb28c08ca5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="运行队列框架示意图"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-04f63b38f7d3d3e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5cc06bd0d28ff28a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="计算机结构示意图"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a0e0796f275d0715.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="自动加载驱动的整个流程"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4291633d2bfaeda4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="VFS架构图"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5f2704876980fae5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Linux目录结构"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-503a53cf6d612eac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="VFS对象关系示意图"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f97261c38c103206.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="打开文件流程"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2c34c5de4ef179e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="读文件流程示意图"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4de9016d9b6e3ab0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="关闭文件流程示意图"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0b41684e5bf50588.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="API框架"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-18b1310b7ffd6910.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="系统服务流程示意图"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f694a9759ea81ed1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="LinuxAPI框架"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8bd38ccfdcc5ec53.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=" KVM 虚拟化"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1b9dd9de2775f877.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="CPU架构图"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-467dccd047a2ea1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="M1芯片"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b247665a719da015.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>]]></content>
      
      
      <categories>
          
          <category> System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入理解 Golang Stack</title>
      <link href="/2021/08/01/golang-stack/"/>
      <url>/2021/08/01/golang-stack/</url>
      
        <content type="html"><![CDATA[<br><!--"https://upload-images.jianshu.io/upload_images/12321605-b6543138cca8bb9c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">--><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="Linux-进程地址空间布局"><a href="#Linux-进程地址空间布局" class="headerlink" title="Linux 进程地址空间布局"></a>Linux 进程地址空间布局</h3><p>我们知道<code>CPU</code>有实模式和保护模式，系统刚刚启动的时候是运行在实模式下，然后经过一系列初始化工作以后，<code>Linux</code>会把<code>CPU</code>的实模式改为保护模式（具体就是修改<code>CPU</code>的<code>CR0寄存器</code>相关标记位），在保护模式下，<code>CPU</code>访问的地址都是虚拟地址(逻辑地址)。<code>Linux</code> 为了每个进程维护了一个单独的虚拟地址空间，虚拟地址空间又分为“<code>用户空间</code>”和“<code>内核空间</code>”。 虚拟地址空间更多相关可以看<a href="/2021/07/25/linux-mem/">Linux内核虚拟地址空间</a>这篇文章。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2cdf1bedff166c2f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Linux-Memory-X86-64.jpg"></p><br><h3 id="Golang-栈内存在虚拟地址空间哪个区域"><a href="#Golang-栈内存在虚拟地址空间哪个区域" class="headerlink" title="Golang 栈内存在虚拟地址空间哪个区域"></a>Golang 栈内存在虚拟地址空间哪个区域</h3><p><code>Golang</code> 的内存管理是用的 <code>TCMalloc</code>（<code>Thread-Caching Malloc</code>）算法, 简单点说就是 <code>Golang</code> 是使用 <code>mmap</code> 函数去操作系统申请一大块内存，然后把内存按照 <code>8Byte~32KB</code> <code>67</code>个 <code>size</code> 类型的 <code>mspan</code>，每个 <code>mspan</code>按照它自身的属性 <code>Size Class</code> 的大小分割成若干个 <code>object</code><a href="https://github.com/golang/go/blob/go1.16.6/src/runtime/sizeclasses.go">（每个span默认是8K）</a>，因为分需要 <code>gc</code> 的 <code>mspan</code> 和不需要 <code>gc</code> 的 <code>mspan</code>（<code>Golang</code>的<code>stack</code>），所以一共有<code>134</code>种类型。</p><p><strong>上面说了 Golang 内存申请是用的 mmap，mmap申请的内存都在虚拟地址空间的“Memory Mapping Segment”，所有Golang 所有的内存使用（包括栈）都是在“Memory Mapping Segment”，并不是在传统应用地址空间划的栈区。</strong></p><p>写<code>Demo</code>个代码验证一下</p><pre><code>func main() {    a := 8    println("a address :  ", &amp;a)    time.Sleep(time.Hour)}// 这里加“-m”来check没有内存逃逸[root@n227-005-021 GoTest ]$ go build -gcflags "-N -l -m"  stack3.go [root@n227-005-021 GoTest ]$ ./stack3a address :   0xc000070f68</code></pre><p>我们可以看到变量 a 的地址是<code>0xc000070f68 </code>，是一个很小的地址，而虚拟地址空间的<code>stack</code>，都是在最上面，地址一般都很大（比如 <code>00007ffd8fe4d000</code>这种），所以我们可以变量 <code>a</code> 肯定不是在<code>stack</code>。我们可以进一步用 <code>pmap</code> 看进程的内存区域验证一下。</p><pre><code>[root@n227-005-021 fanlv ]$ pmap  162277162277:   ./stack30000000000400000    400K r-x-- stack30000000000464000    448K r---- stack300000000004d4000     20K rw--- stack300000000004d9000    200K rw---   [ anon ]000000c000000000  65536K rw---   [ anon ]00007f13d92f3000  36292K rw---   [ anon ]00007f13db664000 263680K -----   [ anon ]00007f13eb7e4000      4K rw---   [ anon ]00007f13eb7e5000 293564K -----   [ anon ]00007f13fd694000      4K rw---   [ anon ]00007f13fd695000  36692K -----   [ anon ]00007f13ffa6a000      4K rw---   [ anon ]00007f13ffa6b000   4580K -----   [ anon ]00007f13ffee4000      4K rw---   [ anon ]00007f13ffee5000    508K -----   [ anon ]00007f13fff64000    384K rw---   [ anon ]00007ffd8fe4d000    132K rw---   [ stack ]00007ffd8ff82000     12K r----   [ anon ]00007ffd8ff85000      8K r-x--   [ anon ] total           702472K</code></pre><p><code>pmap</code> 我们看到<code>000000c000000000 </code>属于 <code>mmap</code> 映射的一个匿名的<code>page([anon])</code>，符合预期。</p><h3 id="常用寄存器"><a href="#常用寄存器" class="headerlink" title="常用寄存器"></a>常用寄存器</h3><table><thead><tr><th>寄存器</th><th>64位名称</th><th>32位名称</th><th>16位名称</th><th>用途</th></tr></thead><tbody><tr><td>AX</td><td>rax</td><td>eax</td><td>ax</td><td>函数返回值一般都存这个里面</td></tr><tr><td>BX</td><td>rbx</td><td>ebx</td><td>bx</td><td>基址(Base)寄存器，常做内存数据的指针, 或者说常以它为基址来访问内存.</td></tr><tr><td>CX</td><td>rcx</td><td>ecx</td><td>cx</td><td>计数器(Counter)寄存器，常做字符串和循环操作中的计数器, 或者用于保存函数调用第4个参数</td></tr><tr><td>DX</td><td>rdx</td><td>edx</td><td>dx</td><td>数据(Data)寄存器，常用于乘、除法和 I/O 指针，或者用于保存函数调用第3个参数</td></tr><tr><td>SI</td><td>rsi</td><td>esi</td><td>si</td><td>来源索引(Source Index)寄存器，常做内存数据指针和源字符串指针，或者用于保存函数调用第2个参数</td></tr><tr><td>DI</td><td>rdi</td><td>edi</td><td>di</td><td>目的索引(Destination Index)寄存器，或者用于保存函数调用第1个参数    ，常做内存数据指针和目的字符串指针</td></tr><tr><td>SP</td><td>rsp</td><td>esp</td><td>sp</td><td>堆栈指针(Stack Point)寄存器，只做堆栈的栈顶指针; 不能用于算术运算与数据传送</td></tr><tr><td>BP</td><td>rbp</td><td>ebp</td><td>bp</td><td>基址指针(Base Point)寄存器，只做堆栈指针, 可以访问堆栈内任意地址, 经常用于中转 ESP 中的数据, 也常以它为基址来访问堆栈; 不能用于算术运算与数据传送</td></tr><tr><td>IP</td><td>rip</td><td>eip</td><td>ip</td><td>指令指针(Instruction Pointer)寄存器，总是指向下一条指令的地址; 所有已执行的指令都被它指向过.</td></tr></tbody></table><p>还有 R8 ~ R15 8个寄存器这里就不详细列出来了， R8用于保存函数调用5个参数，R9用于保存函数调用6个参数</p><p>虽然在<code>x86-64</code>架构下，增加了很多通用寄存器，使得调用惯例(<code>calling convention</code>)变为函数传参可以部分（最多6个）<strong>使用寄存器直接传递</strong>，但是在<code>Golang</code>中，<strong>编译器强制规定函数的传参全部都用栈传递</strong>，不使用寄存器传参。<code>go 1.17</code>以后好像已经开始可以用寄存器传参。</p><p><code>Golang</code>不使用寄存器传参，应该还是为了使生成的伪汇编方便跨平台。这里扩展一个优化点，为了减少函数调用的开销小，可以尽量让函数内联。</p><p>内联的条件：1. 函数语法解析完，<code>token</code>数不超过 80. 2. <code>Interface</code> 的方法调用，不能内联。</p><h3 id="什么是栈帧？"><a href="#什么是栈帧？" class="headerlink" title="什么是栈帧？"></a>什么是栈帧？</h3><p>栈帧，也就是<code>stack frame</code>，其本质就是一种栈，只是这种栈专门用于保存<strong>函数调用过程</strong>中的各种信息（参数，返回地址，本地变量等）。栈帧有栈顶和栈底之分，其中栈顶的地址最低，栈底的地址最高用，<code>BP</code>（栈指针）指向栈底，<code>SP</code>(栈指针)指向栈顶的。</p><p>具体实现，我们看一个<code>Demo</code></p><pre><code>#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;int add(int x, int y){    return x + y;}int main(){    int p = add(2, 6);    printf("add:%d\n", p);    return 0;}</code></pre><p><code>clang add.c -o add.o</code> // 编译，Linux可以用 gcc add.c -o add.o<br><code>otool -tvV add.o</code>  //导出汇编代码 Linux可以用 objdump -d -M at -S add.o</p><pre><code>add.o:(__TEXT,__text) section_add:0000000100003f20    pushq    %rbp0000000100003f21    movq    %rsp, %rbp0000000100003f24    movl    %edi, -0x4(%rbp)0000000100003f27    movl    %esi, -0x8(%rbp)0000000100003f2a    movl    -0x4(%rbp), %eax0000000100003f2d    addl    -0x8(%rbp), %eax0000000100003f30    popq    %rbp0000000100003f31    retq0000000100003f32    nopw    %cs:(%rax,%rax)0000000100003f3c    nopl    (%rax)_main:0000000100003f40    pushq    %rbp0000000100003f41    movq    %rsp, %rbp0000000100003f44    subq    $0x10, %rsp0000000100003f48    movl    $0x0, -0x4(%rbp)0000000100003f4f    movl    $0x2, %edi0000000100003f54    movl    $0x6, %esi0000000100003f59    callq    _add0000000100003f5e    movl    %eax, -0x8(%rbp)0000000100003f61    movl    -0x8(%rbp), %esi0000000100003f64    leaq    0x37(%rip), %rdi                ##literal pool for: "add:%d\n"0000000100003f6b    movb    $0x0, %al0000000100003f6d    callq    0x100003f80                     ##symbol stub for: _printf0000000100003f72    xorl    %ecx, %ecx0000000100003f74    movl    %eax, -0xc(%rbp)0000000100003f77    movl    %ecx, %eax0000000100003f79    addq    $0x10, %rsp0000000100003f7d    popq    %rbp0000000100003f7e    retq</code></pre><p>我们接着<code>lldb add.o</code> <code>Debug</code>一下。我们在Add函数最开始的地方<code>0000000100003f20</code>打上断点，看下调用<code>add</code>过程栈的变化。我们执行<code>x/8xg $rbp</code>先打印下<code>rbp</code>，由下面可以知<code>rbp</code>的地址是<code>0x7ffeefbff580 </code>，<code>rbp</code>地址指向的内容是<code>0x00007ffeefbff590</code></p><pre><code>(lldb) sProcess 43672 stopped* thread #1, queue = 'com.apple.main-thread', stop reason = instruction step into    frame #0: 0x0000000100003f20 add.o`addadd.o`add:-&gt;  0x100003f20 &lt;+0&gt;: pushq  %rbp // 把rbp地址压栈    0x100003f21 &lt;+1&gt;: movq   %rsp, %rbp // rbp = rsp    0x100003f24 &lt;+4&gt;: movl   %edi, -0x4(%rbp) // $(rbp-4) = 2    0x100003f27 &lt;+7&gt;: movl   %esi, -0x8(%rbp) // $(rbp-8) = 6Target 0: (add.o) stopped.(lldb) x/8xg $rbp0x7ffeefbff580: 0x00007ffeefbff590 0x00007fff203fbf3d0x7ffeefbff590: 0x0000000000000000 0x0000000000000001</code></pre><p>执行完<code>movl   %esi, -0x8(%rbp) </code>我们再<code>x/8xg $rbp </code>看下rbp地址，发现<code>rbp</code>地址变成了<code>0x7ffeefbff560 </code> <code>rbp</code>指向了<code>0x00007ffeefbff580</code></p><pre><code>(lldb) sProcess 43672 stopped* thread #1, queue = 'com.apple.main-thread', stop reason = instruction step into    frame #0: 0x0000000100003f2a add.o`add + 10add.o`add:-&gt;  0x100003f2a &lt;+10&gt;: movl   -0x4(%rbp), %eax    0x100003f2d &lt;+13&gt;: addl   -0x8(%rbp), %eax    0x100003f30 &lt;+16&gt;: popq   %rbp    0x100003f31 &lt;+17&gt;: retqTarget 0: (add.o) stopped.(lldb)  x/8xg $rbp0x7ffeefbff560: 0x00007ffeefbff580 0x0000000100003f5e0x7ffeefbff570: 0x00007ffeefbff590 0x0000000000011025</code></pre><p>我们在看下 <code>$rbp-8</code>，发现 <code>$rbp-8</code>的位置数据存的是6 <code>$rbp-4</code>位置存的数据是<code>2</code> ，<code>$rbp</code>数据存的是main函数的<code>rbp</code>地址，<code>$rbp+8</code>的地址是 <code>add</code> 函数返回以后需要执行的代码地址<code>0x00003f5e </code></p><pre><code>(lldb)  x/8xw $rbp-80x7ffeefbff558: 0x00000006 0x00000002 0xefbff580 0x00007ffe0x7ffeefbff568: 0x00003f5e 0x00000001 0xefbff590 0x00007ffe</code></pre><p>栈的图大致如下</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2e4149f286f5ceb0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个栈帧的图里面我们主要关注几点。</p><ol><li>栈是由高地址像低地址扩张，比如<code>main</code>函数里面执行<code>subq $0x10, %rsp</code> 就是申请16个字节的栈空间，执行完<code>main</code>函数再调用<code>addq    $0x10, %rsp</code> 表示释放之前申请的<code>16</code>个字节栈空间。</li><li>每次函数调用都会有一定的栈空间的开销，（老的<code>rbp</code>压栈、函数的返回地址压栈）</li><li>函数执行完以后。会继续执行<code>return address</code>指向的代码。</li></ol><p><strong>这里要理解栈帧是向下扩展的很重要。下面golang的stack扩容缩容判断的时候会用到</strong></p><h3 id="什么是内存逃逸？"><a href="#什么是内存逃逸？" class="headerlink" title="什么是内存逃逸？"></a>什么是内存逃逸？</h3><p>逃逸分析是一种确定指针动态范围的方法。简单来说就是分析在程序的哪些地方可以访问到该指针。编译器会根据变量是否被外部引用来决定是否逃逸：</p><ol><li>如果函数外部没有引用，则优先放到栈中；</li><li>如果函数外部存在引用，则必定放到堆中；</li></ol><p><strong>注意：go 在编译阶段确立逃逸，并不是在运行时。</strong></p><h4 id="逃逸场景（什么情况才分配到堆中）"><a href="#逃逸场景（什么情况才分配到堆中）" class="headerlink" title="逃逸场景（什么情况才分配到堆中）"></a>逃逸场景（什么情况才分配到堆中）</h4><ol><li><p>指针逃逸，<code>Go</code>可以返回局部变量指针，这其实是一个典型的变量逃逸案例，示例代码如下：</p><pre><code> func StudentRegister(name string, age int) *Student {     s := new(Student) //局部变量s逃逸到堆     s.Name = name     s.Age = age     return s } </code></pre></li><li><p>栈空间不足逃逸（空间开辟过大），下面代码<code>Slice()</code>函数中分配了一个<code>1000</code>个长度的切片，是否逃逸取决于栈空间是否足够大。如下：</p><pre><code> func Slice() {      s := make([]int, 1000, 1000) // s 会分配在堆上      for index, _ := range s {         s[index] = index     } }</code></pre></li><li><p>动态类型逃逸（不确定长度大小）。很多函数参数为<code>interface</code>类型，比如<code>fmt.Println(a …interface{})</code>，编译期间很难确定其参数的具体类型，也能产生逃逸。</p><pre><code> func main() {     s := "Escape"     fmt.Println(s) }</code></pre></li><li><p>闭包引用对象逃逸</p><pre><code> func Fibonacci() func() int {     a, b := 0, 1     return func() int {         a, b = b, a+b         return a     } }</code></pre></li></ol><p>可以使用<code>go build -gcflags=-m</code>查看逃逸分析日志</p><h2 id="Golang-栈"><a href="#Golang-栈" class="headerlink" title="Golang 栈"></a>Golang 栈</h2><h3 id="Golang-栈大小变更历史"><a href="#Golang-栈大小变更历史" class="headerlink" title="Golang 栈大小变更历史"></a>Golang 栈大小变更历史</h3><p><code>Go</code> 语言使用用户态线程 <code>Goroutine</code> 作为执行上下文，它的额外开销和默认栈大小都比线程小很多，然而 <code>Goroutine</code> 的栈内存空间和栈结构也在经过很多次变化：</p><p><a href="https://github.com/golang/go/commit/af58f17af936f8d88ccfed96b7a0e9953b4e6010">第一版提交sys·newproc 栈默认是4K</a>、<a href="https://github.com/golang/go/commit/a67258f3801b6aa218c8c2563f0a743b944e5946">malg commit</a> （早期<code>Golang</code>的<code>runtime</code>代码是用c写的）</p><p><a href="https://github.com/golang/go/commit/408238e20bb794d91199c892c68a0989fc924d65">第二次修改 4K -&gt; 8K</a>，主要是提高部分<code>encode</code>和<code>decode</code>的性能</p><pre><code>// 2013-10-03 go1.2rc2Significant runtime reductions:          amd64  386GoParse    -14%  -1%GobDecode  -12% -20%GobEncode  -64%  -1%JSONDecode  -9%  -4%JSONEncode -15%  -5%Template   -17% -14%Benchmark graphs athttp://swtch.com/~rsc/gostackamd64.htmlhttp://swtch.com/~rsc/gostack386.html</code></pre><p><a href="https://github.com/golang/go/commit/1665b006a57099d7bdf5c9f1277784d36b7168d9">第三次修改 8K -&gt; 4K</a></p><pre><code>// 2014-02-27 go1.3beta1runtime: grow stack by copyingOn stack overflow, if all frames on the stack arecopyable, we copy the frames to a new stack twiceas large as the old one.  During GC, if a G is usingless than 1/4 of its stack, copy the stack to a stackhalf its size.</code></pre><p><a href="https://github.com/golang/go/commit/6aee29648fce3af20507787035ae22d06d75d39b">第四次修改 4K -&gt; 8K</a></p><pre><code>// 2014-05-20 go1.3beta2runtime: switch default stack size back to 8kBThe move from 4kB to 8kB in Go 1.2 was to eliminate many stack split hot spots.The move back to 4kB was predicated on copying stacks eliminatingthe potential for hot spots.Unfortunately, the fact that stacks do not copy 100% of the time meansthat hot spots can still happen under the right conditions, and the slowdownis worse now than it was in Go 1.2. There is a real program in issue 8030 thatsees about a 30x slowdown: it has a reflect call near the top of the stackwhich inhibits any stack copying on that segment.Go back to 8kB until stack copying can be used 100% of the time.Fixes issue 8030.</code></pre><p><a href="https://github.com/golang/go/commit/6c934238c93f8f60775409f1ab410ce9c9ea2357">最后、将最小栈内存从8K降低到了2KB</a> ，主要是为了节省内存空间使用</p><pre><code>// 2014-09-17 go1.4beta1runtime: change minimum stack size to 2K.It will be 8K on windows because it needs 4K for the OS.Similarly, plan9 will be 4K.On linux/amd64, reduces size of 100,000 goroutinesfrom ~819MB to ~245MB.Update issue 7514</code></pre><h3 id="分段栈"><a href="#分段栈" class="headerlink" title="分段栈"></a>分段栈</h3><p>分段栈是 <code>Go</code> 语言在 <code>v1.2</code> 版本之前的实现，所有 <code>Goroutine</code> 在栈扩容的时候都会调用<a href="https://github.com/golang/go/blob/go1.2/src/pkg/runtime/stack.c#L196">runtime·newstack:go1.2</a> 分配的内存为<a href="https://github.com/golang/go/blob/go1.2/src/pkg/runtime/stack.h#L79">StackMin + StackSystem</a> 表示，在 <code>v1.2</code> 版本中为<code>StackMin</code> <code>8KB</code>：</p><pre><code>// Called from runtime·newstackcall or from runtime·morestack when a new// stack segment is needed.  Allocate a new stack big enough for// m-&gt;moreframesize bytes, copy m-&gt;moreargsize bytes to the new frame,// and then act as though runtime·lessstack called the function at// m-&gt;morepc.voidruntime·newstack(void){   ...........    // gp-&gt;status is usually Grunning, but it could be Gsyscall if a stack split    // happens during a function call inside entersyscall.    gp = m-&gt;curg;    oldstatus = gp-&gt;status;    framesize = m-&gt;moreframesize;    argsize = m-&gt;moreargsize;    gp-&gt;status = Gwaiting;    gp-&gt;waitreason = "stack split";    newstackcall = framesize==1;    if(newstackcall)        framesize = 0;          if(newstackcall &amp;&amp; m-&gt;morebuf.sp - sizeof(Stktop) - argsize - 32 &gt; gp-&gt;stackguard) {       .........    } else {        // 这里计算栈的空间大小        // allocate new segment.        framesize += argsize;        framesize += StackExtra;    // room for more functions, Stktop.        if(framesize &lt; StackMin)            framesize = StackMin; // 栈最小8K        framesize += StackSystem; // StackSystem  Window-64 是4K，plan9 是9，其他平台是0        gp-&gt;stacksize += framesize;        if(gp-&gt;stacksize &gt; runtime·maxstacksize) { // maxstacksize x64是 1G，x32是250m            runtime·printf("runtime: goroutine stack exceeds %D-byte limit\n", (uint64)runtime·maxstacksize);            runtime·throw("stack overflow");        }        stk = runtime·stackalloc(framesize);        top = (Stktop*)(stk+framesize-sizeof(*top));        free = framesize;    }}    ..............void*runtime·stackalloc(uint32 n){    uint32 pos;    void *v;    if(g != m-&gt;g0)        runtime·throw("stackalloc not on scheduler stack");    if(n == FixedStack || m-&gt;mallocing || m-&gt;gcing) {        if(n != FixedStack) {            runtime·printf("stackalloc: in malloc, size=%d want %d\n", FixedStack, n);            runtime·throw("stackalloc");        }        if(m-&gt;stackcachecnt == 0)            stackcacherefill();        pos = m-&gt;stackcachepos;        pos = (pos - 1) % StackCacheSize;        v = m-&gt;stackcache[pos];        m-&gt;stackcachepos = pos;        m-&gt;stackcachecnt--;        m-&gt;stackinuse++;        return v;    }        // https://github.com/golang/go/blob/go1.2/src/pkg/runtime/malloc.goc#L34    // 这里调用 runtime·mallocgc 去申请内存，指定内存不需要GC    return runtime·mallocgc(n, 0, FlagNoProfiling|FlagNoGC|FlagNoZero|FlagNoInvokeGC);}</code></pre><p>如果通过该方法申请的内存大小为固定的 <code>8KB</code> 或者满足其他的条件，运行时会在全局的栈缓存链表中找到空闲的内存块并作为新 <code>Goroutine</code> 的栈空间返回；在其余情况下，栈内存空间会从堆上申请一块合适的内存。</p><p>当 <code>Goroutine</code> 调用的函数层级或者局部变量需要的越来越多时，运行时会调用 <a href="https://github.com/golang/go/blob/go1.2/src/pkg/runtime/asm_amd64.s#L195">runtime.morestack:go1.2</a> 和 <a href="https://github.com/golang/go/blob/go1.2/src/pkg/runtime/stack.c#L196">runtime.newstack:go1.2</a> 创建一个新的栈空间，这些栈空间虽然不连续，但是当前 <code>Goroutine</code> 的多个栈空间会以链表的形式串联起来，运行时会通过指针找到连续的栈片段：</p><p>一旦 <code>Goroutine</code> 申请的栈空间不在被需要，运行时会调用 <a href="https://github.com/golang/go/blob/go1.2/src/pkg/runtime/asm_amd64.s#L370">runtime.lessstack:go1.2</a> 和 <a href="https://github.com/golang/go/blob/go1.2/src/pkg/runtime/stack.c#L132">runtime.oldstack:go1.2</a> 释放不再使用的内存空间。</p><p>分段栈机制虽然能够按需为当前 <code>Goroutine</code> 分配内存并且及时减少内存的占用，但是它也存在两个比较大的问题：</p><ol><li>如果当前 <code>Goroutine</code> 的栈几乎充满，那么任意的函数调用都会触发栈扩容，当函数返回后又会触发栈的收缩，如果在一个循环中调用函数，栈的分配和释放就会造成巨大的额外开销，<strong>这被称为热分裂问题（Hot split）</strong>；</li><li>一旦 <code>Goroutine</code> 使用的内存越过了分段栈的扩缩容阈值，运行时会触发栈的扩容和缩容，带来额外的工作量；</li></ol><h3 id="连续栈"><a href="#连续栈" class="headerlink" title="连续栈"></a>连续栈</h3><h4 id="什么是连续栈"><a href="#什么是连续栈" class="headerlink" title="什么是连续栈"></a>什么是连续栈</h4><p>连续栈可以解决分段栈中存在的两个问题，其核心原理是每当程序的栈空间不足时，初始化一片更大的栈空间并将原栈中的所有值都迁移到新栈中，新的局部变量或者函数调用就有充足的内存空间。使用连续栈机制时，栈空间不足导致的扩容会经历以下几个步骤：</p><ol><li>在内存空间中分配更大的栈内存空间；</li><li>将旧栈中的所有内容复制到新栈中；</li><li>将指向旧栈对应变量的指针重新指向新栈；</li><li>销毁并回收旧栈的内存空间；</li></ol><p>在扩容的过程中，最重要的是调整指针的第三步，这一步能够保证指向栈的指针的正确性，因为栈中的所有变量内存都会发生变化，所以原本指向栈中变量的指针也需要调整。我们在前面提到过经过逃逸分析的 <code>Go</code> 语言程序的遵循以下不变性 —— 指向栈对象的指针不能存在于堆中，所以指向栈中变量的指针只能在栈上，我们只需要调整栈中的所有变量就可以保证内存的安全了。</p><h4 id="NOSPLIT的自动检测"><a href="#NOSPLIT的自动检测" class="headerlink" title="NOSPLIT的自动检测"></a>NOSPLIT的自动检测</h4><p>我们在编写函数时，编译出汇编代码会发现，在一些函数的执行代码中，编译器很智能的加上了<code>NOSPLIT</code>标记。这个标记可以禁用栈溢出检测<code>prolog</code>，即该函数运行不会导致栈分裂，由于不需要再照常执行栈溢出检测，所以会提升一些函数性能。这是如何做到的</p><pre><code>// cmd/internal/obj/s390x/objz.goif p.Mark&amp;LEAF != 0 &amp;&amp; autosize &lt; objabi.StackSmall {    // A leaf function with a small stack can be marked    // NOSPLIT, avoiding a stack check.    p.From.Sym.Set(obj.AttrNoSplit, true)}</code></pre><p>当函数处于调用链的叶子节点，且栈帧小于<code>StackSmall</code>字节时，则自动标记为<code>NOSPLIT</code>。 <code>x86</code>架构处理与之类似</p><p>自动标记为<code>NOSPLIT</code>的函数，链接器就会知道该函数最多还会使用<code>StackLimit</code>字节空间，不需要栈分裂。</p><p>备注：用户也可以使用<code>//go:nosplit</code>强制指定<code>NOSPLIT</code>属性，但如果函数实际真的溢出了，则会在编译期就报错<code>nosplit stack overflow</code></p><pre><code>$ GOOS=linux GOARCH=amd64 go build -gcflags="-N -l" main.go#command-line-argumentsmain.add: nosplit stack overflow    744    assumed on entry to main.add (nosplit)    -79264    after main.add (nosplit) uses 80008</code></pre><br><h4 id="Goroutine的创建"><a href="#Goroutine的创建" class="headerlink" title="Goroutine的创建"></a>Goroutine的创建</h4><p>程序中执行 <code>go func(){} </code>创建<code>Goroutine</code>的时候，runtime会执行<code>newproc1</code>，这里会先去<code>_p_.gFree</code>列表拿，如果没有空闲的g就会调用<code>malg</code> 去new一个。</p><pre><code>// https://github.com/golang/go/blob/go1.16.6/src/runtime/proc.go#L4065func newproc1(fn *funcval, argp unsafe.Pointer, narg int32, callergp *g, callerpc uintptr) *g {   ..............       newg := gfget(_p_) // 先去P的free list拿 没有的话就调用malg new一个g    if newg == nil {        newg = malg(_StackMin)        casgstatus(newg, _Gidle, _Gdead)        allgadd(newg) // publishes with a g-&gt;status of Gdead so GC scanner doesn't look at uninitialized stack.    }}</code></pre><p><code>new</code> 一个新的<code>Goroutine</code>的时候，会调用<code>stackalloc</code>来申请栈空间。</p><pre><code>//https://github.com/golang/go/blob/go1.16.6/src/runtime/proc.go#L3987// Allocate a new g, with a stack big enough for stacksize bytes.func malg(stacksize int32) *g {    newg := new(g)    if stacksize &gt;= 0 {        // round2 是求2的指数，比如传 6 返回 8        // _StackSystem linux是0、plan9 是512 、 Windows-x64 是4k        stacksize = round2(_StackSystem + stacksize)        systemstack(func() {//切换到 G0 为 newg 初始化栈内存            newg.stack = stackalloc(uint32(stacksize))        })        // 设置 stackguard0 ，用来判断是否要进行栈扩容        newg.stackguard0 = newg.stack.lo + _StackGuard        newg.stackguard1 = ^uintptr(0)        // Clear the bottom word of the stack. We record g        // there on gsignal stack during VDSO on ARM and ARM64.        *(*uintptr)(unsafe.Pointer(newg.stack.lo)) = 0    }    return newg}</code></pre><p>每一个<code>Goroutine</code>的<code>g-&gt;stackguard</code>0都被设置为指向<code>stack.lo </code>+ <code>StackGuard</code>的位置。所以每一个函数在真正执行前都会将<code>SP</code>和<code>stackguard0</code>进行比较。</p><br><h4 id="栈的初始化"><a href="#栈的初始化" class="headerlink" title="栈的初始化"></a>栈的初始化</h4><pre><code>// Number of orders that get caching. Order 0 is FixedStack// and each successive order is twice as large.// We want to cache 2KB, 4KB, 8KB, and 16KB stacks. Larger stacks// will be allocated directly.// Since FixedStack is different on different systems, we// must vary NumStackOrders to keep the same maximum cached size.//   OS               | FixedStack | NumStackOrders//   -----------------+------------+---------------//   linux/darwin/bsd | 2KB        | 4//   windows/32       | 4KB        | 3//   windows/64       | 8KB        | 2//   plan9            | 4KB        | 3_NumStackOrders = 4 - sys.PtrSize/4*sys.GoosWindows - 1*sys.GoosPlan9// 全局的栈缓存，分配 32KB以下内存var stackpool [_NumStackOrders]struct {    item stackpoolItem    _    [cpu.CacheLinePadSize - unsafe.Sizeof(stackpoolItem{})%cpu.CacheLinePadSize]byte // CacheLine对齐，防止Flase Sharding的问题}//go:notinheaptype stackpoolItem struct {    mu   mutex    span mSpanList }// 全局的栈缓存，分配 32KB 以上内存 // heapAddrBits 48 , pageShift 13var stackLarge struct {    lock mutex    free [heapAddrBits - pageShift]mSpanList // free lists by log_2(s.npages)}func stackinit() {    if _StackCacheSize&amp;_PageMask != 0 {        throw("cache size must be a multiple of page size")    }    for i := range stackpool {        stackpool[i].item.span.init()        lockInit(&amp;stackpool[i].item.mu, lockRankStackpool)    }    for i := range stackLarge.free {        stackLarge.free[i].init()        lockInit(&amp;stackLarge.lock, lockRankStackLarge)    }}</code></pre><p>在执行栈初始化的时候会初始化两个全局变量 <code>stackpool</code> 和 <code>stackLarge</code>。<code>stackpool</code> 可以分配小于 <code>32KB </code>的内存，<code>stackLarge</code> 用来分配大于 <code>32KB</code> 的栈空间。</p><br><h4 id="栈内存分配"><a href="#栈内存分配" class="headerlink" title="栈内存分配"></a>栈内存分配</h4><pre><code>// https://github.com/golang/go/blob/go1.16.6/src/runtime/stack.go#L327// Per-P, per order stack segment cache size._StackCacheSize = 32 * 1024// stackalloc allocates an n byte stack.//// stackalloc must run on the system stack because it uses per-P// resources and must not split the stack.////go:systemstackfunc stackalloc(n uint32) stack {    // Stackalloc must be called on scheduler stack, so that we    // never try to grow the stack during the code that stackalloc runs.    // Doing so would cause a deadlock (issue 1547).    thisg := getg() // 必须是g0    if thisg != thisg.m.g0 {        throw("stackalloc not on scheduler stack")    }    if n&amp;(n-1) != 0 {        throw("stack size not a power of 2")    }    if stackDebug &gt;= 1 {        print("stackalloc ", n, "\n")    }    if debug.efence != 0 || stackFromSystem != 0 {        n = uint32(alignUp(uintptr(n), physPageSize))        v := sysAlloc(uintptr(n), &amp;memstats.stacks_sys)        if v == nil {            throw("out of memory (stackalloc)")        }        return stack{uintptr(v), uintptr(v) + uintptr(n)}    }    // Small stacks are allocated with a fixed-size free-list allocator.    // If we need a stack of a bigger size, we fall back on allocating    // a dedicated span.    var v unsafe.Pointer    // _FixedStack: 2K、_NumStackOrders：4 、_StackCacheSize = 32 * 1024    if n &lt; _FixedStack&lt;&lt;_NumStackOrders &amp;&amp; n &lt; _StackCacheSize {// 小于32K        order := uint8(0)        n2 := n        // 大于 2048 ,那么 for 循环 将 n2 除 2,直到 n 小于等于 2048        for n2 &gt; _FixedStack {            order++            n2 &gt;&gt;= 1        }        var x gclinkptr        //preemptoff != "", 在 GC 的时候会进行设置,表示如果在 GC 那么从 stackpool 分配        // thisg.m.p = 0 会在系统调用和 改变 P 的个数的时候调用,如果发生,那么也从 stackpool 分配        if stackNoCache != 0 || thisg.m.p == 0 || thisg.m.preemptoff != "" {            // thisg.m.p == 0 can happen in the guts of exitsyscall            // or procresize. Just get a stack from the global pool.            // Also don't touch stackcache during gc            // as it's flushed concurrently.            lock(&amp;stackpool[order].item.mu)            x = stackpoolalloc(order)// 从 stackpool 分配            unlock(&amp;stackpool[order].item.mu)        } else {                    // 从 P 的 mcache 分配内存            c := thisg.m.p.ptr().mcache            x = c.stackcache[order].list            if x.ptr() == nil {                // 从堆上申请一片内存空间填充到stackcache中                stackcacherefill(c, order)                x = c.stackcache[order].list            }            c.stackcache[order].list = x.ptr().next // 移除链表的头节点            c.stackcache[order].size -= uintptr(n)        }        v = unsafe.Pointer(x) // 获取到分配的span内存块    } else {        // 申请的内存空间过大，从 runtime.stackLarge 中检查是否有剩余的空间        var s *mspan        // 计算需要分配多少个 span 页， 8KB 为一页        npage := uintptr(n) &gt;&gt; _PageShift        log2npage := stacklog2(npage)        // Try to get a stack from the large stack cache.        lock(&amp;stackLarge.lock)        // 如果 stackLarge 对应的链表不为空        if !stackLarge.free[log2npage].isEmpty() {            //获取链表的头节点，并将其从链表中移除            s = stackLarge.free[log2npage].first            stackLarge.free[log2npage].remove(s)        }        unlock(&amp;stackLarge.lock)        lockWithRankMayAcquire(&amp;mheap_.lock, lockRankMheap)        //这里是stackLarge为空的情况        if s == nil {            // 从堆上申请新的内存 span            // Allocate a new stack from the heap.            s = mheap_.allocManual(npage, spanAllocStack)            if s == nil {                throw("out of memory")            }            // OpenBSD 6.4+ 系统需要做额外处理            osStackAlloc(s)            s.elemsize = uintptr(n)        }        v = unsafe.Pointer(s.base())    }    ..........        return stack{uintptr(v), uintptr(v) + uintptr(n)}}</code></pre><p><strong>小于32KB的栈内存分配</strong></p><p>小栈指大小为 <code>2K/4K/8K/16K</code> 的栈，在分配的时候，会根据大小计算不同的 <code>order</code> 值，如果栈大小是 <code>2K</code>，那么 <code>order</code> 就是 <code>0，4K</code> 对应 <code>order</code> 就是 <code>1</code>，以此类推。这样一方面可以减少不同 <code>Goroutine</code> 获取不同栈大小的锁冲突，另一方面可以预先缓存对应大小的 <code>span</code> ，以便快速获取。</p><p><code>thisg.m.p == 0</code>可能发生在系统调用 <code>exitsyscall</code> 或改变 <code>P</code> 的个数 <code>procresize</code> 时，<code>thisg.m.preemptoff != ""</code>会发生在 <code>GC</code> 的时候。也就是说在发生在系统调用 <code>exitsyscall</code> 或改变 P 的个数在变动，亦或是在 <code>GC</code> 的时候，会从 <code>stackpool</code> 分配栈空间，否则从 <code>mcache</code> 中获取。</p><p>在 <code>stackpoolalloc</code> 函数中会去找 <code>stackpool</code> 对应 <code>order</code> 下标的 <code>span</code> 链表的头节点，如果不为空，那么直接将头节点的属性 <code>manualFreeList</code> 指向的节点从链表中移除，并返回；</p><p>如果 <code>list.first</code>为空，那么调用 <code>mheap_</code>的 <code>allocManual</code> 函数从堆中分配 <code>mspan</code></p><p>从 <code>allocManual</code> 函数会分配 <code>32KB</code> 大小的内存块，分配好新的 <code>span</code> 之后会根据 <code>elemsize</code> 大小将 <code>32KB</code> 内存进行切割，然后通过单向链表串起来并将最后一块内存地址赋值给 <code>manualFreeList</code> 。</p><pre><code>func stackpoolalloc(order uint8) gclinkptr {    list := &amp;stackpool[order].item.span    s := list.first    lockWithRankMayAcquire(&amp;mheap_.lock, lockRankMheap)    if s == nil {        // no free stacks. Allocate another span worth.        // 从堆上分配 mspan        // _StackCacheSize = 32 * 1024        s = mheap_.allocManual(_StackCacheSize&gt;&gt;_PageShift, &amp;memstats.stacks_inuse)        if s == nil {            throw("out of memory")        }        // 刚分配的 span 里面分配对象个数肯定为 0        if s.allocCount != 0 {            throw("bad allocCount")        }        if s.manualFreeList.ptr() != nil {            throw("bad manualFreeList")        }        //OpenBSD 6.4+ 系统需要做额外处理        osStackAlloc(s)        // Linux 中 _FixedStack = 2048        s.elemsize = _FixedStack &lt;&lt; order        //_StackCacheSize =  32 * 1024        // 这里是将 32KB 大小的内存块分成了elemsize大小块，用单向链表进行连接        // 最后 s.manualFreeList 指向的是这块内存的尾部        for i := uintptr(0); i &lt; _StackCacheSize; i += s.elemsize {            x := gclinkptr(s.base() + i)            x.ptr().next = s.manualFreeList            s.manualFreeList = x        }        // 插入到 list 链表头部        list.insert(s)    }    x := s.manualFreeList    // 代表被分配完毕    if x.ptr() == nil {        throw("span has no free stacks")    }    // 将 manualFreeList 往后移动一个单位    s.manualFreeList = x.ptr().next    // 统计被分配的内存块    s.allocCount++    // 因为分配的时候第一个内存块是 nil    // 所以当指针为nil 的时候代表被分配完毕    // 那么需要将该对象从 list 的头节点移除    if s.manualFreeList.ptr() == nil {        // all stacks in s are allocated.        list.remove(s)    }    return x}</code></pre><p>如果 <code>mcache</code> 对应的 <code>stackcache</code> 获取不到，那么调用 <code>stackcacherefill</code> 从堆上申请一片内存空间填充到<code>stackcache</code>中。</p><p><code>stackcacherefill</code> 函数会调用 <code>stackpoolalloc</code> 从 <code>stackpool</code> 中获取一半的空间组装成 <code>list</code> 链表，然后放入到 <code>stackcache</code> 数组中。</p><pre><code>func stackcacherefill(c *mcache, order uint8) {     var list gclinkptr    var size uintptr    lock(&amp;stackpool[order].item.mu)    //_StackCacheSize = 32 * 1024    // 将 stackpool 分配的内存组成一个单向链表 list    for size &lt; _StackCacheSize/2 {        x := stackpoolalloc(order)        x.ptr().next = list        list = x        // _FixedStack = 2048        size += _FixedStack &lt;&lt; order    }    unlock(&amp;stackpool[order].item.mu)    c.stackcache[order].list = list    c.stackcache[order].size = size}</code></pre><p><strong>大于等于32KB的栈内存分配</strong></p><p>对于大栈内存分配，运行时会查看 <code>stackLarge</code> 中是否有剩余的空间，如果不存在剩余空间，它也会调用 <code>mheap_.allocManual</code> 从堆上申请新的内存。</p><br><h4 id="栈的扩容"><a href="#栈的扩容" class="headerlink" title="栈的扩容"></a>栈的扩容</h4><p><code>Go</code> 语言中的执行栈由 <code>runtime.stack</code> 表示，该结构体中只包含两个字段，分别表示栈的顶部和栈的底部，每个栈结构体都表示范围为 <code>[lo, hi) </code>的内存空间：</p><pre><code>type stack struct {    lo uintptr    hi uintptr}</code></pre><p>栈的结构虽然非常简单，但是想要理解 <code>Goroutine</code> 栈的实现原理，还是需要我们从编译期间和运行时两个阶段入手：</p><ol><li>编译器会在编译阶段会通过 <code>cmd/internal/obj/x86.stacksplit</code> 在调用函数前插入 <code>runtime.morestack</code> 或者 <code>runtime.morestack_noctxt</code> 函数；</li><li>运行时在创建新的 <code>Goroutine</code> 时会在 <code>runtime.malg</code> 中调用 <code>runtime.stackalloc</code> 申请新的栈内存，并在编译器插入的 <code>runtime.morestack</code> 中检查栈空间是否充足；</li></ol><p>需要注意的是，<code>Go</code> 语言的编译器不会为所有的函数插入 <code>runtime.morestack</code>，它只会在必要时插入指令以减少运行时的额外开销，编译指令 <code>nosplit</code> 可以跳过栈溢出的检查，虽然这能降低一些开销，不过固定大小的栈也存在溢出的风险。本节将分别分析栈的初始化、创建 <code>Goroutine</code> 时栈的分配、编译器和运行时协作完成的栈扩容以及当栈空间利用率不足时的缩容过程。</p><p>在 <code>Goroutine</code> 中会通过 <code>stackguard0</code> 来判断是否要进行栈增长：</p><ul><li><code>stackguard0</code>：<code>stack.lo</code> + <code>StackGuard</code>, 用于<code>stack overlow</code>的检测；</li><li><code>StackGuard</code>：保护区大小，常量<code>Linux</code>上为 <a href="https://github.com/golang/go/blob/go1.16.6/src/cmd/internal/objabi/stack.go#L21">928 字节</a>；</li><li><code>StackSmall</code>：常量大小为 <code>128</code> 字节，用于小函数调用的优化；</li><li><code>StackBig</code>：常量大小为 <code>4096</code> 字节；</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8dd04e53b3137274.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-59ca6d23d703f42f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>需要注意的是，由于栈是由高地址向低地址增长的，所以对比的时候，都是小于才执行扩容。</p><p>当执行栈扩容时，会在内存空间中分配更大的栈内存空间，然后将旧栈中的所有内容复制到新栈中，并修改指向旧栈对应变量的指针重新指向新栈，最后销毁并回收旧栈的内存空间，从而实现栈的动态扩容。</p><p><strong>具体代码实现</strong></p><p><a href="https://github.com/golang/go/blob/go1.16.6/src/runtime/asm_amd64.s#L416">runtime.morestack_noctxt</a> 是用汇编实现的，它会调用到 <code>runtime·morestack</code>，下面我们看看它的实现：</p><pre><code>TEXT runtime·morestack(SB),NOSPLIT,$0-0    // Cannot grow scheduler stack (m-&gt;g0).    // 无法增长调度器的栈(m-&gt;g0)    get_tls(CX)    MOVQ    g(CX), BX    MOVQ    g_m(BX), BX    MOVQ    m_g0(BX), SI    CMPQ    g(CX), SI    JNE    3(PC)    CALL    runtime·badmorestackg0(SB)    CALL    runtime·abort(SB)    // 省略signal stack、morebuf和sched的处理    ...    // Call newstack on m-&gt;g0's stack.    // 在 m-&gt;g0 栈上调用 newstack.    MOVQ    m_g0(BX), BX    MOVQ    BX, g(CX)    MOVQ    (g_sched+gobuf_sp)(BX), SP    CALL    runtime·newstack(SB)    CALL    runtime·abort(SB)    // 如果 newstack 返回则崩溃 crash if newstack returns    RET</code></pre><p><a href="https://github.com/golang/go/blob/go1.16.6/src/runtime/asm_amd64.s#L416">runtime·morestack</a> 做完校验和赋值操作后会切换到 <code>G0</code> 调用 <a href="https://github.com/golang/go/blob/go1.16.6/src/runtime/stack.go#L938">runtime·newstack</a>来完成扩容的操作。</p><pre><code>func newstack() {    thisg := getg()     gp := thisg.m.curg         // 初始化寄存器相关变量    morebuf := thisg.m.morebuf    thisg.m.morebuf.pc = 0    thisg.m.morebuf.lr = 0    thisg.m.morebuf.sp = 0    thisg.m.morebuf.g = 0    ...    // 校验是否被抢占    preempt := atomic.Loaduintptr(&amp;gp.stackguard0) == stackPreempt     // 如果被抢占    if preempt {        // 校验是否可以安全的被抢占        // 如果 M 上有锁        // 如果正在进行内存分配        // 如果明确禁止抢占        // 如果 P 的状态不是 running        // 那么就不执行抢占了        if !canPreemptM(thisg.m) {            // 到这里表示不能被抢占？            // Let the goroutine keep running for now.            // gp-&gt;preempt is set, so it will be preempted next time.            gp.stackguard0 = gp.stack.lo + _StackGuard            // 触发调度器的调度            gogo(&amp;gp.sched) // never return        }    }    if gp.stack.lo == 0 {        throw("missing stack in newstack")    }    // 寄存器 sp    sp := gp.sched.sp    if sys.ArchFamily == sys.AMD64 || sys.ArchFamily == sys.I386 || sys.ArchFamily == sys.WASM {        // The call to morestack cost a word.        sp -= sys.PtrSize    }     ...    if preempt {        //需要收缩栈        if gp.preemptShrink {             gp.preemptShrink = false            shrinkstack(gp)        }        // 被 runtime.suspendG 函数挂起        if gp.preemptStop {            // 被动让出当前处理器的控制权            preemptPark(gp) // never returns        }         //主动让出当前处理器的控制权        gopreempt_m(gp) // never return    }     // 计算新的栈空间是原来的两倍    oldsize := gp.stack.hi - gp.stack.lo    newsize := oldsize * 2     ...     //将 Goroutine 切换至 _Gcopystack 状态    casgstatus(gp, _Grunning, _Gcopystack)     //开始栈拷贝    copystack(gp, newsize)     casgstatus(gp, _Gcopystack, _Grunning)    gogo(&amp;gp.sched)}</code></pre><br><h4 id="栈拷贝"><a href="#栈拷贝" class="headerlink" title="栈拷贝"></a>栈拷贝</h4><p>在开始执行栈拷贝之前会先计算新栈的大小是原来的两倍，然后将 <code>Goroutine</code> 状态切换至 <code>_Gcopystack</code> 状态。</p><pre><code>func copystack(gp *g, newsize uintptr) {     old := gp.stack     // 当前已使用的栈空间大小    used := old.hi - gp.sched.sp     //分配新的栈空间    new := stackalloc(uint32(newsize))    ...     // 计算调整的幅度    var adjinfo adjustinfo    adjinfo.old = old    // 新栈和旧栈的幅度来控制指针的移动    adjinfo.delta = new.hi - old.hi     // 调整 sudogs, 必要时与 channel 操作同步    ncopy := used    if !gp.activeStackChans {        ...        adjustsudogs(gp, &amp;adjinfo)    } else {        // 到这里代表有被阻塞的 G 在当前 G 的channel 中，所以要防止并发操作，需要获取 channel 的锁                 // 在所有 sudog 中找到地址最大的指针        adjinfo.sghi = findsghi(gp, old)         // 对所有 sudog 关联的 channel 上锁，然后调整指针，并且复制 sudog 指向的部分旧栈的数据到新的栈上        ncopy -= syncadjustsudogs(gp, used, &amp;adjinfo)    }     // 将源栈中的整片内存拷贝到新的栈中    memmove(unsafe.Pointer(new.hi-ncopy), unsafe.Pointer(old.hi-ncopy), ncopy)    // 继续调整栈中 txt、defer、panic 位置的指针    adjustctxt(gp, &amp;adjinfo)    adjustdefers(gp, &amp;adjinfo)    adjustpanics(gp, &amp;adjinfo)    if adjinfo.sghi != 0 {        adjinfo.sghi += adjinfo.delta    }     // 将 G 上的栈引用切换成新栈    gp.stack = new    gp.stackguard0 = new.lo + _StackGuard // NOTE: might clobber a preempt request    gp.sched.sp = new.hi - used    gp.stktopsp += adjinfo.delta     // 在新栈重调整指针    gentraceback(^uintptr(0), ^uintptr(0), 0, gp, 0, nil, 0x7fffffff, adjustframe, noescape(unsafe.Pointer(&amp;adjinfo)), 0)     if stackPoisonCopy != 0 {        fillstack(old, 0xfc)    }    //释放原始栈的内存空间    stackfree(old)}</code></pre><ol><li><code>copystack</code> 首先会计算一下使用栈空间大小，那么在进行栈复制的时候只需要复制已使用的空间就好了；</li><li>然后调用 <code>stackalloc</code> 函数从堆上分配一片内存块；</li><li>然后对比新旧栈的 <code>hi</code> 的值计算出两块内存之间的差值 <code>delta</code>，这个 <code>delta</code> 会在调用 <code>adjustsudogs</code>、<code>adjustctxt</code> 等函数的时候判断旧栈的内存指针位置，然后加上 <code>delta</code> 然后就获取到了新栈的指针位置，这样就可以将指针也调整到新栈了；</li><li>调用 <code>memmove</code> 将源栈中的整片内存拷贝到新的栈中；</li><li>然后继续调用调整指针的函数继续调整栈中 <code>txt</code>、<code>defer</code>、<code>panic</code> 位置的指针；</li><li>接下来将 <code>G</code> 上的栈引用切换成新栈；</li><li>最后调用 <code>stackfree</code> 释放原始栈的内存空间；</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-82ea33f8a92282b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><br><h4 id="栈的收缩"><a href="#栈的收缩" class="headerlink" title="栈的收缩"></a>栈的收缩</h4><p>栈的收缩发生在 <code>GC</code> 时对栈进行扫描的阶段：</p><pre><code>func scanstack(gp *g, gcw *gcWork) {    ...     // 进行栈收缩    shrinkstack(gp)    ...}func shrinkstack(gp *g) {    ...    oldsize := gp.stack.hi - gp.stack.lo    newsize := oldsize / 2     // 当收缩后的大小小于最小的栈的大小时，不再进行收缩    if newsize &lt; _FixedStack {        return    }    avail := gp.stack.hi - gp.stack.lo    // 计算当前正在使用的栈数量，如果 gp 使用的当前栈少于四分之一，则对栈进行收缩    // 当前使用的栈包括到 SP 的所有内容以及栈保护空间，以确保有 nosplit 功能的空间    if used := gp.stack.hi - gp.sched.sp + _StackLimit; used &gt;= avail/4 {        return    }    // 将旧栈拷贝到新收缩后的栈上    copystack(gp, newsize)}</code></pre><p>新栈的大小会缩小至原来的一半，如果小于 <code>_FixedStack</code> （<code>2KB</code>）那么不再进行收缩。除此之外还会计算一下当前栈的使用情况是否不足 <code>1/4 </code>，如果使用超过 <code>1/4 </code>那么也不会进行收缩。</p><p>最后判断确定要进行收缩则调用 <code>copystack</code> 函数进行栈拷贝的逻辑。</p><br><h4 id="关于Golang栈思考产生几个有趣的实验"><a href="#关于Golang栈思考产生几个有趣的实验" class="headerlink" title="关于Golang栈思考产生几个有趣的实验"></a>关于Golang栈思考产生几个有趣的实验</h4><br><p><strong>Demo0：Golang栈会自动扩容，是不是永远不会栈溢出？</strong></p><pre><code>func f(i int) int {    if i == 0 || i == 1 {        return i    }    return f(i - 1)}func main() {    println(f(100000000))}</code></pre><p>执行这个函数，程序会报“<code>stack overflow</code>”的<code>excepttion</code>，具体错误日志如下：</p><pre><code>➜  GoTest git:(master) ✗ go run stack.goruntime: goroutine stack exceeds 1000000000-byte limitruntime: sp=0xc0200e0390 stack=[0xc0200e0000, 0xc0400e0000]fatal error: stack overflowruntime stack:runtime.throw(0x1074923, 0xe)    /Users/fanlv/.g/go/src/runtime/panic.go:1117 +0x72runtime.newstack()    /Users/fanlv/.g/go/src/runtime/stack.go:1069 +0x7edruntime.morestack()    /Users/fanlv/.g/go/src/runtime/asm_amd64.s:458 +0x8f</code></pre><p>这个错误是在<code>newstack</code>函数中抛出来的</p><pre><code>if newsize &gt; maxstacksize || newsize &gt; maxstackceiling {    if maxstacksize &lt; maxstackceiling {        print("runtime: goroutine stack exceeds ", maxstacksize, "-byte limit\n")    } else {        print("runtime: goroutine stack exceeds ", maxstackceiling, "-byte limit\n")    }    print("runtime: sp=", hex(sp), " stack=[", hex(gp.stack.lo), ", ", hex(gp.stack.hi), "]\n")    throw("stack overflow")}</code></pre><p>有上面可知，当新的栈大小超过了<code>maxstacksize</code>就会抛出”<code>stack overflow</code>“的异常。<code>maxstacksize</code>是在<code>runtime.main</code>中设置的。<code>64位</code> 系统下栈的最大值<code>1GB</code>、<code>32位</code>系统是<code>250MB</code></p><pre><code>// Max stack size is 1 GB on 64-bit, 250 MB on 32-bit.// Using decimal instead of binary GB and MB because// they look nicer in the stack overflow failure message.if sys.PtrSize == 8 {    maxstacksize = 1000000000} else {    maxstacksize = 250000000}</code></pre><br><p><strong>Demo1：验证栈扩容以后地址变了，对我们是不是写代码的时候会有影响？</strong></p><pre><code>func f(i int) int {    if i == 0 || i == 1 {        return i    }    return f(i - 1)}func main() {          println("demo1: ",demo1())}func demo1() {    var xDemo1 uint64    xAddr := uintptr(unsafe.Pointer(&amp;xDemo1))    println("demo1 before stack copy xDemo1 : ", xDemo1, " xDemo1 pointer: ", &amp;xDemo1)    f(10000000)    xPointer := (*uint64)(unsafe.Pointer(xAddr))    atomic.AddUint64(xPointer, 1)    println("demo1 after stack copy xDemo1 : ", xDemo1, " xDemo1 pointer:", &amp;xDemo1)}</code></pre><p>编译执行上面的代码，输入结果如下，由下面内容可以知道栈扩容过程中，栈上变量的地址的确会发生改变。</p><pre><code>➜  GoTest git:(master) ✗ go build -gcflags "-N -l -m"  stack.go➜  GoTest git:(master) ✗ ./stackdemo1 before stack copy xDemo1 :  0  xDemo1 pointer:  0xc000044740demo1 after stack copy xDemo1 :  0  xDemo1 pointer: 0xc04015ff40</code></pre><br><p><strong>Demo2：变量逃逸到堆上的情况</strong></p><p>上面是变量没有逃逸的情况，我们构造个变量逃逸到堆上的<code>case</code>（其实用<code>fmt.Println</code>打印<code>x变量</code>就导致<code>x逃逸</code>，笔者最开始使用<code>fmt.Println</code>打印<code>x变量</code>发现栈扩容的情况下，<code>x地址</code>也一直不会变），代码如下：</p><pre><code>// f 和 main 函数代码如demo1func demo2() {    var xDemo2 uint64    println("demo2 before stack copy xDemo2 : ", xDemo2, " xDemo2 pointer: ", &amp;xDemo2)    f(10000000)    atomic.AddUint64(&amp;xDemo2, 1)    println("demo2 after stack copy xDemo2 : ", xDemo2, " xDemo2 pointer:", &amp;xDemo2)}</code></pre><p><code>demo2</code>中，我们直接用取地址的方式去调用<code>atomic.AddUint64(&amp;xDemo2, 1)</code>，看编译日志可以知道<code>xDemo2</code>这个变量逃逸到堆上了。逃逸 到堆上的变量地址是不会变的。这样也符合预期。</p><pre><code>➜  GoTest git:(master) ✗ go build -gcflags "-N -l -m"  stack.go#command-line-arguments./stack.go:36:6: moved to heap: xDemo2➜  GoTest git:(master) ✗ ./stackdemo2 before stack copy xDemo2 :  0  xDemo2 pointer:  0xc0000180b8demo2 after stack copy xDemo2 :  1  xDemo2 pointer: 0xc0000180b8</code></pre><br><p><strong>Demo3：go run方式去运行代码</strong></p><p><code>demo3</code> 中，我们直接返回变量的地址，这个时候我们知道，变量肯定已经逃逸到堆上。我们用<code>go run</code>的方式去跑下面代码，地址会变吗？</p><pre><code>func demo3() *uint64 {    var xDemo3 uint64 = 8    println("demo3 before stack copy xDemo3 : ", xDemo3, " xDemo3 pointer: ", &amp;xDemo3)    f(1000)    println("demo3 after stack copy xDemo3 : ", xDemo3, " xDemo3 pointer:", &amp;xDemo3)    return &amp;xDemo3}</code></pre><p>输入日志如下，我们发现逃逸到堆上的变量地址也变了，这个是为什么？</p><pre><code>➜  GoTest git:(master) ✗ go run stack.godemo3 before stack copy xDemo3 :  0  xDemo3 pointer:  0xc000044770demo3 after stack copy xDemo3 :  0  xDemo3 pointer: 0xc000117f70</code></pre><p>其实执行<code>go run</code>的时候，它就是想编译在运行，执行<code>go build</code>的时候没有“<code>禁止内联</code>”，所以<code>demo3</code>函数就发生了内联，所以变量也不会逃逸到堆上了。我们可以<code>go build -gcflags "-m"  stack.go</code>看下，输出日志如下。<code>can inline demo3 </code>，这个时候<code>demo3</code>已经内联了。</p><pre><code>➜  GoTest git:(master) ✗ go build -gcflags "-m"  stack.go#command-line-arguments./stack.go:37:6: can inline demo3./stack.go:14:7: inlining call to demo3./stack.go:38:6: moved to heap: xDemo3</code></pre><p>虽然逃逸分析日志还有<code>moved to heap: xDemo3 </code>，但是其实这个时候已经没有逃逸了，具体我们可以导出汇编代码 <code>otool -tvV stack &gt;&gt; ~/Desktop/stack.s</code> 看下。可以看到main里面已经没有<code>demo3</code>的调用了。<code>xDemo3</code>这边变量还是在栈上<code>$0x8, 0x10(%rsp)</code></p><pre><code>_main.main:000000000105c920    movq    %gs:0x30, %rcx000000000105c929    cmpq    0x10(%rcx), %rsp000000000105c92d    jbe    0x105ca21000000000105c933    subq    $0x20, %rsp000000000105c937    movq    %rbp, 0x18(%rsp)000000000105c93c    leaq    0x18(%rsp), %rbp000000000105c941    movq    $0x8, 0x10(%rsp)000000000105c94a    callq    _runtime.printlock000000000105c94f    leaq    0x18e96(%rip), %rax000000000105c956    movq    %rax, (%rsp)000000000105c95a    movq    $0x22, 0x8(%rsp)000000000105c963    callq    _runtime.printstring000000000105c968    movq    0x10(%rsp), %rax000000000105c96d    movq    %rax, (%rsp)000000000105c971    callq    _runtime.printuint000000000105c976    leaq    0x16ccd(%rip), %rax000000000105c97d    movq    %rax, (%rsp)000000000105c981    movq    $0x13, 0x8(%rsp)000000000105c98a    callq    _runtime.printstring000000000105c98f    leaq    0x10(%rsp), %rax000000000105c994    movq    %rax, (%rsp)000000000105c998    callq    _runtime.printpointer000000000105c99d    nopl    (%rax)000000000105c9a0    callq    _runtime.printnl000000000105c9a5    callq    _runtime.printunlock000000000105c9aa    movq    $0x3e8, (%rsp)  000000000105c9b2    callq    _main.f000000000105c9b7    callq    _runtime.printlock000000000105c9bc    leaq    0x18bf7(%rip), %rax000000000105c9c3    movq    %rax, (%rsp)000000000105c9c7    movq    $0x21, 0x8(%rsp)000000000105c9d0    callq    _runtime.printstring000000000105c9d5    movq    0x10(%rsp), %rax000000000105c9da    movq    %rax, (%rsp)000000000105c9de    nop000000000105c9e0    callq    _runtime.printuint000000000105c9e5    leaq    0x16b62(%rip), %rax000000000105c9ec    movq    %rax, (%rsp)000000000105c9f0    movq    $0x12, 0x8(%rsp)000000000105c9f9    callq    _runtime.printstring000000000105c9fe    leaq    0x10(%rsp), %rax000000000105ca03    movq    %rax, (%rsp)000000000105ca07    callq    _runtime.printpointer000000000105ca0c    callq    _runtime.printnl000000000105ca11    callq    _runtime.printunlock000000000105ca16    movq    0x18(%rsp), %rbp000000000105ca1b    addq    $0x20, %rsp000000000105ca1f    nop000000000105ca20    retq000000000105ca21    callq    _runtime.morestack_noctxt000000000105ca26    jmp    _main.main</code></pre><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/luozhiyun/p/14619585.html">https://www.cnblogs.com/luozhiyun/p/14619585.html</a></p><p><a href="http://www.huamo.online/2019/06/25/%E6%B7%B1%E5%85%A5%E7%A0%94%E7%A9%B6goroutine%E6%A0%88/">http://www.huamo.online/2019/06/25/%E6%B7%B1%E5%85%A5%E7%A0%94%E7%A9%B6goroutine%E6%A0%88/</a></p><p><a href="https://www.cnblogs.com/shijingxiang/articles/12200355.html">https://www.cnblogs.com/shijingxiang/articles/12200355.html</a></p><p><a href="https://www.cnblogs.com/shijingxiang/articles/12200355.html">https://draveness.me/golang/docs/part3-runtime/ch07-memory/golang-stack-management/</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Golang </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux内核虚拟地址空间</title>
      <link href="/2021/07/25/linux-mem/"/>
      <url>/2021/07/25/linux-mem/</url>
      
        <content type="html"><![CDATA[<br><h2 id="x86-32位虚拟地址空间"><a href="#x86-32位虚拟地址空间" class="headerlink" title="x86-32位虚拟地址空间"></a>x86-32位虚拟地址空间</h2><p>就我们所知，<code>Linux</code>内核一般将处理器的虚拟地址空间划分为两个部分。底部比较大的部分用于用户进程，顶部则专用于内核。虽然（在两个用户进程之间的）上下文切换期间会改变下半部分，<strong>但虚拟地址空间的内核部分总是保持不变</strong>。</p><br><p><code>Linux</code>将虚拟地址空间划分为：<code>0~3G</code>为用户空间，<code>3~4G</code>为内核空间</p><img alt="cover" src="https://upload-images.jianshu.io/upload_images/12321605-cb891da74fe7ebaa.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><!-- ![Linux-Memory-X86-32](https://upload-images.jianshu.io/upload_images/12321605-b62889a2b3c2eb38.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240) --><p><a href="https://raw.githubusercontent.com/fanlv/blog/9f9a531bcf4876759ef216060175dba9702415f2/backup/Content/Foundation/images/Linux%20-%20Memory%20-%20X86-32.jpg">点我查看原图</a></p><br><h3 id="用户地址空间"><a href="#用户地址空间" class="headerlink" title="用户地址空间"></a>用户地址空间</h3><br><h4 id="保留区-0x08048000"><a href="#保留区-0x08048000" class="headerlink" title="保留区 - 0x08048000"></a>保留区 - 0x08048000</h4><p>位于虚拟地址空间的最低部分，未赋予物理地址。任何对它的引用都是非法的，用于捕捉使用空指针和小整型值指针引用内存的异常情况。</p><p>它并不是一个单一的内存区域，而是对地址空间中受到操作系统保护而禁止用户进程访问的地址区域的总称。大多数操作系统中，极小的地址通常都是不允许访问的，如<code>NULL</code>。<code>C语言</code>将无效指针赋值为0也是出于这种考虑，因为0地址上正常情况下不会存放有效的可访问数据。</p><p>因为“历史原因”前面 <code>128.28125MB</code> 属于保留空间。</p><p><a href="https://www.quora.com/On-Linux-why-does-the-text-segment-start-at-0x08048000-What-is-stored-below-that-address">On Linux, why does the text segment start at 0x08048000? What is stored below that address?</a></p><p><a href="https://unix.stackexchange.com/questions/466389/where-are-the-kernel-stack-frames-for-c-run-time-startup-functions-and-fr">Where are “the kernel stack”, “Frames for C run-time startup functions”, and “Frame for main()” in the memory layout of a program?</a></p><br><h4 id="代码段-text"><a href="#代码段-text" class="headerlink" title="代码段(text)"></a>代码段(text)</h4><p>代码段也称正文段或文本段，通常用于存放程序执行代码(即<code>CPU</code>执行的机器指令)。一般<code>C语言</code>执行语句都编译成机器代码保存在代码段。通常代码段是可共享的，因此频繁执行的程序只需要在内存中拥有一份拷贝即可。代码段通常属于只读，以防止其他程序意外地修改其指令(对该段的写操作将导致段错误)。某些架构也允许代码段为可写，即允许修改程序。</p><p>代码段指令根据程序设计流程依次执行，对于顺序指令，只会执行一次(每个进程)；若有反复，则需使用跳转指令；若进行递归，则需要借助栈来实现。</p><p>代码段指令中包括操作码和操作对象(或对象地址引用)。若操作对象是立即数(具体数值)，将直接包含在代码中；若是局部数据，将在栈区分配空间，然后引用该数据地址；若位于<code>BSS</code>段和数据段，同样引用该数据地址。</p><p>代码段最容易受优化措施影响。</p><br><h4 id="数据段-Data"><a href="#数据段-Data" class="headerlink" title="数据段(Data)"></a>数据段(Data)</h4><p>数据段通常用于存放程序中已初始化且初值不为<code>0</code>的全局变量和静态局部变量。数据段属于静态内存分配(静态存储区)，可读可写。</p><p>数据段保存在目标文件中(在嵌入式系统里一般固化在镜像文件中)，其内容由程序初始化。例如，对于全局变量<code>int gVar = 10</code>，必须在目标文件数据段中保存<code>10</code>这个数据，然后在程序加载时复制到相应的内存。</p><p>数据段与<code>BSS段</code>的区别如下： </p><ul><li><p><code>BSS段</code>不占用物理文件尺寸，但占用内存空间；数据段占用物理文件，也占用内存空间。对于大型数组如<code>int ar0[10000] = {1, 2, 3, ...}</code>和<code>int ar1[10000]</code>，<code>ar1</code>放在<code>BSS段</code>，只记录共有<code>10000*4</code>个字节需要初始化为<code>0</code>，而不是像<code>ar0</code>那样记录每个数据<code>1、2、3...</code>，此时<code>BSS</code>为目标文件所节省的磁盘空间相当可观。</p></li><li><p>当程序读取数据段的数据时，系统会出发缺页故障，从而分配相应的物理内存；当程序读取<code>BSS段</code>的数据时，内核会将其转到一个全零页面，不会发生缺页故障，也不会为其分配相应的物理内存。</p></li></ul><p>运行时数据段和<code>BSS段</code>的整个区段通常称为数据区。某些资料中“数据段”指代数据段 + <code>BSS段</code> + 堆。</p><br><h4 id="BSS段"><a href="#BSS段" class="headerlink" title="BSS段"></a>BSS段</h4><p><code>BSS(Block Started by Symbol)</code>段中通常存放程序中以下符号：</p><ul><li>未初始化的全局变量和静态局部变量</li><li>初始值为<code>0</code>的全局变量和静态局部变量(依赖于编译器实现)</li><li>未定义且初值不为<code>0</code>的符号(该初值即<code>common block</code>的大小)</li></ul><p><code>C语言</code>中，未显式初始化的静态分配变量被初始化为<code>0</code>(算术类型)或空指针(指针类型)。由于程序加载时，<code>BSS</code>会被操作系统清零，所以未赋初值或初值为<code>0</code>的全局变量都在<code>BSS</code>中。<code>BSS段</code>仅为未初始化的静态分配变量预留位置，在目标文件中并不占据空间，这样可减少目标文件体积。但程序运行时需为变量分配内存空间，故目标文件必须记录所有未初始化的静态分配变量大小总和(通过<code>start_bss</code>和<code>end_bss</code>地址写入机器代码)。当加载器(<code>loader</code>)加载程序时，将为<code>BSS段</code>分配的内存初始化为0。在嵌入式软件中，进入<code>main()</code>函数之前<code>BSS段</code>被<code>C</code>运行时系统映射到初始化为全零的内存(效率较高)。</p><p>注意，尽管均放置于<code>BSS段</code>，但初值为<code>0</code>的全局变量是强符号，而未初始化的全局变量是弱符号。若其他地方已定义同名的强符号(初值可能非 <code>0</code>)，则弱符号与之链接时不会引起重定义错误，但运行时的初值可能并非期望值(会被强符号覆盖)。因此，定义全局变量时，若只有本文件使用，则尽量使用<code>static</code>关键字修饰；否则需要为全局变量定义赋初值(哪怕<code>0</code>值)，保证该变量为强符号，以便链接时发现变量名冲突，而不是被未知值覆盖。</p><p>某些编译器将未初始化的全局变量保存在<code>common段</code>，链接时再将其放入<code>BSS段</code>。在编译阶段可通过<code>-fno-common</code>选项来禁止将未初始化的全局变量放入<code>common段</code>。</p><p>此外，由于目标文件不含<code>BSS段</code>，故程序烧入<code>存储器(Flash)</code>后<code>BSS段</code>地址空间内容未知。<code>U-Boot</code>启动过程中，将<code>U-Boot</code>的<code>Stage2</code>代码(通常位于<code>lib_xxxx/board.c</code>文件)搬迁(拷贝)到<code>SDRAM</code>空间后必须人为添加清零<code>BSS段</code>的代码，而不可依赖于<code>Stage2</code>代码中变量定义时赋<code>0</code>值。</p><blockquote><p>【扩展阅读】BSS历史</p><p><code>BSS</code>(<code>Block Started by Symbol</code>，以符号开始的块)一词最初是UA-SAP汇编器(<code>United Aircraft Symbolic Assembly Program</code>)中的伪指令，用于为符号预留一块内存空间。该汇编器由美国联合航空公司于<code>20世纪50</code>年代中期为<code>IBM 704</code>大型机所开发。</p><p>后来该词被作为关键字引入到了<code>IBM 709</code>和<code>7090/94</code>机型上的标准汇编器<code>FAP(Fortran Assembly Program)</code>，用于定义符号并且为该符号预留指定字数的未初始化空间块。</p><p>在采用段式内存管理的架构中(如<code>Intel 80x86</code>系统)，<code>BSS段</code>通常指用来存放程序中未初始化全局变量的一块内存区域，该段变量只有名称和大小却没有值。程序开始时由系统初始化清零。</p><p><code>BSS段</code>不包含数据，仅维护开始和结束地址，以便内存能在运行时被有效地清零。<code>BSS</code>所需的运行时空间由目标文件记录，但<code>BSS</code>并不占用目标文件内的实际空间，即<code>BSS节段</code>应用程序的二进制映象文件中并不存在。</p></blockquote><br><h4 id="堆-heap"><a href="#堆-heap" class="headerlink" title="堆(heap)"></a>堆(heap)</h4><p>堆用于存放进程运行时动态分配的内存段，可动态扩张或缩减。堆中内容是匿名的，不能按名字直接访问，只能通过指针间接访问。当进程调用<code>malloc(C)/new(C++)</code>等函数分配内存时，新分配的内存动态添加到堆上(扩张)；当调用<code>free(C)/delete(C++)</code>等函数释放内存时，被释放的内存从堆中剔除(缩减) 。</p><p>分配的堆内存是经过字节对齐的空间，以适合原子操作。堆管理器通过链表管理每个申请的内存，由于堆申请和释放是无序的，最终会产生内存碎片。堆内存一般由应用程序分配释放，回收的内存可供重新使用。若程序员不释放，程序结束时操作系统可能会自动回收。</p><p>堆的末端由<code>break</code>指针标识，当堆管理器需要更多内存时，可通过系统调用<code>brk()</code>和<code>sbrk()</code>来移动<code>break</code>指针以扩张堆，一般由系统自动调用。</p><p>使用堆时经常出现两种问题：</p><ol><li>释放或改写仍在使用的内存(“内存破坏”)；</li><li>未释放不再使用的内存(“内存泄漏”)。当释放次数少于申请次数时，可能已造成内存泄漏。泄漏的内存往往比忘记释放的数据结构更大，因为所分配的内存通常会圆整为下个大于申请数量的2的幂次(如申请<code>212B</code>，会圆整为<code>256B</code>)。</li></ol><p>注意，堆不同于数据结构中的”堆”，其行为类似链表。</p><blockquote><p>【扩展阅读】栈和堆的区别</p><p>①管理方式：栈由编译器自动管理；堆由程序员控制，使用方便，但易产生内存泄露。</p><p>②生长方向：栈向低地址扩展(即”向下生长”)，是连续的内存区域；堆向高地址扩展(即”向上生长”)，是不连续的内存区域。这是由于系统用链表来存储空闲内存地址，自然不连续，而链表从低地址向高地址遍历。</p><p>③空间大小：栈顶地址和栈的最大容量由系统预先规定(通常默认<code>2M</code>或<code>10M</code>)；堆的大小则受限于计算机系统中有效的虚拟内存，<code>32</code>位<code>Linux</code>系统中堆内存可达<code>2.9G</code>空间。</p><p>④存储内容：栈在函数调用时，首先压入主调函数中下条指令(函数调用语句的下条可执行语句)的地址，然后是函数实参，然后是被调函数的局部变量。本次调用结束后，局部变量先出栈，然后是参数，最后栈顶指针指向最开始存的指令地址，程序由该点继续运行下条可执行语句。堆通常在头部用一个字节存放其大小，堆用于存储生存期与函数调用无关的数据，具体内容由程序员安排。</p><p>⑤分配方式：栈可静态分配或动态分配。静态分配由编译器完成，如局部变量的分配。动态分配由<code>alloca</code>函数在栈上申请空间，用完后自动释放。堆只能动态分配且手工释放。</p><p>⑥分配效率：栈由计算机底层提供支持：分配专门的寄存器存放栈地址，压栈出栈由专门的指令执行，因此效率较高。堆由函数库提供，机制复杂，效率比栈低得多。<code>Windows</code>系统中<code>VirtualAlloc</code>可直接在进程地址空间中分配一块内存，快速且灵活。</p><p>⑦分配后系统响应：只要栈剩余空间大于所申请空间，系统将为程序提供内存，否则报告异常提示栈溢出。</p><p>操作系统为堆维护一个记录空闲内存地址的链表。当系统收到程序的内存分配申请时，会遍历该链表寻找第一个空间大于所申请空间的堆结点，然后将该结点从空闲结点链表中删除，并将该结点空间分配给程序。若无足够大小的空间(可能由于内存碎片太多)，有可能调用系统功能去增加程序数据段的内存空间，以便有机会分到足够大小的内存，然后进行返回。，大多数系统会在该内存空间首地址处记录本次分配的内存大小，供后续的释放函数(如<code>free/delete</code>)正确释放本内存空间。</p><p>此外，由于找到的堆结点大小不一定正好等于申请的大小，系统会自动将多余的部分重新放入空闲链表中。</p><p>⑧碎片问题：栈不会存在碎片问题，因为栈是先进后出的队列，内存块弹出栈之前，在其上面的后进的栈内容已弹出。而频繁申请释放操作会造成堆内存空间的不连续，从而造成大量碎片，使程序效率降低。</p><p>可见，堆容易造成内存碎片；由于没有专门的系统支持，效率很低；由于可能引发用户态和内核态切换，内存申请的代价更为昂贵。所以栈在程序中应用最广泛，函数调用也利用栈来完成，调用过程中的参数、返回地址、栈基指针和局部变量等都采用栈的方式存放。所以，建议尽量使用栈，仅在分配大量或大块内存空间时使用堆。</p><p>使用栈和堆时应避免越界发生，否则可能程序崩溃或破坏程序堆、栈结构，产生意想不到的后果。</p></blockquote><br><h4 id="内存映射段-mmap"><a href="#内存映射段-mmap" class="headerlink" title="内存映射段(mmap)"></a>内存映射段(mmap)</h4><p>此处，内核将硬盘文件的内容直接映射到内存, 任何应用程序都可通过<code>Linux</code>的<code>mmap()</code>系统调用或<code>Windows</code>的<code>CreateFileMapping()/MapViewOfFile()</code>请求这种映射。内存映射是一种方便高效的文件<code>I/O</code>方式， 因而被用于装载动态共享库。用户也可创建匿名内存映射，该映射没有对应的文件, 可用于存放程序数据。在<code>Linux</code>中，若通过<code>malloc()</code>请求一大块内存，<code>C运行库</code>将创建一个匿名内存映射，而不使用堆内存。”大块” 意味着比阈值 <code>MMAP_THRESHOLD</code>还大，缺省为<code>128KB</code>，可通过<code>mallopt()</code>调整。</p><p><strong>PS:  内存映射端在Linux 2.6.7以前是向上增长的，在2.6.7之后改为向下增长</strong>。</p><p>更多可以查看<code>《深入理解操作系统内核》4.2.1章节</code></p><br><h4 id="栈-stack"><a href="#栈-stack" class="headerlink" title="栈(stack)"></a>栈(stack)</h4><p>栈又称堆栈，由编译器自动分配释放，行为类似数据结构中的栈(先进后出)。堆栈主要有三个用途：</p><ul><li>为函数内部声明的非静态局部变量(<code>C语言</code>中称“自动变量”)提供存储空间。</li><li>记录函数调用过程相关的维护性信息，称为栈帧(<code>Stack Frame</code>)或过程活动记录(<code>Procedure Activation Record</code>)。它包括函数返回地址，不适合装入寄存器的函数参数及一些寄存器值的保存。除递归调用外，堆栈并非必需。因为编译时可获知局部变量，参数和返回地址所需空间，并将其分配于BSS段。<br>临时存储区，用于暂存长算术表达式部分计算结果或<code>alloca()</code>函数分配的栈内内存。</li><li>持续地重用栈空间有助于使活跃的栈内存保持在CPU缓存中，从而加速访问。进程中的每个线程都有属于自己的栈。向栈中不断压入数据时，若超出其容量就会耗尽栈对应的内存区域，从而触发一个页错误。此时若栈的大小低于堆栈最大值<code>RLIMIT_STACK</code>(通常是8M)，则栈会动态增长，程序继续运行。映射的栈区扩展到所需大小后，不再收缩。</li></ul><p><code>Linux</code>中<code>ulimit -s</code>命令可查看和设置堆栈最大值，当程序使用的堆栈超过该值时, 发生栈溢出(<code>Stack Overflow</code>)，程序收到一个段错误(<code>Segmentation Fault</code>)。注意，调高堆栈容量可能会增加内存开销和启动时间。</p><p>堆栈既可向下增长(向内存低地址)也可向上增长, 这依赖于具体的实现。本文所述堆栈向下增长。</p><p>堆栈的大小在运行时由内核动态调整。</p><br><h3 id="内核地址空间"><a href="#内核地址空间" class="headerlink" title="内核地址空间"></a>内核地址空间</h3><br><h4 id="直接映射区（896M）"><a href="#直接映射区（896M）" class="headerlink" title="直接映射区（896M）"></a>直接映射区（896M）</h4><p>所谓的直接映射区，就是这一块空间是连续的，和物理内存是非常简单的映射关系，其实就是虚拟内存地址减去 <code>3G</code>，就得到物理内存的位置。</p><pre><code>__pa(vaddr) 返回与虚拟地址 vaddr 相关的物理地址；__va(paddr) 则计算出对应于物理地址 paddr 的虚拟地址。// PAGE_OFFSET =&gt; 3G  0x0c0000000#define __va(x)      ((void *)((unsigned long)(x)+PAGE_OFFSET))#define __pa(x)    __phys_addr((unsigned long)(x))#define __phys_addr(x)    __phys_addr_nodebug(x)#define __phys_addr_nodebug(x)  ((x) - PAGE_OFFSET)</code></pre><p>这 <code>896M</code> 还需要仔细分解。在系统启动的时候，物理内存的前 <code>1M</code> 已经被占用了，从 <code>1M</code> 开始加载内核代码段，然后就是内核的<code>全局变量</code>、<code>BSS</code> 等，也是 <code>ELF</code> 里面涵盖的。这样内核的代码段，全局变量，<code>BSS</code> 也就会被映射到 <code>3G</code> 后的虚拟地址空间里面。具体的物理内存布局可以查看。</p><pre><code>cat /proc/iomem.....00100000-bffdbfff : System RAM  01000000-01a02fff : Kernel code  01a03000-021241bf : Kernel data  02573000-02611fff : Kernel bss  25000000-34ffffff : Crash kernel.....</code></pre><p>在内核运行的过程中，如果碰到系统调用创建进程，会创建 <code>task_struct</code> 这样的实例，内核的进程管理代码会将实例创建在 <code>3G</code> 至 <code>3G+896M</code> 的虚拟空间中，当然也会被放在物理内存里面的前 <code>896M</code> 里面，相应的页表也会被创建。</p><p>在内核运行的过程中，会涉及内核栈的分配，内核的进程管理的代码会将内核栈创建在 <code>3G</code> 至 <code>3G+896M</code> 的虚拟空间中，当然也就会被放在物理内存里面的前 <code>896M</code> 里面，相应的页表也会被创建。</p><p><a href="https://time.geekbang.org/column/article/95715">进程空间管理</a></p><br><h4 id="高端内存-HIGH-MEMORY"><a href="#高端内存-HIGH-MEMORY" class="headerlink" title="高端内存 - HIGH_MEMORY"></a>高端内存 - HIGH_MEMORY</h4><p><code>x86-32</code>下特有的（<code>x64下没有这个东西</code>），因为内核虚拟空间只有<code>1G</code>无法管理全部的内存空间。</p><p>当内核想访问高于<code>896MB</code>物理地址内存时，从<code>0xF8000000 ~ 0xFFFFFFFF</code>地址空间范围内找一段相应大小空闲的逻辑地址空间，借用一会。借用这段逻辑地址空间，建立映射到想访问的那段物理内存（即填充内核<code>PTE</code>页面表），临时用一会，用完后归还。这样别人也可以借用这段地址空间访问其他物理内存，实现了使用有限的地址空间，访问所有所有物理内存。如下图。</p><p>从上面的描述，我们可以知道高端内存的最基本思想：借一段地址空间，建立临时地址映射，用完后释放，达到这段地址空间可以循环使用，访问所有物理内存。</p><p>看到这里，不禁有人会问：万一有内核进程或模块一直占用某段逻辑地址空间不释放，怎么办？若真的出现的这种情况，则内核的高端内存地址空间越来越紧张，若都被占用不释放，则没有建立映射到物理内存都无法访问了。</p><p><a href="http://ilinuxkernel.com/?p=1013">Linux内核高端内存</a></p><br><h4 id="VMALLOC-OFFSET"><a href="#VMALLOC-OFFSET" class="headerlink" title="VMALLOC_OFFSET"></a>VMALLOC_OFFSET</h4><p>系统会在<code>low memory</code>和<code>VMALLOC</code>区域留8M，防止访问越界。因此假如理论上<code>vmalloc size</code>有<code>300M</code>，实际可用的也是只有<code>292M</code>。</p><pre><code>include/asm-x86/pgtable_32.h #define VMALLOC_OFFSET (8*1024*1024)</code></pre><p>这个缺口可用作针对任何内核故障的保护措施。如果访问越界地址（即无意地访问物理上不存在的内存区），则访问失败并生成一个异常，报告该错误。如果<code>vmalloc</code>区域紧接着直接映射，那么访问将成功而不会注意到错误。在稳定运行的情况下，肯定不需要这个额外的保护措施，但它对开发尚未成熟的新内核特性是有用的。</p><p><code>《深入理解Linux内核》3.4</code></p><br><h4 id="VMALLOC"><a href="#VMALLOC" class="headerlink" title="VMALLOC"></a>VMALLOC</h4><p>虚拟内存中连续、但物理内存中不连续的内存区，可以在<code>vmalloc</code>区域分配。该机制通常用于用户过程，内核自身会试图尽力避免非连续的物理地址。内核通常会成功，因为大部分大的内存块都在启动时分配给内核，那时内存的碎片尚不严重。但在已经运行了很长时间的系统上，在内核需要物理内存时，就可能出现可用空间不连续的情况。此类情况，主要出现在动态加载模块时。</p><pre><code>include/asm-x86/pgtable_32.h #define VMALLOC_START (((unsigned long) high_memory + \  2*VMALLOC_OFFSET-1) &amp; ~(VMALLOC_OFFSET-1)) #ifdef CONFIG_HIGHMEM #define VMALLOC_END (PKMAP_BASE-2*PAGE_SIZE) #else #define VMALLOC_END (FIXADDR_START-2*PAGE_SIZE) #endif</code></pre><p><code>vmalloc</code>区域的起始地址，取决于在直接映射物理内存时，使用了多少虚拟地址空间内存（因此也依赖于上文的<code>high_memory</code>变量）。内核还考虑到下述事实，即两个区域之间有至少为<code>VMALLOC_OFFSET</code>的一个缺口，而且<code>vmalloc</code>区域从可被<code>VMALLOC_OFFSET</code>整除的地址开始。</p><p><code>VMALLOC_START</code> 到 <code>VMALLOC_END</code> 之间称为内核动态映射空间，也即内核想像用户态进程一样 <code>malloc</code> 申请内存，在内核里面可以使用 <code>vmalloc</code>。假设物理内存里面，<code>896M</code> 到 <code>1.5G</code> 之间已经被用户态进程占用了，并且映射关系放在了进程的页表中，内核 <code>vmalloc</code> 的时候，只能从分配物理内存 <code>1.5G</code> 开始，就需要使用这一段的虚拟地址进行映射，映射关系放在专门给内核自己用的页表里面。</p><p>使用<code>vmalloc</code>的最著名的实例是内核对模块的实现。因为模块可能在任何时候加载，如果模块数据比较多，那么无法保证有足够的连续内存可用，特别是在系统已经运行了比较长时间的情况下。如果能够用小块内存拼接出足够的内存，那么使用<code>vmalloc</code>可以规避该问题。</p><p>内核中还有大约<code>400</code>处地方调用了<code>vmalloc</code>，特别是在设备和声音驱动程序中。</p><p>因为用于<code>vmalloc</code>的内存页总是必须映射在内核地址空间中，因此使用<code>ZONE_HIGHMEM</code>内存域的页要优于其他内存域。这使得内核可以节省更宝贵的较低端内存域，而又不会带来额外的坏处。因此，<code>vmalloc</code>（连同其他映射函数在<code>3.5.8节</code>讨论）是内核出于自身的目的（并非因为用户空间应用程序）使用高端内存页的少数情形之一。</p><p><code>《深入理解Linux内核》3.4</code></p><p><a href="https://time.geekbang.org/column/article/95715">进程空间管理</a></p><br><h4 id="持久映射"><a href="#持久映射" class="headerlink" title="持久映射"></a>持久映射</h4><p>内核专门为此留出一块线性空间，从 <code>PKMAP_BASE</code> 到 <code>FIXADDR_START</code> ，用于映射高端内存。在 <code>2.6内核</code>上，这个地址范围是 <code>4G-8M</code> 到 <code>4G-4M</code> 之间。这个空间起叫”<code>内核永久映射空间</code>”或者”<code>永久内核映射空间</code>”。这个空间和其它空间使用同样的页目录表，对于内核来说，就是 <code>swapper_pg_dir</code>，对普通进程来说，通过 <code>CR3 寄存器</code>指向。通常情况下，这个空间是 <code>4M</code> 大小，因此仅仅需要一个页表即可，内核通过来 <code>pkmap_page_table</code> 寻找这个页表。通过 <code>kmap()</code>，可以把一个 <code>page</code> 映射到这个空间来。由于这个空间是 <code>4M</code>大小，最多能同时映射 <code>1024</code> 个 <code>page</code>。因此，对于不使用的的 <code>page</code>，及应该时从这个空间释放掉（也就是解除映射关系），通过 <code>kunmap() </code>，可以把一个 <code>page</code> 对应的线性地址从这个空间释放出来。</p><pre><code> include/asm-x86/highmem.h #define LAST_PKMAP 1024 #define PKMAP_BASE ( (FIXADDR_BOOT_START -PAGE_SIZE*(LAST_PKMAP + 1)) &amp; PMD_MASK )</code></pre><p>如果需要将高端页帧长期映射（作为持久映射）到内核地址空间中，必须使用<code>kmap</code>函数。需要映射的页用指向<code>page</code>的指针指定，作为该函数的参数。该函数在有必要时创建一个映射（即，如果该页确实是高端页），并返回数据的地址。</p><p>如果没有启用高端支持，该函数的任务就比较简单。在这种情况下，所有页都可以直接访问，因此只需要返回页的地址，无需显式创建一个映射。</p><p>如果确实存在高端页，情况会比较复杂。类似于<code>vmalloc</code>，内核首先必须建立高端页和所映射到的地址之间的关联。还必须在虚拟地址空间中分配一个区域以映射页帧，最后，内核必须记录该虚拟区域的哪些部分在使用中，哪些仍然是空闲的。</p><p><code>pkmap_count</code>（在<code>mm/highmem.c</code>定义）是一容量为<code>LAST_PKMAP</code>的整数数组，其中每个元素都对应于一个持久映射页。它实际上是被映射页的一个使用计数器，语义不太常见。该计数器计算了内核使用该页的次数加<code>1</code>。如果计数器值为<code>2</code>，则内核中只有一处使用该映射页。计数器值为<code>5</code>表示有<code>4</code>处使用。一般地说，计数器值为<code>n</code>代表内核中有<code>n-1</code>处使用该页。</p><p>和通常的使用计数器一样，<code>0</code>意味着相关的页没有使用。计数器值<code>1</code>有特殊语义。这表示该位置关联的页已经映射，但由于<code>CPU</code>的<code>TLB</code>没有更新而无法使用，此时访问该页，或者失败，或者会访问到一个不正确的地址。</p><p>内核利用下列数据结构，来建立物理内存页的page实例与其在虚似内存区中位置之间的关联：</p><pre><code>mm/highmem.c struct page_address_map {  struct page *page;  void *virtual;  struct list_head list; };</code></pre><p>该结构用于建立<code>page→virtual</code>的映射（该结构由此得名）。<code>page</code>是一个指向全局<code>mem_map</code>数组中的<code>page</code>实例的指针，<code>virtual</code>指定了该页在内核虚拟地址空间中分配的位置。</p><p>刚才描述的<code>kmap</code>函数不能用于中断处理程序，因为它可能进入睡眠状态。如果<code>pkmap</code>数组中没有空闲位置，该函数会进入睡眠状态，直至情形有所改善。因此内核提供了一个备选的映射函数，其执行是原子的，逻辑上称为<code>kmap_atomic</code>。该函数的一个主要优点是它比普通的<code>kmap</code>快速。但它不能用于可能进入睡眠的代码。因此，它对于很快就需要一个临时页的简短代码，是非常理想的。<code>kmap_atomic</code>的定义在<code>IA-32</code>、<code>PPC</code>、<code>Sparc32</code>上是特定于体系结构的，但这3种实现只有非常细微的差别。其原型是相同的。</p><p><code>《深入理解Linux内核》3.5.8</code></p><p><a href="http://ilinuxkernel.com/?p=1013">Linux内核高端内存</a></p> <br><h4 id="固定映射-临时映射"><a href="#固定映射-临时映射" class="headerlink" title="固定映射/临时映射"></a>固定映射/临时映射</h4><p><code>FIXADDR_START</code> 到 <code>FIXADDR_TOP</code>(<code>0xFFFF F000</code>) 的空间，称为固定映射区域，主要用于满足特殊需求。</p><p>这块空间具有如下特点：<br>（1）每个 <code>CPU</code> 占用一块空间<br>（2）在每个 <code>CPU</code> 占用的那块空间中，又分为多个小空间，每个小空间大小是 <code>1</code> 个 <code>page</code>，每个小空间用于一个目的，这些目的定义在 <code>kmap_types.h</code> 中的 <code>km_type</code> 中。</p><p>当要进行一次固定映射的时候，需要指定映射的目的，根据映射目的，可以找到对应的小空间，然后把这个空间的地址作为映射地址。这意味着一次固定映射会导致以前的映射被覆盖。通过 <code>kmap_atomic()</code> 可实现固定映射。</p><p>固定映射是与物理地址空间中的固定页关联的虚拟地址空间项，但具体关联的页帧可以自由选择。它与通过固定公式与物理内存关联的直接映射页相反，虚拟固定映射地址与物理内存位置之间的关联可以自行定义，关联建立后内核总是会注意到的。</p><p>最后一个内存段由固定映射占据。这些地址指向物理内存中的随机位置。相对于内核空间起始处的线性映射，在该映射内部的虚拟地址和物理地址之间的关联不是预设的，而可以自由定义，但定义后不能改变。固定映射区域会一直延伸到虚拟地址空间顶端。</p><pre><code>include/asm-x86/fixmap_32.h #define __FIXADDR_TOP 0xfffff000 #define FIXADDR_TOP ((unsigned long)__FIXADDR_TOP) #define __FIXADDR_SIZE (__end_of_permanent_fixed_addresses &lt;&lt; PAGE_SHIFT) #define FIXADDR_START (FIXADDR_TOP -__FIXADDR_SIZE)</code></pre><p>固定映射地址的优点在于，在编译时对此类地址的处理类似于常数，内核一启动即为其分配了物理地址。此类地址的解引用比普通指针要快速。内核会确保在上下文切换期间，对应于固定映射的页表项不会从TLB刷出，因此在访问固定映射的内存时，总是通过TLB高速缓存取得对应的物理地址。</p><p>对每个固定映射地址都会创建一个常数，加入到<code>fixed_addresses</code>枚举值列表中。</p><pre><code>include/asm-x86/fixmap_32.h enum fixed_addresses {  FIX_HOLE,  FIX_VDSO,  FIX_DBGP_BASE,  FIX_EARLYCON_MEM_BASE, #ifdef CONFIG_X86_LOCAL_APIC  FIX_APIC_BASE, /* 本地CPU APIC信息，在SMP系统上需要 */ #endif ... #ifdef CONFIG_HIGHMEM  FIX_KMAP_BEGIN, /* 保留的页表项，用于临时内核映射 */  FIX_KMAP_END = FIX_KMAP_BEGIN+(KM_TYPE_NR*NR_CPUS)-1, #endif ...  FIX_WP_TEST,  __end_of_fixed_addresses };</code></pre><p>内核提供了<code>fix_to_virt</code>函数，用于计算固定映射常数的虚拟地址。</p><pre><code>include/asm-x86/fixmap_32.h static __always_inline unsigned long fix_to_virt(const unsigned int idx) {  if (idx &gt;= __end_of_fixed_addresses)  __this_fixmap_does_not_exist();  return __fix_to_virt(idx); }</code></pre><p>编译器优化机制会完全消除if语句，因为该函数定义为内联函数，而且其参数都是常数。这样的优化是有必要的，否则固定映射地址实际上并不优于普通指针。形式上的检查确保了所需的固定映射地址在有效区域中。<code>__end_of_fixed_adresses</code>是<code>fixed_addresses</code>的最后一个成员，定义了最大的可能数字。如果内核访问的是无效地址，则调用伪函数<code>__this_fixmap_does_not_exist</code>（没有定义）。在内核链接时，这会导致错误信息，表明由于存在未定义符号而无法生成映像文件。因此，此种内核故障在编译时即可检测，而不会在运行时出现。</p><p>在引用有效的固定映射地址时，if语句中的比较总是会通过。由于比较的两个操作数都是常数，该条件判断语句实际上不会执行，在编译优化的过程中会直接消除。</p><p><code>__fix_to_virt</code>定义为宏。由于<code>fix_to_virt</code>是内联函数，其实现代码会直接复制到查询固定映射地址的代码处。该宏定义如下：</p><pre><code>include/asm-x86/fixmap_32.h #define __fix_to_virt(x) (FIXADDR_TOP -((x) &lt;&lt; PAGE_SHIFT)) </code></pre><p>从顶部开始（不是按照常理从底部开始），内核回退n页，以确定第n个固定映射项的虚拟地址。这个计算同样也只使用了常数，编译器能够在编译时计算结果。根据上文提到的内存划分，地址空间中对应的虚拟地址尚未用于其他用途。固定映射虚拟地址与物理内存页之间的关联是由<code>set_fixmap(fixmap, page_nr)</code>和<code>set_fixmap_nocache</code>建立的（未讨论后者的实现）。这两个函数只是将页表中的对应项与物理内存中的一页关联起来。不同于<code>set_fixmap</code>，<code>set_fixmap_nocache</code>在必要情况下，会停用所涉及页帧的硬件高速缓存。</p><p>在最后一个区域可以通过 <code>kmap_atomic</code> 实现临时内核映射。假设用户态的进程要映射一个文件到内存中，先要映射用户态进程空间的一段虚拟地址到物理内存，然后将文件内容写入这个物理内存供用户态进程访问。给用户态进程分配物理内存页可以通过 <code>alloc_pages()</code>，分配完毕后，按说将用户态进程虚拟地址和物理内存的映射关系放在用户态进程的页表中，就完事大吉了。这个时候，用户态进程可以通过用户态的虚拟地址，也即 <code>0</code> 至 <code>3G</code> 的部分，经过页表映射后访问物理内存，并不需要内核态的虚拟地址里面也划出一块来，映射到这个物理内存页。但是如果要把文件内容写入物理内存，这件事情要内核来干了，这就只好通过 <code>kmap_atomic</code> 做一个临时映射，写入物理内存完毕后，再 <code>kunmap_atomic</code> 来解映射即可。</p><p><code>《深入理解Linux内核》3.5.8</code></p><p><a href="http://ilinuxkernel.com/?p=1013">Linux内核高端内存</a>、</p><p><a href="https://time.geekbang.org/column/article/95715">进程空间管理</a></p><br><h3 id="物理内存"><a href="#物理内存" class="headerlink" title="物理内存"></a>物理内存</h3><br><h4 id="0-1M"><a href="#0-1M" class="headerlink" title="0~1M"></a>0~1M</h4><p>前<code>4 KiB</code>是第一个页帧，一般会忽略，因为通常保留给<code>BIOS</code>使用。接下来的<code>640 KiB</code>原则上是可用的，但也不用于内核加载。其原因是，该区域之后紧邻的区域由系统保留，用于映射各种<code>ROM</code>（通常是<code>系统BIOS</code>和<code>显卡ROM</code>）。不可能向映射<code>ROM</code>的区域写入数据。但内核总是会装载到一个连续的内存区中，如果要从<code>4 KiB</code>处作为起始位置来装载内核映像，则要求内核必须小于<code>640 KiB</code>。</p><p><code>《深入理解Linux内核》3.4.2</code></p><br><h4 id="ZONE-DMA、ZONE-NORMAL、ZONE-HIGHMEM"><a href="#ZONE-DMA、ZONE-NORMAL、ZONE-HIGHMEM" class="headerlink" title="ZONE-DMA、ZONE_NORMAL、ZONE_HIGHMEM"></a>ZONE-DMA、ZONE_NORMAL、ZONE_HIGHMEM</h4><p>在x86架构中内存有三种区域：<code>ZONE_DMA</code>，<code>ZONE_NORMAL</code>，<code>ZONE_HIGHMEM</code>，不同类型的区域适合不同需要。在32位系统中结构中，<code>1G</code>(内核空间)/<code>3G</code>(用户空间) 地址空间划分时，三种类型的区域如下:</p><p><code>ZONE_DMA</code>              内存开始的<code>16MB</code></p><p><code>ZONE_NORMAL</code>       <code>16MB</code>~`896MB`</p><p><code>ZONE_HIGHMEM</code>     <code>896MB</code> ~ 结束</p><ul><li><p><code>ZONE-DMA</code> (<code>16M</code>)<br>  它是低内存的一块区域,这块区域由标准工业架构(<code>Industry Standard Architecture</code>)设备使用，适合<code>DMA</code>内存。这部分区域大小和CPU架构有关，在<code>x86架构</code>中，该部分区域大小 限制为<code>16MB</code>。</p><p>  该区域的物理页面专门供<code>I/O</code>设备的<code>DMA</code>使用。之所以需要单独管理DMA的物理页面，是因为DMA使用物理地址访问内存，不经过<code>MMU</code>，并且需要连续的缓冲区，所以为了能够提供物理上连续的缓冲区，必须从物理地址空间专门划分一段区域用于<code>DMA</code>。</p><p>  <code>DMA</code> 技术就是我们在主板上放一块独立的芯片。在进行内存和 <code>I/O </code>设备的数据传输的时候，我们不再通过 <code>CPU</code> 来控制数据传输，而直接通过 <code>DMA</code> 控制器（<code>DMA Controller</code>，简称 <code>DMAC</code>）。这块芯片，我们可以认为它其实就是一个协处理器（<code>Co-Processor</code>）。</p></li><li><p><code>ZONE_NORMAL</code>(<code>16~896M</code>)</p><p>  <code>ZONE_NORMAL</code>的范围是<code>16M~896M</code>，该区域的物理页面是内核能够直接使用的。属于直接映射区</p></li><li><p><code>ZONE_HIGHMEM</code>（<code>896M~结束</code>）</p><p>  是系统中剩下的可用内存,但因为内核的地址空间有限,这部分内存不直接映射到内核。</p></li></ul><p><a href="http://ilinuxkernel.com/?p=1013">Linux内核高端内存</a></p><br><h4 id="临时内存页表、内核镜像"><a href="#临时内存页表、内核镜像" class="headerlink" title="临时内存页表、内核镜像"></a>临时内存页表、内核镜像</h4><p>内核启动过程中，存在一个实模式保护模式的切换过程。在<code>Linux</code>启动的最初阶段，内核刚刚被装入内存时，分页功能还未启用，此时是直接存取物理地址的（或者说线性地址就等于物理地址）。但初始化完成后，内核也需要有自己的虚拟地址空间（<code>1个G大小</code>），该虚拟地址空间的地址映射关系，会被作为模版拷贝到其他进程的内核地址空间中。</p><p>临时内核页表只用来映射物理地址的前8M空间内容。目的是允许<code>CPU</code>在实模式（直接存取物理地址）和保护模式（根据虚拟地址映射）之间切换的过程中，都能对这前<code>8M</code>的地址进行访问。（假如内核使用的全部内存可以存放在<code>8M</code>的空间里，因为一个页表可以映射<code>4M</code>的地址，所以<code>8M</code>的空间需要两个页表，也就是需要两个页目录项。这两张页表我们称为临时内核页表<code>pg0</code>和<code>pg1</code>。（页表的作用，参见地址映射））</p><p><code>Linux Kernel</code> 有自己專屬的 <code>Page Directory</code> 及 <code>Page Table</code><br>在系統初始化時，會先建立 2個 <code>Page Table</code> – 包含 <code>2048</code>個 <code>Page</code>，共 <code>8MB</code> 的記憶體空間<br>這 <code>8MB</code> 是 <code>Linux</code> 開機最少需要的記憶體大小，而且保留給 <code>Kernel</code> 使用</p><p><code>Kernel Page Global Directory</code> 是以變數 <code>swapper_pg_dir</code> 表示其資料結構可視為含有 <code>1024</code> 個元素的 <code>pgd_t</code>型態的陣列實體記憶體位址為 <code>0x00101000</code></p><p><code>Kernel Page Table</code> (第 <code>0</code>及第 <code>1</code>個 Table) 是以變數 <code>pg0</code> 及 <code>pg1</code> 表示其資料結構可視為含有 <code>1024</code>個元素的 <code>pte_t</code> 型態的陣列實體記憶體位址分別為 <code>0x00102000</code> 及 <code>0x00103000</code></p><p><code>Linux Page</code> 初始化的動作定義於 <code>arch/i386/kernel/head.S</code>因為開機時僅需要 <code>8MB</code>，所以只要初始化 <code>2</code>個 <code>Page Table</code> 便可，即 <code>pg0</code> 及 <code>pg1</code>其餘的 <code>Page Table</code> 均填入 <code>0</code> 的值</p><p>又為了讓這 <code>2</code>個 <code>Page Table</code> 可以被 <code>Real Mode</code> 及 <code>Protect Mode</code> 所存取<code>Kernel Page Global Directory</code> 中的第 <code>0</code>、第 <code>768</code>個 Entry以及第 <code>1</code>、第 <code>769</code>個 <code>Entry</code>分別會設為相同的 <code>Page Table</code> 的實體記憶體位址，如圖所示</p><p>初始化完成後，得到以下的結果：</p><pre><code>swapper_pg_dir[0] = swapper_pg_dir[768] = 0x00102007swapper_pg_dir[1] = swapper_pg_dir[769] = 0x00103007</code></pre><p><code>pg0</code> 加上 <code>pg1</code> 定址到實體記憶體 <code>0x00000000</code> - <code>0x007FFFFF</code>，共 <code>8MB</code> 的分頁</p><p>根據 <code>pgd_t</code> 及 <code>pte_t</code> 的欄位格式可得知：</p><ol><li><code>bit</code> <code>0-11</code> 為 <code>0x007</code>，表示 <code>Enable</code> 旗號 <code>Present</code>、<code>Read/Write</code>、<code>User/Supervisor</code>（详见下面#页表描述符章节）</li><li><code>bit</code> <code>12-31</code> 為 <code>Base Address</code></li></ol><p><code>《深入理解Linux虚拟内存管理》</code>、</p><p><a href="https://blog.csdn.net/fullofwindandsnow/article/details/8565512">Linux内存管理总结-系统初始化</a></p><p><a href="http://parrotshen.blogspot.com/2008/01/test.html">Linux Memory Paging - 3</a></p><p><a href="https://www.cnblogs.com/4a8a08f09d37b73795649038408b5f33/p/10154324.html">linux启动过程中建立临时页表</a></p><p><a href="https://www.cnblogs.com/tolimit/p/4585803.html">linux内存源码分析 - 页表的初始化</a></p><br><h2 id="x86-64位虚拟地址空间"><a href="#x86-64位虚拟地址空间" class="headerlink" title="x86-64位虚拟地址空间"></a>x86-64位虚拟地址空间</h2><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2cdf1bedff166c2f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Linux-Memory-X86-64.jpg"></p><p><a href="https://raw.githubusercontent.com/fanlv/blog/main/backup/Content/Foundation/images/Linux-Memory-X86-64.jpg">点我查看原图</a></p><br><h3 id="ZONE-DMA、ZONE-DMA32、ZONE-NORMAL"><a href="#ZONE-DMA、ZONE-DMA32、ZONE-NORMAL" class="headerlink" title="ZONE_DMA、ZONE_DMA32、ZONE_NORMAL"></a>ZONE_DMA、ZONE_DMA32、ZONE_NORMAL</h3><ul><li><code>ZONE_DMA</code> 标记适合<code>DMA</code>的内存域。该区域的长度依赖于处理器类型。在<code>IA-32</code>计算机上，一般的限制是<code>16 MiB</code>，这是由古老的ISA设备强加的边界。但更现代的计算机也可能受这一限制的影响。</li><li><code>ZONE_DMA32</code> 标记了使用<code>32位</code>地址字可寻址、适合<code>DMA</code>的内存域。显然，只有在<code>64位</code>系统上，两种<code>DMA</code>内存域才有差别。在<code>32位</code>计算机上，本内存域是空的，即长度为<code>0 MiB</code>。在<code>Alpha</code>和<code>AMD64</code>系统上，该内存域的长度可能从<code>0</code>到<code>4GiB</code>。</li><li><code>ZONE_NORMAL</code>标记了可直接映射到内核段的普通内存域。这是在所有体系结构上保证都会存在的唯一内存域，但无法保证该地址范围对应了实际的物理内存。例如，如果<code>AMD64</code>系统有<code>2 GiB</code>内存，那么所有内存都属于<code>ZONE_DMA32</code>范围，而<code>ZONE_NORMAL</code>则为空。</li></ul><br><h2 id="其他基础知识"><a href="#其他基础知识" class="headerlink" title="其他基础知识"></a>其他基础知识</h2><br><h3 id="页表描述符（page-table-descriptor）"><a href="#页表描述符（page-table-descriptor）" class="headerlink" title="页表描述符（page table descriptor）"></a>页表描述符（page table descriptor）</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c58943107fd21df0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li>P（<code>Present</code>） - 为<code>1</code>表明该<code>page</code>存在于当前物理内存中，为<code>0</code>则<code>PTE</code>的其他部分都失去意义了，不用看了，直接触发<code>page fault</code>。<code>P</code>位为<code>0</code>的PTE也不会有对应的<code>TLB entry</code>，因为早在P位由<code>1</code>变为<code>0</code>的时候，对应的<code>TLB</code>就已经被<code>flush</code>掉了。</li><li>G （<code>Global</code>）- 这个标志位在这篇文章中有介绍，主要是用于<code>context switch</code>的时候不用<code>flush</code>掉<code>kernel</code>对应的<code>TLB</code>，所以这个标志位在<code>TLB entry</code>中也是存在的。</li><li>A（<code>Access</code>） - 当这个<code>page</code>被访问（读/写）过后，硬件将该位置<code>1</code>，<code>TLB</code>只会缓存<code>access</code>的值为<code>1</code>的<code>page</code>对应的映射关系。软件可将该位置<code>0</code>，然后对应的<code>TLB</code>将会被<code>flush</code>掉。这样，软件可以统计出每个<code>page</code>被访问的次数，作为内存不足时，判断该<code>page</code>是否应该被回收的参考。</li><li>D （<code>Dirty</code>）- 这个标志位只对<code>file backed</code>的<code>page</code>有意义，对<code>anonymous</code>的<code>page</code>是没有意义的。当<code>page</code>被写入后，硬件将该位置<code>1</code>，表明该<code>page</code>的内容比外部<code>disk/flash</code>对应部分要新，当系统内存不足，要将该<code>page</code>回收的时候，需首先将其内容<code>flush</code>到外部存储。之后软件将该标志位清0。</li><li>R/W（<code>Read/Write</code>） - 置为<code>1</code>表示该<code>page</code>是<code>writable</code>的，置为<code>0</code>则是<code>readonly</code>，对只读的<code>page</code>进行写操作会触发<code>page fault</code>。</li><li>U/S（<code>User/Supervisor</code>） - 置为<code>0</code>表示只有<code>supervisor</code>（比如操作系统中的<code>kernel</code>）才可访问该<code>page</code>，置为<code>1</code>表示<code>user</code>也可以访问。</li><li>PCD（<code>Page Cache Disabled</code>）- 置为<code>1</code>表示<code>disable</code>，即该<code>page</code>中的内容是不可以被<code>cache</code>的。如果置为<code>0</code>（<code>enable</code>），还要看<code>CR0寄存器</code>中的<code>CD位</code>这个总控开关是否也是<code>0</code>。</li><li>PWT （<code>Page Write Through</code>）- 置为<code>1</code>表示该<code>page</code>对应的<code>cache</code>部分采用<code>write through</code>的方式，否则采用<code>write back</code>。</li></ul><p><strong>64位页面描述符：</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e919d40d408e8c17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-204714b4708fda1d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><br><h3 id="system-map"><a href="#system-map" class="headerlink" title="system.map"></a>system.map</h3><p>每次编译内核时，都生成一个文件System.map并保存在源代码目录下。除了所有其他（全局）变量、内核定义的函数和例程的地址，该文件还包括图3-11给出的常数的值。</p><p><a href="https://raw.githubusercontent.com/fanlv/blog/main/backup/Content/Foundation/x86-debian-10.10.0-System.map-4.19.0-17-686-pae">x86-debian-10.10.0-System.map</a></p><p><a href="https://raw.githubusercontent.com/fanlv/blog/main/backup/Content/Foundation/x64-System.map-4.14.81.bm.15-amd64">x64-Debian-9-System.map</a></p><pre><code>fanlv@debian:~$ head -10 /boot/System.map-4.19.0-17-686-pae000001c9 A kexec_control_code_size01000000 A phys_startup_32c1000000 T _stextc1000000 T _textc1000000 T startup_32c100009b W xen_entryc10000a0 T start_cpu0c10000b0 T startup_32_smpc1000218 T verify_cpuc1000314 T pvh_start_xen</code></pre><p>符号类型：大写为全局符号，小写为局部符号<br>A：该符号的值是不能改变的，等于<code>const</code><br>B：该符号来自于未初始化代码段<code>bss段</code><br>C: 该符号是通用的，通用的符号指未初始化的数据。当链接时，多个通用符号可能对应一个名称，如果该符号在某一个位置定义，这个通用符号被当做未定义的引用。不明白，内核中也没有该类型的符号<br>D: 该符号位于初始化的数据段<br>G: 位于初始化数据段，专门对应小的数据对象，比如<code>global int x</code>,对应的大数据对象为 数组类型等<br>I： 到其他符号的间接引用，是对于<code>a.out</code>文件的<code>GNU</code>扩展，使用非常少<br>N：调试符号<br>R：只读代码段的符号<br>S：<code>BSS段</code>（未初始化数据段）的小对象符号<br>T：代码段符号，全局函数，<code>t</code>为局部函数<br>U：未定义的符号<br>V：该符号是一个<code>weak object</code>，当其连接到为定义的对象上上，该符号的值变为<code>0</code><br>W： 类似于<code>V</code><br>—： 该符号是<code>a.out</code>文件中的一个<code>stabs symbol</code>，获取调试信息<br>？： 未知类型的符号</p><p><a href="https://blog.csdn.net/chun_1959/article/details/45786769">system.map文件详解</a></p><br><h3 id="N-UMA-模型中的内存组织"><a href="#N-UMA-模型中的内存组织" class="headerlink" title="(N)UMA 模型中的内存组织"></a>(N)UMA 模型中的内存组织</h3><p>有两种类型计算机，分别以不同的方法管理物理内存</p><ol><li><code>UMA</code>计算机（一致内存访问，<code>uniform memory access</code>）将可用内存以连续方式组织起来（可能有小的缺口）。SMP系统中的每个处理器访问各个内存区都是同样快。<ul><li>SMP(<code>Symmetric Multi-Processor</code>)所谓对称多处理器结构，是指服务器中多个<code>CPU</code>对称工作，无主次或从属关系。各<code>CPU</code>共享相同的物理内存，每个<code>CPU</code>访问内存中的任何地址所需时间是相同的，因此<strong>SMP也被称为一致存储器访问结构(UMA：Uniform Memory Access)</strong></li></ul></li><li><code>NUMA</code>计算机（非一致内存访问，<code>non-uniform memory access</code>）总是多处理器计算机。系统的各个<code>CPU</code>都有本地内存，可支持特别快速的访问。各个处理器之间通过总线连接起来，以支持对其他<code>CPU</code>的本地内存的访问，当然比访问本地内存慢些。</li></ol><p>PS：还有个MPP(<code>Massive Parallel Processing</code>)，这里不做讨论。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7a5a953fd520ed72.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>现在我们用的OS基本都是NUMA内存模式</strong>，可以通过<code>lscpu</code> 查看<code>numa</code>相关信息</p><pre><code>dp-xxx(xxx@default:prod):message#lscpuArchitecture:          x86_64CPU op-mode(s):        32-bit, 64-bitByte Order:            Little EndianCPU(s):                92On-line CPU(s) list:   0,1Off-line CPU(s) list:  2-91Thread(s) per core:    0Core(s) per socket:    23Socket(s):             2NUMA node(s):          2Vendor ID:             GenuineIntelCPU family:            6Model:                 85Model name:            Intel(R) Xeon(R) Platinum 8260 CPU @ 2.40GHzStepping:              7CPU MHz:               2399.998BogoMIPS:              4799.99Virtualization:        VT-xHypervisor vendor:     KVMVirtualization type:   fullL1d cache:             32KL1i cache:             32KL2 cache:              4096KL3 cache:              16384KNUMA node0 CPU(s):     0-45NUMA node1 CPU(s):     46-91</code></pre><p>和 <code>numactl --hardware</code> 查看</p><pre><code>[root@n227-005-021 fanlv ]$ numactl --hardwareavailable: 1 nodes (0)node 0 cpus: 0 1 2 3 4 5 6 7node 0 size: 15787 MBnode 0 free: 2025 MBnode distances:node   0  0:  10 </code></pre><p><strong>NUMA特性禁用</strong></p><pre><code>一、检查OS是否开启NUMA#numactl --hardware二、Linux OS层面禁用NUMA1、修改 grub.conf#vi /boot/grub/grub.conf#/* Copyright 2010, Oracle. All rights reserved. */ default=0timeout=5hiddenmenuforeground=000000background=ffffffsplashimage=(hd0,0)/boot/grub/oracle.xpm.gz title Trying_C0D0_as_HD0root (hd0,0)kernel /boot/vmlinuz-2.6.18-128.1.16.0.1.el5 root=LABEL=DBSYS ro bootarea=dbsys rhgb quiet console=ttyS0,115200n8 console=tty1 crashkernel=128M@16M numa=offinitrd /boot/initrd-2.6.18-128.1.16.0.1.el5.img 2、重启Linux操作系统#/sbin/reboot 3、确认OS层面禁用NUMA是否成功#cat /proc/cmdlineroot=LABEL=DBSYS ro bootarea=dbsys rhgb quiet console=ttyS0,115200n8 console=tty1 crashkernel=128M@16M numa=off</code></pre><br><h3 id="三种内存模型"><a href="#三种内存模型" class="headerlink" title="三种内存模型"></a>三种内存模型</h3><br><h4 id="什么是内存模型？"><a href="#什么是内存模型？" class="headerlink" title="什么是内存模型？"></a>什么是内存模型？</h4><p>这里的内存模型，是指<code>Linux</code>内核用怎么样的方式去管理物理内存，一个物理内存页（<code>4k</code>），内核会用一个<a href="https://github.com/torvalds/linux/blob/master/include/linux/mm_types.h#L70">page（64Byte）</a>（类似物理页的<code>meta</code>）去记录<code>Physics Page Number</code>相关信息，下面几种内存模型是讲如何存储这个物理机的<code>meta</code>信息（<code>page</code>），保证内核能快速根据<code>PFN/PPN</code>找到<code>Page</code>，也可以根据<code>Page</code>快速算出<code>PFN</code>。</p><p><code>Linux</code>内存模型发展经历了三个模式，分别为<code>FLATMEM</code>、<code>DISCONTIGMEM</code>、<code>SPARSEMEM</code>。</p><p>PS：这里说下PFN和PPN是一个东西。具体可以看<a href="https://compas.cs.stonybrook.edu/~nhonarmand/courses/fa17/cse306/slides/06-paging.pdf">这个PPT第4页</a></p><br><h4 id="FLATMEM-flat-memory-model"><a href="#FLATMEM-flat-memory-model" class="headerlink" title="FLATMEM (flat memory model)"></a>FLATMEM (flat memory model)</h4><p><code>FLATMEM</code>内存模型是<code>Linux</code>最早使用的内存模型，那时计算机的内存通常不大。<code>Linux</code>会使用一个<code>struct page mem_map[x]</code>的数组根据PFN去依次存放所有的<code>strcut page</code>，且<code>mem_map</code>也位于内核空间的线性映射区，所以根据<code>PFN(页帧号)</code>即可轻松的找到目标页帧的<code>strcut page</code>。</p><pre><code>#define __pfn_to_page(pfn)    (mem_map + ((pfn) - ARCH_PFN_OFFSET))#define __page_to_pfn(page)    ((unsigned long)((page) - mem_map) + \                 ARCH_PFN_OFFSET)</code></pre><p>而对于FLATMEM来说，如果其管理的的物理内存本身是连续的还好说，如果不连续的话，那么中间一部分物理地址是没有对应的物理内存的，形象的说就像是一个个洞（<code>hole</code>），这会浪费<code>mem_map</code>数组本身占用的内存空间。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c8c7c21d83278669.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><br><h4 id="DISCONTIGMEM-discontiguous-memory-model"><a href="#DISCONTIGMEM-discontiguous-memory-model" class="headerlink" title="DISCONTIGMEM (discontiguous memory model)"></a>DISCONTIGMEM (discontiguous memory model)</h4><p>对于物理地址空间不存在空洞(<code>holes</code>)的计算机来说，<code>FLATMEM</code>无疑是最优解。可物理地址中若是存在空洞的话，<code>FLATMEM</code>就显得格外的浪费内存，因为<code>FLATMEM</code>会在<code>mem_map</code>数组中为所有的物理地址都创建一个<code>struct page</code>，即使大块的物理地址是空洞，即不存在物理内存。可是为这些空洞这些<code>struct page</code>完全是没有必要的。</p><p>那什么情况下物理内存是不连续的？那就要说到后来出现的<code>NUMA</code>。为了有效的管理NUMA模式下的物理内存，一种被称为不连续内存模型(<code>discontiguous memory model</code>)的实现于<code>1999年</code>被引入<code>Linux</code>系统。</p><p><strong>DISCONTIGMEM是个稍纵即逝的内存模型，在SPARSEMEM出现后即被完全替代</strong>，且当前的<code>Linux kernel</code>默认都是使用<code>SPARSEMEM</code>，所以介绍<code>DISCONTIGMEM</code>的意义不大。</p><pre><code>//PS. 这段源码在Linux最新的代码中已经找不到#define __pfn_to_page(pfn)            \({    unsigned long __pfn = (pfn);        \    unsigned long __nid = arch_pfn_to_nid(__pfn);  \    NODE_DATA(__nid)-&gt;node_mem_map + arch_local_page_offset(__pfn, __nid);\})#define __page_to_pfn(pg)                        \({    const struct page *__pg = (pg);                    \    struct pglist_data *__pgdat = NODE_DATA(page_to_nid(__pg));    \    (unsigned long)(__pg - __pgdat-&gt;node_mem_map) +            \     __pgdat-&gt;node_start_pfn;                    \})</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c5734ee77b6e12f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><br><h4 id="SPARSEMEM-sparse-memory-model"><a href="#SPARSEMEM-sparse-memory-model" class="headerlink" title="SPARSEMEM (sparse memory model)"></a>SPARSEMEM (sparse memory model)</h4><p>稀疏内存模型是当前内核默认的选择，从<code>2005年</code>被提出后沿用至今，但中间经过几次优化，包括：<code>CONFIG_SPARSEMEM_VMEMMAP</code>和<code>CONFIG_SPARSEMEM_EXTREME</code>的引入，这两个配置通常是被打开的。</p><p><strong><code>CONFIG_SPARSEMEM_VMEMMAP:</code></strong></p><pre><code>/* memmap is virtually contiguous.  */#define __pfn_to_page(pfn)    (vmemmap + (pfn))#define __page_to_pfn(page)    (unsigned long)((page) - vmemmap)// /linux/arch/x86/include/asm/pgtable_64.h#define vmemmap ((struct page *)VMEMMAP_START)  </code></pre><p>PS：引入 <code>vmemmap</code> 的核心思想就是用空间（虚拟地址空间）换时间，<strong>2008年以后，SPARSEMEM_VMEMMAP 成为 x86-64 唯一支持的内存模型</strong>，因为它只比<code>FLAGMEM</code>开销稍微大一点点，但是比<code>DISCONTIGMEM</code>要高效的多。详见<a href="https://lwn.net/Articles/789304/">这里</a></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-26470483d3f7872a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-953e1dc0f94b6573.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>关于这三种内存模型，网上很多文章讲的很混乱，推荐看这篇：<br><a href="https://chasinglulu.github.io/2019/05/29/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E3%80%8Cmemory-model%E3%80%8D/">内存模型「memory model」</a></p><p><a href="https://lwn.net/Articles/839737/">Reducing page structures for huge pages</a></p><br><h3 id="CPU寻址方式"><a href="#CPU寻址方式" class="headerlink" title="CPU寻址方式"></a>CPU寻址方式</h3><p>实模式和保护模式都是<code>CPU</code>的工作模式，而<code>CPU</code>的工作模式是指<code>CPU</code>的寻址方式、寄存器大小等用来反应<code>CPU</code>在该环境下如何工作的概念。</p><br><h4 id="实模式"><a href="#实模式" class="headerlink" title="实模式"></a>实模式</h4><p>实模式的“实”体现在程序中用到的地址都是<strong>真实的物理地址</strong>，“段基址:段内偏移地址”产生的逻辑地址就是物理地址，即程序员可见的地址完全是真实的内存地址。</p><p>在实模式下，内存寻址方式和<code>8086</code>相同，由<code>16位</code>段寄存器的内容乘以<code>16</code>（左移<code>4</code>位）作为段基址，加上<code>16</code>位段偏移地址形成<code>20</code>位的物理地址，最大寻址空间<code>1MB</code>，最大分段<code>64KB</code>。可以使用<code>32位</code>指令，即<code>32位</code>的<code>x86 CPU</code>也可以兼容实模式，此时的实模式相当于高速的<code>8086</code>（<strong>32位CPU的实模式可以使用32位下的资源</strong>）。在3<code>2位CPU</code>下，<strong>系统复位或加电时都是以实模式启动，然后再切换为保护模式</strong>。在实模式下，所有的段都是可以读、写和可执行的。</p><p><code>8086CPU</code>的实模式开创性地提出了地址分段的概念，改变了在它之前的<code>CPU</code>只能“硬编码”，程序无法重定位的缺点。然而实模式还是有很多缺陷，其中最主要的是实模式的安全隐患。在实模式下，用户程序和操作系统拥有同等权利，因为实模式下没有特权级。此外，程序可以随意修改自己的段基址，加上实模式下对地址的访问就是实实在在的物理地址，因此程序可以随意修改任意物理地址，甚至包括操作系统所在的内存，这给操作系统带来极大的安全问题。</p><br><h4 id="保护模式"><a href="#保护模式" class="headerlink" title="保护模式"></a>保护模式</h4><p>保护模式下，<code>CPU</code>访问的所有地址都是逻辑地址（段寄存器都为<code>0</code>的话，逻辑地址就是虚拟地址），<code>CPU</code>会通过”<code>分段</code>“或者“<code>分页</code>“方式来查寻到对应的物理地址。</p><p>在保护模式下，全部 <code>32</code> 条地址线有效，可寻址高达 <code>4 GB</code> 的物理地址空间。扩充的存储器<strong>段式管理机制</strong>和可选的<strong>页式管理机制</strong>，不仅为存储器共享和保护提供了硬件支持，而且为实现虚拟存储器提供了硬件支持，支持多任务，能够快速地进行任务切换和保护任务环境。四个特权级和完善的特权检查机制，既能实现资源共享又能保证代码和数据的安全及任务的隔离。</p><p><strong>总的来说，我们现在的系统CPU都是在保护模式下，并且用的是页管理机制（查页表方式）来访问内存。</strong></p><p><strong>CPU寻址相关的寄存器：</strong></p><p>控制寄存器（<code>CR0~CR3</code>）用于控制和确定处理器的操作模式以及当前执行任务的特性。4个控制寄存器都是32位的。</p><ul><li><code>CR0</code>：<strong>含有控制CPU操作模式和状态的标识</strong></li><li><code>CR1</code>：保留不用</li><li><code>CR2</code>：存储导致页错误的线性地址</li><li><code>CR3</code>：<strong>含有页目录表的物理内存基址</strong></li></ul><p><strong>CR0中的保护控制位：</strong></p><p><strong>PE：CR0的位0是启用保护（Protection Enable）标志。（CR0的最低位）</strong></p><p>当设置该位时即开启了保护模式，当复位时即进入实地址模式。这个标志仅开启段级保护，而没有启用分页机制。若要启用分页机制，那么<code>PE</code>和<code>PG</code>都要置位。</p><p><strong>PG：CR0的位31是分页（Paging）标志。（CR0的最高位）</strong></p><p>当设置该位时即开启了分页机制，当复位时则禁止分页机制，此时所有线性地址等同于物理地址。</p><p>注意，在开启这个标志之前必须已经开启PE标志，否则<code>CPU</code>会产生一个一般保护性异常。</p><p>改变<code>PG</code>位的代码必须在线性地址空间和物理地址空间中具有相同地址，这部分具有相同地址的代码在分页和未分页世界之间起着桥梁的作用。</p><p>如果<code>PE=0、PG=0</code>，处理器工作在实地址模式下。（兼容早期的实模式操作系统）</p><p>如果<code>PE=1、PG=0</code>，处理器工作在无分页机制的段保护模式下（兼容段式管理的操作系统）</p><p>如果<code>PE=1、PG=1</code>，处理器工作在段页式保护模式下</p><p>在系统刚上电时，处理器被复位成<code>PE=0</code>和<code>PG=0</code>（即实模式状态），以允许引导代码在启用分段和分页机制之前能够初始化这些寄存器和数据结构。</p><br><h4 id="PAE-32位系统如何突破4G限制？"><a href="#PAE-32位系统如何突破4G限制？" class="headerlink" title="PAE - 32位系统如何突破4G限制？"></a>PAE - 32位系统如何突破4G限制？</h4><p>在<code>x86 CPU</code>中，只有<code>32位</code>地址总线，也就意味着只有<code>4G</code>地址空间。为了实现在<code>32位</code>系统中使用更多内存，<code>Intel CPU</code> 提供了 <code>PAE</code> (<code>Pyhsical Address Extensions</code>)机制，这样可以让操作系统使用超过<code>4G</code>的物理内存。</p><p><code>PAE</code>机制的打开，需要设置<code>CR0</code>、<code>CR4</code>控制寄存器和<code>IA32_EFER</code> <code>MSR</code> 寄存器，设置值<code>CR0.PG=1</code>，<code>CR4.PAE=1</code> 和 <code>IA32_EFER.LME=0</code>。 但<code>PAE</code>机制打开后，<code>MMU</code>会将<code>32位</code>线性地址转换为<code>52位</code>物理地址，尽快物理地址是<code>52位</code>（4PB），但线性地址仍然为<code>32位</code>，即进程可使用的物理内存不超过<code>4GB</code>。</p><br><h4 id="Linux-如何从实模式切换到保护模式"><a href="#Linux-如何从实模式切换到保护模式" class="headerlink" title="Linux 如何从实模式切换到保护模式"></a>Linux 如何从实模式切换到保护模式</h4><p>在使用启动装载器（如<code>LILO</code>、<code>GRUB</code>等）将内核载入物理内存之后，将通过跳转语句，将控制流切换到内存中适当的位置，来调用<code>arch/x86/boot/header.S</code>中的汇编语言“函数”<code>setup</code>。这是可能的，因为<code>setup</code>函数总是位于目标文件中的同<br>一位置。</p><p>该代码执行下列任务，这需要许多汇编代码。</p><ol><li>它检查内核是否加载到内存中正确的位置。为此，它使用一个4字节的特征标记，该标记在编<br>译时集成到内核映像中，并且总是位于物理内存中一个不变的正确位置。</li><li>它确定系统内存的大小。</li><li>初始化显卡。</li><li>将内核映像移动到内存中的某个位置，使得在后续的解压缩期间，映像不会自阻其路。</li><li><strong>将CPU切换到保护模式</strong>。</li></ol><p>具体实现，内核会调用 <a href="https://github.com/torvalds/linux/blob/5bfc75d92efd494db37f5c4c173d3639d4772966/arch/x86/boot/pmjump.S#L24"><code>protected_mode_jump</code></a> 把CPU从实模式切换到保护模式。核心代码如下</p><pre><code>movl    %cr0, %edxorb    $X86_CR0_PE, %dl    #Protected modemovl    %edx, %cr0</code></pre><br><h3 id="分页模式"><a href="#分页模式" class="headerlink" title="分页模式"></a>分页模式</h3><br><h4 id="内核视角"><a href="#内核视角" class="headerlink" title="内核视角"></a>内核视角</h4><p>由上面我们知道，<code>CPU</code>读取数据过程是先去<code>CR3</code>寄存器拿到<code>pdg</code>，然后然后查一级一级页表，最终拿到物理地址的高<code>40位</code>基地址。<code>CR3寄存器</code>和每个程序的页表内容都是由<code>Linux</code>内核来维护的。<br><br></p><h4 id="应用程序页表"><a href="#应用程序页表" class="headerlink" title="应用程序页表"></a>应用程序页表</h4><p><code>Linux</code>内核通过一个被称为进程描述符的 <a href="https://github.com/torvalds/linux/blob/master/include/linux/sched.h#L661">task_struct</a> 结构体来管理进程，这个结构体包含了一个进程所需的所有信息。程序内存相关的都存在 <a href="https://github.com/torvalds/linux/blob/master/include/linux/mm_types.h#L394">mm_struct</a> 中 <code>mm_struct</code> 有一个<code>pgd_t * pgd;</code>就是最顶级的目录的地址。内核在做程序切换的时候会调用 <code>pick_next_task</code> -&gt; <code>context_switch</code> -&gt; <a href="https://github.com/torvalds/linux/blob/9269d27e519ae9a89be8d288f59d1ec573b0c686/arch/x86/mm/tlb.c#L428"><code>switch_mm_irqs_off</code></a> -&gt; <a href="https://github.com/torvalds/linux/blob/9269d27e519ae9a89be8d288f59d1ec573b0c686/arch/x86/mm/tlb.c#L269"><code>load_new_mm_cr3</code></a> -&gt; </p><p>这里需要注意两点。第一点，<code>cr3</code> 里面存放当前进程的顶级 <code>pgd</code>，这个是硬件的要求。<code>cr3</code> 里面需要存放 <code>pgd</code> 在物理内存的地址，不能是虚拟地址。因而 <code>load_new_mm_cr3</code> 里面会使用 <code>__pa</code>，将 <code>mm_struct</code> 里面的成员变量 <code>pgd</code>（<code>mm_struct</code> 里面存的都是虚拟地址）变为物理地址，才能加载到 <code>cr3</code> 里面去。</p><p>第二点，用户进程在运行的过程中，访问虚拟内存中的数据，会被 <code>cr3</code> 里面指向的页表转换为物理地址后，才在物理内存中访问数据，这个过程都是在用户态运行的，地址转换的过程无需进入内核态。</p><pre><code>static void load_new_mm_cr3(pgd_t *pgdir, u16 new_asid, bool need_flush){    unsigned long new_mm_cr3;    if (need_flush) {        invalidate_user_asid(new_asid);        new_mm_cr3 = build_cr3(pgdir, new_asid);    } else {        new_mm_cr3 = build_cr3_noflush(pgdir, new_asid);    }    /*     * Caution: many callers of this function expect     * that load_cr3() is serializing and orders TLB     * fills with respect to the mm_cpumask writes.     */    write_cr3(new_mm_cr3);}</code></pre><p><code>write_cr3 </code>相对就比较简单，一个<code>mov</code>汇编指令来设置<code>cr3</code>寄存器的值</p><pre><code>static inline void write_cr3(unsigned long x){    PVOP_ALT_VCALL1(mmu.write_cr3, x,            "mov %%rdi, %%cr3", ALT_NOT(X86_FEATURE_XENPV));}</code></pre><br><h4 id="内核页表"><a href="#内核页表" class="headerlink" title="内核页表"></a>内核页表</h4><p>和用户态页表不同，在系统初始化的时候，我们就要创建内核页表了。<a href="https://github.com/torvalds/linux/blob/master/arch/x86/include/asm/pgtable_64.h#L19">内核页表定义如下</a>：</p><pre><code>extern p4d_t level4_kernel_pgt[512];extern p4d_t level4_ident_pgt[512];extern pud_t level3_kernel_pgt[512];extern pud_t level3_ident_pgt[512];extern pmd_t level2_kernel_pgt[512];extern pmd_t level2_fixmap_pgt[512];extern pmd_t level2_ident_pgt[512];extern pte_t level1_fixmap_pgt[512 * FIXMAP_PMD_NUM];extern pgd_t init_top_pgt[];    struct mm_struct init_mm = {  .mm_rb    = RB_ROOT,  .pgd    = swapper_pg_dir,  .mm_users  = ATOMIC_INIT(2),  .mm_count  = ATOMIC_INIT(1),  .mmap_sem  = __RWSEM_INITIALIZER(init_mm.mmap_sem),  .page_table_lock =  __SPIN_LOCK_UNLOCKED(init_mm.page_table_lock),  .mmlist    = LIST_HEAD_INIT(init_mm.mmlist),  .user_ns  = &amp;init_user_ns,  INIT_MM_CONTEXT(init_mm)};</code></pre><p>定义完了内核页表，接下来是初始化内核页表，在系统启动的时候 <code>start_kernel</code> 会调用 <code>setup_arch</code>。</p><pre><code>void __init setup_arch(char **cmdline_p){#ifdef CONFIG_X86_32    memcpy(&amp;boot_cpu_data, &amp;new_cpu_data, sizeof(new_cpu_data));    /*     * copy kernel address range established so far and switch     * to the proper swapper page table     */    clone_pgd_range(swapper_pg_dir     + KERNEL_PGD_BOUNDARY,            initial_page_table + KERNEL_PGD_BOUNDARY,            KERNEL_PGD_PTRS);    load_cr3(swapper_pg_dir);    .......}static inline void load_cr3(pgd_t *pgdir){    write_cr3(__sme_pa(pgdir));}</code></pre><p>内核的页表初始化是在 <code>arch\x86\kernel\head_64.S</code> 中。这段代码比较难看懂，占不去深究。大概知道内核是怎么维护页表的就好。</p><br><h4 id="硬件视角"><a href="#硬件视角" class="headerlink" title="硬件视角"></a>硬件视角</h4><br><h5 id="CPU翻译虚拟地址到物理地址过程"><a href="#CPU翻译虚拟地址到物理地址过程" class="headerlink" title="CPU翻译虚拟地址到物理地址过程"></a>CPU翻译虚拟地址到物理地址过程</h5><p><img src="https://upload-images.jianshu.io/upload_images/12321605-16b2286da60c1b08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><br><h5 id="CPU读取数据过程"><a href="#CPU读取数据过程" class="headerlink" title="CPU读取数据过程"></a>CPU读取数据过程</h5><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ff93072e983e0e68.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-877f131a5206033e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><br><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><strong>《深入理解Linux虚拟内存管理》</strong></p><p><strong>《深入理解操作系统》</strong></p><p><strong>《深入理解Linux内核》</strong></p><p><a href="http://ilinuxkernel.com/?p=1013">http://ilinuxkernel.com/?p=1013</a></p><p><a href="https://jekton.github.io/2018/11/18/linux-page-table-setup/">https://jekton.github.io/2018/11/18/linux-page-table-setup/</a></p><p><a href="https://blog.csdn.net/fullofwindandsnow/article/details/8565512">https://blog.csdn.net/fullofwindandsnow/article/details/8565512</a></p><p><a href="https://www.cnblogs.com/tolimit/p/4585803.html">https://www.cnblogs.com/tolimit/p/4585803.html</a></p><p><a href="https://www.cnblogs.com/4a8a08f09d37b73795649038408b5f33/p/10154324.html">https://www.cnblogs.com/4a8a08f09d37b73795649038408b5f33/p/10154324.html</a></p><p><a href="http://parrotshen.blogspot.com/2008/01/test.html">http://parrotshen.blogspot.com/2008/01/test.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/67053210">https://zhuanlan.zhihu.com/p/67053210</a></p><p><a href="https://zhuanlan.zhihu.com/p/220068494">https://zhuanlan.zhihu.com/p/220068494</a></p><p><a href="https://toutiao.io/posts/6r7pjh/preview">https://toutiao.io/posts/6r7pjh/preview</a></p><p><a href="https://www.cnblogs.com/chenwb89/p/operating_system_002.html">https://www.cnblogs.com/chenwb89/p/operating_system_002.html</a></p><p><a href="http://www.wowotech.net/memory_management/memory_model.html">http://www.wowotech.net/memory_management/memory_model.html</a></p><p><a href="https://chasinglulu.github.io/2019/05/29/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E3%80%8Cmemory-model%E3%80%8D/">https://chasinglulu.github.io/2019/05/29/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E3%80%8Cmemory-model%E3%80%8D/</a></p><p><a href="https://elinux.org/images/b/b0/Introduction_to_Memory_Management_in_Linux.pdf">https://elinux.org/images/b/b0/Introduction_to_Memory_Management_in_Linux.pdf</a></p><p><a href="https://pdfs.semanticscholar.org/5b92/9e20c9203232ac8aefbab9d905499f4bde25.pdf">https://pdfs.semanticscholar.org/5b92/9e20c9203232ac8aefbab9d905499f4bde25.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Memory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Envoy 编译调试</title>
      <link href="/2021/07/16/envoy-debug/"/>
      <url>/2021/07/16/envoy-debug/</url>
      
        <content type="html"><![CDATA[<h2 id="Debian9-上编译调试"><a href="#Debian9-上编译调试" class="headerlink" title="Debian9 上编译调试"></a>Debian9 上编译调试</h2><p>主要参考Envoy官方的<a href="https://github.com/envoyproxy/envoy/tree/main/bazel">Bazel编译文档</a></p><ol><li><p>下载bazelisk-linux-amd64</p><pre><code> sudo wget -O /usr/local/bin/bazel https://github.com/bazelbuild/bazelisk/releases/latest/download/bazelisk-linux-amd64 sudo chmod +x /usr/local/bin/bazel</code></pre></li><li><p>安装依赖</p><pre><code> sudo apt-get install \    autoconf \    automake \    cmake \    curl \    libtool \    make \    ninja-build \    patch \    python3-pip \    unzip \    virtualenv</code></pre></li><li><p>下载llvm编译器</p><pre><code> md llvm // 新建目录 wget https://github.com/llvm/llvm-project/releases/download/llvmorg-12.0.0/clang+llvm-12.0.0-x86_64-linux-gnu-ubuntu-16.04.tar.xz  tar xf clang+llvm-12.0.0-x86_64-linux-gnu-ubuntu-16.04.tar.xz  mv clang+llvm-12.0.0-x86_64-linux-gnu-ubuntu-16.04 src // 修改名字 #保存到环境变量 .zshrc （不保存也可以，下面脚本会写的user.bazelrc中去） export PATH=$PATH:/home/fanlv/llvm/src/bin export PATH=$PATH:/home/fanlv/llvm/src/include export PATH=$PATH:/home/fanlv/llvm/src/lib export PATH=$PATH:/home/fanlv/llvm/src/libexec export PATH=$PATH:/home/fanlv/llvm/src/share</code></pre></li><li><p><code>Debian9.x </code>默认是<code>python3.5</code>，build过程需要用到<code>Jinja2 - 3.0.1</code> 必须要<code>python 3.6</code>以上版本，如果是3.6以上可以忽略这一步。</p><pre><code> apt-get autoremove python3.5 python3.5-dev sudo apt-get install dirmngr sudo gcc vim /etc/apt/sources.list deb http://mirrors.163.com/ubuntu/ bionic main sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 3B4FE6ACC0B21F32 echo 'APT::Default-Release "stable";' | sudo tee -a /etc/apt/apt.conf.d/00local sudo apt-get update sudo apt-get -t bionic install python3.6 python3.6-dev python3-distutils python3-pip ln -s /usr/bin/python3.6 /usr/bin/python3</code></pre></li><li><p>clone envoy源码</p><pre><code> //当前commit b62dae29a5dd06b7f689899b26974d9567a98f0e git clone git@github.com:envoyproxy/envoy.git</code></pre></li><li><p>配置Bazel使用llvm编译器</p><pre><code> cd envoy bazel/setup_clang.sh /home/fanlv/llvm/src // 这个是llvm 文件位置 echo "build --config=libc++" &gt;&gt; user.bazelrc #--config=libc++ means using clang + libc++ #--config=clang means using clang + libstdc++ #no config flag means using gcc + libstdc++</code></pre></li><li><p>开始编译</p><pre><code> cd envoy bazel build -c dbg --verbose_failures --verbose_explanations  --config=libc++ //source/exe:envoy-static</code></pre></li></ol><ol start="8"><li><p>动态库找不到错误报错</p><pre><code> python3 ../../tools/run.py ./bytecode_builtins_list_generator gen/builtins-generated/bytecodes-builtins-list.h ./bytecode_builtins_list_generator: error while loading shared libraries: libc++abi.so.1: cannot open shared object file: No such file or directory</code></pre><p> 解决方式，更多参考<a href="https://github.com/envoyproxy/envoy/pull/9024">这里</a></p><pre><code> ln -s /data00/home/fanlv/llvm/src/lib/* /usr/lib/</code></pre></li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3ad5af16e2f5f0ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="调试效果"></p><h2 id="Mac-上编译调试"><a href="#Mac-上编译调试" class="headerlink" title="Mac 上编译调试"></a>Mac 上编译调试</h2><ol><li><p>bazel安装同上一</p></li><li><p>安装依赖</p><pre><code> brew install coreutils wget cmake libtool automake ninja clang-format autoconf aspell </code></pre></li><li><p>编译</p><pre><code> bazel build --explain=file.txt --verbose_explanations --verbose_failures //source/exe:envoy-static // 带符号表的 bazel build -c dbg //source/exe:envoy-static --copt=-Wno-inconsistent-missing-override --spawn_strategy=standalone --genrule_strategy=standalone </code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> Envoy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《说透芯片》</title>
      <link href="/2021/07/05/sya-chip/"/>
      <url>/2021/07/05/sya-chip/</url>
      
        <content type="html"><![CDATA[<h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="晶体管"><a href="#晶体管" class="headerlink" title="晶体管"></a>晶体管</h3><p>想了解晶体管，你得先了解它的“前身”——电子管。电子管是中文翻译后的名称，英文原文其实是真空管。从这个名字，你可以想象，它其实是把参与工作的金属薄片，也就是电极，封装在一个真空的容器内，真空容器一般指的是玻璃瓶。</p><p>整个电子行业，并不是从芯片开始的，而是从电子管开始。电子管最鼎盛时期的代表作，就是世界上的第一台电子计算机。这台 1946 年诞生的电子计算机，占地 150 平方米，重达 30 吨，里面的电路使用了 17468 只电子管、7200 只电阻、10000 只电容、50 万条线。</p><p>这台计算机虽然运算速度不快，但基本具备了现代计算机的主要结构和功能，这也是电子管能达到的最高成就了。其实从上图你也可以看出来，电子管最大的缺点就是，真空容器对于电子产品来说体积太大了。</p><p>如果人类停留在电子管技术上，所用的电子设备，就会因为需要多个真空电子管而变得体积庞大，成本昂贵，还需要轻拿轻放。</p><p>这个时候，科学家们开始积极寻找可以取代电子管的固体元器件材料：一种合适的半导体。</p><p>什么是半导体呢？官方说法，半导体是指常温下导电性能介于导体与绝缘体之间的一类材料。简单地说，导体导电，绝缘体不导电，而半导体，在不同电流控制下可以表现出不同的导电，或者不导电的特性，这个特征和真空电子管做电信号放大器的特性吻合，因此半导体可以被用来做固体电子元器件的材料。</p><p>科学家们对半导体材料的研究结果就是，<strong>半导体晶体管复刻了真空电子管的功能，可以全面地取而代之</strong>。使用半导体材料制成的晶体管，最大的优势就是可以不断缩小尺寸， 这为电子设备的微型化提供了可能。</p><p><strong>更小的体积、更快的速度、更可靠的稳定性，让半导体做的晶体管取代电子管成为了整个电子行业的基本元器件</strong>。这也是晶体管被称为是二十世纪最重要发明的原因。发明者肖克利、巴丁、布拉顿三人因此获得了 1956 年的诺贝尔物理学奖。</p><h3 id="集成电路"><a href="#集成电路" class="headerlink" title="集成电路"></a>集成电路</h3><p>有了晶体管，集成电路也就成为可能。</p><p>把多个晶体管和其它的电子元器件小型化，微型化集成在一起，以减少电器的大小，这个思路就是集成电路。关于两者的关系，你可以理解为，集成电路就是由大量晶体管搭建的。严谨一点说，集成电路的最小单元是逻辑门，逻辑门是由晶体管搭建而成。可以说，半导体行业，就是拿晶体管去堆集成电路的行业。</p><p>现代的集成电路是由德州仪器的工程师杰克·基尔比在 1958 年发明的，当时发明的是锗集成电路，他本人也因此荣获 2000 年诺贝尔物理学奖。这里我还想提一个人，Intel 的第一任 CEO 罗伯特·诺伊斯，他后来发明了现在应用更广的硅集成电路，让集成电路真正进入了商用时代。可惜诺伊斯在 1990 年早逝，没有领到诺贝尔奖。</p><p>在点开晶体管、集成电路这个全新的科技树之后，半导体行业，就走上了缩小晶体管体积 -&gt; 扩大集成电路规模 -&gt; 构建性能更强价格更优的电子设备 -&gt; 再次缩小晶体管体积 -&gt; 构建更大规模的集成电路支持更多功能 -&gt; 构建新一代性能更强价格更优的电子设备的高速路。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-701e6e3ee3fe2733.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>对于集成电路的发展你可以看下图，在最初的 20 年，集成电路的规模迅速扩大，单个集成电路可以集成的晶体管数目从 1 个发展到上百万个，增长了 10 万倍，而且增长势头不减，超大规模集成电路、特大规模集成电路、巨大规模集成电路… … 形容词都不够用了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-99a392590edac582.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>后来，业界索性放弃用集成电路的规模来定义行业发展阶段，而改用晶体管的特征尺寸来标识。你现在听到手机芯片的 28nm、20nm、14nm、10nm、7nm、5nm，这些数字都是晶圆工厂的制造工艺的名称，虽然并不直接代表晶体管的尺寸，但也是有所关联的。工艺制程的数字越小，意味着晶体管体积越小，这样单位面积可以集成的晶体管数目就越多，也就是所谓的晶体管密度高。</p><p>下图是用制造工艺名称来标识的半导体行业发展路线图。横坐标是时间轴，纵坐标是工艺制程。你可以看到从 1987 年到 2019 年，制造工艺从 3 微米发展到了 5 纳米。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-329a70703ebed0de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>你可能要问，这个制造工艺代表什么呢？我给你举个例子，对于 iPhone12 里用的应用处理器 A14，我们一般都说是 5nm 的工艺。行内的人会留意到这是一个用台积电 N5 工艺生产的，面积为 88mm² 的芯片。N5 工艺指的就是台积电 5nm 工艺。</p><p>苹果披露 A14 应用处理器是一颗集成了 118 亿个晶体管的芯片。如果是关心制造工艺的人，就会算一下，台积电 N5 工艺，官方的晶体管密度是 173 MTr/mm2，就是每平方毫米的面积上可以集成 1.73 亿个晶体管。那么苹果在 88 平方毫米的面积上集成了 118 亿个晶体管，是相当不错的数字了，算是非常高效地利用了最先进工艺带来的高密度。</p><p>当然，评价一颗芯片，不能这么简单地只看晶体管密度，至少还要看 PPA 衡量标准，也就是 Power 功耗、 Performance 性能、Area 面积，这是后话，在接下来的课程中我会再讲到。</p><h3 id="摩尔定律"><a href="#摩尔定律" class="headerlink" title="摩尔定律"></a>摩尔定律</h3><p>看到上面那张图的时候，或者你听到我说 28nm、20nm、14nm、10nm、7nm、5nm 这组数字的时候，你心中或许有疑问，为什么选这些数字？有什么规律么？你问到重点了。基本上这是个相邻两个数字差 0.7 倍的数字序列，你看，10nmx0.7=7nm，7nmx0.7≈5nm，都是这样的规律。如果把晶体管的特征尺寸理解成正方形的边长，边长缩小 0.7 倍，0.7x0.7=0.49，那么一个正方形的面积就相当于小了一半。</p><p>这种相差 0.7 倍的数字序列，想表达的意思就是，在 5nm 制造工艺下，晶体管的体积应该是前一代 7nm 工艺的一半，换另一个数据来说就是晶体管密度可以高一倍。前面说台积电 N5 工艺的密度是 173MTr/mm2，N7 是 96.5MTr/mm2。如果再加上时间轴的描述，你会发现台积电在 2018 年开始量产 N7 工艺的芯片，2020 开始量产 N5 工艺的芯片，两年工艺一更新。到这里，恭喜你，你已经自行发现了半导体行业的黄金定律：摩尔定律。</p><p>摩尔定律是由英特尔公司联合创始人戈登·摩尔提出的概念，定律本身很简单：<strong>半导体芯片上集成的晶体管和电阻数量将每隔 24 月增加一倍</strong>。 从上面的推导过程，你可以知道，是晶体管的特征尺寸每代缩小 0.7 倍，因此单位面积可以集成的晶体管密度可以提高一倍。你可以在行业里的每一条产品线，每一个公司的发展历史，每一次技术革新背后看到摩尔定律的影子。</p><p>再看看下面这张业界著名的摩尔定律图，这张图里横坐标是时间轴，纵坐标是一个实际的芯片产品所集成的晶体管数目，这意味着不仅仅技术上可以制造出密度翻倍的芯片，而且半导体公司能充分使用多出的晶体管，来设计出性能更高功能更强的芯片，并实现其经济价值。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-18c8f1dd2fb0be6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>某种程度上，<strong>你可以用摩尔定律当尺子丈量半导体行业内的技术进步、产品迭代、公司发展。跟上摩尔定律的，就可以打一个“good”的标签</strong>，落后于摩尔定律的，可以给一个“？”，超越摩尔定律的，可以打一个“行业赢家”或者“very good”的标签。</p><p>例如台积电一年一次工艺提升的稳定输出，苹果每年一代新的芯片，这些就是“good”的公司。</p><p>而以英伟达 Nvidia 为首的 AI 公司，提供的算力以 2 年 10 倍的超摩尔速度增长，可以说“very good”。还有一些公司，开始说“摩尔定律在变慢”，因为它们 5 年才出一代新工艺，属于自己掉队，然后还想通过改规则，掩耳盗铃。嘻嘻，我就不提名字了。</p><p>上面我也说了，摩尔定律它不是一个自然规律，而是人类创造力的定律，因此如果我们对自己有信心的话，给摩尔定律续命的方式多得是。例如，在设计和制造两个环节已经被充分挖掘了之后，一直被认为是技术门槛较低的封装环节，也开始技术创新加速。2.5D、3D 等异构封装技术遍地开花。</p><h3 id="从一部iPhone手机看芯片的分类"><a href="#从一部iPhone手机看芯片的分类" class="headerlink" title="从一部iPhone手机看芯片的分类"></a>从一部iPhone手机看芯片的分类</h3><p>首先，我可以负责任地告诉你，<strong>芯片肯定不全是集成电路</strong>。芯片里面，大约只有 80% 属于集成电路，其余的都是光电器件、传感器和分立器件，行业内把这些器件称为 O-S-D（Optoelectronic, Sensor, Discrete）。</p><p>为了方便你理解，我按照半导体行业的专业分类方式，做了下面这张行业分类图。图里的数据都来自半导体行业权威的市场研究机构 IC Insights 的最新市场统计。能把这张图和别人讲清楚，我觉得你至少是一个半导体行业初级市场研究人员的水平了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e47884b5628f2ce0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>有这张图还不够，我另外整理了一张 iPhone X 物料表，在正中间那一列，我标明了每个物料所属的半导体种类。而且这张表的顺序，跟前面的分类图，基本上可以一一对应起来。考虑到公开信息的准确性，这张表我参考了知名市场研究机构 IHS Markit 的 iPhone X 拆解报告。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5807dee66dec0ee7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>对照完两张图，从宏观看，有两个信息你需要关注：</p><ul><li>第一，一部手机 80% 都是集成电路。所以，苹果是半导体产品全球排名第一的买家，这个就不难理解了吧？</li><li>第二，一部 iPhone 几乎把前面分类图中的重要品类都用到了。你看，手机一点都不简单吧？</li></ul><p>从技术上，再看一下 A11 这颗处理器的细节，它集成了 6 颗 ARMv8 的 CPU 核，2 大 4 小；3 颗 GPU 核，一个神经网络处理器 NPU 用来加速人工智能算法，一个（照相机）图像信号处理器 ISP。**这是一颗高度集成的 SoC (系统级芯片，System-on-Chip)**。</p><p>高度集成也是手机芯片的特点，像在 PC 或者服务器上，CPU、GPU、NPU 往往是三颗独立的芯片。对于 iPhone 手机来说，整个 iOS 系统都是跑在应用处理器上的，可以说手机中最重要的一颗芯片就是应用处理器了，系统是否顺滑，游戏是否顺畅，全看应用处理器的芯片。这也是苹果、华为、三星都要自研应用处理器的原因。</p><p>在 A11 下面其实还压着一个存储芯片 DRAM（动态随机存取存储器，Dynamic Random Access Memory）。存储芯片，顾名思义，就是存储数据的芯片，也叫存储器。手机中的全部信息，包括输入的原始数据、应用程序、中间运行结果和最终运行结果都保存在存储器中。除此之外，iPhone X 还有一块重要的存储芯片，就是在下图中最大的一块芯片，东芝的 NAND Flash。</p><p>DRAM 和 NAND Flash 的区别很好理解，我类比下。在 PC 机上，我们俗称的内存条，其实就是 DRAM，固态硬盘（SSD）就是 NAND Flash。</p><h3 id="数字-IC"><a href="#数字-IC" class="headerlink" title="数字 IC"></a>数字 IC</h3><p>集成电路的英文是 Integrated Circuit，数字 IC 就是数字集成电路。回到专业视角，如果从用途上分类，数字集成电路可以简洁地分为，做计算控制的逻辑芯片和保存数据的存储芯片。不过业界习惯，把标准程度非常高的 CPU、GPU、MCU 合并为 MPU 微处理器来单独统计，把应用相关度高的 ASIC（下文会解释）和 SoC 算作逻辑芯片。</p><h3 id="CPU：计算设备的运算核心和控制核心"><a href="#CPU：计算设备的运算核心和控制核心" class="headerlink" title="CPU：计算设备的运算核心和控制核心"></a>CPU：计算设备的运算核心和控制核心</h3><p>CPU（Central Processing Unit）是计算设备的运算核心和控制核心。它的功能主要是解释计算机指令以及处理计算机软件中的数据。CPU 的标准性很高，是最能体现摩尔定律的产品。</p><p>苹果的手机应用处理器，永远使用的是最顶级的工艺。iPhone X 的 A11，在 2017 年上市的时候，就用了当时最先进的 10nm 工艺。到了 2020 年，苹果的应用处理器都已经用上 5nm 的工艺制程，桌面和服务器端才跟进到了 7nm。</p><h3 id="GPU：图形处理器"><a href="#GPU：图形处理器" class="headerlink" title="GPU：图形处理器"></a>GPU：图形处理器</h3><p>GPU（Graphics Processing Unit）也叫图形处理器，主要用来满足图像计算要求。相对来说 CPU 擅长逻辑判断和串行数据运算，而一个图片的每一个像素都需要相同的计算处理，GPU 就擅长图形计算这种并行的任务。因为 GPU 这种并行度高的特征，在品类上还衍生出弱化图像能力，专注于计算的通用 GPU。一般来说，通用 GPU 的数据处理性能是 CPU 的 10 倍、20 倍，甚至更高。</p><p>作为加速器存在的 GPU，比 CPU 还要激进。摩尔定律中处理器性能每隔两年翻 1 倍，而英伟达的 CEO，Jason Huang，归纳说 GPU 将推动 AI 性能实现每年翻 1 倍，这个规律还被业界称为黄氏定律。</p><h3 id="ASIC：为解决特定应用问题而定制设计的集成电路"><a href="#ASIC：为解决特定应用问题而定制设计的集成电路" class="headerlink" title="ASIC：为解决特定应用问题而定制设计的集成电路"></a>ASIC：为解决特定应用问题而定制设计的集成电路</h3><p>为解决特定应用问题而定制设计的集成电路，就是 ASIC（Application Specific IC）。当 ASIC 规模够大，逐渐通用起来，某类 ASIC 就会有一个专有名称，成为一个品类。例如现在用来解决人工智能问题的神经网络处理器。</p><p>标准的 CPU 芯片，往往要配上不同的外围芯片，比如 Intel 管理外设的芯片组 Chipset，加速图形的 GPU，这样才能构成系统。而随着工艺制程的不断演进，我们有能力把越来越多的外围芯片集成进 CPU 芯片中，于是就有了 SoC。SoC 因其高集成度、高效率的特点，是目前 IC 设计的主流。SoC 也算是 ASIC 的一种。</p><p>相较于我们常见的 CPU、GPU 等通用型芯片，ASIC 芯片的计算能力和计算效率都可以根据特定的需要进行定制，定制么，肯定体积小、功耗低、计算效率高，在这些方面有优势。但是缺点就是入门门槛高，这里的门槛，包括资金、技术，还有时间。</p><h3 id="存储芯片：DRAM-和-NAND-Flash"><a href="#存储芯片：DRAM-和-NAND-Flash" class="headerlink" title="存储芯片：DRAM 和 NAND Flash"></a>存储芯片：DRAM 和 NAND Flash</h3><p>前面有介绍，数字 IC 中 2/3 是逻辑芯片，1/3 就是存储芯片。存储芯片就两个主要品类 DRAM 和 NAND Flash，占了 98% 的比例，其余可以忽略不计。</p><p>存储芯片在设计方面跟前面的 CPU、GPU、ASIC 这类逻辑芯片有很大不同。CPU、GPU、ASIC 重在功能设计、逻辑设计。而存储芯片的设计比较简单，基本都是重复单元，但是对时序和布局布线有挑战性。</p><p>好了，专业概念到这里就告一段落，让我们再回到 iPhone X 的物料表，讲一讲剩下 30% 的事情。存储芯片之后我列了三行模拟芯片，有射频芯片、电源芯片。那么为什么它们会被归类到模拟芯片，什么是模拟芯片呢？</p><h3 id="iPhone-X-中的模拟-IC"><a href="#iPhone-X-中的模拟-IC" class="headerlink" title="iPhone X 中的模拟 IC"></a>iPhone X 中的模拟 IC</h3><p>上面我提到数字 IC 的时候没有展开讲概念，这里你可以跟模拟 IC 对比来看：处理数字信号的就是数字 IC，处理模拟信号的就是模拟 IC。它们两个是相对的。其实如果要逻辑严密，集成电路的分类应该还列上数模混合 IC 共三种，而实际上你可以理解为，以数字电路为主的归类到数字 IC，以模拟电路为主的归类到模拟 IC，两大类方便你记忆。</p><p>数字 IC 基本上是一个追逐摩尔定律的品类，尽量采用最新工艺，利用新工艺制程带来的晶体管密度的提升，来提高性能同时降低成本。</p><p>相对来说，模拟 IC 则更多的追求电路速度、分辨率、功耗等参数方面的提升，强调的是高信噪比、低失真、低耗电和高稳定性，因而产品一旦达到设计目标就具备长久的生命力，生命周期可长达 10 年以上。行业里有“一年数字，十年模拟”的说法。</p><p><strong>那么，数字信号和模拟信号又是什么？</strong></p><p>简单来说，0 和 1，就是数字信号。而声音、光、气压、无线电信号（Radio Frequency，也被翻译成射频，射频信号），这些现实中的信号，基本上都是连续的信号，而不是简单的用“有 -1”，“无 -0”来表示，它们都是模拟信号。</p><p>数字 IC 这块，你好歹在日常中有见过、听过，甚至买过。而对于模拟 IC，你可能就不熟悉了。但手机其实就是一个大量使用模拟 IC 的电子设备。如果按照整个半导体行业的出货量来看，模拟 IC 的数量是超过数字 IC 的，但是单价不高，因此在销售收入上占比也不高。</p><p>看下图，Quadplexer 四路复用器芯片，实现手机芯片频段载波聚合功能，载波聚合，是一种增加传输带宽的手段，把几个分散的频段通道整合成为一个更宽的数据通道；RF Switch 射频开关芯片，处理无线信号通道转换；NFC 芯片，用来处理近距无线通讯信号的；Wireless Charging 芯片，这个你熟悉，支持无线充电；还有 Audio Amp 音频放大器芯片等等，这些就都是在 iPhone X 中的模拟 IC。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-56f8545dc8daaf04.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>总的来说，射频器件、电源管理装置和数模 / 模数转换器是模拟 IC 的三大主要产品。</p><ul><li>射频器件是处理无线电信号的核心器件，包括 5G 信号、蓝牙、WIFI、NFC 等，凡是需要无线连接的地方必备射频器件，手机是射频器件的一个重要应用场景。</li><li>任何电子设备都需要电源管理装置。电源管理芯片的任务就是完成电能的变换、分配、检测及其它电能管理。电源管理芯片占模拟芯片销售份额接近三成。射频芯片和电源管理芯片在手机里非常重要，你可以对着物料表，找一找。</li><li>模数和数模转换器是模拟信号与数字信号之间起桥梁作用的电路。A/D 是模拟量到数字量的转换， D/A 是数字量到模拟量的转换，它们的道理是完全一样的，只是转换方向不同。例如我们要播放一首歌曲，歌曲是以数字形式存储的，手机经过一系列的数模转换，把数字信号变成连续的声音信号，通过麦克风播放出来，这就是一个 DAC(Digital to Aanalog Controller, 数模转换器) 数模转换过程。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d7f655906550a0b1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>一个小知识点，模拟 IC 的设计涉及了更加复杂的信号环节，并且其设计的自动化程度远不及数字 IC，通常需要大量的人工干预决定取舍。相对于数字 IC，模拟 IC 的设计对工程师的经验，权衡矛盾等方面的能力要求更为严格。所以，模拟 IC 设计被称为一门艺术。</p><h3 id="一颗芯片到底是如何诞生的"><a href="#一颗芯片到底是如何诞生的" class="headerlink" title="一颗芯片到底是如何诞生的"></a>一颗芯片到底是如何诞生的</h3><p>为了方便大家记忆理解，我做了一张完整的流程图。重点有市场需求分析、芯片设计、晶圆制造、封装测试这四个流程，最后，封装测试好的芯片成品会交给设备厂商，完成电子设备硬件的制造组装和软件安装。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0286b5c865eb09ac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果只看芯片设计，它主要包含需求分析、架构设计、逻辑设计、物理实现和验证等几个部分。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-dfdef687507939b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果说芯片项目和其他产品项目有什么不同，那就是芯片项目是人类历史上最细微也是最宏大的工程，研发投入大，项目收益高，<strong>试错成本极高</strong>。</p><p>以 iPhone 的 A11 应用处理器为例，这颗处理器包含 43 亿的晶体管，工程师们要在有限的项目时间内，实现手机应用处理器所需要完成的功能任务，还需要在 88mm² 指甲盖大小的面积下放置下 43 亿晶体管和它们自己的连线，再要经过缜密的验证工程，以保证它在 2 亿台手机上都能正常工作 2-3 年。</p><p><strong>FPGA</strong></p><p>本来在买商业现成芯片，和定制芯片或者自制芯片之间，还有 FPGA 一个选择。FPGA 可以通过硬件编程语言，把芯片内的标准阵列器件重新组合，形成新的电路，完成类似定制芯片的功能，可以节省投片生产的开销和时间。</p><p>但是，代价就是单片的价格偏高，功耗偏高，因此 FPGA 在数据中心的应用较多，在手机终端领域极少。</p><p><strong>需求分析</strong></p><p>其实可以想见当时一定是有多个芯片公司，发现了手机应用处理器这个真实需求。在芯片公司发现市场需求之后，通常会进行市场调查，总结出一个通用的市场需求清单出来。一般这个需求清单，会包括主要应用场景分析、软件栈、竞争分析、性能与定位、需求量与投资回报比分析、行规与标准，主要配套芯片的市场供应情况预测等多项内容。最终形成的就是一份市场需求文档。</p><p>每个公司的文档流程不一样，有的公司，在做市场需求文档之前，还要做商业需求文档，重点在于分析投入产出比。在市场需求文档之后，还要做深入的产品需求文档和细分的功能需求文档。</p><p><strong>架构设计</strong></p><p>我们说，一个项目是否有商业价值，主要看需求分析。而一个芯片是否做得好，80% 是由架构设计决定的。</p><p>在拿到需求分析文档后，高层设计人员，往往是以架构师为主的团队，需要开会对产品需求逐条进行可行性分析，并在此基础上确定基本架构和模块分解，最终设计出一个系统架构。我可以给你看看，苹果 A12 的一张系统架构图，你可以感受一下。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9d3bbc6a0cc3eff8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在基本架构确定之后，芯片的大目标，就是对 Power 功耗、 Performance 性能、Area 面积，也就是业内常提到的 PPA 衡量标准，要有明确的规划，这一般也就确定了要选择的制造工厂和工艺制程。接着，芯片设计公司要把需要向供应商购买的 IP 和自研的 IP，把它们的交付时间和接口逐一确认。苹果的 CPU 核一直是自研的，GPU，则从 IP 供应商那里购买过很长一段时间。</p><p>如果采用先进封装技术进行小芯片（chiplet）的设计，此时封装方案和初步的布局规划也都应该确定了。在大多数情况下，一个芯片里面只封装一个集成电路硅片，但是有时候为了更高的性能，或者高密度的设计，当然还有成本因素，需要把多个硅片封装在一起，这种技术手段就叫先进封装。</p><p>到架构设计这一步，最终输出的就是一份产品规格书和高层架构设计文档。<strong>这是一个将市场需求，翻译为可实现的芯片架构过程</strong>。</p><p>架构确定了之后，通常架构团队和算法工程师团队会建模仿真，确保功能、性能、吞吐量等指标可实现。有了一个可行的芯片设计方案和芯片原型模型之后，架构团队就可以把文档和芯片模型移交给设计团队，开始逻辑设计。设计团队，会先输出微架构或者模块设计文档，然后进入编码阶段。</p><p><strong>逻辑设计</strong></p><p>前端设计与验证</p><p>前端设计（逻辑设计）一般用硬件描述语言，例如 Verilog，将架构师的设计用编码实现。大型芯片项目，设计也是分层次的。先进行模块设计，底层的模块写完之后，把新写的模块、商业 IP、复用的旧 IP 等整合在一起，形成一个完整的设计。</p><p>其实硬件描述语言，看起来跟 C 语言颇为类似，不过写 C 语言的人，心里想着“hello world”，写 Verilog HDL 的人，心里想着电路图。</p><p>我给你展示一段编码，用 Verilog HDL 写的寄存器传输级设计如下图：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d5c4f1aa214ffbb4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>跟其它的软件项目一样，在设计的同时，验证也要并行进行。验证是芯片设计中最为耗时耗力的工序，ARM 的技术白皮书有统计，一般一个项目的 40% 资源是用在验证阶段的。</p><p><strong>逻辑综合</strong></p><p>设计验证完成之后，还有一个步骤，叫逻辑综合 （Logic Synthesis），就是用 EDA 工具把寄存器传输级设计 RTL 描述变网表（Netlist），非常类似于编译器把 C 语言翻译成机器语言的过程。从这一步开始，芯片的设计就和具体的晶圆代工厂和具体工艺绑定在一起，设计开始具有物理特征了。</p><p>网表表示的电路如下图，它其实就是描述电路元件相互之间的连接关系。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e4c0e57a303322ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>逻辑综合，对于半导体设计公司不难，但是却是 EDA 公司的核心产品之一。在实现流程中，就背后算法而言，综合一定是最难最复杂的步骤。一个晶圆厂，并不仅仅要有先进工艺，提供给设计公司的设计工具包 PDK（Process Design Kit）和 EDA 厂商的支持也非常重要。</p><p><strong>DFT（可测试性设计）</strong></p><p>除了以上两步，前端设计还有一个步骤就是 DFT（Design For Test）。所谓 DFT，就是预先规划并插入各种用于芯片测试的逻辑电路。芯片制造后期，在封测阶段中，很多测试需要依赖 DFT 的设计。</p><p>完成以上工作后，前端设计团队就可以将生成的网表交给后端实现团队，开始物理设计了。当然这个过程不是一蹴而就的，前端设计工程师往往要多次，不同层次的反复综合、验证，各种设计规则检查，既要确保设计的正确性，又要保证设计的布局布线可行且优化。</p><p>整个逻辑设计阶段，你可以这样理解：<strong>架构师写在文档上的指标与功能，是需要设计团队通过一行行的代码实现出来的。</strong></p><p><strong>物理实现</strong></p><p>在前端设计结束后，后端也就是物理实现需要完成布局布线，这个时候，需要把网表转换成制造工厂可以看懂的文件，也就是转化为制造工厂可以用来制造光罩的图形文件。</p><p>后端设计的主要步骤可以总结为：布局规划 Floorplan→布局 Placement→时钟树综合 CTS→布线 Routing →物理验证。</p><p>布局规划就是在总体上确定各种电路的摆放位置，它是后端实现中最为重要的一个环节。我这里放了一张图，你可以看下苹果 A11 的布局规划是怎么样的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3d0589e8020a587c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>芯片的面积、时序收敛、稳定性、走线难易，基本上都是受布局规划的影响。在实际项目中，往往此时还有未完成的模块，就要预留位置。</p><p>规划之后，在指定区域摆放元器件，就是布局，而把各个元器件连接起来，就是布线。一颗芯片的树状的时钟信号线路非常重要，往往需要单独布线，因此还有一个专门的名称：时钟树综合。最后，要做验证。</p><p>我说得简单，其实这就是在指尖上建高楼，在小小的芯片上放置上百亿个晶体管，纳米级的单位，幸而有 EDA 工具辅助，这不是人力所及的工程。</p><p><strong>一颗芯片做得好不好，在决策阶段取决于市场需求理解的是否深刻，在逻辑设计阶段取决于工程师的能力强不强，而在物理实现阶段基本取决于 EDA 工具玩得好不好。</strong></p><p>在芯片设计进入纳米时代之后，布局布线的复杂度呈指数增长，从布局规划到布局布线，时钟树综合，每一步涉及到的算法在近年都有颠覆性的革新。这些步骤，都高度的依赖 EDA 工具。要对 EDA 工具有深度理解，并且要理解 EDA 工具背后的方法学。</p><p>在项目后期，特别是在最后阶段发现个别电路小问题 ，可以进行工程更改（Engineering Change Order,ECO）。ECO 有专门的 EDA 工具和流程，我就不展开说了。</p><p>物理设计完成之后就形成了下图展示的电路图。图中可以看到蓝、红等不同颜色，每种颜色就代表着一张光罩。这个时候的芯片设计就可以以 GDSII 的文件格式从设计公司移交给芯片代工厂了。自此，设计完成，制造流程开始。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c9cba112f428fade.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果仔细看每个晶体管，都是由硅基底（Silicon Substrate）、氧化层（Oxide）、鳍（Fin）、栅（Gate）构成的。鳍的高度，宽度都在十几、几十纳米；栅极长度，高度也在几十纳米的范围之内，是非常精致的纳米级器件。下图已经是简化过的抽象模型，随着半导体工艺的演进，实际的器件，其实比下图要更加复杂。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8f7e046b0b669280.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而芯片的制造，就是要在平方毫米的尺寸上，制造上亿个纳米级晶体管的过程。我们在讲芯片的物理实现的时候说过，每个晶体管，都是被工程师们精心设计，放置在特定的位置上，要完成一定功能的，因此在制造过程中，任何一个晶体管的失效，都会导致最终芯片的一部分，或者整个芯片的失效。</p><p><strong>芯片制造：晶圆厂的王国</strong></p><p><strong>上游：晶圆材料准备</strong></p><p>这阶段的重点是提炼单晶硅锭。生产单晶硅锭的公司会将硅元素，从沙子中提取出来，经过高温整形、多次提纯等手段得到高纯度的硅（EGS）。然后再将纯硅熔化，抽出圆柱形的单晶硅锭。</p><p>硅锭切割之后，变成一片一片的圆盘，再经过打磨抛光，一片纯硅晶圆，就准备好了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-951178bd0e62333c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>直径为 300 毫米的纯硅晶圆，俗称 12 寸晶圆，大约是 400 美金。但尺寸并不是衡量硅晶圆的最重要指标，纯度才是。日本的信越公司可以生产 13 个 9 纯度的晶圆。</p><p>其实，日本的信越、SUMCO（三菱住友株式会社），在晶圆市场上占据了近 60% 的市场份额，这让晶圆市场排名第三的台湾环球晶圆非常有压力，在 2020 年开展了并购排在第四的德国世创的计划。</p><p>其实半导体产业的上游原材料，不仅仅是晶圆，像在光刻胶、键合引线、模压树脂及引线框架等重要材料方面，日本的企业在全球都占有很高份额。可以这么说，如果没有日本材料企业，全球的半导体制造都要受挫。因此 2019 年日本限制向韩国出口三种半导体核心原料（含氟聚酰亚胺、高纯度氟化氢、光阻剂）时，曾经一度让三星实际掌门人不得不四处奔走，为保证生产而努力。</p><p><strong>中游：晶圆加工过程</strong></p><p>有了硅晶圆，下一步就是把设计团队交付的电路图，通过光罩，移植到硅晶圆上。这个加工过程，有上百个步骤，一般需要 2 到 5 个月。感谢现代社会的精细分工，即使是半导体从业人员，甚至是在晶圆厂工作的人，只要不涉及工艺的研发，也无需完全掌握这些步骤，只需要各司其职，共同协作就好。</p><p>我把加工的过程简化为 7 个步骤，给你逐步讲解一下。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e26df12be3c456af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>第一步，纯硅晶圆，在加工之前， 需要打磨抛光。</p><p>第二步，抛光完成后，通过高温，或者其他方式，在晶圆表面产生一层薄薄的二氧化硅保护膜，叫做镀膜。</p><p>第三步，光刻，光刻是整个过程中最重要的一个环节。这个环节的重要性体现在两个方面，第一，它是设计和制造进行联系的唯一环节；第二它是最昂贵的一个环节。</p><p>光刻的一个重要输入是光罩。在上一讲中，设计团队交付的 GDS II 文件，会被用来制成如下的光罩。这个光罩，有点像印钞时候的母版，也是设计和制造之间的纽带。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-89fe602e3382a1be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>光罩，有专业的公司可以生产。因为光罩和晶圆制造紧密衔接，所以一般晶圆制造厂，都有自己的专业工厂来生产自身需要的光罩。为了制造一款芯片需要上百道工序，光罩也是不只有一张的，在 14nm 工艺制程上，大约需要 60 张光罩，7nm 可能需要 80 张光罩甚至更多。光罩层数的增加，也就代表着成本的增加。工艺提升，带来的光罩层数的增多，算是先进工艺成本越来越高的原因之一。</p><p>使用特定波长的光，透过光罩，照射在涂有光刻胶的晶圆上，光罩上芯片的设计图像，就复制到晶圆上了，这就是光刻，这一步是由光刻机完成的，光刻机是芯片制造中光刻环节的核心设备。你可以把光刻理解为，就是用光罩这个母版，一次次在晶圆上印电路的过程。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-bd0120caca45aec7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>刚刚我说，光刻是最贵的一个环节，一方面是光罩越来越多，越来越贵，另一方面光刻机也很贵。光刻机是半导体制造设备中价格占比最大，也是最核心的设备。2020 年荷兰公司 ASML 的极紫外光源（EUV）光刻机每台的平均售价是 1.45 亿欧元，而且全世界独家供货，年产量 31 台，有钱也未必能买得到。</p><p>回顾光刻机的发展历史，从 1960 年代的接触式光刻机、接近式光刻机，到 1970 年代的投影式光刻机，1980 年代的步进式光刻机，到步进式扫描光刻机、浸入式光刻机和现在的深紫外光源（DUV）和极紫外光源（EUV）光刻机，一边是设备性能的不断提高，另一边是价格逐年上升，且供应商逐渐减少。</p><p>到了 EUV 光刻机，ASML 就是独家供货了。</p><p>EUV 作为下一代技术的代表，不需要多重曝光，一次就能曝出想要的精细图形，没有超纯水和晶圆接触，在产品生产周期、光学邻近校正（OPC）的复杂程度、工艺控制、良率等方面的优势明显。是 7nm 以下工艺必备的。</p><p>光罩加光刻机，让光刻这个步骤 “贵”了起来。</p><p>好，光刻之后，就到了第四步蚀刻，蚀刻就是去除多余物质。</p><p>第五步，离子注入，在真空的环境下进行离子注射，在光刻的晶圆电路里注入导电材料，这样可以改变对应区域的导电特性。</p><p>第六步，电镀，电镀这个步骤主要是用来制作铜导线的。</p><p>一颗芯片通常有多层，每一层的线路不同，功能不同，使用的膜的材质不同。所以每一层的光罩也不同。因此，以上的 1-6 步需要根据实际的集成电路的设计层数重复进行几次到数十次。这里可以有一个小知识点，如果芯片有一些小 Bug，可以只对其中几层的光罩进行修正，而不用重做全部的光罩。</p><p>第七步，测试，测试永远是重要环节。对于晶圆的测试可以分成两类，一类是制造工艺相关的晶圆验收测试，一类是电路功能测试。当然，当晶圆切割完毕，在封装之前，还会再进行测试。不过对于整个晶圆进行测试，效率要高过切割之后。</p><p>这里我们可以复习一下上一讲逻辑设计中提到的 DFT 环节。在进行电路的前端设计时，就预先设计并插入用来测试的电路，这就是 DFT。后期，无论是晶圆的电路功能测试，还是封测阶段中最后的测试，都是依赖 DFT 设计的。</p><p>芯片制造的中游环节，到这里就介绍完了。我一定要提一下价值问题，这么昂贵的工厂制造环节，究竟可以增值多少呢？据国际商业策略 IBS 公司（International Business Strategies）的推算，台积电一片 5nm 晶圆的加工费高达 12500 美金。根据台积电的财报推算，台积电平均每片晶圆可以产生近 4000 美金（300mm 晶圆）的利润。无论是哪个数字，对比 400 美金的纯硅晶圆原料来说，这都是一个至少增值 10 倍的高价值的加工过程。</p><p><strong>下游：封装与测试</strong></p><p>当晶圆制造进行测试之后，就会被送往下游的 IC 封装测试厂实施切割、封装和进一步的测试。</p><p>整个晶圆在切割成单片之后，会针对每一个单片（芯片）进行电气测试。在封装前，还会使用显微镜对芯片进行复检。提前检测出有瑕疵的芯片，可以减少后续流程上的成本开销。</p><p>封装的主要目的是将半导体材料集中在一个保护壳内，防止物理损坏和化学腐蚀。相对于测试，封装对芯片的最终形态，影响更大。</p><p>半导体封装技术有三次大的技术进步：第一次是在 20 世纪 80 年代从引脚（Pin）插入式封装到表面贴片封装，表面贴片封装极大地提高了印刷电路板上的组装密度；第二次是在 20 世纪 90 年代球型矩阵封装的出现，满足了市场对高引脚的需求，改善了半导体器件的性能。</p><p>第三次是 2D、2.5D、3D 封装等，混合了芯片堆叠、异构封装的先进封装。先进封装的最主要推动力来自手机，因为手机有着对封装面积最小的极致追求，其次推动力来自于数据中心对高性能芯片的追求。</p><p>关于 2D、2.5D、3D 封装，我有一个朋友有个非常形象的比喻，“摊大饼摊不下去了，搞个千层饼试试”。总之，所谓的先进封装，就是把单独设计和制造的组件或者小芯片（chiplet），封装在一起，如下图。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-378c3bb962ca11ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>封装之后，还会对每个芯片进行最后的测试。这个环节是由 ATE 专业自动测试设备完成的。因为一般的公司，都是租用 ATE 机台，所以每个芯片的测试计划，测试时间等，都是成本的一部分。一般的经验是，一颗芯片的 1/3 的成本，是花在封测阶段的。</p><p>基于测试的结果，将具有相同能力的芯片归属一类，可以根据芯片的最高工作频率，稳定性等规格制定等级，以便定价。该芯片就会被打标示，分类芯片的规格、型号及出厂日期等丝印，等待打包出厂了。有部分缺陷的芯片，在很多时候是可以作为低规格的正式产品出厂的。要做到这点，必须在架构设计阶段，就要规划好。一个优秀的架构师，考虑的不仅仅是功能和性能的竞争性，整个流程中的每一个与芯片成本相关的问题，都必须提前考虑周到。</p><p>到这里，芯片制造的过程我就讲完了，目前除了极少数半导体设计公司还有工厂之外，芯片的制造是都由专业的晶圆厂代为制造，是晶圆厂的王国领地。上一讲我们提到的芯片设计，和这一讲分析的芯片制造与封测，并不需要一个公司完成，我给你总结了一个产业链的分工合作示意图：</p><h3 id="产业链分工视角：设计、制造、封装、测试的分工合作"><a href="#产业链分工视角：设计、制造、封装、测试的分工合作" class="headerlink" title="产业链分工视角：设计、制造、封装、测试的分工合作"></a>产业链分工视角：设计、制造、封装、测试的分工合作</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fe918fe88069f6a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="IDM-模式"><a href="#IDM-模式" class="headerlink" title="IDM 模式"></a>IDM 模式</h3><p>在半导体行业开始的三十多年，所有的半导体公司，都是从设计到制造全部自己完成。在下一讲介绍 Intel 的时候，我会提到当 IBM 向 Intel 订购芯片，Intel 会设计、制造、测试、封装，然后直接交付芯片。这种模式业界称为集成设备制造商，也就是 IDM 模式。</p><p>这种模式，在 1987 年台积电创立之后，慢慢地改变了。台积电这样的代工厂，专注于芯片制造这个环节，服务整个半导体产业。越来越多的创业公司，选择放弃需要高额资金的建厂环节，专注在价值更高的设计芯片环节。设计和制造开始了分离。</p><p>在摩尔定律的推进下，半导体行业高速运转，半导体工艺从 45nm，到 28nm，到 16nm 不断演进，随着晶体管体积不断变小，建造工厂的成本剧增。能够维持一定的生产规模，进行良性的商业循环的 IDM 模式公司在减少，甚至专业的晶圆厂都在逐步减少之中。</p><p>下图是一张工厂对应工艺制程的总表。到了 10nm 工艺制程的时候，全世界晶圆厂也就只有三家了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7de463e18c3cbc6c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>那些原来的 IDM 模式的公司都怎样了呢，有的完全剥离工厂，成为无厂的纯设计公司，例如 AMD、IBM。有的放弃跟随新工艺，成为轻工厂模式，例如博通，德州仪器。</p><h2 id="Top10-半导体公司"><a href="#Top10-半导体公司" class="headerlink" title="Top10 半导体公司"></a>Top10 半导体公司</h2><h3 id="Intel的发展史就是一部芯片行业的创新史"><a href="#Intel的发展史就是一部芯片行业的创新史" class="headerlink" title="Intel的发展史就是一部芯片行业的创新史"></a>Intel的发展史就是一部芯片行业的创新史</h3><p>如果从现代集成电路被发明开始计算半导体行业的历史，Intel 基本上是最早的一批半导体公司。可以说 Intel 的历史基本上就是半部半导体行业发展史。</p><p>Intel 从 1992 年就被财富杂志评为最大的半导体供应商，从此开始连续占据半导体行业排名第一的位置，仅仅在 2017 和 2018 两年，输给三星。这里插一句，常年第二的三星，走了一条和 Intel 截然不同的路线，Intel 之后我就分析三星。到了 2019 年和 2020 年的行业第一，还是 Intel。</p><p>更牛的是，Intel 不仅仅排名第一，而且是非常有质量的第一。2021 年 1 月份，著名市场研究公司 Gartner 披露的 2020 年半导体厂商营收排名前十名里，第一名 Intel 的营收超 700 亿美元，比第 6 名到第 10 名的总和还多，占整个半导体市场的 15.6%。我这里提供了一张饼图，你可以自行感受一下。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3a6353d49ce40ef1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>四个发展阶段</strong></p><p>我不知道你是不是听过仙童公司，它经常被叫做硅谷的黄埔军校。80 年代初出版的著名畅销书《硅谷热》中写到：“硅谷大约 70 家半导体公司的半数，是仙童公司的直接或间接后裔。在仙童公司供职是进入遍布于硅谷各地的半导体业的途径。1969 年在森尼维尔举行的一次半导体工程师大会上，400 位与会者中，未曾在仙童公司工作过的还不到 24 人。”</p><p>1968 年 8 月，仙童八叛逆之中的诺伊斯与摩尔一起辞职，离开仙童，创办了 Intel 公司，而后格鲁夫作为第三名员工加入，这是一个重要的起点。以此为始，Intel 开启了它辉煌的旅程。为了方便理解，我把 Intel 的发展历史划分为以下四个阶段。</p><ul><li>第一阶段，发展初期，从 1968 年公司创立到 1992 年，Intel 虽然无奈退出存储器市场，但是随着 PC 市场的兴起，Intel 确立了以 CPU 为核心的产品路线，并确立了行业第一的地位。</li><li>第二阶段，上升期，也可以称为前 Tick-Tock 时代，从 1992 年到 2005 年，跟随第一次互联网行业的大爆发，Intel 与 AMD 联手拿下服务器市场，同时通过高速高质量的产品迭代，彻底跑赢 AMD，坐稳行业第一。</li><li>第三阶段，从 2005 年到 2015 年，Intel 提出 Tick-Tock 战略，持续领跑 ICT 行业，虽然保持了行业第一，但是在这个阶段，Intel 错失了移动互联网市场。</li><li>第四阶段，从 2015 年之后，是 Intel 的徘徊期。因为错失了移动互联网，Tick-Tock 战略也出现了停摆，多元化经营策略效果亦并不明显，这个阶段的 Intel，虽然在行业第一的高位上，但是和第二梯队的距离，前所未有的被拉近了。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5c0b14c51c537be7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>完败存储器市场</strong></p><p>现在的 Intel 是以处理器闻名的。但是在 1968 公司刚成立的时候， Intel 推出的第一款产品是 64bit 的 3101 静态随机访问存储器 SRAM，注意，是存储器。一直到公司成立的 4 年之后，Intel 才推出了全球第一个微处理器 4004。</p><p>在 Intel 发展初期，存储芯片是这家创业公司的重要产品，现在你听说的全球第一块双极型半导体存储芯片、第一个 DRAM、第一个 1KB SRAM、第一块 EEPROM，都是 Intel 的作品。</p><p>总体来说，存储类半导体是非常依赖工厂能力的。而 Intel 在初创时期，重要的战略之一就是全力建设自己的制造工厂。</p><p>在那个时候，Intel 拥有存储芯片 100% 的市场份额。这意味着什么呢？即使把目光拉回到今天，整个半导体行业的销售额中，集成电路占 82%，而存储器是集成电路市场中最大的产品品类，约占集成电路市场的 30%，是一个 1100 亿美金的巨型市场。</p><p>如果没有竞争，市场一帆风顺，Intel 左手内存，右手处理器，两条腿走路，估计就会成为拥有超 50% 市场份额的超级巨无霸。但是很不巧，就在这个时候，Intel 遇上了高速发展的日本半导体行业。</p><p>日本企业怎么赢的？这段历史，我觉得中国政府和半导体行业可以借鉴。1976 年日本政府制订了“VLSI（超大规模集成电路）计划”，选定在存储器市场发力，组织多公司参与研发，投入大笔资金，饱和式攻坚，可谓举国之力。</p><p>VLSI 计划开启第 4 年（1980 年），在惠普对 16K DRAM 内存的竞标中，日本的 NEC、日立和富士通完胜美国的 Intel、德州仪器和莫斯泰克（当时美国存储器领域的头部玩家）。</p><p>更让人大跌眼镜的是，美国质量最好的 DRAM，不合格率比日本最差的公司还要高 6 倍。1985 年，日本电气 NEC 凭借存储器产品登上全球半导体厂商榜首，并在之后连续 7 年稳坐头把交椅。</p><p>可以说，Intel 在公司的发展初期，选择对了产品（半导体最大的品类，存储器），而且技术创新领先（各种世界第一），也结合了自己的强项（工厂生产）。从一个初创公司角度，无论在技术，商业上都没大问题，但是输给了强大的竞争对手：日本政府。</p><p>在存储器和处理器之间的选择是战败之选。1985 年，Intel 在总裁安迪·格鲁夫（2 年后出任 CEO）的主导下关闭了 7 座工厂，裁员 7200 人，结束了存储器业务。</p><p><strong>选择 CPU 作为主产品</strong></p><p>回到 CPU 这条产品线上，其实 Intel 的胜出，也没有那么一帆风顺。不过 CPU，是纯粹的技术产品的竞争，大家竞争的是研发效率。</p><p>1971 年，Intel 发布的 4004 还只是微处理器的雏形，只能用来做计算器。即使是计算器，也需要 4001、4002、4003、4004 这组芯片套件，共 10 片，才能搭建完成。直到 1974 年的第三代 8080，才被认为是一款真正可用的微处理器。</p><p>因为 8080 的出现，IT 行业的分工合作模式有了一些变化。在 8080 之前，当时的计算机系统厂商，如 IBM、惠普，需要独立完成整个计算机系统，从处理器到终端，再到操作系统、编译器和应用软件。</p><p>8080 出现之后，一些小公司，例如 MITS，用 8080 搭建了第一个商用的个人电脑 Altair 8800，那个时候比尔·盖茨，还没有创建微软，就开始给 Altair 8800 写软件了。</p><p>Altair 8800 系统和 Intel 8080 芯片的结合，让系统厂商看到了更大的机会。是的，他们可以采购商业的半导体芯片，来快速搭建系统。看，Intel 不仅仅创造半导体产品，还在创造新的商业模式。</p><p>当时的 CPU 市场已经行情看涨，有 60 多家半导体公司进入，市场上最成功的 8 位处理器是 Zilog 的 Z80。Zilog 是 Intel 离职的技术骨干创立的公司，技术实力很强，同时期的摩托罗拉，也很优秀。这就是真实的世界，任何一个有前景的赛道上，都是拥挤的。</p><p>这里交代一下 Zilog 的结局，它没有赢，虽然没有死但是影响力可以忽略不计（这也是一段很有意思的商业故事，你感兴趣可以查查）。Z80 的成功，某种程度是 Z80 和 Intel 8080 指令完全兼容，软件可以复用的相互成就。想想系统厂商，可以按自己的喜好，自由地二选一。而软件公司，写一个软件，可以在两种处理器上运行，享受大一倍的市场。</p><p>最后 Intel 成功的关键在于，它在设计下一代产品 8086 的时候，保持了指令集的向后兼容性（就是新款 CPU 依然可以运行老版本的软件），但是 Zilog 并没有，而市场喜欢这种指令集的向后兼容性。</p><p>这里要给所有的硬件小伙伴画一个重点，此时微软还是个刚刚成立的十几人的小公司，还没有强大的 Wintel 联盟，而 Intel 已经是个万人公司了。这是一个蛮荒时代，Intel 抓住了这个时间窗，确立了一个在软硬件之间的标准。</p><p>硬件公司立软硬件接口标准，其实非常有难度，因为这意味着将软件公司锁定在一个平台上。在半导体行业，Intel 最先意识到指令集向后兼容的标准价值，也因此，它战胜了 Zilog 的 Z80，随后微软把这个标准价值进一步放大，几乎成为 IT 行业的底层标准，这也确立了 Intel 和微软随后多年的行业双霸主地位。</p><p>持续努力的人，运气不会太差。Intel 在公司创立 10 年后，迎来了第一款重量级的处理器产品，8086 处理器，和随后的简化版 8088。</p><p>8086 就是一个 16bit 的处理器（4004 是 4bit 的处理器），有将近 2.9 万个晶体管 （是 4004 的 10 倍，8 年集成晶体管的数目增长了 10 倍，完美的摩尔定律速度），芯片的面积为 33mm² （4004 是 12mm^2，面积仅仅大了 3 倍)，主频为 5 MHz（对比 4004 的 108KHz，快了 46 倍），3.2 微米的制程。从 1971 的 10 微米，经过 6 微米节点，到 1978 年的 3.2 微米制程，可以看到那个年代，工艺节点的进步速度之快，而且 Intel 就是领跑者。我放了一张 8086 Die（裸片）的照片如下。</p><p>如果请 Intel 自己选一个历史最重要产品，那肯定是 8088。为什么呢？其实 8088，在产品规格上跟 8086 几乎一致，它是在完成 8086 之后，进行裁剪的简化版。这不是一个技术上的突破产品，但这是一次商业上的绝对成功。1981 年 IBM 选定它作为自己 PC 机的处理器，开启了 Intel CPU 的问鼎之路。Intel 还把拿下这个重要订单的销售名字，写入自己的公司史中。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-efd2201f786b98bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>CPU 问鼎之路</strong></p><p>从 8088 应用到 IBM PC 机上开始，个人电脑真正走进了人们的工作和生活之中，这是 PC 时代的开始，也是半导体行业的第一个风口，Intel 的问鼎之路，就是靠着 PC 市场的推动。</p><p>为什么选择 8088？其实 IBM 在摩托罗拉和 Intel 之间犹豫了蛮久，传说是因为当时苹果的 Macintosh 就采用了据说技术更领先的摩托罗拉芯片 68020，而 IBM 不想走相同的路线，所以选择了 8088。8088 单从性能上看，还不如自家的 8086，但是 8bit 的数据通道，让外设的价格更低。这里划个重点，单品的性能很重要，但是更重要的是设备整体的总成本合理。</p><p>PC，所谓的个人电脑是整个世界进入信息化的开端。正是依靠廉价且高速提升性能的 Intel 的 CPU，以及 IBM 一开始就采取的开放标准策略，使大批的兼容机生产厂商，和大量提供标准配件的厂商，分工合作，共同拉低了计算机系统的成本，让每个公司每个家庭（注意，是家庭不是个人，手机才让每个人）都可以拥有强大的算力，才有今天繁荣的 IT 行业。</p><p>赢下 IBM PC 设计，基本上 Intel 的处理器产品发展就走上了快车道，而且因为 IBM 同时选了微软的操作系统，这也为后来的 Wintel，Windows+Intel 的生态奠定了大局。</p><p>在整个 80 年代，Intel 就是在 8086 芯片的基础上，不断推出 80286（1982 年）、80386（1985 年）、80486（1990 年）等迭代版本，也因此，Intel 的芯片架构被称为“x86”系列。x 就是 2，3，4… … 代表产品系列的数字。1992 年，Intel 终于凭借处理器 CPU 在 PC 市场的胜利，成为世界最大的半导体供应商。</p><p><strong>充分竞争</strong></p><p>如果说 80 年代，Intel 是和一众日本企业在存储器市场上搏杀。那么在放弃存储器业务，Intel 把火力集中在处理器市场上之后，最强的竞争者就是同架构的兄弟 AMD。</p><p>这个成立时间仅仅比 Intel 晚一年的 AMD，是一个既有技术先进性又有故事性的公司。AMD 的创始人杰里·桑德斯，可不像 Intel 的三位创始人一样是技术出身，他是销售出身，创业之路非常艰难。</p><p>这里要同样要感谢 IBM。IBM 在决定采用 Intel 的 8088 做 PC 机的时候，为了防止因生产问题而出现断供，几乎是强制地促成 AMD 成为 8088 的第二供应商。Intel 向 AMD 开放技术，全面授权 AMD 生产 x86 系列处理器，而 AMD 则放弃了自己的竞争产品，成为 Intel 后备供应商。</p><p>8086/8088 处理器不仅仅是 AMD 在生产，它们的生产厂商还包括前面提过的，靠存储器做过 7 年行业老大的日本电气 NEC，以及富士通 Fujitsu、美国军工企业 Harris、日本老牌企业 OKI、德国西门子 Simens AG、德州仪器 TI、日本三菱 Mitsubishi。为什么这么多日企？前面在讲惜败存储器市场的时候说了，日本政府给力，1980 年代，半导体公司前十名，日企占 6 名。日本半导体业的兴衰史，可以单独开一节，这里不提。</p><p>1985 年，Intel 在做出放弃存储器业务，以处理器为主的决策之后，第一个操作就是“停止授权”，同时挥舞专利大棒清除那些克隆 8086 的半导体厂商，它意图也很明确，就是爷要摆脱备胎们，独霸整个处理器市场。</p><p>其他克隆 8086/8088 处理器的半导体厂商，基本都退出市场。只有 AMD 据理力争，要求 Intel 继续执行 1982 年签署的有效期为 10 年的“交叉授权”协议。为此，双方打起了官司。</p><p>这场官司旷日持久，最终判决直到 1995 年才兑现。AMD 从 Intel 那里得到了 1000 万美元经济补偿，并于 1993 年拿到了生产 386 处理器的资格，Intel 则从 AMD 这边总计得到了 5800 万美元的专利授权费。</p><p>其实，对 Intel 而言，5800 万美元的专利费可以说微不足道，重要的是，在微处理器和 PC 高速发展的关键时期，Intel 借此成功地抑制住了 AMD 的发展和壮大。</p><p>虽然 AMD 错过了 CPU 发展的关键时期，但是<strong>总算是赢得了 x86 指令集的永久知识产权</strong>，可以继续设计 x86 的处理器。AMD 1996 年收购设计公司 NexGen，1997 年迅速推出 K6 处理器。在三个版本的 K6 之后，AMD 进入到有史以来最成功的 K7 架构节点。</p><p>1999 年，AMD 基于 K7 微架构的 Athlon 处理器诞生，综合性能超越同频的 Pentium III（奔腾），让所有的 DIY 用户为之震惊，后来，AMD 又抢先发布了 1GHz CPU。虽然 K7 最终版 Athlon XP 口碑很好，但因为主频落后，始终屈居在 Pentium 4 之下。</p><p>AMD 在 2003 年抢先发布了 64bit ISA 的 K8 Athlon 64，2005 年发布了双核 Athlon 64 X2，由 32bit ISA，升级到 64bit ISA，在单核升双核的两个关键技术节点上反超了 Intel。 2006 年第一季度，AMD 达到了它的第一个最高峰，占据将近 25% 的市场份额。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4aaed339bc282f64.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Intel 在这个期间，也是明星产品不断。在 486 之后，把 586 重新命名为 Pentium(奔腾) 系列。然后分离出针对工作站和服务器市场的 Pentium Pro（1995），就是后来的 Xeon(至强) 系列，同时继续推出 Pentium II（1997）、 Pentium III （1999），2001 年的 Pentium 4 把主频提到 2.0GHz，终于在和 AMD 之战中取得了性能的关键优势，后来演进为 Core（酷睿）系列。</p><p>其实在 90 年代末，Intel 还出了低端的 Celeron（赛扬）系列，和给予高度希望却不太成功的 Itanium（安腾）系列。不过，某种程度上，赛扬和安腾都是竞争之下的产物。Xeon、Core，才是 Intel 的主流产品线，市场最终还是看产品说话。</p><p>结束这场战斗的是 Intel 2006 年发布的高性能双核酷锐 Core，而 AMD 连续几代产品性能落后，又因为 2006 年收购 ATI 债台高筑，之后十年碌碌无为。Intel 稳稳地拿到了近 80% 的市场份额，坐稳了行业老大的位置，收入连创新高。</p><p>一个市场的双供应商，其实是一个非常有益的状态，生态可以共享共建，消费者在两者之间反复横跳，可以极大地促进创新。1997-2006 年，AMD 与 Intel 展开针锋相对的拉锯战，是处理器市场的黄金岁月，两家竞争的市场超级繁荣，技术创新速度更是飞快。</p><p>一方面体现在 PC 市场的出货量从 8000 万台，增长到 2.4 亿台，另一方面 PC 的平均价格从 2000 美金之上，下降到 726 美金（这是包括了笔记本电脑的平均售价），整个世界从中获益，中国的联想、方正，台湾的宏基，也是在这个时间段成长为一线系统厂商的。</p><p>PC 市场是个主战场，相邻的服务器市场上其实也很热闹。从 1980 年到 2005 年，x86 双雄与微软并肩，战胜众多古典 RISC 架构厂商。2005 年 Windows 服务器（就是 x86 服务器）的市场份额第一次超过 Unix 服务器市场份额，到达了逆转点。</p><p><strong>Tick-Tock 策略</strong></p><p>从 2007 年开始，Intel 宣布了著名的时钟“嘀嗒”(Tick-Tock) 战略模式，把摩尔定律具体化了。“嘀嗒”意为钟摆的一个周期，“嘀”Tick 代表芯片工艺提升、晶体管变小，而“嗒”Tock 代表工艺不变，芯片核心架构的升级。一个“嘀嗒”代表完整的芯片发展周期，耗时两年。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-39a8c8939dee90a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>错失移动互联网</strong></p><p>但是这个期间，Intel 犯了一个要命的错误，长期的第一，让它忽略了一个新兴的市场：智能手机处理器。它拿下了苹果的 Mac，却拒绝给 iPhone 设计处理器。如果有评选，这是 Intel 公司史上最贵的失误吧。Intel 花了 100 亿美金，试图纠正这个错误，仍然以失败黯然离场。</p><p>而且更糟糕的是，到了 2016 年，工厂在研发新工艺方面，频频出现失误、延期、良率问题，一句话总结就是：工艺的进步，突然停止了，Intel Tick-Tock 的摩尔时钟停摆了。</p><p>这时，Intel 尝试把 Tick-Tock 变成 PAO 策略，就是所谓的 Process（制程）-Architecture（架构）-Optimization（优化）三步走战略，3 年更新一次制程，但是实际上也并不成功。出现了 14nm 被打磨到 14nm+++ 的优化，优化，再优化的局面。</p><p>在 2016 年 -2020 年这段时间，Intel 还尝试通过收购 FPGA 厂商 Altera、以色列自动驾驶技术公司 Mobileye、AI 芯片设计公司 Habana 来尝试多元化的道路。但是，至少从财务报表上看，最近 3 年，就是 2018、2019、2020 年，收入最高的仍然是 PC 业务 CCG，和服务器业务 DCG，其它业务的收入，相比之下，都低了一个数量级。</p><p>到这里，我总结一下。错失手机处理器市场，对 Intel 是个损失，但是 Tick-Tock 的停摆才是致命伤。如果行业里的其它竞争者，可以搭乘台积电这样的工厂，保持摩尔速度，那么 Intel 再多的优势也会有被耗尽的一天。在半导体这个行业中，不出错的按照摩尔定律前进创新，才是一个公司的安身立命之本。</p><p>历史真是有轮回性，2007 年帕特·基辛格向全世界介绍了 Tick-Tock 战略。2021 年 3 月 23 号，回归 Intel 出任第 8 任 CEO 不到一个月的帕特，向全世界宣布要重启 Tick-Tock 战略，以及要升级 IDM 模式到 2.0。</p><h3 id="三星：资本与技术共舞的40年"><a href="#三星：资本与技术共舞的40年" class="headerlink" title="三星：资本与技术共舞的40年"></a>三星：资本与技术共舞的40年</h3><p>有“最懂经济的市长”之称的黄奇帆说，一个公司的成功三要素是：市场、资金和技术。每个公司都想三要素齐备，但是如果因地缘问题，没法拥有一个有规模的本土市场，那就得在资金和技术方面多下功夫，以资金和技术赢得市场。三星就是这方面的楷模，一手技术，一手资本，打造了业界独一份的端到端的完整产业链。</p><p>2017、2018 年三星电子的半导体业务分别以 659 亿美金、832 亿美金的销售额，超过我们上一讲介绍的 Intel，登顶世界半导体公司的第一。</p><p>三星电子靠的是什么呢？靠的是存储芯片，就是让 Intel 败北的存储芯片业务。</p><p>单从三星电子的半导体业务看，三星电子很像 Intel，是 IDM 模式，不过它是靠多年存储芯片市场第一，站稳半导体行业第二的位置的。但三星电子并不仅仅止步于存储芯片，它还控制着全球手机产业链的命脉，除了手机的存储器外，手机的液晶面板也做到全球第一。而且三星电子的手机业务，也排名世界第二。从这个角度看，三星电子很像苹果，它出手机，还自己设计手机应用处理器。</p><p>但是与苹果专注于手机业务不同，三星电子深度参与产业链，无论是液晶面板、存储芯片，还是处理器，都供应给其它手机厂商。2017 年，三星电子把芯片代工业务独立出来，跟台积电竞争，很巧的是，芯片代工这项业务，三星电子也做到行业第二。</p><p>三星电子绝对不是一个简单的“苹果 +Intel+ 台积电”的三合一的公司，它的商业策略和技术路径都是非常独特的。它从制造开始，沿着产业链一步步攀升，直至集大成的智能手机。这是一个真正的端到端打通，拥有完整产业链的一家垂直公司。而看上面三家公司，苹果没有工厂，Intel 没有手机，台积电只有制造。</p><p>三星电子的成功，不只是技术领先，或是大手笔投资，单方面因素成就的，这是资本与技术长期相辅相成的结果。</p><p>下面我画了一个 2020 财年三星电子的收入比例图。可以看出存储芯片业务在三星电子中占据了举足轻重的位置，同时三星存储芯片业务在全世界半导体行业中也一样的至关重要。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-83012c3774d16651.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>另外，你还可以看看下面这张图，显示了存储市场的两个主要产品：DRAM 和 NAND Flash 在 2019 年的供应商市场份额。通过之前的学习，你已经了解了在存储芯片市场，DRAM 和 NAND Flash 大约占了 98% 的市场销售额。而在这两个市场中，三星都是有压倒性优势的第一。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-222919023e14186a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>既然说三星是一手技术，一手资本，打造了业界独一份的端到端的完整产业链，对于一个完整的产业链肯定不是一篇文章能讲清楚的，这一讲我就通过分析最重点的三项业务：存储芯片，代工和手机处理器来鸟瞰一下。三星强的地方还很多，例如拥有智能手机屏市场份额的 50%，MEMS 传感器排名第二，这还仅仅是三星电子，三星集团就更厉害了。</p><p><strong>三星存储芯片业务：逆周期投资</strong></p><p>而 1983 年三星电子才刚刚开始，三星从当时规模还小的美光 Micron，拿到了 64K DRAM 的技术授权。技术授权，往往是行业后来者迅速赶上行业先进者的行之有效的方式。但是 1984 年，三星刚推出 64K DRAM，内存价格就暴跌，直接遭遇到行业的第一次衰退周期。当时内存价格从每片 4 美元跌至每片 30 美分，此时三星的成本是每片 1.3 美元。</p><p>换句话说，每生产 1 片亏损 1 美元。到 1986 年底，累计亏损 3 亿美元，当时三星电子的股权资本全部亏空，要靠集团输血。</p><p>那在这次存储芯片行业衰退周期，三星是怎么实现转机的呢？在此期间，美国提供了超过 20 亿美金的资金援助，韩国政府不但组织产学研共同投资，也提供了 6000 万美金的研发资金支持。三星电子，仅仅用了 3 年时间就一口气掌握了 16K 到 256K DRAM 的关键技术。</p><p>到了 1987 年，苦熬多年的三星电子终于迎来行业转机。当年，美国向日本半导体企业发起反倾销诉讼，双方达成出口限制协议。受此影响，DRAM 价格回升，三星趁势崛起，不但实现了盈利，还开始在技术上领先日本。</p><p>1992 年，三星率先推出全球第一个 64M DRAM，并于当年超越日本 NEC，成为全球最大的 DRAM 制造商。两年后，又率先推出 256M DRAM。三星的崛起，还带动整个韩国形成一个内存产业集群，除了三星，韩国的现代（2001 年后改称 SK 海力士）也跻身世界前三强。</p><p>这里说一句，在 1993 年，全球内存芯片市场又开始下滑，迎来第二次衰退周期，三星再次采用逆周期的投资策略，投资兴建 8 英寸硅片生产线来生产 DRAM。相反，日本因为房地产泡沫破灭，整个行业退缩，最后在政府的主导下，将半导体行业企业一分为二，日立、NEC、三菱的 DRAM 部门整合成立了国企尔必达。</p><p>这里重点说一下“逆周期”投资策略，要想“逆周期”投资，首先你得有钱。所谓逆周期，就是行业处在下行周期，不赚钱，甚至赔钱的时候，还进行投资。那投资的钱，从哪里来？这时候三星多业务的优势就体现出来了，存储业务在下行周期，但是还有手机业务在赚钱。三星集团的持续输血，让三星电子的逆周期投资成为可能。其次，你要有勇气，就是在一个业务赔钱的时候，“做赔钱生意”的勇气，这是三星最难得的意志，是对技术的笃定，也是对长期精神的信仰。</p><p>行业的第三次衰退周期随着 2008 年金融危机爆发到来，DRAM 价格雪崩，从 2.25 美元狂跌至 0.31 美元。就在此时，三星把握机会，再次做出逆周期投资的决策：将三星电子上一年的利润全部用于扩大产能，故意扩大行业的亏损。很快，DRAM 价格就跌破材料成本。最先破产的是德国巨头奇梦达，这也成就了紫光集团接手奇梦达的机缘。</p><p>日本的尔必达苦苦支撑数年，最终于 2012 年被美光收购，另一巨头东芝的闪存业务，也在 2017 年被美国贝恩资本收购。自此日本彻底退出存储芯片业务。整个 DRAM 行业只剩下三星、SK 海力士和美光三大玩家。</p><p>进入 2016 年，在大数据、云计算、比特币挖矿等需求的带动下，存储芯片进入“超级上升周期”，三星存储芯片的业务收入，从 2017 年一季度到 2018 年一季度，增长 118%。三星电子的半导体业务在 2017、2018 年超越 Intel，成为行业第一。同样受益于存储芯片的周期，2017 年，SK 海力士、美光也分别上升为整个半导体行业的 3、4 名。</p><p>让我们复习一下世界半导体公司前 10 强，和它们对应的市场份额。三星、SK 海力士、美光，存储行业的前三的营收，基本上占整个半导体行业的 1/4。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5607b6c4ffd0e720.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在存储业务上，三星一手靠技术，自 1992 年起，三星的每一代产品、工艺都是领先竞争对手的；另一手靠大规模投资，特别是逆周期的投资，不仅仅帮助自己成为行业第一，而且还带动了韩国内存产业群。</p><p><strong>三星代工业务：后发先至的逆袭之路</strong></p><p>三星代工业务，一方面是存储芯片业务积累的先进制造工艺能力输出，另一方面逻辑芯片对于制程的提升，也反过来让其它业务收益。这是垂直型公司的优势，有关联的业务之间，商业上可以抱团一起赢单，技术上可以相互提升能力。</p><p>三星的代工业务开始于 2005 年，一开始不过是产能与技术的溢出。在存储芯片业务的下降周期时，把自己过剩的产能和技术拿出来，卖点钱增加半导体业务部门的收入。这个起步时间相对于台积电，非常晚。但是从 2005 年到 2020 年的十几年内，三星的代工业务，一路 PK 掉 20 多家公司，成为唯二提供 7nm 工艺的两家代工厂之一。而且三星在 GAA（Gate-All-Around，环绕式栅极技术）的最新技术研发上，有领先之势。</p><p>代工业务，也是一项既拼制造技术，又拼投资的业务。三星在代工业务上，一手抓技术赶超，一手放手投资，后发先到的逆袭之路，才是我国产业界需要学习的地方。</p><p>前面提到，三星进入代工业务的底气，来自于在存储芯片的竞争中，建立起的先进工艺的制造技术。但是三星的代工业务，面对每一个节点上的大客户订单，似乎都是生死存亡之战。</p><p>在 28nm 制程上，台积电是 2011 年最先进行 28nm 制程量产的，然后在 2012 年攻克了 28nm HKMG 制程。三星则是在 2012 年实现 28nm 的量产，并于 2013 年导入了 28nm HKMG，而 GF 公司和 UMC 台联电则在 2013 年才开始量产 28nm 芯片。至此，从技术角度看，三星晶圆代工业务已经进入前二。</p><p>而在 2014 年，苹果把 A8 的订单转给台积电，这大大的刺激了三星加快转进 20nm 及以下先进制程的研发。2015 年 2 月，三星率先进行 14nm FinFET 芯片量产，重新拿回了苹果订单。历史上，非常罕见的出现了，一颗芯片 A9，同时在两个代工厂生产的局面。</p><p>这样的同款手机不同处理器的对比是非常残酷的。很快就有用户发现了，苹果 iPhone 6s，同样的 A9，三星版本和台积电版本续航能力有差异，虽然苹果官方声称差异仅有 2%-3%，但是在专业测评机构的极致测试情况下，显示有 20%-30% 的差异。这次与台积电的较量失败，明显伤害了三星的代工业务。</p><p>接着的 10nm 制程，三星落后于台积电。于是三星率先在 7nm 节点导入了 EUV 技术，却再次因为良率问题，错失先机。目前，三星宣布跳过 4nm，采用 GAA 架构实现 3nm，试图在 3nm 这代反超。</p><p>三星技术上激进，资金投入上更是。相对来说，台积电是小心翼翼地在维护着投资与资金流的平衡。三星在晶圆代工业务上疯狂进行扩张，其依仗就是高额的研发投入。</p><p>而纵观三星历年来的年报，2010 年时，三星对半导体的投资就首次超过了 100 亿美元，此后三星在半导体的投入也一直维持在百亿美元以上。IC Insights 曾经发布的调查报告显示，三星的半导体资本支出由 2016 年的 113 亿美元，增长到 2017 年的 260 亿美元，其投资金额已经成为了 Intel 及台积电全年资本支出的总和。</p><p>2019 年 4 月三星电子又宣布，计划在 10 年之内投资 1160 亿美元用于推动半导体领域的研发和生产技术。虽然台积电也投资不小，但是台积电的市场份额是三星的 3 倍。相对市场份额来说，三星绝对是按照更大的目标进行的投资。</p><p>其实，在 Intel 那讲，有关于半导体公司是保持 IDM 模式好，还是“设计 + 代工模式”更好的讨论？在三星这里，做存储芯片的制造和逻辑芯片制造，其实还是有不同的。如果三星把自己的目标仅仅定位为存储第一，甚至行业第一，那代工业务都不是那么重要。但是如果从端到端产业链的完整性上，晶圆工厂，特别是先进制程的晶圆工厂，是阿喀琉斯之踵。没有的话，那这个产业链就是不完整的。</p><p>再次引用同一位业内大佬的话，“国家与国家之间的竞争，是端到端产业链能力的竞争”。</p><p>说到 IDM 模式，三星的工厂，在最开始的时候，是给自己的高端手机 SoC 使用的。甚至到目前为止，三星自用的比例仍然超过 50%。所以，三星的工厂到底是产能与技术的溢出？还是有实力冲击行业第一的核心竞争力？我觉得这个问题，三星也是到了 2018 年，才明确下来的。三星要冲击行业第一，把工厂的制造能力打造成最核心竞争力。</p><p><strong>三星的手机处理器</strong></p><p>三星做手机处理器的起始点并不高，是从给苹果做定制服务开始的。</p><p>而且据说苹果并不满意三星的定制芯片，因此才推出自研的 A4 处理器。客观的说，三星做了苹果要求的产品，但是苹果没有得到自己想要的处理器，这是苹果架构团队的问题，不是三星的问题。</p><p>但是问题在于，三星的重心到底是做手机业务，还是在做手机处理器业务？对比苹果，苹果的策略很清晰，它专注在手机业务，做手机处理器只是为了更好地做手机。苹果的手机处理器不外卖，苹果的手机也不外购处理器。三星的手机，既用自己的处理器，也用高通的处理器。三星的手机处理器，也对外卖给小米、OPPO。无论是同机型不同处理器的配置，还是给竞争者供应手机处理器的操作，都挺迷惑的。</p><p>其实，三星的这种全产业链模式的弊端就在这里，例如它替苹果代工，生产手机处理器，又同时自己也设计手机处理器，虽然三星自己声称部门与部门之间，有很好的保密体系，但终究是瓜田李下。2012 年苹果起诉三星专利侵权，就是这种生意模式的弊端的体现。</p><p>对于大手机品牌，拥有自己的处理器肯定是有助于竞争的。</p><p>对于手机处理器设计部门，供货给自己的手机部门，自产自销天然有优势，而且手机部门的系统知识，对设计部门收益良多。但是，手机处理器部门的利润和价值，也往往被手机部门所淹没。</p><p>手机处理器，是技术和市场偏重的产品。前面说了，三星擅长技术和资金。但它本土市场有限是一个硬伤。美国有苹果，也有高通；中国有华为、小米、vivo、OPPO，中美都是有很大规模的本土市场的。没有一个有规模的本土市场，必然要拿其它的两项来补齐。</p><p>三星在手机芯片这块，因为想在技术上领先，在 2015 年到 2019 年四年时间，尝试了自研的 CPU 核：猫鼬，猫鼬虽然曾经阶段性超过高通，但是 ARM 的公版 CPU 性能更为优秀，且更新节奏稳定，在高通转向 ARM 公版之后，三星团队最终也放弃了投资巨大，但是收效甚微的自研 CPU 核的努力，也回归 ARM 的公版核路线。</p><p>不过，三星在技术上的追求并没有停止。2019 年在宣布放弃自研 CPU 核之后，2020 年，有爆料称，三星和 AMD 在合作研发移动 GPU。果然，2021 年三星官方证实了这一消息。CPU 这条路不通，就试 GPU 这条路，三星一直是一个屡败屡战的战士。</p><p>而且三星不仅仅有处理器，还有基带芯片、图像传感器、IC、DDI 等产品，都让三星对手机设计的任何一个新动态，有敏锐的感觉。我想说，一个企业有市场前瞻性，有技术能力，持续迭代，仍然在市场中，就会一直有机会。</p><p>手机处理器有三大技术能力：CPU、GPU 和基带，三星在 CPU 方向，4 年花了上千亿人民币，最终以失败退场；三星和 AMD 的合作，算是三星在 GPU 上的努力，效果尚未知。在基带方面，2020 年底三星虽然发布了首款 5nm 工艺 5G 基带芯片，但尚未自用。</p><p>跟排在行业第一的存储芯片业务，以及稳居第二，冲击第一的代工业务相比，手机处理器这个品类，算是三星的半导体业务中唯一徘徊的业务了。</p><h3 id="台积电：摩尔定律的忠实执行者"><a href="#台积电：摩尔定律的忠实执行者" class="headerlink" title="台积电：摩尔定律的忠实执行者"></a>台积电：摩尔定律的忠实执行者</h3><p>前两讲，我给你讲了 Intel、三星这两家公司，一个是处理器芯片第一，一个是存储芯片第一，都是好公司，都是业界楷模。如果 Intel 和三星出现意外停工停产的话，估计整个半导体行业会忙乱一阵，疯狂补位，瓜分它们的市场，重新排定座次；行业之外，都不会有什么太大影响。</p><p>但是，今天讲的台积电就不一样了。如果台积电出现点问题，那整个半导体行业，可能死一半吧。行业之外，别的不说，手机价格应该会贵一倍，或者只有三星代工的芯片可以买了。</p><p>当然我说得有点夸张，不过这是有前例的。2018 年 8 月 7 号，因为电脑病毒的攻击，台积电出现全线停产的问题，影响到已经定在 9 月 12 号上市的 iPhone 9 所需要的 A12 处理器的生产。当时 iPhone 9 的市场价，很快就被黄牛们炒到离谱的地步。</p><p>那个时候，台积电决定优先生产苹果的 A12 处理器，将其它的处理器推后生产（其实就是华为麒麟）。当时台湾媒体，一直夸奖华为非常有绅士风度。因为华为的处理器也是由台积电生产的，而且华为的处理器生产量也不比苹果少，加之华为新机的发布也并不比苹果晚多久。由此，苹果和台积电都欠了华为一个很大的人情。</p><p>华为的礼让，是有回报的。2019 年在美国发布实体清单之前，台积电积极帮助华为。它和高通、联发科、AMD、英伟达等公司沟通，请他们释放出来一部分 7nm 芯片产能来给华为公司应急生产制造。</p><p>其实台积电不仅仅是半导体行业的咽喉要处，它还改写了半导体产业的格局。它是摩尔定律的忠实守护者，在 Intel 无法维持 Tick-Tock 的行业节奏之后，是台积电和苹果搭队，默默地为整个行业提供持续创新动力。其实在第四节我有提到，半导体一半设计，一半制造。在制造这方面，台积电以一己之力，支持了半个行业。这一讲，就让我介绍一下这家无冕之王。</p><p><strong>重塑产业格局：激励创新</strong></p><p>在第四节有提过，在台积电出现之前，半导体公司都是 IDM 模式。从设计到制造全部自己完成。AMD 的创始人兼第一任 CEO 曾经说过“真男人都有工厂”。在很长的一段时间内，芯片设计和工艺制程两者是紧耦合的，被看作是半导体公司的核心竞争力。行业老大 Intel，至今仍然坚持这条路线。</p><p>但是，让我给你一个数字，你就能清楚的知道，如果半导体公司采取 IDM 模式，需要什么样的实力。</p><p>一家并不是最新工艺的晶圆厂的建设成本为 30 亿美金，每年的折旧成本 + 运营成本约为 10 亿美金，这意味着半导体公司的营业额，必须在 50 亿美金以上。以 2020 年无厂半导体设计公司的收入来看，只有 5 家公司：高通、博通、英伟达、AMD 和联发科够这个标准（排在第 6 的 Xilinx 收入是 30 亿美金），但是这 5 家头部设计公司，全都是用的建设成本至少 100 亿美金起的先进工艺进行设计。</p><p>台积电开创了芯片制造的代工业务，让半导体行业可以在 IDM 模式之外，出现“无厂设计公司 + 代工工厂”的模式。甚至，原有的 IDM 模式的公司，也可以有选择的保留部分工厂，而将其它的芯片生产制造，特别是昂贵的先进工艺制造，外包给代工工厂，变成“轻工厂”模式。</p><p>无厂设计公司，大大的降低了半导体公司的创业资金门槛，促进了行业繁荣。换一个角度描述，如果没有台积电开创的这种 “无厂设计公司 + 代工工厂”的模式，大约 70% 的半导体公司都不存在了。可以说，没有台积电就没有目前的半导体行业格局。</p><p>但是，开创一个新的产业模式从来都不是一件容易的事。在台积电之前，从来没有一个专业的只从事晶圆制造的工厂。晶圆厂的成功，是要产能满负荷运转的。即使到了 1995 年，有专业研究机构统计，全世界的设计公司的芯片，全给台积电，也不够喂饱台积电。</p><p>无厂设计公司 + 代工厂，这个模式想运转起来，需要三个条件：</p><ol><li>代工工厂的工艺水平不能太差，不能拖设计的后腿，而这个“差”，是跟同期的 IDM 的工艺水平相比较的。</li><li>设计公司的设计水平也不能太差，而且要有一定的出货量，才能填满代工厂产能。</li><li>与同一个市场中竞争的 IDM 模式相比，要有良好的投资回报比，才能持续发展。</li></ol><p>换句话说，台积电要成功必须得同时满足这三个条件，分别代表了三个层面：要有工艺水平、有客户需求、要比 IDM 模式赚钱。 事实上，它也很好地完成了。我来一条线一条线跟你说。</p><p><strong>提升工艺水平，成为行业第一</strong></p><p>在台积电建立的头十年，在工艺技术上与当时的德州仪器、摩托罗拉等欧美大型 IDM 相比，还是有差距的。它真正的崛起有两个转折点，经过这两个转折点，才逐步奠定了它的地位。</p><p>第一个转折点是 2000 年，台积电在铜铝工艺切换的时候，开始自主研发基于铜制程 0.13 微米技术，而不再仰仗欧美公司的技术转让。正如英伟达的 CEO 黄仁勋说：“0.13 制程改造了台积电。”领先的制程工艺，让台积电成为代工厂的技术第一。</p><p>第二个转折点是 2007 年，台积电的林本坚提出浸没式光刻设想后，ASML 开始与台积电合作开发浸没式光刻机，并在 2007 年成功推出第一台浸没式光刻机。技术能力延展到制造设备侧，才是核心能力。而凭借这个能力，台积电决心“抢先半步”，走 40、28、20nm 的路线，自此和 Intel 的 45、32、22nm 路线分裂。接着创始人张忠谋的回归，大手笔投资 28nm 制程，将技术先进巩固为商业成功。</p><p>真正要到 2018 年，台积电按时按计划发布了 7nm 制程并进入量产，才真正反超了一直困扰在 10nm 良率问题上的 Intel，开始领跑整个业界。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e7c6f07f24e3b23f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>先进的客户需求成重要助推器</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5f453f9d7b17ca98.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>回顾早期历史，1995 年台积电和 Altera 就合作在美国建晶圆厂，为芯片设计公司提供代工服务了。对 Altera 这家公司，你可能不熟悉，这是 FPGA 双雄之一，2016 年被 Intel 收购。FPGA 对于半导体制造工艺非常重要，因为 FPGA 的构造是大面积的重复单元，因此往往可以用来测试工艺制程。而 FPGA 这个品类，是强依赖最新工艺来提升竞争力的，这也是后来 Intel 要收购 Altera 的原因之一。</p><p>接下来，我们看看台积电的客户们。</p><p>高通，不太主动披露它的芯片制造商，但是 1998 年，高通的基带芯片 MSM3000，就是用的台积电 0.35 微米工艺生产的。之前几代，可分别用的是 LSI、IBM 和 Intel。而且 2002 年，高通的 CDMA 产品线的最佳供应商奖项是颁给了台积电。这是在台积电拿下苹果订单的 10 多年前。</p><p>英伟达，是从 1995 年就投奔台积电了。漫长的合作历史上，常有精品，例如英伟达 2020 年 5 月发布的 Ampere A100，542 亿的晶体管，826 平方微米的 N7 工艺单 Die，这个芯片做成 PCIe 卡之后，售价为 12500 美金。</p><p>在英伟达之前，只有博通能设计这么大的芯片了。博通也是台积电的很早期客户。</p><p>2014 年台积电推出了基于 FinFET 工艺的 20nm 芯片。这一年台积电拿下了最重要的一张订单：苹果的 A8。从此，苹果成为每年为台积电贡献超 20% 收入的超级大客户。</p><p>苹果手机芯片一年一代的更新节奏，反向制定了台积电的工艺升级的节奏。而台积电的工艺制程升级的节奏，也给全行业定了基调。其实从 2016 年，Intel 的 Tick- Tock 停摆之后，半导体行业的速度是台积电确立的，稳稳的摩尔定律速度。</p><p><strong>与 IDM 模式竞争，保持产能与需求之间的平衡</strong></p><p>其实，在台积电最初创立时期，Intel 是帮了台积电一把。但是，代工这种模式，本质是在和 IDM 模式的厂商在竞争制造机会。</p><p>到了 2000 年，dotcom 泡沫破裂，很多半导体公司的投入减少。同时，12 寸厂成为新建晶圆厂主流，但一座晶圆厂造价高达 25 至 30 亿美元，不仅中小 IDM 负担不起，大型 IDM 要投资也常显吃力。此时台积电的技术实力也已经超越 Intel 之外的其它 IDM。</p><p>2010 年，IDM 已较少自己生产 40nm 制程的芯片了，而台积电成功地预测到“做到 28nm 时几乎所有 IDM 都要靠代工”， 因此大规模投资 28nm 的产线，一举奠定胜局。</p><p>整体而言，IDM 放弃自建晶圆厂已是大趋势，比如 AMD 便在 2008 年底将芯片制造剥离出去。近 20 年来只有所谓“轻晶圆厂”（fab-light）或“无晶圆厂”（Fabless）模式的兴起，而没有芯片设计公司反过来成为 IDM。</p><p>总结看，IDM 模式，无论从产能预测、保密，还是工艺技术与设计结合的角度看，全部自己完成制造，有优势。但是，靠一个公司的产品填满产能，这对公司的研发能力有超强要求。台积电制胜的秘诀就是用最快的速度、最好的制程生产最多的芯片。</p><p>当台积电可以提供给全行业最好的制程的时候，全行业的产能都自发的集中过来。而且在制程领先这个条件下，客户，特别是大客户关心的优先级，就是产能优先，价格第二了。特别在同一个市场拼杀的竞争者，每个都恨不得把产能全包，一个 Wafer 都不给其它人剩下。因此台积电的利润率，一直行业领先。</p><p>“让晶圆厂持续高产能运转”，在产能和需求之间保持平衡，这是一项艺术。</p><p><strong>稳健投资：摩尔定律的忠实执行者</strong></p><p>“摩尔定律”使晶圆代工企业不断追求更先进的制程，而能在一两个原子大小的空间中雕刻回路的设备自然越来越昂贵，动辄数千万美元。为达到规模经济以抵消前期巨额投入，晶圆代工企业又追求更大的工厂和产能。</p><p>台积电在不断追求更先进的制程路上，靠得不仅仅是技术的创新，还有不被行业繁荣与萧条的波动左右，持续大手笔投资的长远眼光。最难能可贵的是，我们这里说的投资不是像三星一样的逆周期投资，台积电用的是自身赚到的资金， 以远高于行业其它公司的金额，长期投资。在台积电精致地平衡业务赚到的营业现金流量（正现金流）和投资活动付出现金流量（负现金流）下，按照金融相关人士说，台积电“画出了一张教科书般的收支图表”。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8a3beb8b2edf05e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>比起三星的大开大合式、用其它业务补贴式的投资策略。台积电这种稳健的投资，在研发和制造均需要巨额资金投入的半导体行业里，确保了自己的长期健康发展。</p><p>台积电曾在提出的财务指标中强调，2018 年的投入资本回报率（ROIC）必须达到 15% 到 20%。投入资本回报率是显示投向业务活动的资金能在多大程度上产生回报的指标，台积电过去 5 年的平均值为 21.4%，高于 Intel 和三星，Intel 为 15.7%，三星约为 8%。</p><p>既赢，又稳，这背后离不开领导者的经验、魄力与前瞻性。多年来，台积电一直保持着“高投入 - 低成本 - 高销量 - 高投入”的正向循环。</p><p><strong>苹果：需要单独讲讲的超级客户</strong></p><p>整个计算机硬件行业，有 3 次风口：第一次是 PC 机，第二次是手机与移动互联网，第三次是这一轮的 AI、5G 与 IoT。第一次的 PC，成就了行业第一的 Intel。第二次的手机浪潮，成就的就是台积电，而这里的关键票，是苹果投下的。</p><p>苹果并不是一个半导体设计公司，苹果的前三代处理器，都是由三星设计并且生产的。直到 A4，苹果才进入自主研发的阶段。但是这个时候芯片生产还是三星在做。三星的代工价格，基本上是成本价，并且和存储芯片做了捆绑销售包，和只有代工服务的台积电相比，优势明显。因此苹果的 A4、A5、A6、A7 也都是三星生产的。</p><p>但是三星的问题是，它的手机业务和苹果有竞争关系。三星 Galaxy 系列手机搭载的自己生产的猎户座（Exynos）系列手机处理器，看起来跟苹果的相似度太高。这时候问题就出现了，我们讲代工业务有一个特点，代工厂可以拿到设计公司的最终成果，因此客户对代工厂的信任度必须要高。2011 年苹果正式起诉三星 Galaxy 系列产品抄袭 iPhone 和 iPad，三星又反起诉苹果侵犯其 10 项技术专利，苹果与三星的专利诉讼战几乎遍及全世界，这严重破坏了信任度。</p><p>台积电这边在争取苹果订单的时候，一方面遇到价格问题，台积电是一个纯代工工厂，肯定不能像三星那样，只是拿苹果的订单解决剩余产能，可以接受成本价。另一方面，台积电的产能基本满载，如果想拿下苹果订单，就需要给苹果单独建厂。另外，这里还涉及知识产权等问题，也是困难重重。最终台积电 20nm 制程领先三星，且产能扩张完毕，用实力说话，才首度拿下 iPhone 6 的 A8 处理器全部订单。</p><p>而三星并不是一个会轻易认输的公司。它直接跳过 20nm，首先宣布量产 14nm，重新赢回了苹果 A9 的部分订单。这对刚刚扩大产能的台积电来说，真是个大危机。但正如我说过的，危险和机会，总是并存的。我在讲三星的时候说了，iPhone 6s 装载三星版 A9 和台积电版 A9，被发现在功耗上有显著的差异：台积电的芯片明显较三星的省电。对三星来说，这是一个悲剧，相反，台积电因此一战成名。从此，苹果的订单，再也没有离开过台积电。</p><p>从官方披露的信息上看，苹果将在 2021 年推出 5nm+ 的 A15 处理器，2022 年推出 3nm 的 A16 处理器，2023 年推出 3nm+ 的 A17 处理器，目前，台积电和苹果正在携手研发 2nm 芯片，预计 2024 年推出 2nm 的 A18 处理器。</p><p>为什么我说苹果是“超级客户”？2019 年、2020 年，苹果都是世界上最大的半导体产品买家，台积电所有业务中有 25％直接或间接来自苹果，除了代工苹果的自研芯片以外，苹果购买高通、博通、AMD、德州仪器、意法半导体、恩智浦和瑞萨等公司的产品也是台积电代工。想想，三星是一直手机出货量第一，是半导体产品的第二大买家（前 2019 年之前是最大买家），如果没有拿到苹果这一超级客户，后果不敢想。</p><h3 id="Nvidia-与AI-芯片：超越摩尔定律"><a href="#Nvidia-与AI-芯片：超越摩尔定律" class="headerlink" title="Nvidia 与AI 芯片：超越摩尔定律"></a>Nvidia 与AI 芯片：超越摩尔定律</h3><p>另外要提一下，财经投资的朋友们可不是只动嘴巴夸夸英伟达，他们真的下场投资。如果论营收，看 2021 年 4 月中旬的数字，Intel 是英伟达的近 5 倍，但是如果论市值，英伟达是 Intel 的 1.5 倍。为什么？凭什么？营收算是行业内部给予一个公司的评价，真金白银的买产品；而市值，就是大众对于公司的社会价值的肯定，那么大众肯定的是什么？是跟我一样，看到的是超行业、超摩尔定律的速度？还是大众对于 AI 这个大浪潮更有想象力？</p><p><strong>GPU 开局</strong></p><p>让我先从英伟达的主要产品说起。看看下面这张英伟达财报中的表格，2020 年，英伟达收购成功 Mellanox，表格上“Compute&amp;Networking”那一行的收入就来源于此，如果不算这部分，英伟达就是一个 GPU 的公司。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c729f68cb2203c7d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>关于绘图的任务，我可以给你讲讲大概的原理。现在的显示器分辨率是 1080p，就是说这个显示器上的所有的图形，都是由水平方向 1920 行，垂直方向 1080 列的光点矩阵组成，算一下 1920x1080，也就是 207 万个光点组成。</p><p>这些光点会被记录成一个二维数组，就是一张图。每一个光点，专业上称为像素，是由红绿蓝三种基本颜色调和而成。而这三个颜色，都是以 0-255 之间的一个数字表示。也就是一个像素，就是 3 个数字，例如一个红色的像素，就是（255，0，0）。那么，要生成一张红色的图，在 1080p 的分辨率下，就需要提供 207 万个（255，0，0）红色像素的数组。而且这只是一张静态的图像，如果是视频，每秒有 60 帧图像，那一秒就需要处理上亿个像素了。</p><p>其实，在 GPU 处理图像，特别是 3D 图像的时候，倒不是一个像素一个像素处理的，而是把 3D 图形转换成可以在 2D 屏幕上展现出来的，由顶点构成的无数个三角形。然后，根据每个三角形的三个顶点，把这个三角形所覆盖区域换算成像素，然后再做颜色效果，基本上就得到了屏幕上的最终效果。</p><p>从我上面说的图像处理的原理，可以看到，绘图任务需要并行处理海量数据，这对擅长做串行数据处理的 CPU 来说，既不合适，又负担很重。于是在 1980-1990 年代，图形加速卡这种外设开始出现。当 1993 年黄仁勋创建英伟达的时候，市面上有 100 多家图形加速相关的公司。这是一个可怕的在性能、标准、市场层面的混战，当时的市场主要是游戏和 PC 市场。</p><p>当然 PC 和游戏市场的残酷，也给了英伟达两个非常宝贵的经验：第一，超摩尔速度，就是被业界称为“黄氏定律”的：“半年更新，一年换代”；第二，就是软硬件之间的标准的重要性。</p><p>在游戏市场，英伟达和竞争对手竞争的早期，微软的 Direct X 可以说是能左右游戏开发生死的翻云覆雨手，它是一套优秀的应用程序编程接口，既为软件开发者提供与硬件的无关性，又为硬件开发提供策略。说白了，它就在做软硬件之间的标准。</p><p>这也是英伟达花 10 年苦功，投入 CUDA 软件生态建设，把软硬件之间的标准，移到了自己一侧的动力。关于 CUDA，我下面会讲到。</p><p>对于 GPU 开局这部分内容，当然我的重点不是分析传统的 GPU 市场，老实说，回顾英伟达的公司历史，至少从股价市值的方面，市场给予了 GPU 这个产品正常认可度。在 2016 年之前，英伟达的市值和跟营收曲线，基本上相符。GPU 这个产品存在的价值，就是从 CPU 上卸载图形处理工作，是依附在 PC 和游戏机这两个市场的。因此长期以来，英伟达是被放在 Intel 这个坐标系下评价的。这也是为什么如果固守 PC 和游戏机这两个已经成熟的市场，英伟达的发展有限。</p><p>英伟达的转折点，或者用理论一点的术语描述，英伟达的第二曲线是我下面要说的 GPGPU 与 CUDA。</p><p><strong>GPGPU：点亮并行计算的科技树</strong></p><p>2007 年，英伟达首席科学家 David Kirk 非常前瞻性地提出 GPGPU 的概念，把英伟达和 GPU 从单纯图形计算拓展为通用计算，强调并行计算，鼓励开发者用 GPU 做计算，而不是局限在图形加速这个传统的领域。GPGPU，前面这个 GP，就是 General Purpose 通用的意思。</p><p>而且英伟达这样一个芯片公司，以 12 分的努力和投入，开始建设 CUDA 这样的软件框架。GPGPU 和 CUDA 让英伟达从计算机图形学，开始走出来，把并行计算这个重要任务记在自己的名下。</p><p>这里我要重点说一下 CUDA（Compute Unified Device Architecture，统一计算架构），CUDA 不仅仅是一个 GPU 计算的框架，它对下抽象了所有的英伟达出品的 GPU，对上构建了一个通用的编程框架，<strong>它实质上制定了一个 GPU 和上层软件之间的接口标准</strong>。</p><p>前面有提到，在 GPU 市场的早期竞争中，英伟达认识到软硬件之间的标准的重要性，花了 10 年苦功，投入 CUDA 软件生态建设，把软硬件之间的标准，变成自己的核心竞争力。英伟达可以说是硬件公司中软件做得最好的。</p><p>同样是生态强大，Wintel 的生态是微软帮忙建的，ARM-Android 的生态是 Google 建的，而 GPU-CUDA 的生态是英伟达自建的。</p><p>这个标准有多重要？这么说吧，一流企业定标准，二流企业做品牌，三流企业做产品。在所有的半导体公司中，制定出软件与硬件之间的标准，而且现在还算成功的，只有 3 个，一个是 x86 指令集，一个是 ARM 指令集，还有一个就是 CUDA 了。</p><p>2007 年，英伟达推出了基于 CUDA 的 GPGPU beta 版，之后公司的所有 GPU 都支持 CUDA，因此其实 GPGPU 和 GPU 可以完全通用。各种编程语言的工程师纷纷用英伟达的 GPU 进行开发，大大增强了 GPU 的开放性和通用性。而且因为英伟达是用 GPU 进行并行计算这个领域的第一，因此所有这个领域的软件、应用，都支持了 CUDA，CUDA 实际上成为并行计算的事实标准。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3fbf950f9622c22c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>GPGPU 提出之后，在提供算力这个方面上，每个英伟达的 GPU 都有了一个统一的框架。英伟达在算力和存储带宽两个方向上，对比 CPU，都以超 10 倍，甚至 100 倍的优势领先，如上图。GPU 相对 CPU 的 TOPS per Watt（花费每瓦特电能可以获得的算力）的差异竞争优势，它的本质就是将晶体管花在计算上，而不是逻辑判断上。</p><p>在提出 GPGPU 和做 CUDA 的时候，英伟达面向的科学计算，就是冲着 HPC 和超算这个市场去的。也确实挺成功的。2020 年超级计算机 TOP500 更新榜单，可以看到 TOP10 的超级计算机中有 8 台采用了英伟达 GPU、InfiniBand 网络技术，或同时采用了两种技术。TOP500 榜单中，有 333 套（三分之二）采用了英伟达的技术。</p><p>但是 HPC 和超算市场有限。有易用统一的 CUDA 编程标准，有算力和存储带宽都远超 CPU 的计算能力，有明确的技术优势，万事具备，英伟达的腾飞就差一个应用面更广大的通用问题了。</p><p><strong>AI 的风口下，英伟达腾飞</strong></p><p>深度学习依赖于数学和统计学计算。深度学习中的用到的各种神经网络，例如人工神经网络（ANN）、卷积神经网络（CNN）和循环神经网络（RNN），都是典型的并行结构，每个节点的计算简单且独立，但是数据庞大，通常深度学习的模型需要几百亿甚至几万亿的矩阵运算。因此深度学习的成功有两个要素：一，海量的训练数据；二，高速的并行计算能力。</p><p>几百亿甚至几万亿的矩阵运算，这一听，就很 GPU，因为 GPU 在进行图像渲染时，就是做大量的矩阵乘法运算。</p><p>每秒进行大量的矩阵乘法运算，这就是图形渲染和深度学习统一的地方。</p><p>统一的问题，给出统一的答案：英伟达 GPU。</p><p>搞算法的科学家，在用 CPU 的年代里无所作为，于是努力改进自己的算法，希望能少用一点计算，但并没有太大效果。后来，有人用上了 GPU，算法不变，算力提升 10 倍，效果立马就不一样了。混沌大学创办人李善友总是说，要注意差 10 倍的那种变量，一个方向上出现了差 10 倍的一个变量，一定有大事要发生了！</p><p>英伟达一直保持相对于 CPU 的 10 倍的计算力，这个 10 倍速变量，让它成为一个合适问题的关键答案，也完美地验证了超越摩尔定律的都是好公司的业界通识。</p><p>当然，作为从业人士，我其实是想提一个小悖论的。GPU 的计算性能比 CPU 高 10 倍，如果人类社会的任务比例是逻辑计算，就是适合 CPU，那些任务是 1，数据计算是 10 的话，CPU 和 GPU 的比例，是 1:1，英伟达的最高点就是跟 Intel 同规模。只有逻辑计算是 1，数据计算是 100，CPU 和 GPU 的比例，才是逆转的 1:10，英伟达才会有 Intel 的 10 倍规模。你可以理解我说的么？欢迎给我留言，我们一起讨论。</p><h3 id="德州仪器：模拟芯片的无冕之王"><a href="#德州仪器：模拟芯片的无冕之王" class="headerlink" title="德州仪器：模拟芯片的无冕之王"></a>德州仪器：模拟芯片的无冕之王</h3><p>今天我分享的这个老牌厂商，经过两次风口，起飞降落之后，最终决定另辟蹊径，在模拟器市场获得成功，它就是德州仪器。德州仪器的中国区总裁谢兵，曾经说过“企业的目标是做强而不是做大”，我觉得这句话非常精准的解释了德州仪器的公司策略。</p><p>让我还是从历史开始，看看德州仪器的三次转型历程。</p><p><strong>第一次转型：搭上半导体行业的火箭</strong></p><p>德州仪器成立在 1930 年，最初是一个小型石油和天然气公司，叫“地球物理业务公司”(GSI)。这么早的历史，让德州仪器有望成为第一个半导体行业的百年老店，毕竟半导体是在 1947 年才发明的。GSI 的生意一直不错，到了二战期间，又得到了国防电子产品的订单，开始将信号处理技术应用在潜艇侦测、空军雷达系统等产品上。</p><p>这里画一下重点：电子产品，信号处理技术，我们看到它在向科技方向转变了。</p><p>在国防电子产品上的经验告诉他们，半导体行业是一个极具前景的行业，这个“火箭”马上要起飞了。因此 1946 年，GSI 创建了电子设备实验室和制造厂，1951 年重组并更名为“德州仪器”。1954 年，德州仪器购买了生产晶体管的专利，直接杀入晶体管制造和销售的市场。</p><p>德州仪器在半导体发展早期，斩获颇丰：有 1954 年研制的第一个商用硅晶体管，还有 1958 年新员工杰克·基尔比研制出世界上第一块集成电路，德州仪器也是当时唯一能批量生产硅晶体管的公司。</p><p>接着德州仪器进入微处理器领域，但在微处理器领域，它遇到了 Intel。</p><p>德州仪器的开局，其实非常漂亮，两个行业第一，不亚于 Intel。但是德州仪器，并不是一个传统的半导体公司。它一开始就产品线很长。它有着电子设备商的特征，初期有很多消费电子产品，例如数字钟表、电子手表、便携式计算器、家用电脑等等。</p><p>一个“不专心”的半导体公司，遇到一个专心做芯片的公司，你认为结果会怎么样呢？</p><p>现在我们都知道结果了，Intel 是行业老大。而分析一场 40 多年前的产品竞争，我觉得也没有意义。因此让我们跳过过程，一起看看，在微处理器领域失败的德州仪器，后面又做了什么。</p><p>德州仪器在 1979 年，输掉了和 Intel 的微处理器的比赛之后，开始制定微处理器之后的未来路线。它制定了发展数字信号处理器（DSP）和嵌入式处理器的策略。1983 年，TMS320 DSP 系列及其衍生产品开始贡献德州仪器总利润的近一半，有非常多的电子产品采用。其实，从这个时间开始，德州仪器的产品就有着明显的多用户、多使用场景的特征了。</p><p>也就是这个 TMS320 产品家族中的 C5000 系列，在 1990 年代末被诺基亚和爱立信选中，德州仪器因此进入了手机市场。</p><p><strong>第二次转型：进入手机处理器市场</strong></p><p>在二十世纪 90 年代末到二十一世纪初，德州仪器一方面转让了国防、打印机、电脑、DRAM 等一系列业务，另一方面开始大力拓展手机芯片市场。这里给你说一个小八卦，当时在德州仪器任职，后来创建台积电的张忠谋，就是因为德州仪器放弃 DRAM 而离开的。</p><p>复习一下，我在介绍三星手机处理器的时候有说过，手机处理器，有三项主要技术：CPU、GPU 和基带。德州仪器的 OMAP 系列处理器，CPU 用了 ARM，GPU 用了 Imagination 公司的 PowerVR，跟后来苹果 A 系列用的是一模一样的，德州仪器自己家也有信号质量非常过硬的 GSM/GPRS，就是 2G 基带芯片，因此三项技术皆强。</p><p>客户方面，德州仪器拿下了 2G 时代最强的诺基亚和摩托罗拉，一时间风头无两。</p><p>但是，凡事都怕但是，这个时候高通催动信息世界升级到了让自己占有绝对技术优势的 CDMA 时代，就是 3G 技术。刚我们说手机应用处理器有三项关键技术：CPU，GPU 和基带。没有 3G 基带芯片的 OMAP 处理器，跟瘸了一条腿似的，根本无法在市场立足。基带，也是高通纵横手机市场多年的杀手锏，至今每个手机也还在交“高通税”。</p><p>好在德州仪器一直是一个多元化布局的公司。在手机芯片业务最好的时候，也没有耽误给车企造芯片，早在上世纪 80 年代，德州仪器就为福特和通用打造了名为 TLC542 的车载器件 8 位 ADC，2003 年还针对汽车推出了第一款信息娱乐系统，成为为数不多的几个汽车处理器供应商之一。</p><p>因此在决定放弃手机移动芯片之后，德州仪器义无反顾地杀入模拟芯片市场。当然德州仪器的能够转型成功，也是因为那些“新”方向本来就是其原有业务中的强势产品。即使在没有以模拟芯片为主的时候，1995 年它在模拟市场上，也是排名第五的。</p><p><strong>第三次转型：模拟市场加冕之路</strong></p><p>德州仪器的第三次转型之路，也是个非常经典的企业转型案例。它主要在三个层面上做了努力：第一是在公司经营层面，消减无关业务，从“大而全”转为“专而精”。第二，大举收购模拟芯片公司，扩大市场份额，产品品类，引进人才，快速确立自己的行业第一地位。第三，在产品技术方面，大举建厂并率先引入 12 寸晶圆制造工艺，带着数字市场的先进经验降维打击模拟市场的原有同行。</p><p><strong>转型：消减无关业务</strong></p><p>所谓转型，放弃之前的强势产品线，这个其实非常见管理功力。多元化的布局，也并不是什么都做。放弃，才能把资源聚集在最有竞争力的产品和市场上。我大概梳理了个表格，你可以看看从二十世纪初开始，德州仪器是如何消减数字芯片业务的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-07693a6020f44464.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>收购：市场份额与产品品类，引进人才</strong></p><p>整体来说，模拟芯片业务稳定，又赚钱，但从公司经营的角度看，它的问题是品类特别多，市场比较零散。其次模拟芯片的生命周期长，一个新的设计，必须要等待合适的时间窗才有机会替换老产品。还有，模拟产品非常依赖资深工程师，非标准化的设计和生产。这三方面综合起来，收购公司就可以快速提高市场占有率，增加产品品类，同时引进人才。</p><p>这里要说一下，美国的公司，一直非常重视通过收购和并购来增强公司的竞争力。麦肯锡曾经给出半导体产业的一个建议就是每个公司应该每年用自己市值的 3%-5%，来收购相邻技术的公司，扩张自己的领域。而德州仪器在收购公司方面一直比较活跃，我也列了一个德州仪器收购公司的时间表。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-debee2becc1ae676.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>2011 年德州仪器的模拟芯片销售额约为 64 亿美元，尤其是并购了美国国家半导体，给公司带来了 5 千名员工，以及多达 4.5 万种模拟集成电路产品和客户设计工具，极大程度地扩展了公司的模拟业务。</p><p>1995 年德州仪器在模拟供应商中还排在第 5 位，而从 2011 年收购美国国家半导体公司之后，模拟供应商的第一把交椅就一直就是德州仪器的了。</p><p><strong>建厂：构建最佳成本结构</strong></p><p>德州仪器的第一可不仅仅靠收购，在做模拟芯片这件事上，也是有独到手段的。</p><p>模拟芯片从产品技术上看，更高阶的工艺制程不一定能改善芯片的性能，一般公司会采用和设计匹配的工艺，因此模拟芯片以 8 寸产线为主，0.18um/0.13um 以上的制程。</p><p>但是从数字芯片市场拼杀过的德州仪器，则不同，它是抓住传统模拟芯片制造工艺落后的劣势痛点，引进 12 寸晶圆，这一手，就是数字 IC 中最崇尚的扩大晶圆，减小晶体管，虽然对性能并无影响，但是斩获了 40% 的成本优势（未封装的芯片的成本）。在一个不是很讲摩尔定律的领域，用摩尔定律打败现存的竞争者。</p><p>2009 年末，德州仪器以 1.725 亿美元的价格从破产的 DRAM 制造商奇梦达公司那里购买了 12 寸晶圆的制造工具，并将其转移到德克萨斯州现有的 Richardson 晶圆工厂中，成为第一家在 12 寸晶圆设备上制造模拟芯片的公司。</p><p>2014 年德州仪器宣布将较老的 DMOS6 工厂转换为 12 寸晶圆的模拟芯片生产。</p><p>2017 年，德州仪器的模拟芯片收入中，大约一半来自使用 12 寸晶圆制造的设备。12 寸晶圆的高毛利，让德州仪器，对比其它的模拟同行，具有更强大，且持续的成本架构。</p><p>德州仪器 70% 以上的生产制造是在 9 个国家的 17 家内部工厂中完成的。模拟 IC 本身就跟工厂工艺强绑定，而且生产产能非常重要，在这点上德州仪器做的非常出色。德州仪器的晶圆产能可以排入晶圆厂产能的前 10，在中国成都也是有工厂的。</p><p>自 2013 年开始到现在，德州仪器在模拟 IC 市场的营收就是行业老二亚德诺半导体（ADI）的两倍。</p><p>没有一款电子设备不需要模拟芯片，在模拟芯片领域，德州仪器是无可争议的市场领导者，在整个半导体界有着举足轻重的地位。</p><p>但是德州仪器的传奇，可没有止步于模拟芯片第一。它从来不是被动转型的公司，它是主动改革的弄潮儿。</p><p><strong>主动改革的重要举措：取消代理分销</strong></p><p>德州仪器最近最大的一次震惊业内，特别是亚洲地区的操作是 2019 年下半年接连取消了安富利、WPI 和文晔的代理权。仅仅保留了一家全球代理商 ARROW 的代理权。这等于全面进入了直销模式。</p><p>要知道，德州仪器有 8 万多个产品，10 万多客户。2/3 的收入是来自代理。这种取消代理，依靠在线销售模式和直销的操作，在业界历史上，从来没有过。</p><p>取消代理，一个直观的好处是德州仪器可以直接访问到客户的项目，每个项目的每颗芯片机会，对客户所作所为有非常直观的洞见，这是靠代理的竞争者所没有的优势。当然，砍掉代理商的利润，肯定也可以提高毛利。</p><p>但是如德州仪器 CEO 所说，在取消代理一年之后，它的 2/3 收入直接来自客户。这个数字，可能会再高一点点，但是不会高很多了。这也意味着，低毛利的小客户的流失。</p><h3 id="其他半导体公司：美国双通与欧洲双雄"><a href="#其他半导体公司：美国双通与欧洲双雄" class="headerlink" title="其他半导体公司：美国双通与欧洲双雄"></a>其他半导体公司：美国双通与欧洲双雄</h3><p><strong>5 年内的前十名</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b1d5d3de7481c2c1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>美国双通：高通与博通</strong></p><p>其实每一个电子设备，可以抽象为计算、存储和网络（也可以叫 I/O 输入和输出）三个主要的功能模块。CPU 做串行逻辑计算，GPU 做并行计算，Intel 和英伟达，就是凭借完成计算任务的能力，位列行业前茅。说到存储模块，三星、SK 海力士、美光、东芝，这些公司都是做存储的。</p><p>还有一类，就是做网络。按照核心技术来分，高通和博通应该归为做网络的公司。从它们的名字可以看出来，都是跟通信有关系的。</p><p>高通（QUALity COMMunications），我觉得中文的翻译还是很传神的，就是高质量的通信的意思。让我先来从这家公司开始。</p><p><strong>高通：CDMA 标准的制定者</strong></p><p>前面有说过，一流公司订标准。高通就是一家制定了 CDMA 无线通信标准，然后从中获益至今的公司。CDMA 无线通信标准就是 3G 通信的主要技术标准。</p><p>如果按照无线通信的发展历史看，1G 是模拟制式的时代，用的是 FDMA，频分复用的技术。到了 2G 时代，欧洲同志们率先结盟，制定了 GSM 的标准，推广到全球。谁立的标准谁获益，诺基亚就是凭着 2G 标准崛起的。而高通，主推的通信性能更高的码分复用 CDMA，最后成为 3G 的标准。</p><p>CDMA 系统在频谱的利用上有较大优势，可以更加高效地利用频谱资源，其实质就是可以支持更多的用户使用，拥有很大的商业价值。</p><p>但是从技术优势，要转换为商业优势，是要花费巨大代价的。最初的高通，从拜访各国政府推动标准的制定，到说服每个运营商投资建网，与北电（Nortel）合作提供网络设备，到和手机生产商合作，提供 CDMA 手机，甚至自己动手制造支持 CDMA 的手机。可以说，高通公司动手参与了 CDMA 产业链的每一个环节。</p><p>但是高通的这种产业链垂直集成的生意模式，并不是它的本意。它跟其它的美国公司类似，定位自己为先进技术 CDMA 的代表，而不是生产商。高通在初期参与设备制造、手机制造，只是为了推广标准，建立一个完整产业链。</p><p>经过高通的努力，1993 年，CDMA 被美国电信工业协会采纳，成为行业标准；1995 年，CDMA 在香港实现商用，1996 年登陆韩国；到了 1999 年，国际电信联盟把 CDMA 选作是 3G 技术。</p><p>1999 年，高通就决定不再做手机和系统设备的业务，转而完全集中在价值更高的技术开发和芯片研发上。这才有了现在的高通设计公司第一的行业地位。</p><p>其实，手机处理器这个市场，堪称半导体公司的坟场，德州仪器的 OMAP 系列、英伟达 Tegra、Intel SoFIA、Marvell 美满电子的 ARMAD、Freescale 飞思卡尔的 i.MX 现在都从手机市场上消失了。</p><p>我们说手机处理器有三大技术能力：CPU、GPU 和基带，ARM 证明了自己在高能效 CPU 上的能力，GPU 还是一场正在进行中的战斗，高通则靠基带稳稳地站住了行业第一的位置。</p><p>对于中国来说，出身网络市场的华为，本来是最有希望超越高通的，可惜中国的芯片产业链不够给力，被卡住脖子。高通的背后是世界顶级的产业链，有强烈的美国属性。</p><p><strong>博通：半导体公司的并购史</strong></p><p>如果说高通是无线通信的第一，那么博通就是有线通信之王。博通对自己的描述是 99.9% 的互联网流量都通过至少一片博通的芯片。很长一段时间，博通拥有以太网交换商业芯片 90% 以上的市场份额。具市场分析公司 Linley Group 的数据，2015 年，博通以太网交换芯片的市场份额是 94.5%。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5bd43bf62eac3ab0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>为什么这么强？我给你看看博通的 Tomahawk 系列侧重于高性能的交换芯片的路标，图如下。你可以看到基本上就是每两年交换带宽加倍，标准的摩尔定律速度。如前所述，按照摩尔定律速度发展的公司，都是好公司。而且如果手机处理器芯片属于优衣库那样的大众成衣，交换芯片就属于芯片中的轻奢品，价格昂贵，是产品经理眼中的好产品。</p><p>不过博通近年来更多的出现在公司并购的新闻中。世界著名的咨询公司麦肯锡，有一个半导体产业的报告。其中说到发展半导体行业的时候，收购与合并是一个重要手段。每个公司应该每年用自己市值的 3%-5%，来收购相邻技术的公司，扩张自己的领域。</p><p>但是博通，不是这种小步迭代的状态。博通的历史简直是一部半导体公司的并购史。1991 年创建的“老博通”，从 2000 年到 2015 年，买了至少 16 家公司。“新博通”的前身安华高科技（Avago Technologies Limited），是从惠普剥离出来的半导体公司，在 2008 年之后也是活跃地进行公司收购和并购。最大的手笔，当然是 2015 年，以小吃大，以 370 亿美元并购博通，并采用“博通”作为合并之后的公司名，业界一般称为“新博通”。之后的新博通，更是采取“激进的买买买”策略。</p><p>下面是博通的官方并购路线。想想，这还是收购高通未遂的状态。如果没有美国政府的干预，2018 年博通以 1300 亿美金成功收购高通，我都不能想象现在的半导体行业格局。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2c5bac2e267e7460.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这样的大规模收购并购之后，我前面说的网络芯片业务，大约只占博通公司收入的 1/4 了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f1dcd675c3e12904.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>欧洲双雄：恩智浦和英飞凌</strong></p><p>如果看半导体公司的前 10，我们能看到恩智浦和英飞凌，如果把名单拉到前 20，或者看再早几年的数据，我们还能看到意法半导体。</p><p>如果大家有兴趣，可以看看这三个公司的出身，例如恩智浦，前身是著名的飞利浦公司的半导体业务部。恩智浦在收购了美国的飞思卡尔半导体之后，这才超越英飞凌成了欧洲老大，可见收购至少对排名有贡献。</p><p>英飞凌，也是系出名门，英飞凌科技公司前身是西门子集团的半导体部门，于 1999 年独立，2000 年就完成了上市。</p><p>意法成立于 1987 年，是意大利 SGS 半导体公司和法国汤姆逊半导体公司合并后的新企业。</p><p>我为什么把欧洲这三家企业放一起说？并不是因为它们是欧洲公司，而是因为它们都是汽车供应链上的半导体企业。其实在半导体行业内，我们以前分析一家公司不太看地域，而是看细分行业，比如手机、汽车。如果看汽车行业，我们常常谈论的就是欧洲三杰（英飞凌、恩智浦和意法半导体），再加一个日本的瑞萨和上一讲提到的德州仪器，2020 年，这五家公司占据了汽车半导体市场的 49% 市场份额。</p><p>因为汽车行业在自动驾驶技术发展，以及特斯拉横空出世之前，都是 MCU（微控制器）的天下，因此这些汽车供应链上的半导体企业的 MCU 也都很强，还有传感器。说直白一点，之前的芯片厂商靠的都是汽车产业的技术溢出效果。</p><p>半导体器件，到底是服务电子设备的。一个半导体公司，如果已经上车了，那么努力把车用半导体解决方案做全做强，就是一个非常合理的产品矩阵设计策略。而在一个主要市场内，解决了技术与规模问题之后，向相邻市场进行技术与产品的溢出，也是常见的策略。理解这两个市场策略之后，我们再看恩智浦，英飞凌，意法半导体和瑞萨的发展方向，基本上就非常清晰了。</p><p>车用半导体市场本来格局清晰，但是在新能源车、车联网、自动驾驶来了之后，基本上所有的头部半导体公司，以及互联网巨头们都下场了，创业公司也蜂拥而至。但是友情提示一下，这四家传统车用半导体公司，都是有工厂的，属于 IDM 或者是轻工厂的模式，在控制成本结构上，有一定的主动权，不会轻易出局。</p><h2 id="中国的机会"><a href="#中国的机会" class="headerlink" title="中国的机会"></a>中国的机会</h2><h3 id="2000-年之前：国家计划与政策扶持"><a href="#2000-年之前：国家计划与政策扶持" class="headerlink" title="2000 年之前：国家计划与政策扶持"></a>2000 年之前：国家计划与政策扶持</h3><p>1956 年，拿历史书上的话说，是中国半导体行业发展史上的具有里程碑意义的一年。那一年，在我国十二年科学技术发展远景规划中，半导体科学技术被列为当时国家新技术四大紧急措施之一。那个时候，产业界还在电子管时代，集成电路尚未发明。</p><p>而到了七十年代初，集成电路价高利厚，需求巨大，引起了全国建设集成电路生产企业的热潮，共有四十多家集成电路工厂建成。中国 IC 行业形成了“两霸”：南霸是上无十九厂，北霸则为 878 厂。</p><p>80 年底，中国进入改革开放时期。一方面国家期望企业摸索在开放市场中的发展能力，另一方面国防的需求骤减，这些原因对半导体产业的冲击非常明显。</p><p>但是国家的策略很清晰，那就是集中力量建设半导体制造能力。因此才有了 “八五”规划，全国重点支持的 5 个集成电路重点企业：无锡华晶、绍兴华越、上海贝岭、上海飞利浦和北京首钢日电。1990 年，908 工程，无锡华晶成立。1996 年，909 工程，上海华虹 NEC 合资成立。目前半导体制造企业，多半出自此时，包括成为时代分水岭的中芯国际集成电路。</p><p>其实，中国的半导体产业初期服务军工，自研能力不低，但是几乎全是国有企业。离开了政策的扶持，离开了军工的订单，这些国企并没有很好的度过军转民这一个弯，甚至在 80 年代改革开放后，进口半导体产品的冲击下，几乎停滞。2000 年左右，中国芯片设计公司，不足 100 家，总收入不到一亿美元。</p><h3 id="2000-年-2014-年：市场带动产业发展"><a href="#2000-年-2014-年：市场带动产业发展" class="headerlink" title="2000 年 -2014 年：市场带动产业发展"></a>2000 年 -2014 年：市场带动产业发展</h3><p>2000 年，中芯国际成立，可以成为中国半导体历史上的另一个里程碑事件，因为中芯国际一开始就奔着“世界先进水平”的目标去的。仅仅在开工建设 13 个月后，2001 年 9 月 25 日，中芯国际一厂投产，4 个月后量产，投产和量产速度均创造了世界之最。</p><p>不仅如此，中芯国际其实也是属于在经济衰退期建厂的。利用 2000 年 dotcom 泡沫破裂，连带的整个半导体产业低迷期，中芯国际用合理成本在 3 年内建成了 4 条 8 英寸生产线和 1 条 12 英寸生产线，2004 年完成上市，2005 年跻身世界晶圆代工厂前三。</p><p>大手笔地投资建厂是一方面，中国的市场也给了中国半导体业更多机会。中国从 2005 年就开始成为全球半导体产品的最大市场。具体有哪些市场呢？我们一个一个来说。</p><p><strong>电信市场</strong></p><p>80 年代，中国电信市场还是“七国八制”时代，就是说我们总共有来自七个国家的八种制式的机型或网络。之后，有“巨大中华”之称的四大公司，包括巨龙通信、大唐电信还有华为和中兴，在 90 年代中后期快速崛起。一方面以质高价廉的设备替代了国外进口的高价设备，另一方面，由于成本的下降，中国运营商也蓬勃发展，开始步入大规模建网周期，整个国家的电信基础设施水平快速提升。</p><p>电信市场的这波操作，最终的结果是，华为和中兴不但拿下国内市场，而且开始在全球市场进入第一梯队。</p><p>电信设备的国际竞争化，让华为和中兴很快意识到芯片的重要性。华为在 1991 年成立的集成电路设计中心基础上，2004 年成立了海思半导体公司。中兴 1996 年就成立了 IC 设计部，专门从事芯片研发，并在 2003 年 11 月，成立了中兴微电子技术有限公司（简称“中兴微电子”）。</p><p>在电信设备市场，海思和中兴微电子领先对手半年推出产品，靠的就是他们比别人先研发出的自己的芯片。</p><p>这种在电信市场中积累的设计高性能复杂芯片的能力，让海思在后来的安防芯片、服务器芯片、手机应用处理器和基带芯片、人工智能芯片上都能快速出类拔萃，进入世界第一梯队。</p><p>当然最终把海思送上世界半导体公司前十的（2020 年的上半年），且成为台积电第二大客户的，还是手机应用处理器麒麟系列。这也是半导体产品的特点，一颗好产品，在一个大市场中，拥有惊人的出货量和收益。</p><p>再多说一点，手机处理器不仅要比拼工艺先进性，因为工艺先进性直接影响功耗和晶体管密度，反应在手机上，就是待机时间和性能高低，还要比拼先进工艺的良率爬坡能力。</p><p>先进工艺能领先很重要，可以凭借工艺领先拿下时间敏感的手机客户，但是先进工艺的性能，产能和良率也同样重要。高通处理器出现过几次功耗过高的问题，都发生在台积电与三星代工厂切换的阶段。相对而言，苹果、海思、联发科 MTK 都稳定地在台积电生产，保证了先进工艺的性能提升，产能和良率。</p><p><strong>手机市场</strong></p><p>上面有提到海思凭借麒麟，在 2020 年上半年拿到全世界半导体排名前十，成为台积电第二大客户。但是中国繁荣的手机市场，可不仅仅只出了一家海思。</p><p>2001 年，展讯通信有限公司（以下简称展讯）成立，并随着中国特有的山寨机市场，赚到第一桶金，先后在 2.5G 和 3G 芯片上做出了不错的成绩。2007 年，展讯在纳斯达克上市。</p><p>另外一家公司，锐迪科（RDA），成立于 2004 年 4 月，是射频 IC、混合信号芯片和手机功率放大器 PA 的专家。锐迪科不仅仅在手机市场，还在无线连接和广播通信市场有多款创新产品。2010 年成功登陆纳斯达克的时候，锐迪科已经是中国市场第一名的功率放大器、蓝牙、FM 与 DVB-S 调谐器芯片供应商，GSM 基带的第二供应商。</p><p>后来源自锐迪科系的创业公司，包括翱捷科技、恒玄科技等 8 家公司，撑起了中国射频芯片领域的半壁江山。</p><p>如果从产品互补，两个同城兄弟联合起来做大做强，一起对抗同在手机市场，处在第二梯队的联发科的角度看，展讯和锐迪科是应该合并的。但是合并过程并不顺利，紫光集团在 2013、2014 年分别收购了展讯和锐迪科之后，2018 年才将两者合并为紫光展锐。合并之后的紫光展锐在手机应用处理器市场的市场份额可排在世界第六，无厂半导体设计公司的世界第 10。</p><p>其实手机市场中，中国半导体公司除了应用处理器和基带芯片，还有指纹识别芯片、图像传感器芯片等等。汇顶科技成立于 2002 年，几乎所有手机都用了它家的指纹识别芯片，仅有三星采用了高通的芯片。豪威科技，后被威尔科技收购，在 CMOS 图像传感器芯片市场排名全球第三。</p><p><strong>比特币矿机</strong></p><p>因为比特币及其他数字货币发展迅猛的缘故，专用的矿机 ASIC 芯片发展速度有目共睹。说到矿机 ASIC 芯片上，由于挖矿的成本大头是电力，芯片功耗越低成本就越低，所以矿机 ASIC 芯片这两年在制程工艺升级上比移动处理器还要激进。</p><p>其实说起来，近几年英伟达和 AMD 的收入中，来自比特币挖矿这个市场的比例不低。而这个市场中专业做矿机芯片的，是两个中国企业比特大陆和嘉楠耘智。</p><p>比特大陆成立于 2013 年，不但在矿机芯片上世界第一，而且公司依靠自己在高性能芯片上的设计能力，积极拓展人工智能和高性能计算市场。比特大陆曾经一度超越海思，成为台积电大陆地区第一大客户。</p><p>同年成立的嘉楠科技则是仅次于比特大陆的第二大矿机芯片厂商，虽然在矿机芯片上的营收位列第二，但是运气好，2019 年成功在美国纳斯达克上市。嘉楠科技同样要把从先进 ASIC 矿机芯片设计中积累的技术优势，投入在人工智能和高效能计算上。</p><p><strong>其它市场上活跃的中国半导体公司</strong></p><p>其实，这个时期成立的半导体公司还有很多，例如靠白牌平板电脑起家的瑞芯微、全志。凭借 NOR Flash 芯片进入苹果供应链的兆易创新，还有核高基项目重点支持的 CPU 公司：兆芯、龙芯、飞腾、海光等。还有 2014 年，长电科技以 7.8 亿美元收购新加坡星科金朋，成为全球封测第三。</p><p>国际上，半导体产业格局稳定，巨头们忙着大鱼吃小鱼，搞收购兼并的时候，中国的半导体产业凭借着中国市场得天独厚的优势走出自己的节奏。</p><h3 id="2014-年后：投资产业时代"><a href="#2014-年后：投资产业时代" class="headerlink" title="2014 年后：投资产业时代"></a>2014 年后：投资产业时代</h3><p>2014 年国家集成电路产业大基金成立，简称大基金，一期幕资 1387 亿元，是当时国务院批准的最大规模的产业投资基金。大基金一改以往项目为主的资助形式，通过股权投资的方式，扶植产业链上的龙头企业，“只投行业前三”，加大对芯片制造业的投资力度，且兼顾设计和封测。</p><p>中央政府资金，一向对于社会上商业资本有很强的带动作用。对于国家来说，掌握先进技术的公司（领头羊）、市场、资金是发展的三要素，这一轮是资金先开局，推动国内半导体产业发展进入第三阶段。第三阶段，有一个企业你需要重点了解，那就是紫光集团。</p><h3 id="紫光集团"><a href="#紫光集团" class="headerlink" title="紫光集团"></a>紫光集团</h3><p>紫光集团是这个时期最为瞩目的企业，特别是几个大手笔的国内外企业并购重组活动，成为行业内都在关注的事情。先是 2013 年，用 17.8 亿美元收购展讯；2014 年，9 亿美元在海外私有化锐迪科；2015 年 5 月，从惠普手里收购了新华三 51% 的股权；同年对美光和美国西部数据（WD）发起收购邀约，被拒绝之后，紫光的策略从收购转为自建。2016 年 12 月，合并武汉新芯，成立长江存储，与西数合资成立紫光西数。此外还有一系列的封测和晶圆厂的动作。</p><p>长江存储在量产 64 层 NAND Flash 之后，2020 年首发 192 层 3D NAND，被预测 2021 会拿下 8% 的 NAND Flash 份额。同时，在存储芯片领域，中国还有一家公司叫做长鑫存储，长鑫存储以唯一一家中国公司的名号，杀入 DRAM 领域。从世界著名的行业分析公司 Yole 公司的报告上，我看到长江存储和长鑫存储与三星、SK 海力士、美光和 Intel 齐头并进，还挺激动的。</p><p>算上前面提过的兆易创新在 NOR Flash 的成就，我觉得在存储芯片领域，中国可以算得上有一份不错的成绩。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9105e0b1b0ec638b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>有金融行业的人总结，大基金一期在 2018 年年底，芯片设计投资占 19.7%，制造占 47.8%，封测业占 11%，半导体材料占 1.4%，半导体设备占 1.2%，产业生态建设占 19%，这是一份又全面又重点突出的投资图谱。</p><h3 id="中国的巨大市场机会"><a href="#中国的巨大市场机会" class="headerlink" title="中国的巨大市场机会"></a>中国的巨大市场机会</h3><p>我们整个专栏的第二部分是按照半导体公司销售收入世界前十的表格为索引的，这里让我给你另外一张表：半导体产品的十大买家。你可以看到，这张表里，大陆公司占了 4 个。表里的 BBK，是步步高集团，Garnter 是把一加、Oppo、vivo、Realme、爱酷都算在了步步高集团里。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-001f03e2576c7ebb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>中国不但拥有前十大买家中的 4 家，而且中国自 2005 年以来，就一直是世界上最大的 IC 市场，并且自那时开始市场规模就一直在增长。</p><p>据 IC Insights 报告指出，2020 年，中国集成电路市场增至 1,434 亿美元，约占全球市场的 35%。如果再仔细看逻辑芯片、微处理器、DRAM 存储芯片的所占比例，基本上跟全球比例一致，可以说中国的半导体需求，是跟全球保持一致的。</p><p>虽然中国是集成电路的最大消费国，但是中国公司出产的芯片，只占到全世界产量的 5%。</p><p>其实无论外面议论得多热闹，中国产业界自己，对于现状还是非常清晰的。中国半导体协会曾经总结过国产芯片的比例，特别是在计算机系统、通用电子系统、通信装备、存储设备、显示及视频系统中的核心芯片占比，你可以看下面这张图。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9bdcff716caeec1b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>35% 的全球半导体产品市场，这是一个巨大的市场空间，中国公司仅仅贡献了 5% 的产品，在核心芯片领域几乎可以忽略中国的市场份额，这里就是和国际产业的差距。但同时，我们也应该看到，10 大半导体产品买家中有 4 家中国公司，这就是机会。</p><p>另一个发展的机会，是国家对半导体产业的期望。我在上一讲中反复提到集成电路发展一直是国家重要战略之一。2014 年出台的《国家集成电路产业发展纲要》和 2015 年的《中国制造 2025》文件中有明确提出：到 2020 年，集成电路产业与国际先进水平的差距逐步缩小；2020 年中国芯片自给率要达到 40%，2025 年要达到 50%。</p><p>站在 2021 年看，半导体产业是没有完成这个 40% 的国家目标的。而且即使到 2025，40% 的目标也很难完成。不过，在存储芯片市场，有长江存储（YMTC）和长鑫存储（CXMT）的双雄崛起，无论是技术领先性，还是市场份额上，国产存储芯片市场，也许还有望达到 2025 的目标。在非存储市场，海思受阻之后的中国半导体设计行业，挑战非常大。</p><p>看过中国市场现状之后，让我们看看前沿技术这边。一般前沿技术的创新，有机会打破既有的格局，是后来者的可能破局点。当然，先锋变先烈的事情年年有，而且是半导体行业特别多。这个行业最终是要落实到制造本身。</p><p>再前沿的技术，也是工厂一颗芯片一颗芯片制造出来的。例如今年 5 月 6 号 IBM 首发了 2nm 工艺芯片的新闻，能制造出 2nm 工艺的芯片，和能稳定地高良率地大量制造出 2nm 芯片，中间至少有两年的时间差，需要晶圆工厂、设计公司、EDA 公司等多方面的共同努力。</p><p><strong>破局点：先进技术方向</strong></p><p>半导体前沿技术的推动力主要来自两个方面，一个是数据中心的 CPU、云端人工智能芯片、数据中心专用处理器，例如 DPU、交换芯片，这类“大”芯片的驱动。另一个是海量的手机芯片，手机芯片对于性能、功耗、尺寸以及经济成本的敏感性，都需要半导体产业不断创新。</p><p>这两个市场都是极度追求最先进制程工艺，才能将最新设计落地。在芯片制造和设计领域，我们来看看都有哪些机会呢？</p><p><strong>芯片制造：光刻机、晶体管架构、先进封装</strong></p><p>芯片制造这个环节的所有努力都是在延续摩尔定律。</p><p>目前看，得益于极紫外光刻机的技术进步和晶体管的新架构，在未来 8-10 年，摩尔定律仍然生效。光刻机有一个重要的技术指标，就是分辨率。光学投影的原理公式我就略过不讲了，直接说结论：短波长光源是提高光刻机分辨率的有效方法。因此看光刻机的发展历史，就从紫外光源（UV）、深紫外光源（DUV），发展到了现在的极紫外光源（EUV）。</p><p>当光源的分辨率不够的时候，还可以采取多重曝光技术进行加强。但是多重曝光技术，需要相应地增加光罩层数，增加了经济成本。到了 1nm 节点上，晶圆工厂有望使用极紫外光刻的下一代光刻机。</p><p>新光刻机，不换光源，也不搞多重曝光，而是换透镜来增加分辨率，把多重曝光重新转回为一次曝光，可以有效减少光罩的层数。这样经济成本、良率、制造时间都可以得到改善。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-cc12f52d1cf7c9de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>晶体管的架构呢，也是持续稳定地演进的。标准单元的架构从 FinFET，到 Nanosheets，再到 Forksheets、CFETs，这是一条通往 1nm 工艺的路标。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7780dbb286261823.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>NAND 存储可以在现有的架构上，持续扩展，可以堆叠更多的层数，还可以换 FinFET 的晶体管。DRAM 的缩放虽然在放缓，但是可以用极紫外光刻技术做 3D DRAM。因此目前看，<strong>存储芯片技术在短时间内没有技术瓶颈。</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-dcb98d0114a5374a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>一方面是先进制程接着往 1nm 方向演进，另一方面先进封装技术的经济收益在持续上升。这么说吧，以前是架构、工艺制程的 Tick-Tock 的研究，后面大约是架构、工艺制程、换封装的三节式演进了。</p><p>未来 SoC 设计，会更重视功能分区，把追求最新工艺的逻辑部分，追求高密度的存储芯片，和在新工艺上收益不显著的模拟部分，分开设计，然后再封装在一起，取得最佳的设计性价比。看看最新一代的 Intel 和 AMD 的云端大芯片，都是多 Die（裸片）异构封装了。</p><p>什么叫多 Die 异构封装呢？看下面 Intel 的最新 7nm GPU 的芯片，把不同功能多个小硅片封装在一起。这里是 16 个 CPU Die、8 个 Cache Die、8 个 HBM Die、2 个 IO Die，还有几个未标明功能的填充或者测试用 Die，封装成一个 GPU 的芯片。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6213a17d1dcf6f82.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>芯片设计：存内计算、整 Wafer 芯片、光子计算和量子计算</strong></p><p>比如，打破冯·诺伊曼架构，做存内计算。存内计算，顾名思义就是直接在存储内做计算。其具体实现方式有若干条技术路径。例如有望提高能效比 10 倍以上的模拟存内计算，就非常适合人工智能在边缘计算的场景。</p><p>还有超摩尔定律的光子芯片、量子计算等。特别是光子芯片，相对于硅芯片，在延时、带宽、功耗方面都有超 10 倍的优势，而且已经有公司在进行商业化实现了。</p><p>让人欣慰的是，在这些前沿领域，都有中国公司的身影。</p><p>相对于这些非常前沿的技术，我个人倒是比较看好 2019 年在 Hotchips 上亮相的 Cerebras 公司，这家初创公司用下图这块晶圆做出了史上最大的芯片。首先，我要给你一个知识背景：光刻机有一个加工的最大尺寸，一般是 858mm²，而 Cerebras 和台积电紧密合作，做了一个 46255mm²，1.2T 个晶体管的世界第一大芯片。这也是超摩尔定律的一个突破。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-58754094b180f9f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我再补充一句，先进封装技术也可以帮助芯片设计公司打破这个光刻机加工尺寸的限制，博通和台积电也在合作中，“大”尺寸芯片是一个让人向往的方向。</p><p>针对芯片工厂的就业机会，我给你总结了一张表格，你可以保存。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2ac3c2c670f0ac8a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>]]></content>
      
      
      <categories>
          
          <category> System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
            <tag> Chip </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DDD在Message服务的实践</title>
      <link href="/2021/06/25/ddd-implemennt/"/>
      <url>/2021/06/25/ddd-implemennt/</url>
      
        <content type="html"><![CDATA[<h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>我们<code>message</code>服务经过<code>4</code>年的迭代，项目的可维护性、代码的可读性、功能的可扩展性越来越差，经常改一个小功能牵一发而动全身，新来的同学对<code>message</code>服务不能快速上手，而且因为代码可读性不高，导致新同学修代码经常会出现<code>bug</code>。还有一部分原因是大多数人都是第一次用<code>Golang</code>，对项目的分层也没有一个统一的规范，导致最后成了一个四不像的框架。</p><p>然后我们今年的两个主要目标，一是项目的稳定性、二是为了支持<code>KA</code>私有划部署战略，我们需要合并微服务数量，降低运维和部署成本。</p><p>基于上背景，老板让我牵头对<code>Message</code>服务基于<code>DDD</code>做一次大规模重构，重构目标很明确：</p><ol><li>对项目有个明确的分层，提高项目代码可读性、可维护性、可扩展性。降低新人上手成本。</li><li>升级<code>RPC</code>框架，合并<code>Message</code>相关服务，提高优化项目资源利用率。</li><li>合理分层，方便后续扩展，比如做<code>存储</code>的高可用。</li><li>产出一个最佳实践规范，作为整个<code>Messenger</code>项目指导规范。</li></ol><h2 id="二、架构方案"><a href="#二、架构方案" class="headerlink" title="二、架构方案"></a>二、架构方案</h2><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2cc2ee62447df8af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1c2ee27646713bac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ad25686d0d9d878f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="三、应用代码分层"><a href="#三、应用代码分层" class="headerlink" title="三、应用代码分层"></a>三、应用代码分层</h2><h3 id="3-1-基本架构思想"><a href="#3-1-基本架构思想" class="headerlink" title="3.1 基本架构思想"></a>3.1 基本架构思想</h3><p>目前使用的是<code>整洁架构/洋葱架构</code>的思想，领域层是项目核心，里面存放业务的核心逻辑。</p><ol><li>对上<code>Domain Service</code>抽象自己的对外的提供的能力，定义好接口给<code>Application</code>层服务调用。</li><li>对下<code>Domain Service</code>抽象依赖的接口，通过依赖反转方式让底层的<code>Infrastructure</code>去实现相关逻辑。</li><li>注意内层对象的不能依赖外层对象。<code>Domain Service</code>之间也不能互相调用。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c63c554e4115bb48?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a90b40c0686d827f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="3-2-项目分层"><a href="#3-2-项目分层" class="headerlink" title="3.2 项目分层"></a>3.2 项目分层</h3><p>我们可以把 <code>Message</code>、<code>Read</code>、<code>Pin</code>、<code>Urgent</code>、<code>Reaction</code>几个模块看成<code>Message</code>服务域下的一个个聚合，聚合和聚合相互独立，聚合之间可以通过<code>EventBus</code>来相互通信，或者通过<code>Local Call</code>的方式来调用。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ef7a9c26e3aaf8bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5b40b3f135527bbd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="Interfaces（用户接口层）"><a href="#Interfaces（用户接口层）" class="headerlink" title="Interfaces（用户接口层）"></a>Interfaces（用户接口层）</h4><p>这一层主要做数据解析操作，如果是<code>RPC</code>服务，数据解析逻辑一般<code>RPC</code>框架都已经给我们做好了，没啥好说的，<code>Http</code>，这一层主要解析请求数据，然后转<code>DTO</code>。</p><h4 id="Application（应用层）"><a href="#Application（应用层）" class="headerlink" title="Application（应用层）"></a>Application（应用层）</h4><ol><li>权限校验、参数校验。</li><li>发送或订阅领域事件等。</li><li><code>Domain Obejct</code> 组装（简单接口可以不用<code>DO</code>）</li><li><code>Domain Service</code> 服务的组合、编排。 </li><li><code>Response</code>的组装返回。</li></ol><h4 id="Domain（领域层）"><a href="#Domain（领域层）" class="headerlink" title="Domain（领域层）"></a>Domain（领域层）</h4><p><code>Domain</code>层，主要包括<code>Domain Service</code>和<code>Domain Object</code>。在基于充血模型的<code>DDD</code>开发模式中，大部分业务逻辑都在<code>Domain Object</code>中，<code>Domain Service</code>类变得很薄。</p><p><code>Domain Service</code>类主要有下面这样几个职责：</p><ol><li><code>Domain Service</code>类负责与<code>Repository</code>交流。之所以让<code>Domain Service</code>与<code>Repository</code>打交道，而不是让领域模型与<code>Repository</code>打交道，那是因为我们想<strong>保持领域模型的独立性</strong>，不与任何其他层的代码（<code>Repository</code> 层的代码）或开发框架耦合在一起，将流程性的代码逻辑（比如从<code>DB</code>中取数据、映射数据）与领域模型的业务逻辑解耦，让领域模型更加可复用。</li><li><code>Domain Service</code>负责一些非功能性及与三方系统交互的工作。比如幂等、事务、调用其他系统的<code>RPC</code>接口等，都可以放到<code>Service</code>类中。</li><li><code>Domain Service</code>类负责跨领域模型的业务聚合功能。</li><li><code>Domain Service</code> 应该相互独立，<code>Domain Service</code>里面不能直接调用其他<code>Domain Service</code>的服务，比如在<code>pushDomainService</code>调用<code>NewPackDomainService().Pack()</code>。 </li><li><code>Domain Object</code>，充血模型，里面存放一些核心逻辑，比如<code>Pack</code>时候，<code>Domain Object</code>中会有<code>Thrift Message</code> 转 <code>Protobuf Message</code> 数据组装的若干逻辑。</li></ol><h4 id="Infrastructure（基础层）"><a href="#Infrastructure（基础层）" class="headerlink" title="Infrastructure（基础层）"></a>Infrastructure（基础层）</h4><p>基础层贯穿所有层，为各层提供基础资源服务</p><ol><li><code>Infrastructure/Repository</code> 提供存储相关能力。</li><li><code>Infrastructure/Service</code> 提供比如消息加解密、<code>Risk</code>检查、部分带<code>LocalCache</code> 或者<code>CtxCacheRPC</code>服务接口。</li><li><code>Infrastructure/RPC</code> 提供RPC能力，这个里面只做简单的<code>RPC</code>调用，参数解析返回，如果有<code>LocalCache</code>相关，需要封装到<code>Infrastructure/Service</code>中去（理论上<code>Domain</code>层不应该直接依赖<code>Infrastructure/RPC</code>包，都需要抽象为接口封装为<code>Infrastructure/Service</code>，考虑到<code>IDL</code>一般都是向前兼容的，为了开发方便所以<code>Application</code>层和<code>Domain</code>层都直接依赖了<code>RPC</code>包）。</li><li><code>Infrastructure/Pkg</code> 提供一些基础工具包，比如<code>fg</code>、<code>metrics</code>、<code>idgen</code>、<code>errror</code>包相关的，这个包理论上不能有具体的业务逻辑。</li></ol><h3 id="3-3-关于各层之间数据传递"><a href="#3-3-关于各层之间数据传递" class="headerlink" title="3.3 关于各层之间数据传递"></a>3.3 关于各层之间数据传递</h3><p><code>DO</code>、<code>PO</code>、<code>VO</code> 存在的意义是什么？</p><ol><li>为了尽量减少每层之间的耦合，把职责边界划分明确，每层都会维护自己的数据对象，层与层之间通过接口交互。数据从下一层传递到上一层的时候，将下一层的数据对象转化成上一层的数据对象，再继续处理。虽然这样的设计稍微有些繁琐，每层都需要定义各自的数据对象，需要做数据对象之间的转化，但是分层清晰。对于非常大的项目来说，结构清晰是第一位的！</li><li><code>DO</code>、<code>PO</code>、<code>VO</code> 并非完全一样。每一层各个对象还是有一些区别。</li><li><code>DO</code>、<code>PO</code>、<code>VO</code>三个类虽然代码重复，但功能语义不重复，从职责上讲是不一样的。所以，也并不能算违背<code>DRY</code>原则。</li></ol><p>不同分层之间的数据对象该如何互相转化呢？</p><p>当下一层的数据通过接口调用传递到上一层之后，我们需要将它转化成上一层对应的数据对象类型。比如，<code>Domain</code> 层从 <code>Repository</code>层获取的<code>Entity</code>之后，将其转化成<code>DTO</code>，再继续业务逻辑的处理。<br>具体可以参考 <a href="https://time.geekbang.org/column/article/183007">https://time.geekbang.org/column/article/183007</a></p><ol><li><code>IDL</code>生成的 <code>RPC Request/Response</code> 对象我们可以认为是<code>DTO</code>对象。</li><li><code>Redis</code> 持久化的对象比如<code>pbpersistent.PersistentMessage</code>需要收敛在<code>Infrastructure</code>层，<code>Domain</code>和<code>Application</code>不应该感知到这个类型。</li><li><code>PO</code>对象也不应该让<code>Domain</code>和<code>Application</code> 层感知到。比如 <code>MessageEntity</code>实体。</li><li>不能通过<code>context</code>隐式传递自定义的参数，所有的数据传递必须显示传递。</li><li><code>context</code>中的公共参数只能在<code>Application</code>层获取，到<code>Domain</code>层和<code>Infrastructure</code>层必须显示传递。</li><li>不想重复定义<code>Domain Obeject</code> ，可以通过组合的方式组合<code>DTO</code>对象（包括自定义<code>DTO</code>对象和<code>Thrift</code>生成的对象）</li><li>如果一个对象被多层用到了，这个对象可以放到<code>types/dto</code>文件夹下。注意<code>types/dto</code>应该都是贫血对象，理论上只能依赖<code>idl_gen</code>这种<code>dto</code>包，不应该依赖其他层的任何包。</li></ol><h3 id="3-4-服务内聚合之间如何通信"><a href="#3-4-服务内聚合之间如何通信" class="headerlink" title="3.4 服务内聚合之间如何通信"></a>3.4 服务内聚合之间如何通信</h3><h4 id="3-4-1-基于事件"><a href="#3-4-1-基于事件" class="headerlink" title="3.4.1 基于事件"></a>3.4.1 基于事件</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f7d32a97fa93b293.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="3-4-2-Local-Call"><a href="#3-4-2-Local-Call" class="headerlink" title="3.4.2 Local Call"></a>3.4.2 Local Call</h4><p><code>kitex</code>服务可以直接使用<code>kitex</code>生成的<code>Interface</code>，<code>kite</code>服务可以自己定义各个聚合需要暴露的<code>Interface</code>。<code>Localcall</code>只应该依赖<code>kitex/kite</code>生成的结构 。</p><p>优点：后续要把某个聚合再拆出去以后，只需要把<code>Localcall</code>包名改成<code>RPC</code>即可，不需要修改其他逻辑。</p><pre><code>package localcallimport (   "context"   "git.byted.org/ee/go/kitex_gen/lark/im/message")var localSVC message.MessageServicefunc InitLocalSVC(svc message.MessageService) {   localSVC = svc}func Pack(ctx context.Context, request *message.PackRequest) (r *message.PackResponse, err error) {   return localSVC.Pack(ctx, request)}// 在main初始化func main() {   ....   localcall.InitLocalSVCImpl(new(MessageServiceImpl))   ...}  </code></pre><h2 id="四、编码规范"><a href="#四、编码规范" class="headerlink" title="四、编码规范"></a>四、编码规范</h2><h3 id="4-1-基本原则"><a href="#4-1-基本原则" class="headerlink" title="4.1 基本原则"></a>4.1 基本原则</h3><ol><li><p>单一职责（<code>SRP</code>）：<code>Single Responsibility Principle</code>，一个类只负责完成一个职责或者功能。不要设计大而全的类，要设计粒度小、功能单一的类。单一职责原则是为了实现代码高内聚、低耦合，提高代码的复用性、可读性、可维护性。</p></li><li><p>开闭原则（<code>OCP</code>）：<code>Open Closed Principle</code>，对扩展开放，对修改关闭。添加一个新的功能，应该是通过在已有代码基础上扩展代码（新增模块、类、方法、属性等），而非修改已有代码（修改模块、类、方法、属性等）的方式来完成。</p></li><li><p>里式替换（<code>LSP</code>）：<code>Liskov Substitution Principle</code> 子类对象（<code>object of subtype/derived class</code>）能够替换程序（<code>program</code>）中父类对象（<code>object of base/parent class</code>）出现的任何地方，并且保证原来程序的逻辑行为（<code>behavior</code>）不变及正确性不被破坏。举例： 是拿父类的单元测试去验证子类的代码。如果某些单元测试运行失败，就有可能说明，子类的设计实现没有完全地遵守父类的约定，子类有可能违背了里式替换原则。</p></li><li><p>接口隔离原则（<code>ISP</code>）：<code>Interface Segregation Principle</code> 调用方不应该被强迫依赖它不需要的接口。</p></li><li><p>依赖反转原则（<code>DIP</code>）： <code>Dependency Inversion Principle</code> 高层模块（<code>high-level modules</code>）不要依赖低层模块（<code>low-level</code>）。高层模块和低层模块应该通过抽象（<code>abstractions</code>）来互相依赖。除此之外，抽象（<code>abstractions</code>）不要依赖具体实现细节（<code>details</code>），具体实现细节（<code>details</code>）依赖抽象（<code>abstractions</code>）。举例 <code>Domain</code> 层不依赖 <code>Infrastructure</code> 层具体实现，只依赖<code>Domain</code>自己抽象的 <code>Interface</code></p></li></ol><h3 id="4-2-CodeReview-标准"><a href="#4-2-CodeReview-标准" class="headerlink" title="4.2 CodeReview 标准"></a>4.2 CodeReview 标准</h3><ol><li>可维护性（<code>maintainability</code>），落实到编码开发，所谓的“维护”无外乎就是修改 bug、修改老的代码、添加新的代码之类的工作。所谓“代码易维护”就是指，在不破坏原有代码设计、不引入新的 <code>bug</code> 的情况下，能够快速地修改或者添加代码。所谓“代码不易维护”就是指，修改或者添加代码需要冒着极大的引入新 <code>bug</code> 的风险，并且需要花费很长的时间才能完成。</li><li>可读性（<code>readability</code>），我们在编写代码的时候，时刻要考虑到代码是否易读、易理解。除此之外，代码的可读性在非常大程度上会影响代码的可维护性。毕竟，不管是修改 bug，还是修改添加功能代码，我们首先要做的事情就是读懂代码。代码读不大懂，就很有可能因为考虑不周全，而引入新的 bug。我们需要看代码是否符合编码规范、命名是否达意、注释是否详尽、函数是否长短合适、模块划分是否清晰、是否符合高内聚低耦合等等。你应该也能感觉到，从正面上，我们很难给出一个覆盖所有评价指标的列表。这也是我们无法量化可读性的原因。</li><li>可扩展性（<code>extensibility</code>），代码的可扩展性表示，我们在不修改或少量修改原有代码的情况下，通过扩展的方式添加新的功能代码。说直白点就是，代码预留了一些功能扩展点，你可以把新功能代码，直接插到扩展点上，而不需要因为要添加一个功能而大动干戈，改动大量的原始代码。</li><li>灵活性（<code>flexibility</code>），从刚刚举的场景来看，如果一段代码易扩展、易复用或者易用，我们都可以称这段代码写得比较灵活。所以，灵活这个词的含义非常宽泛，很多场景下都可以使用。</li><li>简洁性（<code>simplicity</code>），有一条非常著名的设计原则，你一定听过，那就是 <code>KISS</code> 原则：“<code>Keep It Simple，Stupid</code>”。这个原则说的意思就是，尽量保持代码简单。代码简单、逻辑清晰，也就意味着易读、易维护。我们在编写代码的时候，往往也会把简单、清晰放到首位。不过，很多编程经验不足的程序员会觉得，简单的代码没有技术含量，喜欢在项目中引入一些复杂的设计模式，觉得这样才能体现自己的技术水平。实际上，思从深而行从简，真正的高手能云淡风轻地用最简单的方法解决最复杂的问题。这也是一个编程老手跟编程新手的本质区别之一。</li><li>可复用性，代码的可复用性可以简单地理解为，尽量减少重复代码的编写，复用已有的代码。</li><li>可测试性，相对于前面六个评价标准，代码的可测试性是一个相对较少被提及，但又非常重要的代码质量评价标准。代码可测试性的好坏，能从侧面上非常准确地反应代码质量的好坏。代码的可测试性差，比较难写单元测试，那基本上就能说明代码设计得有问题。</li></ol><h3 id="4-3-关于重构"><a href="#4-3-关于重构" class="headerlink" title="4.3 关于重构"></a>4.3 关于重构</h3><p>总结一下重构的做法，其实就是“分段实施”，将要解决的问题根据优先级、重要性、实施难度等划分为不同的阶段，每个阶段聚焦于一个整体的目标，集中精力和资源解决一类问题。<br>这样做有几个好处：</p><ol><li>每个阶段都有明确目标，做完之后效果明显，团队信心足，后续推进更加容易。</li><li>每个阶段的工作量不会太大，可以和业务并行。</li><li>每个阶段的改动不会太大，降低了总体风险。</li></ol><p><strong>优先级排序 -&gt; 问题分类 -&gt; 先易后难 -&gt; 循序渐进</strong></p><p><strong>重构不是简单功能搬运，把老的项目功能迁移到新的项目就完事了，这样的重构没有任何收益。重构的时候更多的是需要考虑、可读性、可维护性、可扩展性等几个方面。</strong></p><p>需求开发的时候可能工期紧，或者没想好怎么实现相关功能。后续可以多思考有没有更好的方式去实现相关功能，持续优化老的功能，让项目的代码质量越来越高。</p><h3 id="4-4-函数"><a href="#4-4-函数" class="headerlink" title="4.4 函数"></a>4.4 函数</h3><ol><li>尽量最短小 <code>golint</code> 行数超过60行会有警告。<ul><li><code>BadCase</code> ： 一个函数400-500行，变量声明就10~20行。给阅读者极大的心理负担，需要花大量时间去理解。</li></ul></li><li>函数应该做一件事（单一原则）, 函数应该做一件事。做好这件事。只做这一件事。</li><li>函数参数,函数请求参数和返回参数(除去<code>context</code>)，理论上不应该超过<code>3</code>个，超过三个可以考虑封装为类。如果参数过多，可以尝试用那个下面几种方式解决：<ol><li>首先可以考虑是不是函数过于复杂，一个大函数能否拆成几个小函数。</li><li>如果函数不能拆，比如<code>new</code>一个对象的需要传递很多参数，这种情况可以考虑使用“构建者模式”来重构。</li><li>将参数封装为对象的方式，来处理参数过多的情况</li></ol></li><li>关于标示参数<br> <img src="https://upload-images.jianshu.io/upload_images/12321605-683cb911a4e6421e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li><li><code>If/else Switch</code>逻辑过于复杂<ul><li><code>BadCase </code>: 函数过长，每个分支逻辑过多。</li><li><code>GoodCase</code>：用多态特性，把相关逻辑封装到各自的类中去</li></ul></li></ol><h3 id="4-5-注释"><a href="#4-5-注释" class="headerlink" title="4.5 注释"></a>4.5 注释</h3><p>注释不能美化糟糕的代码，写注释的时候，可以多想下能否用代码表达清楚，能用函数或变量时表达清楚，就别用注释。<br>注释一定要跟着代码变动一起修改，不然不如不写注释。</p><h4 id="4-5-1-坏注释场景"><a href="#4-5-1-坏注释场景" class="headerlink" title="4.5.1 坏注释场景"></a>4.5.1 坏注释场景</h4><ol><li>喃喃自语，如果只是因为你觉得应该或者因为过程需要就添加注释，那就是无谓之举。如果你决定写注释，就要花必要的时间确保写出最好的注释。</li><li>多余的注释，读这段注释花的时间没准比读代码花的时间还要长。</li><li>误导性注释，尽管初衷可嘉，程序员还是会写出不够精确的注释。</li><li>废话注释，有时，你会看到纯然是废话的注释。它们对于显然之事喋喋不休，毫无新意。</li><li>位置标记，有时，程序员喜欢在源代码中标记某个特别位置。例如，最近我在程序中看到这样一行：// Actions //////////////////////////////////把特定函数趸放在这种标记栏下面，多数时候实属无理。鸡零狗碎，理当删除—特别是尾部那一长串无用的斜杠。</li><li>注释掉的代码，直接把代码注释掉是讨厌的做法。别这么干！其他人不敢删除注释掉的代码。他们会想，代码依然放在那儿，一定有其原因，而且这段代码很重要，不能删除。注释掉的代码堆积在一起，就像破酒瓶底的渣滓一般。</li><li>短函数不需要太多描述。为只做一件事的短函数选个好名字，通常要比写函数头注释要好。</li><li>代码里面的 长期没有清理掉的TODO </li></ol><h4 id="4-5-2-好的注释场景"><a href="#4-5-2-好的注释场景" class="headerlink" title="4.5.2 好的注释场景"></a>4.5.2 好的注释场景</h4><ol><li>用注释来提供基本信息也有其用处。</li><li>对意图的解释,注释不仅提供了有关实现的有用信息，而且还提供了某个决定后面的意图。</li><li>阐释, 注释把某些晦涩难明的参数或返回值的意义翻译为某种可读形式，也会是有用的。通常，更好的方法是尽量让参数或返回值自身就足够清楚；但如果参数或返回值是某个标准库的一部分，或是你不能修改的代码，帮助阐释其含义的代码就会有用。</li><li>警示， 用于警告其他程序员会出现某种后果的注释也是有用的。</li><li>TODO 注释，有时，有理由用//TODO 形式在源代码中放置要做的工作列表。</li><li>放大，注释可以用来放大某种看来不合理之物的重要性。</li></ol><h3 id="4-6-错误和日志"><a href="#4-6-错误和日志" class="headerlink" title="4.6 错误和日志"></a>4.6 错误和日志</h3><ol><li>所有错误<code>new</code>、<code>wrap</code>、收敛到<code>errno</code>包</li><li>返回错误的场景，不需要打印错误，直接<code>wrap</code>一下<code>err</code>返回就行了，在中间件里面统一打<code>Error</code>。</li><li>不返回错误的场景，吞掉的<code>errror</code> 可以打印一条<code>warn</code>日志。</li></ol><h3 id="4-7-关于命名"><a href="#4-7-关于命名" class="headerlink" title="4.7 关于命名"></a>4.7 关于命名</h3><h4 id="4-7-1-名副其实"><a href="#4-7-1-名副其实" class="headerlink" title="4.7.1 名副其实"></a>4.7.1 名副其实</h4><p>名副其实说起来简单。我们想要强调，这事很严肃。选个好名字要花时间，但省下来的时间比花掉的多。<strong>注意命名，而且一旦发现有更好的名称，就换掉旧的</strong>。这么做，读你代码的人（包括你自己）都会更开心。</p><p>变量、函数或类的名称应该已经答复了所有的大问题。它该告诉你，它为什么会存在，它做什么事，应该怎么用。如果名称需要注释来补充，那就不算是名副其实。</p><h4 id="4-7-2-名称长短"><a href="#4-7-2-名称长短" class="headerlink" title="4.7.2 名称长短"></a>4.7.2 名称长短</h4><p>名称长短应与其作用域大小相对应。比如for循环的i，简单明了。如果名称作用域比较大，不推荐使用缩写。</p><h4 id="4-7-3-关于Constants类"><a href="#4-7-3-关于Constants类" class="headerlink" title="4.7.3 关于Constants类"></a>4.7.3 关于Constants类</h4><ol><li> <code>Constants</code> 类拆解为功能更加单一的多个类，比如跟 <code>MySQL</code> 配置相关的常量，我们放到 <code>MysqlConstants</code> 类中；跟 <code>Redis</code> 配置相关的常量，我们放到 <code>RedisConstants</code> 类中。<code>FG</code>的配置放到<code>FG</code>包中、<code>Metric</code>点位放<code>Metric</code>包中。</li><li>剩下的多个地方用到了，就放<code>Constants</code>包中。</li></ol><h4 id="4-7-4-关于Utils类"><a href="#4-7-4-关于Utils类" class="headerlink" title="4.7.4 关于Utils类"></a>4.7.4 关于Utils类</h4><ol><li>在<code>Infrastructure/utils</code>中添加函数的时候，需要想下能不能拆到<code>Infrastructure/pkg</code> 或者其他地方去，实在不知道放哪，再放<code>Infrastructure/utils</code>。<br><img src="https://upload-images.jianshu.io/upload_images/12321605-1e6ec6fff53d0e5e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li></ol><h4 id="4-7-5-关于Common、Helper"><a href="#4-7-5-关于Common、Helper" class="headerlink" title="4.7.5 关于Common、Helper"></a>4.7.5 关于Common、Helper</h4><ol><li>尽量不要用 common 和 helper 命名文件和类，不然最后这个类、文件会变成一个大杂烩，大家什么都往这里面放。</li></ol><p><a href="https://dave.cheney.net/2019/01/08/avoid-package-names-like-base-util-or-common">avoid-package-names-like-base-util-or-common</a></p><h3 id="4-8-关于提交-MergeRequest"><a href="#4-8-关于提交-MergeRequest" class="headerlink" title="4.8  关于提交 MergeRequest"></a>4.8  关于提交 MergeRequest</h3><p>为方便进行代码的<code>Review</code>，一次提交尽量不要超过<code>200</code>行（除过完整的业务功能模块）</p><h3 id="4-9-可测试性"><a href="#4-9-可测试性" class="headerlink" title="4.9 可测试性"></a>4.9 可测试性</h3><p>代码可测试性的好坏，能从侧面上非常准确地反应代码质量的好坏。代码的可测试性差，比较难写单元测试，那基本上就能说明代码设计得有问题。</p><p>单元测试是保证服务稳定性的重要手段之一，时间允许的情况下，开发时间和编写单测时间应该可以达到1:1。在时间排期紧张的情况下，优先保证核心逻辑的测试覆盖率，<code>message</code>项目约定新的<code>Feature</code>开发，每次代码合入测试覆盖率不能低于 <code>60%</code>。</p><p>重构后的服务，必须要保证每个接口都是有回归测试<code>case</code>的，这个作为接口灰度的一个卡点交付物，没有回测<code>case</code>，就不允许接口灰度上线。</p><h2 id="五、参考资料"><a href="#五、参考资料" class="headerlink" title="五、参考资料"></a>五、参考资料</h2><p><a href="https://book.douban.com/subject/30333919/">《整洁架构》 </a></p><p><a href="https://book.douban.com/subject/5442024/">《整洁代码》 </a></p><p><a href="https://time.geekbang.org/column/intro/100037301">DDD 实战课</a></p><p><a href="https://time.geekbang.org/column/article/183007">《设计模式之美》 </a></p>]]></content>
      
      
      <categories>
          
          <category> Architecture </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Golang </tag>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《ETCD实战》</title>
      <link href="/2021/04/11/note/etcd/"/>
      <url>/2021/04/11/note/etcd/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="ectd-常见问题"><a href="#ectd-常见问题" class="headerlink" title="ectd 常见问题"></a>ectd 常见问题</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6a4c12fdbe8c1b3d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="etcd-基础"><a href="#etcd-基础" class="headerlink" title="etcd 基础"></a>etcd 基础</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7a51863c44d3fbbb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="etcd-实践"><a href="#etcd-实践" class="headerlink" title="etcd 实践"></a>etcd 实践</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0045c746c46a13d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="etcd-v2-功能"><a href="#etcd-v2-功能" class="headerlink" title="etcd v2 功能"></a>etcd v2 功能</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-846c8936494bfca4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="etcd-v2-存在的问题"><a href="#etcd-v2-存在的问题" class="headerlink" title="etcd v2 存在的问题"></a>etcd v2 存在的问题</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4d41e6351ebb71e1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li>第一，etcd v2 不支持范围查询和分页。分页对于数据较多的场景是必不可少的。在 Kubernetes 中，在集群规模增大后，Pod、Event 等资源可能会出现数千个以上，但是 etcd v2 不支持分页，不支持范围查询，大包等 expensive request 会导致严重的性能乃至雪崩问题。</li><li>第二，etcd v2 不支持多 key 事务。在实际转账等业务场景中，往往我们需要在一个事务中同时更新多个 key。</li><li>然后是 Watch 机制可靠性问题。Kubernetes 项目严重依赖 etcd Watch 机制，然而 etcd v2 是内存型、不支持保存 key 历史版本的数据库，只在内存中使用滑动窗口保存了最近的 1000 条变更事件，当 etcd server 写请求较多、网络波动时等场景，很容易出现事件丢失问题，进而又触发 client 数据全量拉取，产生大量 expensive request，甚至导致 etcd 雪崩。</li><li>其次是性能瓶颈问题。etcd v2 早期使用了简单、易调试的 HTTP/1.x API，但是随着 Kubernetes 支撑的集群规模越来越大，HTTP/1.x 协议的瓶颈逐渐暴露出来。比如集群规模大时，由于 HTTP/1.x 协议没有压缩机制，批量拉取较多 Pod 时容易导致 APIServer 和 etcd 出现 CPU 高负载、OOM、丢包等问题。</li><li>另一方面，etcd v2 client 会通过 HTTP 长连接轮询 Watch 事件，当 watcher 较多的时候，因 HTTP/1.x 不支持多路复用，会创建大量的连接，消耗 server 端过多的 socket 和内存资源。</li><li>同时 etcd v2 支持为每个 key 设置 TTL 过期时间，client 为了防止 key 的 TTL 过期后被删除，需要周期性刷新 key 的 TTL。</li><li>实际业务中很有可能若干 key 拥有相同的 TTL，可是在 etcd v2 中，即使大量 key TTL 一样，你也需要分别为每个 key 发起续期操作，当 key 较多的时候，这会显著增加集群负载、导致集群性能显著下降。</li><li>最后是内存开销问题。etcd v2 在内存维护了一颗树来保存所有节点 key 及 value。在数据量场景略大的场景，如配置项较多、存储了大量 Kubernetes Events， 它会导致较大的内存开销，同时 etcd 需要定时把全量内存树持久化到磁盘。这会消耗大量的 CPU 和磁盘 I/O 资源，对系统的稳定性造成一定影响。</li></ul><h3 id="k8s-为什么使用-etcd"><a href="#k8s-为什么使用-etcd" class="headerlink" title="k8s 为什么使用 etcd"></a>k8s 为什么使用 etcd</h3><ul><li>一方面当时包括 Consul 在内，没有一个开源项目是十全十美完全满足 Kubernetes 需求。而 CoreOS 团队一直在聆听社区的声音并积极改进，解决社区的痛点。用户吐槽 etcd 不稳定，他们就设计实现自动化的测试方案，模拟、注入各类故障场景，及时发现修复 Bug，以提升 etcd 稳定性。</li><li>另一方面，用户吐槽性能问题，针对 etcd v2 各种先天性缺陷问题，他们从 2015 年就开始设计、实现新一代 etcd v3 方案去解决以上痛点，并积极参与 Kubernetes 项目，负责 etcd v2 到 v3 的存储引擎切换，推动 Kubernetes 项目的前进。同时，设计开发通用压测工具、输出 Consul、ZooKeeper、etcd 性能测试报告，证明 etcd 的优越性。</li></ul><h3 id="etcd-v3-优化了什么？"><a href="#etcd-v3-优化了什么？" class="headerlink" title="etcd v3 优化了什么？"></a>etcd v3 优化了什么？</h3><p>etcd v3 就是为了解决以上稳定性、扩展性、性能问题而诞生的。</p><ul><li>在内存开销、Watch 事件可靠性、功能局限上，它通过引入 B-tree、boltdb 实现一个 MVCC 数据库，数据模型从层次型目录结构改成扁平的 key-value，提供稳定可靠的事件通知，实现了事务，支持多 key 原子更新，同时基于 boltdb 的持久化存储，显著降低了 etcd 的内存占用、避免了 etcd v2 定期生成快照时的昂贵的资源开销。</li><li>性能上，首先 etcd v3 使用了 gRPC API，使用 protobuf 定义消息，消息编解码性能相比 JSON 超过 2 倍以上，并通过 HTTP/2.0 多路复用机制，减少了大量 watcher 等场景下的连接数。</li><li>其次使用 Lease 优化 TTL 机制，每个 Lease 具有一个 TTL，相同的 TTL 的 key 关联一个 Lease，Lease 过期的时候自动删除相关联的所有 key，不再需要为每个 key 单独续期。</li><li>最后是 etcd v3 支持范围、分页查询，可避免大包等 expensive request。</li></ul><h3 id="etcd-v3-基础架构"><a href="#etcd-v3-基础架构" class="headerlink" title="etcd v3  基础架构"></a>etcd v3  基础架构</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f43d1b5966e1970d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li>Client 层：Client 层包括 client v2 和 v3 两个大版本 API 客户端库，提供了简洁易用的 API，同时支持负载均衡、节点间故障自动转移，可极大降低业务使用 etcd 复杂度，提升开发效率、服务可用性。</li><li>API 网络层：API 网络层主要包括 client 访问 server 和 server 节点之间的通信协议。一方面，client 访问 etcd server 的 API 分为 v2 和 v3 两个大版本。v2 API 使用 HTTP/1.x 协议，v3 API 使用 gRPC 协议。同时 v3 通过 etcd grpc-gateway 组件也支持 HTTP/1.x 协议，便于各种语言的服务调用。另一方面，server 之间通信协议，是指节点间通过 Raft 算法实现数据复制和 Leader 选举等功能时使用的 HTTP 协议。</li><li>Raft 算法层：Raft 算法层实现了 Leader 选举、日志复制、ReadIndex 等核心算法特性，用于保障 etcd 多个节点间的数据一致性、提升服务可用性等，是 etcd 的基石和亮点。</li><li>功能逻辑层：etcd 核心特性实现层，如典型的 KVServer 模块、MVCC 模块、Auth 鉴权模块、Lease 租约模块、Compactor 压缩模块等，其中 MVCC 模块主要由 treeIndex 模块和 boltdb 模块组成。</li><li>存储层：存储层包含预写日志 (WAL) 模块、快照 (Snapshot) 模块、boltdb 模块。其中 WAL 可保障 etcd crash 后数据不丢失，boltdb 则保存了集群元数据和用户写入的数据。</li></ul><h2 id="一致性读"><a href="#一致性读" class="headerlink" title="一致性读"></a>一致性读</h2><h3 id="串行读-Serializable"><a href="#串行读-Serializable" class="headerlink" title="串行读(Serializable)"></a>串行读(Serializable)</h3><p>这种直接读状态机数据返回、无需通过 Raft 协议与集群进行交互的模式，在 etcd 里叫做<strong>串行 (Serializable) 读</strong>，它具有低延时、高吞吐量的特点，适合对数据一致性要求不高的场景。</p><p>这里为了帮助你更好的理解读流程，我先简单提下写流程。如下图所示，当 client 发起一个更新 hello 为 world 请求后，若 Leader 收到写请求，它会将此请求持久化到 WAL 日志，并广播给各个节点，若一半以上节点持久化成功，则该请求对应的日志条目被标识为已提交，etcdserver 模块异步从 Raft 模块获取已提交的日志条目，应用到状态机 (boltdb 等)。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ce8de03ddd7686eb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="线性读"><a href="#线性读" class="headerlink" title="线性读"></a>线性读</h3><p>前面我们聊到串行读时提到，它之所以能读到旧数据，主要原因是 Follower 节点收到 Leader 节点同步的写请求后，应用日志条目到状态机是个异步过程，那么我们能否有一种机制在读取的时候，确保最新的数据已经应用到状态机中？</p><p>其实这个机制就是叫 ReadIndex，它是在 etcd 3.1 中引入的，我把简化后的原理图放在了上面。当收到一个线性读请求时，它首先会从 Leader 获取集群最新的已提交的日志索引 (committed index)，如上图中的流程二所示。</p><p>Leader 收到 ReadIndex 请求时，为防止脑裂等异常场景，会向 Follower 节点发送心跳确认，一半以上节点确认 Leader 身份后才能将已提交的索引 (committed index) 返回给节点 C(上图中的流程三)。</p><p>C 节点则会等待，直到状态机已应用索引 (applied index) 大于等于 Leader 的已提交索引时 (committed Index)(上图中的流程四)，然后去通知读请求，数据已赶上 Leader，你可以去状态机中访问数据了 (上图中的流程五)。</p><p>以上就是线性读通过 ReadIndex 机制保证数据一致性原理， 当然还有其它机制也能实现线性读，如在早期 etcd 3.0 中读请求通过走一遍 Raft 协议保证一致性， 这种 Raft log read 机制依赖磁盘 IO， 性能相比 ReadIndex 较差。</p><p>总体而言，KVServer 模块收到线性读请求后，通过架构图中流程三向 Raft 模块发起 ReadIndex 请求，Raft 模块将 Leader 最新的已提交日志索引封装在流程四的 ReadState 结构体，通过 channel 层层返回给线性读模块，线性读模块等待本节点状态机追赶上 Leader 进度，追赶完成后，就通知 KVServer 模块，进行架构图中流程五，与状态机中的 MVCC 模块进行进行交互了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3368ebb43c1137e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="MVCC"><a href="#MVCC" class="headerlink" title="MVCC"></a>MVCC</h2><p>流程五中的多版本并发控制 (Multiversion concurrency control) 模块是为了解决上一讲我们提到 etcd v2 不支持保存 key 的历史版本、不支持多 key 事务等问题而产生的。</p><p>它核心由内存树形索引模块 (treeIndex) 和嵌入式的 KV 持久化存储库 boltdb 组成。</p><p>首先我们需要简单了解下 boltdb，它是个基于 B+ tree 实现的 key-value 键值库，支持事务，提供 Get/Put 等简易 API 给 etcd 操作。</p><p>那么 etcd 如何基于 boltdb 保存一个 key 的多个历史版本呢?</p><p>比如我们现在有以下方案：方案 1 是一个 key 保存多个历史版本的值；方案 2 每次修改操作，生成一个新的版本号 (revision)，以版本号为 key， value 为用户 key-value 等信息组成的结构体。</p><p>很显然方案 1 会导致 value 较大，存在明显读写放大、并发冲突等问题，而方案 2 正是 etcd 所采用的。boltdb 的 key 是全局递增的版本号 (revision)，value 是用户 key、value 等字段组合成的结构体，然后通过 treeIndex 模块来保存用户 key 和版本号的映射关系。</p><p>treeIndex 与 boltdb 关系如下面的读事务流程图所示，从 treeIndex 中获取 key hello 的版本号，再以版本号作为 boltdb 的 key，从 boltdb 中获取其 value 信息。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0655f1f122af857d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="treeIndex"><a href="#treeIndex" class="headerlink" title="treeIndex"></a>treeIndex</h3><p>treeIndex 模块是基于 Google 开源的内存版 btree 库实现的，为什么 etcd 选择上图中的 B-tree 数据结构保存用户 key 与版本号之间的映射关系，而不是哈希表、二叉树呢？在后面的课程中我会再和你介绍。</p><p>treeIndex 模块只会保存用户的 key 和相关版本号信息，用户 key 的 value 数据存储在 boltdb 里面，相比 ZooKeeper 和 etcd v2 全内存存储，etcd v3 对内存要求更低。</p><p>简单介绍了 etcd 如何保存 key 的历史版本后，架构图中流程六也就非常容易理解了， 它需要从 treeIndex 模块中获取 hello 这个 key 对应的版本号信息。treeIndex 模块基于 B-tree 快速查找此 key，返回此 key 对应的索引项 keyIndex 即可。索引项中包含版本号等信息。</p><h3 id="Revision概念"><a href="#Revision概念" class="headerlink" title="Revision概念"></a>Revision概念</h3><p>etcd存储数据时，实际存储引擎中存放的key并不是我们实际PUT的KV中的K，而是以数据的revision作为key，value中存放的是数据的KV。</p><p>revision由2部分组成：{main revision, sub revision}。每次事务main revision都会递增，同一个事务中，每次操作sub revision都会自增1。这两个结合就能保证每次key唯一而且是递增的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2ef64b1e6c19b3e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>举个例子：比如通过批量接口两次更新两对键值，第一次写入数据时，写入&lt;key1,value1&gt;和&lt;key2,value2&gt;，在Etcd这边的存储看来，存放的数据就是这样的：</p><p> revision={1,0}, key=key1, value=value1<br> revision={1,1}, key=key2, value=value2</p><p>而在第二次更新写入数据&lt;key1,update1&gt;和&lt;key2,update2&gt;后，存储中又记录（注意不是覆盖前面的数据）了以下数据：</p><p>revision={2,0}, key=key1, value=update1<br>revision={2,1}, key=key2, value=update2</p><p>对于客户端来说，每次操作的时候是根据Key来进行操作的，所以这里就需要一个Key映射到当前revision的操作了，为了做到这个映射关系，Etcd引入了一个内存中的Btree索引，整个操作过程如下面的流程所示。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1656c71d0bd70123.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>查询时，先通过内存中的btree索引来查询该key对应的keyIndex结构体，然后再根据这个结构体才能去boltdb中查询真实的数据返回。</p><p><a href="https://github.com/etcd-io/etcd/blob/master/server/mvcc/key_index.go">keyIndex相关实现可参考源码</a></p><h3 id="buffer"><a href="#buffer" class="headerlink" title="buffer"></a>buffer</h3><p>在获取到版本号信息后，就可从 boltdb 模块中获取用户的 key-value 数据了。不过有一点你要注意，并不是所有请求都一定要从 boltdb 获取数据。</p><p>etcd 出于数据一致性、性能等考虑，在访问 boltdb 前，首先会从一个内存读事务 buffer 中，二分查找你要访问 key 是否在 buffer 里面，若命中则直接返回。</p><h3 id="boltdb"><a href="#boltdb" class="headerlink" title="boltdb"></a>boltdb</h3><p>若 buffer 未命中，此时就真正需要向 boltdb 模块查询数据了，进入了流程七。</p><p>我们知道 MySQL 通过 table 实现不同数据逻辑隔离，那么在 boltdb 是如何隔离集群元数据与用户数据的呢？答案是 bucket。boltdb 里每个 bucket 类似对应 MySQL 一个表，用户的 key 数据存放的 bucket 名字的是 key，etcd MVCC 元数据存放的 bucket 是 meta。</p><p>因 boltdb 使用 B+ tree 来组织用户的 key-value 数据，获取 bucket key 对象后，通过 boltdb 的游标 Cursor 可快速在 B+ tree 找到 key hello 对应的 value 数据，返回给 client。</p><h2 id="etcd一个写请求是如何执行的？"><a href="#etcd一个写请求是如何执行的？" class="headerlink" title="etcd一个写请求是如何执行的？"></a>etcd一个写请求是如何执行的？</h2><p>首先 client 端通过负载均衡算法选择一个 etcd 节点，发起 gRPC 调用。然后 etcd 节点收到请求后经过 gRPC 拦截器、Quota 模块后，进入 KVServer 模块，KVServer 模块向 Raft 模块提交一个提案，提案内容为“大家好，请使用 put 方法执行一个 key 为 hello，value 为 world 的命令”。</p><p>随后此提案通过 RaftHTTP 网络模块转发、经过集群多数节点持久化后，状态会变成已提交，etcdserver 从 Raft 模块获取已提交的日志条目，传递给 Apply 模块，Apply 模块通过 MVCC 模块执行提案内容，更新状态机。</p><p>与读流程不一样的是写流程还涉及 Quota、WAL、Apply 三个模块。crash-safe 及幂等性也正是基于 WAL 和 Apply 流程的 consistent index 等实现的，因此今天我会重点和你介绍这三个模块。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-751ba712a40e8ddd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="Quota-模块"><a href="#Quota-模块" class="headerlink" title="Quota 模块"></a>Quota 模块</h3><p>我们先从此模块的一个常见错误说起，你在使用 etcd 过程中是否遇到过”etcdserver: mvcc: database space exceeded”错误呢？</p><p>哪些情况会触发这个错误呢？</p><p>一方面默认 db 配额仅为 2G，当你的业务数据、写入 QPS、Kubernetes 集群规模增大后，你的 etcd db 大小就可能会超过 2G。</p><p>另一方面我们知道 etcd v3 是个 MVCC 数据库，保存了 key 的历史版本，当你未配置压缩策略的时候，随着数据不断写入，db 大小会不断增大，导致超限。</p><p>当 etcd server 收到 put/txn 等写请求的时候，会首先检查下当前 etcd db 大小加上你请求的 key-value 大小之和是否超过了配额（quota-backend-bytes）。</p><p>如果超过了配额，它会产生一个告警（Alarm）请求，告警类型是 NO SPACE，并通过 Raft 日志同步给其它节点，告知 db 无空间了，并将告警持久化存储到 db 中。</p><p>最终，无论是 API 层 gRPC 模块还是负责将 Raft 侧已提交的日志条目应用到状态机的 Apply 模块，都拒绝写入，集群只读。</p><p>最后你需要注意配额（quota-backend-bytes）的行为，默认’0’就是使用 etcd 默认的 2GB 大小，你需要根据你的业务场景适当调优。如果你填的是个小于 0 的数，就会禁用配额功能，这可能会让你的 db 大小处于失控，导致性能下降，不建议你禁用配额。</p><h3 id="KVServer-模块"><a href="#KVServer-模块" class="headerlink" title="KVServer 模块"></a>KVServer 模块</h3><p>通过流程二的配额检查后，请求就从 API 层转发到了流程三的 KVServer 模块的 put 方法，我们知道 etcd 是基于 Raft 算法实现节点间数据复制的，因此它需要将 put 写请求内容打包成一个提案消息，提交给 Raft 模块。不过 KVServer 模块在提交提案前，还有如下的一系列检查和限速。</p><h3 id="Preflight-Check"><a href="#Preflight-Check" class="headerlink" title="Preflight Check"></a>Preflight Check</h3><p>为了保证集群稳定性，避免雪崩，任何提交到 Raft 模块的请求，都会做一些简单的限速判断。如下面的流程图所示，首先，如果 Raft 模块已提交的日志索引（committed index）比已应用到状态机的日志索引（applied index）超过了 5000，那么它就返回一个”etcdserver: too many requests”错误给 client。</p><p>然后它会尝试去获取请求中的鉴权信息，若使用了密码鉴权、请求中携带了 token，如果 token 无效，则返回”auth: invalid auth token”错误给 client。</p><p>其次它会检查你写入的包大小是否超过默认的 1.5MB， 如果超过了会返回”etcdserver: request is too large”错误给给 client。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9e6c8d27185e179d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="Propose"><a href="#Propose" class="headerlink" title="Propose"></a>Propose</h3><p>最后通过一系列检查之后，会生成一个唯一的 ID，将此请求关联到一个对应的消息通知 channel，然后向 Raft 模块发起（Propose）一个提案（Proposal），提案内容为“大家好，请使用 put 方法执行一个 key 为 hello，value 为 world 的命令”，也就是整体架构图里的流程四。</p><p>向 Raft 模块发起提案后，KVServer 模块会等待此 put 请求，等待写入结果通过消息通知 channel 返回或者超时。etcd 默认超时时间是 7 秒（5 秒磁盘 IO 延时 +2*1 秒竞选超时时间），如果一个请求超时未返回结果，则可能会出现你熟悉的 etcdserver: request timed out 错误。</p><h3 id="WAL-模块"><a href="#WAL-模块" class="headerlink" title="WAL 模块"></a>WAL 模块</h3><p>Raft 模块收到提案后，如果当前节点是 Follower，它会转发给 Leader，只有 Leader 才能处理写请求。Leader 收到提案后，通过 Raft 模块输出待转发给 Follower 节点的消息和待持久化的日志条目，日志条目则封装了我们上面所说的 put hello 提案内容。</p><p>etcdserver 从 Raft 模块获取到以上消息和日志条目后，作为 Leader，它会将 put 提案消息广播给集群各个节点，同时需要把集群 Leader 任期号、投票信息、已提交索引、提案内容持久化到一个 WAL（Write Ahead Log）日志文件中，用于保证集群的一致性、可恢复性，也就是我们图中的流程五模块。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2d55dbd454774fb9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>上图是 WAL 结构，它由多种类型的 WAL 记录顺序追加写入组成，每个记录由类型、数据、循环冗余校验码组成。不同类型的记录通过 Type 字段区分，Data 为对应记录内容，CRC 为循环校验码信息。</p><ul><li>文件元数据记录包含节点 ID、集群 ID 信息，它在 WAL 文件创建的时候写入；</li><li>日志条目记录包含 Raft 日志信息，如 put 提案内容；状态信息记录，包含集群的任期号、节点投票信息等，一个日志文件中会有多条，以最后的记录为准；</li><li>CRC 记录包含上一个 WAL 文件的最后的 CRC（循环冗余校验码）信息， 在创建、切割 WAL 文件时，作为第一条记录写入到新的 WAL 文件， 用于校验数据文件的完整性、准确性等；</li><li>快照记录包含快照的任期号、日志索引信息，用于检查快照文件的准确性。</li></ul><p>首先我们来看看 put 写请求如何封装在 Raft 日志条目里面。下面是 Raft 日志条目的数据结构信息，它由以下字段组成：</p><ul><li>Term 是 Leader 任期号，随着 Leader 选举增加；</li><li>Index 是日志条目的索引，单调递增增加；</li><li>Type 是日志类型，比如是普通的命令日志（EntryNormal）还是集群配置变更日志（EntryConfChange）；</li><li>Data 保存我们上面描述的 put 提案内容。</li></ul><pre><code>    type Entry struct {       Term             uint64    `protobuf:"varint，2，opt，name=Term" json:"Term"`       Index            uint64    `protobuf:"varint，3，opt，name=Index" json:"Index"`       Type             EntryType `protobuf:"varint，1，opt，name=Type，enum=Raftpb.EntryType" json:"Type"`       Data             []byte    `protobuf:"bytes，4，opt，name=Data" json:"Data，omitempty"`    }    </code></pre><p>了解完 Raft 日志条目数据结构后，我们再看 WAL 模块如何持久化 Raft 日志条目。它首先先将 Raft 日志条目内容（含任期号、索引、提案内容）序列化后保存到 WAL 记录的 Data 字段， 然后计算 Data 的 CRC 值，设置 Type 为 Entry Type， 以上信息就组成了一个完整的 WAL 记录。</p><p>最后计算 WAL 记录的长度，顺序先写入 WAL 长度（Len Field），然后写入记录内容，调用 fsync 持久化到磁盘，完成将日志条目保存到持久化存储中。</p><p>当一半以上节点持久化此日志条目后， Raft 模块就会通过 channel 告知 etcdserver 模块，put 提案已经被集群多数节点确认，提案状态为已提交，你可以执行此提案内容了。</p><h3 id="Apply-模块"><a href="#Apply-模块" class="headerlink" title="Apply 模块"></a>Apply 模块</h3><p>执行 put 提案内容对应我们架构图中的流程七，其细节图如下。那么 Apply 模块是如何执行 put 请求的呢？若 put 请求提案在执行流程七的时候 etcd 突然 crash 了， 重启恢复的时候，etcd 是如何找回异常提案，再次执行的呢？</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-40611f161c5d582a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>核心就是我们上面介绍的 WAL 日志，因为提交给 Apply 模块执行的提案已获得多数节点确认、持久化，etcd 重启时，会从 WAL 中解析出 Raft 日志条目内容，追加到 Raft 日志的存储中，并重放已提交的日志提案给 Apply 模块执行。</p><p>然而这又引发了另外一个问题，如何确保幂等性，防止提案重复执行导致数据混乱呢?</p><p>我们在上一节课里讲到，etcd 是个 MVCC 数据库，每次更新都会生成新的版本号。如果没有幂等性保护，同样的命令，一部分节点执行一次，一部分节点遭遇异常故障后执行多次，则系统的各节点一致性状态无法得到保证，导致数据混乱，这是严重故障。</p><p>因此 etcd 必须要确保幂等性。怎么做呢？Apply 模块从 Raft 模块获得的日志条目信息里，是否有唯一的字段能标识这个提案？</p><p>答案就是我们上面介绍 Raft 日志条目中的索引（index）字段。日志条目索引是全局单调递增的，每个日志条目索引对应一个提案， 如果一个命令执行后，我们在 db 里面也记录下当前已经执行过的日志条目索引，是不是就可以解决幂等性问题呢？</p><p>因此我们在实现上，还需要将两个操作作为原子性事务提交，才能实现幂等。正如我们上面的讨论的这样，etcd 通过引入一个 consistent index 的字段，来存储系统当前已经执行过的日志条目索引，实现幂等性。</p><p>Apply 模块在执行提案内容前，首先会判断当前提案是否已经执行过了，如果执行了则直接返回，若未执行同时无 db 配额满告警，则进入到 MVCC 模块，开始与持久化存储模块打交道。</p><h3 id="MVCC-模块"><a href="#MVCC-模块" class="headerlink" title="MVCC 模块"></a>MVCC 模块</h3><p>Apply 模块判断此提案未执行后，就会调用 MVCC 模块来执行提案内容。MVCC 主要由两部分组成，一个是内存索引模块 treeIndex，保存 key 的历史版本号信息，另一个是 boltdb 模块，用来持久化存储 key-value 数据。那么 MVCC 模块执行 put hello 为 world 命令时，它是如何构建内存索引和保存哪些数据到 db 呢？</p><h4 id="treeIndex-1"><a href="#treeIndex-1" class="headerlink" title="treeIndex"></a>treeIndex</h4><p>首先我们来看 MVCC 的索引模块 treeIndex，当收到更新 key hello 为 world 的时候，此 key 的索引版本号信息是怎么生成的呢？需要维护、持久化存储一个全局版本号吗？</p><p>版本号（revision）在 etcd 里面发挥着重大作用，它是 etcd 的逻辑时钟。etcd 启动的时候默认版本号是 1，随着你对 key 的增、删、改操作而全局单调递增。</p><p>因为 boltdb 中的 key 就包含此信息，所以 etcd 并不需要再去持久化一个全局版本号。我们只需要在启动的时候，从最小值 1 开始枚举到最大值，未读到数据的时候则结束，最后读出来的版本号即是当前 etcd 的最大版本号 currentRevision。</p><p>MVCC 写事务在执行 put hello 为 world 的请求时，会基于 currentRevision 自增生成新的 revision 如{2,0}，然后从 treeIndex 模块中查询 key 的创建版本号、修改次数信息。这些信息将填充到 boltdb 的 value 中，同时将用户的 hello key 和 revision 等信息存储到 B-tree，也就是下面简易写事务图的流程一，整体架构图中的流程八。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fd09e01bcc5f58e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="boltdb-1"><a href="#boltdb-1" class="headerlink" title="boltdb"></a>boltdb</h4><p>MVCC 写事务自增全局版本号后生成的 revision{2,0}，它就是 boltdb 的 key，通过它就可以往 boltdb 写数据了，进入了整体架构图中的流程九。</p><p>boltdb 上一篇我们提过它是一个基于 B+tree 实现的 key-value 嵌入式 db，它通过提供桶（bucket）机制实现类似 MySQL 表的逻辑隔离。</p><p>在 etcd 里面你通过 put/txn 等 KV API 操作的数据，全部保存在一个名为 key 的桶里面，这个 key 桶在启动 etcd 的时候会自动创建。</p><p>除了保存用户 KV 数据的 key 桶，etcd 本身及其它功能需要持久化存储的话，都会创建对应的桶。比如上面我们提到的 etcd 为了保证日志的幂等性，保存了一个名为 consistent index 的变量在 db 里面，它实际上就存储在元数据（meta）桶里面。</p><p>写入 boltdb 的 value， 并不是简单的”world”，如果只存一个用户 value，索引又是保存在易失的内存上，那重启 etcd 后，我们就丢失了用户的 key 名，无法构建 treeIndex 模块了。</p><p>因此为了构建索引和支持 Lease 等特性，etcd 会持久化以下信息:</p><ul><li>key 名称；</li><li>key 创建时的版本号（create_revision）、最后一次修改时的版本号（mod_revision）、key 自身修改的次数（version）；</li><li>value 值；</li><li>租约信息（后面介绍）。</li></ul><p>boltdb value 的值就是将含以上信息的结构体序列化成的二进制数据，然后通过 boltdb 提供的 put 接口，etcd 就快速完成了将你的数据写入 boltdb，对应上面简易写事务图的流程二。</p><p>事务提交的过程，包含 B+tree 的平衡、分裂，将 boltdb 的脏数据（dirty page）、元数据信息刷新到磁盘，因此事务提交的开销是昂贵的。如果我们每次更新都提交事务，etcd 写性能就会较差。</p><p>那么解决的办法是什么呢？etcd 的解决方案是合并再合并。</p><p>首先 boltdb key 是版本号，put/delete 操作时，都会基于当前版本号递增生成新的版本号，因此属于顺序写入，可以调整 boltdb 的 bucket.FillPercent 参数，使每个 page 填充更多数据，减少 page 的分裂次数并降低 db 空间。</p><p>其次 etcd 通过合并多个写事务请求，通常情况下，是异步机制定时（默认每隔 100ms）将批量事务一次性提交（pending 事务过多才会触发同步提交）， 从而大大提高吞吐量，对应上面简易写事务图的流程三。</p><p>但是这优化又引发了另外的一个问题， 因为事务未提交，读请求可能无法从 boltdb 获取到最新数据。</p><p>为了解决这个问题，etcd 引入了一个 bucket buffer 来保存暂未提交的事务数据。在更新 boltdb 的时候，etcd 也会同步数据到 bucket buffer。因此 etcd 处理读请求的时候会优先从 bucket buffer 里面读取，其次再从 boltdb 读，通过 bucket buffer 实现读写性能提升，同时保证数据一致性。</p><h2 id="鉴权：如何保护你的数据安全？"><a href="#鉴权：如何保护你的数据安全？" class="headerlink" title="鉴权：如何保护你的数据安全？"></a>鉴权：如何保护你的数据安全？</h2><p><img src="https://upload-images.jianshu.io/upload_images/12321605-cddee559e8142793.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e98fff394c4715fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Lease"><a href="#Lease" class="headerlink" title="Lease"></a>Lease</h2><p>我们今天的主题，Lease，正是基于主动型上报模式，提供的一种活性检测机制。Lease 顾名思义，client 和 etcd server 之间存在一个约定，内容是 etcd server 保证在约定的有效期内（TTL），不会删除你关联到此 Lease 上的 key-value。</p><p>若你未在有效期内续租，那么 etcd server 就会删除 Lease 和其关联的 key-value。</p><p>etcd 在启动的时候，创建 Lessor 模块的时候，它会启动两个常驻 goroutine，如上图所示，一个是 RevokeExpiredLease 任务，定时检查是否有过期 Lease，发起撤销过期的 Lease 操作。一个是 CheckpointScheduledLease，定时触发更新 Lease 的剩余到期时间的操作。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5a293b9ee062032d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="key-如何关联-Lease"><a href="#key-如何关联-Lease" class="headerlink" title="key 如何关联 Lease"></a>key 如何关联 Lease</h3><pre><code># 创建一个TTL为600秒的lease，etcd server返回LeaseID$ etcdctl lease grant 600lease 326975935f48f814 granted with TTL(600s)# 查看lease的TTL、剩余时间$ etcdctl lease timetolive 326975935f48f814lease 326975935f48f814 granted with TTL(600s)， remaining(590s)</code></pre><p>很简单，KV 模块的 API 接口提供了一个”–lease”参数，你可以通过如下命令，将 key node 关联到对应的 LeaseID 上。然后你查询的时候增加 -w 参数输出格式为 json，就可查看到 key 关联的 LeaseID。</p><pre><code>$ etcdctl put node healthy --lease 326975935f48f818OK$ etcdctl get node -w=json | python -m json.tool{    "kvs":[        {            "create_revision":24，            "key":"bm9kZQ=="，            "Lease":3632563850270275608，            "mod_revision":24，            "value":"aGVhbHRoeQ=="，            "version":1        }    ]}</code></pre><h3 id="如何优化-Lease-续期性能"><a href="#如何优化-Lease-续期性能" class="headerlink" title="如何优化 Lease 续期性能"></a>如何优化 Lease 续期性能</h3><p>Lease 续期其实很简单，核心是将 Lease 的过期时间更新为当前系统时间加其 TTL。关键问题在于续期的性能能否满足业务诉求。</p><p>etcd v3 版本为了解决以上问题，提出了 Lease 特性，TTL 属性转移到了 Lease 上， 同时协议从 HTTP/1.x 优化成 gRPC 协议。</p><p>一方面不同 key 若 TTL 相同，可复用同一个 Lease， 显著减少了 Lease 数。另一方面，通过 gRPC HTTP/2 实现了多路复用，流式传输，同一连接可支持为多个 Lease 续期，大大减少了连接数。</p><h3 id="如何高效淘汰过期-Lease"><a href="#如何高效淘汰过期-Lease" class="headerlink" title="如何高效淘汰过期 Lease"></a>如何高效淘汰过期 Lease</h3><p>etcd 早期的时候，淘汰 Lease 非常暴力。etcd 会直接遍历所有 Lease，逐个检查 Lease 是否过期，过期则从 Lease 关联的 key 集合中，取出 key 列表，删除它们，时间复杂度是 O(N)。</p><p>然而这种方案随着 Lease 数增大，毫无疑问它的性能会变得越来越差。我们能否按过期时间排序呢？这样每次只需轮询、检查排在前面的 Lease 过期时间，一旦轮询到未过期的 Lease， 则可结束本轮检查。</p><p>刚刚说的就是 etcd Lease 高效淘汰方案最小堆的实现方法。每次新增 Lease、续期的时候，它会插入、更新一个对象到最小堆中，对象含有 LeaseID 和其到期时间 unixnano，对象之间按到期时间升序排序。</p><p>etcd Lessor 主循环每隔 500ms 执行一次撤销 Lease 检查（RevokeExpiredLease），每次轮询堆顶的元素，若已过期则加入到待淘汰列表，直到堆顶的 Lease 过期时间大于当前，则结束本轮轮询。</p><p>相比早期 O(N) 的遍历时间复杂度，使用堆后，插入、更新、删除，它的时间复杂度是 O(Log N)，查询堆顶对象是否过期时间复杂度仅为 O(1)，性能大大提升，可支撑大规模场景下 Lease 的高效淘汰。</p><p>Lessor 模块会将已确认过期的 LeaseID，保存在一个名为 expiredC 的 channel 中，而 etcd server 的主循环会定期从 channel 中获取 LeaseID，发起 revoke 请求，通过 Raft Log 传递给 Follower 节点。</p><p>各个节点收到 revoke Lease 请求后，获取关联到此 Lease 上的 key 列表，从 boltdb 中删除 key，从 Lessor 的 Lease map 内存中删除此 Lease 对象，最后还需要从 boltdb 的 Lease bucket 中删除这个 Lease。</p><h2 id="MVCC：如何实现多版本并发控制？"><a href="#MVCC：如何实现多版本并发控制？" class="headerlink" title="MVCC：如何实现多版本并发控制？"></a>MVCC：如何实现多版本并发控制？</h2><pre><code># 更新key hello为world1$ etcdctl put hello world1OK# 通过指定输出模式为json,查看key hello更新后的详细信息$ etcdctl get hello -w=json{    "kvs":[        {            "key":"aGVsbG8=",            "create_revision":2,            "mod_revision":2,            "version":1,            "value":"d29ybGQx"        }    ],    "count":1}# 再次修改key hello为world2$ etcdctl put hello world2OK# 确认修改成功,最新值为wolrd2$ etcdctl get hellohelloworld2# 指定查询版本号,获得了hello上一次修改的值$ etcdctl get hello --rev=2helloworld1# 删除key hello$ etcdctl del  hello1# 删除后指定查询版本号3,获得了hello删除前的值$ etcdctl get hello --rev=3helloworld2</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-983f7f459b1f3570.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Apply 模块通过 MVCC 模块来执行 put 请求，持久化 key-value 数据。MVCC 模块将请求请划分成两个类别，分别是读事务（ReadTxn）和写事务（WriteTxn）。读事务负责处理 range 请求，写事务负责 put/delete 操作。读写事务基于 treeIndex、Backend/boltdb 提供的能力，实现对 key-value 的增删改查功能。</p><p>treeIndex 模块基于内存版 B-tree 实现了 key 索引管理，它保存了用户 key 与版本号（revision）的映射关系等信息。</p><p>Backend 模块负责 etcd 的 key-value 持久化存储，主要由 ReadTx、BatchTx、Buffer 组成，ReadTx 定义了抽象的读事务接口，BatchTx 在 ReadTx 之上定义了抽象的写事务接口，Buffer 是数据缓存区。</p><p>etcd 设计上支持多种 Backend 实现，目前实现的 Backend 是 boltdb。boltdb 是一个基于 B+ tree 实现的、支持事务的 key-value 嵌入式数据库。</p><p>treeIndex 与 boltdb 关系你可参考下图。当你发起一个 get hello 命令时，从 treeIndex 中获取 key 的版本号，然后再通过这个版本号，从 boltdb 获取 value 信息。boltdb 的 value 是包含用户 key-value、各种版本号、lease 信息的结构体。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a3a84c6e3b9c02e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="treeIndex-原理"><a href="#treeIndex-原理" class="headerlink" title="treeIndex 原理"></a>treeIndex 原理</h3><p>对于 etcd v2 来说，当你通过 etcdctl 发起一个 put hello 操作时，etcd v2 直接更新内存树，这就导致历史版本直接被覆盖，无法支持保存 key 的历史版本。在 etcd v3 中引入 treeIndex 模块正是为了解决这个问题，支持保存 key 的历史版本，提供稳定的 Watch 机制和事务隔离等能力。</p><p>下面我就为你介绍下，etcd 保存用户 key 与版本号映射关系的数据结构 B-tree，为什么 etcd 使用它而不使用哈希表、平衡二叉树？</p><p>从 etcd 的功能特性上分析， 因 etcd 支持范围查询，因此保存索引的数据结构也必须支持范围查询才行。所以哈希表不适合，而 B-tree 支持范围查询。</p><pre><code>type keyIndex struct {   key         []byte //用户的key名称，比如我们案例中的"hello"   modified    revision //最后一次修改key时的etcd版本号,比如我们案例中的刚写入hello为world1时的，版本号为2   generations []generation //generation保存了一个key若干代版本号信息，每代中包含对key的多次修改的版本号列表}</code></pre><p>keyIndex 中包含用户的 key、最后一次修改 key 时的 etcd 版本号、key 的若干代（generation）版本号信息，每代中包含对 key 的多次修改的版本号列表。那我们要如何理解 generations？为什么它是个数组呢?</p><p>generations 表示一个 key 从创建到删除的过程，每代对应 key 的一个生命周期的开始与结束。当你第一次创建一个 key 时，会生成第 0 代，后续的修改操作都是在往第 0 代中追加修改版本号。当你把 key 删除后，它就会生成新的第 1 代，一个 key 不断经历创建、删除的过程，它就会生成多个代。</p><pre><code>type generation struct {   ver     int64    //表示此key的修改次数   created revision //表示generation结构创建时的版本号   revs    []revision //每次修改key时的revision追加到此数组}</code></pre><p>generation 结构中包含此 key 的修改次数、generation 创建时的版本号、对此 key 的修改版本号记录列表。</p><p>你需要注意的是版本号（revision）并不是一个简单的整数，而是一个结构体。revision 结构及含义如下：</p><pre><code>type revision struct {   main int64    // 一个全局递增的主版本号，随put/txn/delete事务递增，一个事务内的key main版本号是一致的   sub int64    // 一个事务内的子版本号，从0开始随事务内put/delete操作递增}</code></pre><p>revision 包含 main 和 sub 两个字段，main 是全局递增的版本号，它是个 etcd 逻辑时钟，随着 put/txn/delete 等事务递增。sub 是一个事务内的子版本号，从 0 开始随事务内的 put/delete 操作递增。</p><h3 id="MVCC-更新-key-原理"><a href="#MVCC-更新-key-原理" class="headerlink" title="MVCC 更新 key 原理"></a>MVCC 更新 key 原理</h3><p>当你通过 etcdctl 发起一个 put hello 操作时，如下面的 put 事务流程图流程一所示，在 put 写事务中，首先它需要从 treeIndex 模块中查询 key 的 keyIndex 索引信息，keyIndex 中存储了 key 的创建版本号、修改的次数等信息，这些信息在事务中发挥着重要作用，因此会存储在 boltdb 的 value 中。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ac5b5fa820818395.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>其次 etcd 会根据当前的全局版本号（空集群启动时默认为 1）自增，生成 put hello 操作对应的版本号 revision{2,0}，这就是 boltdb 的 key。</p><p>boltdb 的 value 是 mvccpb.KeyValue 结构体，它是由用户 key、value、create_revision、mod_revision、version、lease 组成。它们的含义分别如下：</p><ul><li>create_revision 表示此 key 创建时的版本号。在我们的案例中，key hello 是第一次创建，那么值就是 2。当你再次修改 key hello 的时候，写事务会从 treeIndex 模块查询 hello 第一次创建的版本号，也就是 keyIndex.generations[i].created 字段，赋值给 create_revision 字段；</li><li>mod_revision 表示 key 最后一次修改时的版本号，即 put 操作发生时的全局版本号加 1；</li><li>version 表示此 key 的修改次数。每次修改的时候，写事务会从 treeIndex 模块查询 hello 已经历过的修改次数，也就是 keyIndex.generations[i].ver 字段，将 ver 字段值加 1 后，赋值给 version 字段。</li></ul><p>填充好 boltdb 的 KeyValue 结构体后，这时就可以通过 Backend 的写事务 batchTx 接口将 <strong>key{2,0},value 为 mvccpb.KeyValue</strong> 保存到 boltdb 的缓存中，并同步更新 buffer，如上图中的流程二所示。</p><p>此时存储到 boltdb 中的 key、value 数据如下：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b98cc7e1e9988749.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因为 key hello 是首次创建，treeIndex 模块它会生成 key hello 对应的 keyIndex 对象，并填充相关数据结构。</p><p>keyIndex 填充后的结果如下所示：</p><pre><code>key hello的keyIndex:key:     "hello"modified: &lt;2,0&gt;generations:[{ver:1,created:&lt;2,0&gt;,revisions: [&lt;2,0&gt;]} ]</code></pre><ul><li>key 为 hello，modified 为最后一次修改版本号 &lt;2,0&gt;，key hello 是首次创建的，因此新增一个 generation 代跟踪它的生命周期、修改记录；</li><li>generation 的 ver 表示修改次数，首次创建为 1，后续随着修改操作递增；</li><li>generation.created 表示创建 generation 时的版本号为 &lt;2,0&gt;；</li><li>revision 数组保存对此 key 修改的版本号列表，每次修改都会将将相应的版本号追加到 revisions 数组中。</li></ul><p>但是此时数据还并未持久化，为了提升 etcd 的写吞吐量、性能，一般情况下（默认堆积的写事务数大于 1 万才在写事务结束时同步持久化），数据持久化由 Backend 的异步 goroutine 完成，它通过事务批量提交，定时将 boltdb 页缓存中的脏数据提交到持久化存储磁盘中，也就是下图中的黑色虚线框住的流程四。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5819f0fc04d4ecce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="MVCC-查询-key-原理"><a href="#MVCC-查询-key-原理" class="headerlink" title="MVCC 查询 key 原理"></a>MVCC 查询 key 原理</h3><p>完成 put hello 为 world1 操作后，这时你通过 etcdctl 发起一个 get hello 操作，MVCC 模块首先会创建一个读事务对象（TxnRead），在 etcd 3.4 中 Backend 实现了 ConcurrentReadTx， 也就是并发读特性。</p><p>并发读特性的核心原理是创建读事务对象时，它会全量拷贝当前写事务未提交的 buffer 数据，并发的读写事务不再阻塞在一个 buffer 资源锁上，实现了全并发读。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-511525a8d64a72b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如上图所示，在读事务中，它首先需要根据 key 从 treeIndex 模块获取版本号，因我们未带版本号读，默认是读取最新的数据。treeIndex 模块从 B-tree 中，根据 key 查找到 keyIndex 对象后，匹配有效的 generation，返回 generation 的 revisions 数组中最后一个版本号{2,0}给读事务对象。</p><p>读事务对象根据此版本号为 key，通过 Backend 的并发读事务（ConcurrentReadTx）接口，优先从 buffer 中查询，命中则直接返回，否则从 boltdb 中查询此 key 的 value 信息。</p><h3 id="MVCC-删除-key-原理"><a href="#MVCC-删除-key-原理" class="headerlink" title="MVCC 删除 key 原理"></a>MVCC 删除 key 原理</h3><p>介绍完 MVCC 更新、查询 key 的原理后，我们接着往下看。当你执行 etcdctl del hello 命令时，etcd 会立刻从 treeIndex 和 boltdb 中删除此数据吗？还是增加一个标记实现延迟删除（lazy delete）呢？</p><p>答案为 etcd 实现的是延期删除模式，原理与 key 更新类似。</p><p>与更新 key 不一样之处在于，一方面，生成的 boltdb key 版本号{4,0,t}追加了删除标识（tombstone, 简写 t），boltdb value 变成只含用户 key 的 KeyValue 结构体。另一方面 treeIndex 模块也会给此 key hello 对应的 keyIndex 对象，追加一个空的 generation 对象，表示此索引对应的 key 被删除了。</p><p>当你再次查询 hello 的时候，treeIndex 模块根据 key hello 查找到 keyindex 对象后，若发现其存在空的 generation 对象，并且查询的版本号大于等于被删除时的版本号，则会返回空。</p><p>etcdctl hello 操作后的 keyIndex 的结果如下面所示：</p><pre><code>key hello的keyIndex:key:     "hello"modified: &lt;4,0&gt;generations:[{ver:3,created:&lt;2,0&gt;,revisions: [&lt;2,0&gt;,&lt;3,0&gt;,&lt;4,0&gt;(t)]}，             {empty}]</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-bd73ce3027334152.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>那么 key 打上删除标记后有哪些用途呢？什么时候会真正删除它呢？</p><p>一方面删除 key 时会生成 events，Watch 模块根据 key 的删除标识，会生成对应的 Delete 事件。</p><p>另一方面，当你重启 etcd，遍历 boltdb 中的 key 构建 treeIndex 内存树时，你需要知道哪些 key 是已经被删除的，并为对应的 key 索引生成 tombstone 标识。而真正删除 treeIndex 中的索引对象、boltdb 中的 key 是通过压缩 (compactor) 组件异步完成。</p><h2 id="Watche"><a href="#Watche" class="headerlink" title="Watche"></a>Watche</h2><p>etcd 基于以上介绍的 HTTP/2 协议的多路复用等机制，实现了一个 client/TCP 连接支持多 gRPC Stream， 一个 gRPC Stream 又支持多个 watcher，如下图所示。同时事件通知模式也从 client 轮询优化成 server 流式推送，极大降低了 server 端 socket、内存等资源。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0501ac92c336b802.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="滑动窗口-vs-MVCC"><a href="#滑动窗口-vs-MVCC" class="headerlink" title="滑动窗口 vs MVCC"></a>滑动窗口 vs MVCC</h3><p>介绍完 etcd v2 的轮询机制和 etcd v3 的流式推送机制后，再看第二个问题，事件是如何存储的？ 会保留多久呢？watch 命令中的版本号具有什么作用？</p><p>第二个问题的本质是历史版本存储，etcd 经历了从滑动窗口到 MVCC 机制的演变，滑动窗口是仅保存有限的最近历史版本到内存中，而 MVCC 机制则将历史版本保存在磁盘中，避免了历史版本的丢失，极大的提升了 Watch 机制的可靠性。</p><p>etcd v2 滑动窗口是如何实现的？它有什么缺点呢？</p><p>它使用的是如下一个简单的环形数组来存储历史事件版本，当 key 被修改后，相关事件就会被添加到数组中来。若超过 eventQueue 的容量，则淘汰最旧的事件。在 etcd v2 中，eventQueue 的容量是固定的 1000，因此它最多只会保存 1000 条事件记录，不会占用大量 etcd 内存导致 etcd OOM。</p><pre><code>type EventHistory struct {   Queue      eventQueue   StartIndex uint64   LastIndex  uint64   rwl        sync.RWMutex}</code></pre><p>但是它的缺陷显而易见的，固定的事件窗口只能保存有限的历史事件版本，是不可靠的。当写请求较多的时候、client 与 server 网络出现波动等异常时，很容易导致事件丢失，client 不得不触发大量的 expensive 查询操作，以获取最新的数据及版本号，才能持续监听数据。</p><p>特别是对于重度依赖 Watch 机制的 Kubernetes 来说，显然是无法接受的。因为这会导致控制器等组件频繁的发起 expensive List Pod 等资源操作，导致 APIServer/etcd 出现高负载、OOM 等，对稳定性造成极大的伤害。</p><p>etcd v3 的 MVCC 机制，正如上一节课所介绍的，就是为解决 etcd v2 Watch 机制不可靠而诞生。相比 etcd v2 直接保存事件到内存的环形数组中，etcd v3 则是将一个 key 的历史修改版本保存在 boltdb 里面。boltdb 是一个基于磁盘文件的持久化存储，因此它重启后历史事件不像 etcd v2 一样会丢失，同时你可通过配置压缩策略，来控制保存的历史版本数，在压缩篇我会和你详细讨论它。</p><h3 id="可靠的事件推送机制"><a href="#可靠的事件推送机制" class="headerlink" title="可靠的事件推送机制"></a>可靠的事件推送机制</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-37750e426fa6a9c7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="异常场景重试机制"><a href="#异常场景重试机制" class="headerlink" title="异常场景重试机制"></a>异常场景重试机制</h3><p>若出现 channel buffer 满了，etcd 为了保证 Watch 事件的高可靠性，并不会丢弃它，而是将此 watcher 从 synced watcherGroup 中删除，然后将此 watcher 和事件列表保存到一个名为受害者 victim 的 watcherBatch 结构中，通过异步机制重试保证事件的可靠性。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9cc922a788e4f495.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="历史事件推送机制"><a href="#历史事件推送机制" class="headerlink" title="历史事件推送机制"></a>历史事件推送机制</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8a5b1669ee85cb73.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="高效的事件匹配"><a href="#高效的事件匹配" class="headerlink" title="高效的事件匹配"></a>高效的事件匹配</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c91e3a1301791881.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="事务实现"><a href="#事务实现" class="headerlink" title="事务实现"></a>事务实现</h2><h3 id="事务特性初体验及-API"><a href="#事务特性初体验及-API" class="headerlink" title="事务特性初体验及 API"></a>事务特性初体验及 API</h3><p>etcd v3 为了解决多 key 的原子操作问题，提供了全新迷你事务 API，同时基于 MVCC 版本号，它可以实现各种隔离级别的事务。它的基本结构如下：</p><pre><code>client.Txn(ctx).If(cmp1, cmp2, ...).Then(op1, op2, ...,).Else(op1, op2, …)</code></pre><p>从上面结构中你可以看到，事务 API 由 If 语句、Then 语句、Else 语句组成，这与我们平时常见的 MySQL 事务完全不一样。</p><p>那么 If 语句支持哪些检查项呢？</p><p>首先是 key 的最近一次修改版本号 mod_revision，简称 mod。你可以通过它检查 key 最近一次被修改时的版本号是否符合你的预期。比如当你查询到 Alice 账号资金为 100 元时，它的 mod_revision 是 v1，当你发起转账操作时，你得确保 Alice 账号上的 100 元未被挪用，这就可以通过 mod(“Alice”) = “v1” 条件表达式来保障转账安全性。</p><p>其次是 key 的创建版本号 create_revision，简称 create。你可以通过它检查 key 是否已存在。比如在分布式锁场景里，只有分布式锁 key(lock) 不存在的时候，你才能发起 put 操作创建锁，这时你可以通过 create(“lock”) = “0”来判断，因为一个 key 不存在的话它的 create_revision 版本号就是 0。</p><p>接着是 key 的修改次数 version。你可以通过它检查 key 的修改次数是否符合预期。比如你期望 key 在修改次数小于 3 时，才能发起某些操作时，可以通过 version(“key”) &lt; “3”来判断。</p><p>最后是 key 的 value 值。你可以通过检查 key 的 value 值是否符合预期，然后发起某些操作。比如期望 Alice 的账号资金为 200, value(“Alice”) = “200”。</p><p>If 语句通过以上 MVCC 版本号、value 值、各种比较运算符 (等于、大于、小于、不等于)，实现了灵活的比较的功能，满足你各类业务场景诉求。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-571a74d40acf7c2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="boltdb：如何持久化存储你的key-value数据？"><a href="#boltdb：如何持久化存储你的key-value数据？" class="headerlink" title="boltdb：如何持久化存储你的key-value数据？"></a>boltdb：如何持久化存储你的key-value数据？</h2><h3 id="boltdb-磁盘布局"><a href="#boltdb-磁盘布局" class="headerlink" title="boltdb 磁盘布局"></a>boltdb 磁盘布局</h3><p>在介绍一个 put 写请求在 boltdb 中执行原理前，我先给你从整体上介绍下平时你所看到的 etcd db 文件的磁盘布局，让你了解下 db 文件的物理存储结构。</p><p>boltdb 文件指的是你 etcd 数据目录下的 member/snap/db 的文件， etcd 的 key-value、lease、meta、member、cluster、auth 等所有数据存储在其中。etcd 启动的时候，会通过 mmap 机制将 db 文件映射到内存，后续可从内存中快速读取文件中的数据。写请求通过 fwrite 和 fdatasync 来写入、持久化数据到磁盘。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-846346b1123f194f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>上图是我给你画的 db 文件磁盘布局，从图中的左边部分你可以看到，文件的内容由若干个 page 组成，一般情况下 page size 为 4KB。</p><p>page 按照功能可分为元数据页 (meta page)、B+ tree 索引节点页 (branch page)、B+ tree 叶子节点页 (leaf page)、空闲页管理页 (freelist page)、空闲页 (free page)。</p><p>文件最开头的两个 page 是固定的 db 元数据 meta page，空闲页管理页记录了 db 中哪些页是空闲、可使用的。索引节点页保存了 B+ tree 的内部节点，如图中的右边部分所示，它们记录了 key 值，叶子节点页记录了 B+ tree 中的 key-value 和 bucket 数据。</p><p>boltdb 逻辑上通过 B+ tree 来管理 branch/leaf page， 实现快速查找、写入 key-value 数据。</p><pre><code>// 打开boltdb文件，获取db对象db,err := bolt.Open("db"， 0600， nil)if err != nil {   log.Fatal(err)}defer db.Close()// 参数true表示创建一个写事务，false读事务tx,err := db.Begin(true)if err != nil {   return err}defer tx.Rollback()// 使用事务对象创建key bucketb,err := tx.CreatebucketIfNotExists([]byte("key"))if err != nil {   return err}// 使用bucket对象更新一个keyif err := b.Put([]byte("r94"),[]byte("world")); err != nil {   return err}// 提交事务if err := tx.Commit(); err != nil {   return err}</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b392ead6bab445d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0b9af91f9c9173f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ba226a9d2a3b9240.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1678714314db2118.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="如何回收旧版本数据？"><a href="#如何回收旧版本数据？" class="headerlink" title="如何回收旧版本数据？"></a>如何回收旧版本数据？</h2><p>在07里，我们知道 etcd 中的每一次更新、删除 key 操作，treeIndex 的 keyIndex 索引中都会追加一个版本号，在 boltdb 中会生成一个新版本 boltdb key 和 value。也就是随着你不停更新、删除，你的 etcd 进程内存占用和 db 文件就会越来越大。很显然，这会导致 etcd OOM 和 db 大小增长到最大 db 配额，最终不可写。</p><p>那么 etcd 是通过什么机制来回收历史版本数据，控制索引内存占用和 db 大小的呢？</p><p>这就是我今天要和你分享的 etcd 压缩机制。希望通过今天的这节课，能帮助你理解 etcd 压缩原理，在使用 etcd 过程中能根据自己的业务场景，选择适合的压缩策略，避免 db 大小增长失控而不可写入，帮助你构建稳定的 etcd 服务。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-287671745c668ffc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在了解 etcd 压缩模块实现细节前，我先给你画了一幅压缩模块的整体架构图。从图中可知，你可以通过 client API 发起人工的压缩 (Compact) 操作，也可以配置自动压缩策略。在自动压缩策略中，你可以根据你的业务场景选择合适的压缩模式。目前 etcd 支持两种压缩模式，分别是时间周期性压缩和版本号压缩。</p><p>当你通过 API 发起一个 Compact 请求后，KV Server 收到 Compact 请求提交到 Raft 模块处理，在 Raft 模块中提交后，Apply 模块就会通过 MVCC 模块的 Compact 接口执行此压缩任务。</p><p>Compact 接口首先会更新当前 server 已压缩的版本号，并将耗时昂贵的压缩任务保存到 FIFO 队列中异步执行。压缩任务执行时，它首先会压缩 treeIndex 模块中的 keyIndex 索引，其次会遍历 boltdb 中的 key，删除已废弃的 key。</p><p>以上就是压缩模块的一个工作流程。接下来我会首先和你介绍如何人工发起一个 Compact 操作，然后详细介绍周期性压缩模式、版本号压缩模式的工作原理，最后再给你介绍 Compact 操作核心的原理。</p><p><strong>压缩的本质是回收历史版本，目标对象仅是历史版本，不包括一个 key-value 数据的最新版本，因此你可以放心执行压缩命令，不会删除你的最新版本数据</strong>。不过我在08介绍 Watch 机制时提到，Watch 特性中的历史版本数据同步，依赖于 MVCC 中是否还保存了相关数据，因此我建议你不要每次简单粗暴地回收所有历史版本。</p><p>在生产环境中，我建议你精细化的控制历史版本数，那如何实现精细化控制呢？</p><p>主要有两种方案，一种是使用 etcd server 的自带的自动压缩机制，根据你的业务场景，配置合适的压缩策略即可。</p><p>另外一种方案是如果你觉得 etcd server 的自带压缩机制无法满足你的诉求，想更精细化的控制 etcd 保留的历史版本记录，你就可以基于 etcd 的 Compact API，在业务逻辑代码中、或定时任务中主动触发压缩操作。你需要确保发起 Compact 操作的程序高可用，压缩的频率、保留的历史版本在合理范围内，并最终能使 etcd 的 db 大小保持平稳，否则会导致 db 大小不断增长，直至 db 配额满，无法写入。</p><p>在一般情况下，我建议使用 etcd 自带的压缩机制。它支持两种模式，分别是按时间周期性压缩和保留版本号的压缩，配置相应策略后，etcd 节点会自动化的发起 Compact 操作。</p><h3 id="压缩原理"><a href="#压缩原理" class="headerlink" title="压缩原理"></a>压缩原理</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3478a93864ada710.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fff634ecdd231637.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如上图所示，因此异步压缩任务的第一项工作，就是压缩 treeIndex 模块中的各 key 的历史版本、已删除的版本。为了避免压缩工作影响读写性能，首先会克隆一个 B-tree，然后通过克隆后的 B-tree 遍历每一个 keyIndex 对象，压缩历史版本号、清理已删除的版本。</p><p>假设当前压缩的版本号是 CompactedRev， 它会保留 keyIndex 中最大的版本号，移除小于等于 CompactedRev 的版本号，并通过一个 map 记录 treeIndex 中有效的版本号返回给 boltdb 模块使用。</p><p>为什么要保留最大版本号呢?</p><p>因为最大版本号是这个 key 的最新版本，移除了会导致 key 丢失。而 Compact 的目的是回收旧版本。当然如果 keyIndex 中的最大版本号被打了删除标记 (tombstone)， 就会从 treeIndex 中删除这个 keyIndex，否则会出现内存泄露。</p><p>Compact 任务执行完索引压缩后，它通过遍历 B-tree、keyIndex 中的所有 generation 获得当前内存索引模块中有效的版本号，这些信息将帮助 etcd 清理 boltdb 中的废弃历史版本。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-92ca3bf81e4b4eec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>压缩任务的第二项工作就是删除 boltdb 中废弃的历史版本数据。如上图所示，它通过 etcd 一个名为 scheduleCompaction 任务来完成。</p><p>scheduleCompaction 任务会根据 key 区间，从 0 到 CompactedRev 遍历 boltdb 中的所有 key，通过 treeIndex 模块返回的有效索引信息，判断这个 key 是否有效，无效则调用 boltdb 的 delete 接口将 key-value 数据删除。</p><p>在这过程中，scheduleCompaction 任务还会更新当前 etcd 已经完成的压缩版本号 (finishedCompactRev)，将其保存到 boltdb 的 meta bucket 中。</p><p>scheduleCompaction 任务遍历、删除 key 的过程可能会对 boltdb 造成压力，为了不影响正常读写请求，它在执行过程中会通过参数控制每次遍历、删除的 key 数（默认为 100，每批间隔 10ms），分批完成 boltdb key 的删除操作。</p><h2 id="db大小：为什么etcd社区建议db大小不超过8G？"><a href="#db大小：为什么etcd社区建议db大小不超过8G？" class="headerlink" title="db大小：为什么etcd社区建议db大小不超过8G？"></a>db大小：为什么etcd社区建议db大小不超过8G？</h2><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7f36817e94d1f9da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>最后我们来小结下今天的内容。大 db 文件首先会影响 etcd 启动耗时，因为 etcd 需要打开 db 文件，初始化 db 对象，并遍历 boltdb 中的所有 key-value 以重建内存 treeIndex。</p><p>其次，较大 db 文件会导致 etcd 依赖更高配置的节点内存规格，etcd 通过 mmap 将 db 文件映射到内存中。etcd 启动后，正常情况下读 etcd 过程不涉及磁盘 IO，若节点内存不够，可能会导致缺页中断，引起延时抖动、服务性能下降。</p><p>接着 treeIndex 维护了所有 key 的版本号信息，当 treeIndex 中含有百万级 key 时，在 treeIndex 中搜索指定范围的 key 的开销是不能忽略的，此开销可能高达上百毫秒。</p><p>然后当 db 文件过大后，boltdb 本身连续空闲页的申请、释放、存储都会存在一定的开销。etcd 社区已通过新的 freelist 管理数据结构 hashmap 对其进行优化，将时间复杂度降低到了 O(1)，同时支持事务提交时不持久化 freelist，而是通过重启时扫描 page 重建，以提升 etcd 写性能、降低 db 大小。</p><p>随后我给你介绍了 db 文件过大后，count only、limit、大包查询等 expensive request 对集群稳定性的影响。建议你的业务尽量避免任何 expensive request 请求。</p><p>最后我们介绍了大 db 文件对快照功能的影响。大 db 文件意味着更长的备份时间，而更长的只读事务则可能会导致 db 文件增长。同时 Leader 发送快照与 Follower 基于快照重建都需要较长时间，在集群写请求较大的情况下，可能会陷入死循环，导致落后的 Follower 节点一直无法追赶上 Leader。</p><h2 id="延时：为什么你的etcd请求会出现超时？"><a href="#延时：为什么你的etcd请求会出现超时？" class="headerlink" title="延时：为什么你的etcd请求会出现超时？"></a>延时：为什么你的etcd请求会出现超时？</h2><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7c9474f54e565512.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li>网络质量，如节点之间 RTT 延时、网卡带宽满，出现丢包；</li><li>磁盘 I/O 抖动，会导致 WAL 日志持久化、boltdb 事务提交出现抖动，Leader 出现切换等；</li><li>expensive request，比如大包请求、涉及到大量 key 遍历、Authenticate 密码鉴权等操作；</li><li>容量瓶颈，太多写请求导致线性读请求性能下降等；</li><li>节点配置，CPU 繁忙导致请求处理延时、内存不够导致 swap 等。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7686a36ad76b6e72.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0424f966141eb847.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="内存：为什么你的etcd内存占用那么高？"><a href="#内存：为什么你的etcd内存占用那么高？" class="headerlink" title="内存：为什么你的etcd内存占用那么高？"></a>内存：为什么你的etcd内存占用那么高？</h2><p>在使用 etcd 的过程中，你是否被异常内存占用等现象困扰过？比如 etcd 中只保存了 1 个 1MB 的 key-value，但是经过若干次修改后，最终 etcd 内存可能达到数 G。它是由什么原因导致的？如何分析呢？</p><p>下图是我以 etcd 写请求流程为例，给你总结的可能导致 etcd 内存占用较高的核心模块与其数据结构。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-15fefd6dd53d6e32.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6b0376246a5f18ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>首先是 raftLog。为了帮助 slow Follower 同步数据，它至少要保留 5000 条最近收到的写请求在内存中。若你的 key 非常大，你更新 5000 次会产生较大的内存开销。</p><p>其次是 treeIndex。 每个 key-value 会在内存中保留一个索引项。索引项的开销跟 key 长度、保留的历史版本有关，你可以通过 compact 命令压缩。</p><p>然后是 boltdb。etcd 启动的时候，会通过 mmap 系统调用，将文件映射到虚拟内存中。你可以通过 compact 命令回收旧版本，defrag 命令进行碎片整理。</p><p>接着是 watcher。它的内存占用跟连接数、gRPC Watch Stream 数、watcher 数有关。watch 机制一个不可忽视的内存开销其实是事件堆积的占用缓存，你可以通过相关 metrics 及时发现堆积的事件以及 slow watcher。</p><p>最后我介绍了一些典型的场景导致的内存异常，如大包查询等 expensive request，etcd 中存储了 v2 API 写入的 key， goroutines 泄露以及 etcd lease bug 等。</p><p>希望今天的内容，能够帮助你从容应对 etcd 内存占用高的问题，合理配置你的集群，优化业务 expensive request，让 etcd 跑得更稳。</p><h2 id="如何优化及扩展etcd性能？"><a href="#如何优化及扩展etcd性能？" class="headerlink" title="如何优化及扩展etcd性能？"></a>如何优化及扩展etcd性能？</h2><ul><li>业务应用层，etcd 应用层的最佳实践；</li><li>etcd 内核层，etcd 参数最佳实践；</li><li>操作系统层，操作系统优化事项；</li><li>硬件及网络层，不同的硬件设备对 etcd 性能有着非常大的影响；</li><li>扩展性能，基于 gRPC proxy 扩展读、Watch、Lease 的性能。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-22d988789d96de7d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="ZooKeeper-架构及原理"><a href="#ZooKeeper-架构及原理" class="headerlink" title="ZooKeeper 架构及原理"></a>ZooKeeper 架构及原理</h2><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2cc163e33ffc5fbb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如下面架构图所示，你可以看到 ZooKeeper 中的节点与 etcd 类似，也划分为 Leader 节点、Follower 节点、Observer 节点（对应的 Raft 协议的 Learner 节点）。同时，写请求统一由 Leader 处理，读请求各个节点都能处理。</p><p>不一样的是它们的读行为和共识算法。</p><ul><li>在读行为上，ZooKeeper 默认读可能会返回 stale data，而 etcd 使用的线性读，能确保读取到反应集群共识的最新数据。</li><li>共识算法上，etcd 使用的是 Raft，ZooKeeper 使用的是 Zab。</li></ul><p>那什么是 Zab 协议呢？Zab 协议可以分为以下阶段：</p><ul><li>Phase 0，Leader 选举（Leader Election)。一个节点只要求获得半数以上投票，就可以当选为准 Leader；</li><li>Phase 1，发现（Discovery）。准 Leader 收集其他节点的数据信息，并将最新的数据复制到自身；</li><li>Phase 2，同步（Synchronization）。准 Leader 将自身最新数据复制给其他落后的节点，并告知其他节点自己正式当选为 Leader；</li><li>Phase 3，广播（Broadcast）。Leader 正式对外服务，处理客户端写请求，对消息进行广播。当收到一个写请求后，它会生成 Proposal 广播给各个 Follower 节点，一半以上 Follower 节点应答之后，Leader 再发送 Commit 命令给各个 Follower，告知它们提交相关提案；</li></ul><p>ZooKeeper 在实现中并未严格按论文定义的分阶段实现，而是对部分阶段进行了整合，分别如下：</p><ul><li>Fast Leader Election。首先 ZooKeeper 使用了一个名为 Fast Leader Election 的选举算法，通过 Leader 选举安全规则限制，确保选举出来的 Leader 就含有最新数据， 避免了 Zab 协议的 Phase 1 阶段准 Leader 收集各个节点数据信息并复制到自身，也就是将 Phase 0 和 Phase 1 进行了合并。</li><li>Recovery Phase。各个 Follower 发送自己的最新数据信息给 Leader，Leader 根据差异情况，选择发送 SNAP、DIFF 差异数据、Truncate 指令删除冲突数据等，确保 Follower 追赶上 Leader 数据进度并保持一致。</li><li>Broadcast Phase。与 Zab 论文 Broadcast Phase 一致。</li></ul><h2 id="Consul-架构及原理"><a href="#Consul-架构及原理" class="headerlink" title="Consul 架构及原理"></a>Consul 架构及原理</h2><p><img src="https://upload-images.jianshu.io/upload_images/12321605-caba82b37ababe0b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>从图中你可以看到，它由 Client、Server、Gossip 协议、Raft 共识算法、两个数据中心组成。每个数据中心内的 Server 基于 Raft 共识算法复制日志，Server 节点分为 Leader、Follower 等角色。Client 通过 Gossip 协议发现 Server 地址、分布式探测节点健康状态等。</p><p>那什么是 Gossip 协议呢？</p><p>Gossip 中文名称叫流言协议，它是一种消息传播协议。它的核心思想其实源自我们生活中的八卦、闲聊。我们在日常生活中所看到的劲爆消息其实源于两类，一类是权威机构如国家新闻媒体发布的消息，另一类则是大家通过微信等社交聊天软件相互八卦，一传十，十传百的结果。</p><p>Gossip 协议的基本工作原理与我们八卦类似，在 Gossip 协议中，如下图所示，各个节点会周期性地选择一定数量节点，然后将消息同步给这些节点。收到消息后的节点同样做出类似的动作，随机的选择节点，继续扩散给其他节点。</p><p>最终经过一定次数的扩散、传播，整个集群的各个节点都能感知到此消息，各个节点的数据趋于一致。Gossip 协议被广泛应用在多个知名项目中，比如 Redis Cluster 集群版，Apache Cassandra，AWS Dynamo。</p><p>你需要注意的是，虽然 Consul 天然支持多数据中心，但是多数据中心内的服务数据并不会跨数据中心同步，各个数据中心的 Server 集群是独立的。不过，Consul 提供了Prepared Query功能，它支持根据一定的策略返回多数据中心下的最佳的服务实例地址，使你的服务具备跨数据中心容灾。</p><p>比如当你的 API 网关收到用户请求查询 A 服务，API 网关服务优先从缓存中查找 A 服务对应的最佳实例。若无缓存则向 Consul 发起一个 Prepared Query 请求查询 A 服务实例，Consul 收到请求后，优先返回本数据中心下的服务实例。如果本数据中心没有或异常则根据数据中心间 RTT 由近到远查询其它数据中心数据，最终网关可将用户请求转发给最佳的数据中心下的实例地址。</p><p>了解完 Consul 的 Gossip 协议、多数据中心支持，我们再看看 Consul 是如何处理读请求的呢?</p><ul><li>默认（default）。默认是此模式，绝大部分场景下它能保证数据的强一致性。但在老的 Leader 出现网络分区被隔离、新的 Leader 被选举出来的一个极小时间窗口内，可能会导致 stale read。这是因为 Consul 为了提高读性能，使用的是基于 Lease 机制来维持 Leader 身份，避免了与其他节点进行交互确认的开销。</li><li>强一致性（consistent）。强一致性读与 etcd 默认线性读模式一样，每次请求需要集群多数节点确认 Leader 身份，因此相比 default 模式读，性能会有所下降。</li><li>弱一致性（stale)。任何节点都可以读，无论它是否 Leader。可能读取到陈旧的数据，类似 etcd 的串行读。这种读模式不要求集群有 Leader，因此当集群不可用时，只要有节点存活，它依然可以响应读请求。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-84bf50b2dc5edb80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="etcd-vs-ZooKeeper-vs-Consul"><a href="#etcd-vs-ZooKeeper-vs-Consul" class="headerlink" title="etcd vs ZooKeeper vs Consul"></a>etcd vs ZooKeeper vs Consul</h2><h3 id="并发原语"><a href="#并发原语" class="headerlink" title="并发原语"></a>并发原语</h3><p>etcd 和 ZooKeeper、Consul 的典型应用场景都是分布式锁、Leader 选举，以上场景就涉及到并发原语控制。然而 etcd 和 ZooKeeper 并未提供原生的分布式锁、Leader 选举支持，只提供了核心的基本数据读写、并发控制 API，由应用上层去封装。</p><p>为了帮助开发者更加轻松的使用 etcd 去解决分布式锁、Leader 选举等问题，etcd 社区提供了concurrency 包来实现以上功能。同时，在 etcdserver 中内置了 Lock 和 Election 服务，不过其也是基于 concurrency 包做了一层封装而已，clientv3 并未提供 Lock 和 Election 服务 API 给 Client 使用。 ZooKeeper 所属的 Apache 社区提供了Apache Curator Recipes库来帮助大家快速使用分布式锁、Leader 选举功能。</p><p>相比 etcd、ZooKeeper 依赖应用层基于 API 上层封装，<a href="https://www.consul.io/commands/lock">Consul 对分布式锁就提供了原生的支持</a>，可直接通过命令行使用。</p><h3 id="健康检查、服务发现"><a href="#健康检查、服务发现" class="headerlink" title="健康检查、服务发现"></a>健康检查、服务发现</h3><p>分布式协调服务的另外一个核心应用场景是服务发现、健康检查。</p><p>与并发原语类似，etcd 和 ZooKeeper 并未提供原生的服务发现支持。相反，Consul 在服务发现方面做了很多解放用户双手的工作，提供了服务发现的框架，帮助你的业务快速接入，并提供了 HTTP 和 DNS 两种获取服务方式。</p><p>比如下面就是通过 DNS 的方式获取服务地址：</p><pre><code>$ dig @127.0.0.1 -p 8600 redis.service.dc1.consul. ANY</code></pre><p>最重要的是它还集成了分布式的健康检查机制。与 etcd 和 ZooKeeper 健康检查不一样的是，它是一种基于 client、Gossip 协议、分布式的健康检查机制，具备低延时、可扩展的特点。业务可通过 Consul 的健康检查机制，实现 HTTP 接口返回码、内存乃至磁盘空间的检测。</p><p>Consul 提供了多种机制给你注册健康检查，如脚本、HTTP、TCP 等。</p><p>比如你将如下脚本放在 Agent 相应目录下，当 Linux 机器内存使用率超过 70% 的时候，它会返回告警状态。</p><pre><code>{  ​"check":     ​"id": "mem-util"    ​"name": "Memory utilization"    ​"args":       ​"/bin/sh"      ​"-c"      ​"/usr/bin/free | awk '/Mem/{printf($3/$2*100)}' | awk '{ print($0); if($1 &gt; 70) exit 1;}'    ​]    ​"interval": "10s"    ​"timeout": "1s  }​}</code></pre><p>相比 Consul，etcd、ZooKeeper 它们提供的健康检查机制和能力就非常有限了。</p><p>etcd 提供了 Lease 机制来实现活性检测。它是一种中心化的健康检查，依赖用户不断地发送心跳续租、更新 TTL。</p><p>ZooKeeper 使用的是一种名为临时节点的状态来实现健康检查。当 client 与 ZooKeeper 节点连接断掉时，ZooKeeper 就会删除此临时节点的 key-value 数据。它比基于心跳机制更复杂，也给 client 带去了更多的复杂性，所有 client 必须维持与 ZooKeeper server 的活跃连接并保持存活。</p><h3 id="数据模型比较"><a href="#数据模型比较" class="headerlink" title="数据模型比较"></a>数据模型比较</h3><p>从并发原语、健康检查、服务发现等维度了解完 etcd、ZooKeeper、Consul 的实现区别之后，我们再从数据模型上对比下三者。</p><p>首先 etcd 正如我们在07节 MVCC 和10节 boltdb 所介绍的，它是个扁平的 key-value 模型，内存索引通过 B-tree 实现，数据持久化存储基于 B+ tree 的 boltdb，支持范围查询、适合读多写少，可容纳数 G 的数据。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f41d2a57ac8f3d34.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如上图所示，它是一种层次模型，你可能已经发现，etcd v2 的内存数据模型与它是一样的。ZooKeeper 作为分布式协调服务的祖师爷，早期 etcd v2 的确就是参考它而设计的。</p><p>ZooKeeper 的层次模型中的每个节点叫 Znode，它分为持久性和临时型两种。</p><ul><li>持久性顾名思义，除非你通过 API 删除它，否则它将永远存在。</li><li>临时型是指它与客户端会话绑定，若客户端会话结束或出现异常中断等，它都将被 ZooKeeper server 自动删除，被广泛应用于活性检测。</li></ul><p>同时你创建节点的时候，还可以指定一个顺序标识，这样节点名创建出来后就具有顺序性，一般应用于分布式选举等场景中。</p><p>那 ZooKeeper 是如何实现以上层次模型的呢？</p><p>ZooKeeper 使用的是内存 ConcurrentHashMap 来实现此数据结构，因此具有良好的读性能。但是受限于内存的瓶颈，一般 ZooKeeper 的数据库文件大小是几百 M 左右。</p><p>Consul 的数据模型及存储是怎样的呢？</p><p>它也提供了常用 key-value 操作，它的存储引擎是基于Radix Tree实现的go-memdb，要求 value 大小不能超过 512 个字节，数据库文件大小一般也是几百 M 左右。与 boltdb 类似，它也支持事务、MVCC。</p><h3 id="Watch-特性比较"><a href="#Watch-特性比较" class="headerlink" title="Watch 特性比较"></a>Watch 特性比较</h3><p>正在我在 08 节 Watch 特性中所介绍的，etcd v3 的 Watch 是基于 MVCC 机制实现的，而 Consul 是采用滑动窗口实现的。Consul 存储引擎是基于Radix Tree实现的，因此它不支持范围查询和监听，只支持前缀查询和监听，而 etcd 都支持。</p><p>相比 etcd、Consul，ZooKeeper 的 Watch 特性有更多的局限性，它是个一次性触发器。</p><p>在 ZooKeeper 中，client 对 Znode 设置了 Watch 时，如果 Znode 内容发生改变，那么 client 就会获得 Watch 事件。然而此 Znode 再次发生变化，那 client 是无法收到 Watch 事件的，除非 client 设置了新的 Watch。</p><h3 id="其他比较"><a href="#其他比较" class="headerlink" title="其他比较"></a>其他比较</h3><ul><li>线性读。etcd 和 Consul 都支持线性读，而 ZooKeeper 并不具备。</li><li>权限机制比较。etcd 实现了 RBAC 的权限校验，而 ZooKeeper 和 Consul 实现的 ACL。</li><li>事务比较。etcd 和 Consul 都提供了简易的事务能力，支持对字段进行比较，而 ZooKeeper 只提供了版本号检查能力，功能较弱。</li><li>多数据中心。在多数据中心支持上，只有 Consul 是天然支持的，虽然它本身不支持数据自动跨数据中心同步，但是它提供的服务发现机制、Prepared Query功能，赋予了业务在一个可用区后端实例故障时，可将请求转发到最近的数据中心实例。而 etcd 和 ZooKeeper 并不支持。</li></ul><h2 id="如何构建高可靠的etcd集群运维体系"><a href="#如何构建高可靠的etcd集群运维体系" class="headerlink" title="如何构建高可靠的etcd集群运维体系"></a>如何构建高可靠的etcd集群运维体系</h2><p><img src="https://upload-images.jianshu.io/upload_images/12321605-02a0b48987cc4fd9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> Note </tag>
            
            <tag> ETCD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《设计模式》</title>
      <link href="/2021/03/25/note/design-patterns/"/>
      <url>/2021/03/25/note/design-patterns/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h3 id="什么是设计模式"><a href="#什么是设计模式" class="headerlink" title="什么是设计模式"></a>什么是设计模式</h3><p>设计模式讲的是如何写出可扩展、可读、可维护的高质量代码，所以，它们跟平时的编码会有直接的关系，也会直接影响到你的开发能力。</p><h3 id="为什么要学习设计模式"><a href="#为什么要学习设计模式" class="headerlink" title="为什么要学习设计模式"></a>为什么要学习设计模式</h3><ol><li>应对面试中的设计模式相关问题。</li><li>告别写被人吐槽的烂代码。我见过太多的烂代码，比如命名不规范、类设计不合理、分层不清晰、没有模块化概念、代码结构混乱、高度耦合等等。这样的代码维护起来非常费劲，添加或者修改一个功能，常常会牵一发而动全身，让你无从下手，恨不得将全部的代码删掉重写！</li><li>提高复杂代码的设计和开发能力。</li><li>让读源码、学框架事半功倍。</li><li>为你的职场发展做铺垫</li></ol><h3 id="如何评价代码质量的高低？"><a href="#如何评价代码质量的高低？" class="headerlink" title="如何评价代码质量的高低？"></a>如何评价代码质量的高低？</h3><p>仔细看前面罗列的所有代码质量评价标准，你会发现，有些词语过于笼统、抽象，比较偏向对于整体的描述，比如优雅、好、坏、整洁、清晰等；有些过于细节、偏重方法论，比如模块化、高内聚低耦合、文档详尽、分层清晰等；有些可能并不仅仅局限于编码，跟架构设计等也有关系，比如可伸缩性、可用性、稳定性等。</p><p>为了做到有的放矢、有重点地学习，我挑选了其中几个最常用的、最重要的评价标准，来详细讲解，其中就包括：可维护性、可读性、可扩展性、灵活性、简洁性（简单、复杂）、可复用性、可测试性。接下来，我们逐一讲解一下。</p><p>代码质量的评价有很强的主观性，描述代码质量的词汇也有很多，比如可读性、可维护性、灵活、优雅、简洁等，这些词汇是从不同的维度去评价代码质量的。它们之间有互相作用，并不是独立的，比如，代码的可读性好、可扩展性好就意味着代码的可维护性好。代码质量高低是一个综合各种因素得到的结论。我们并不能通过单一的维度去评价一段代码的好坏。</p><ol><li>可维护性（maintainability），落实到编码开发，所谓的“维护”无外乎就是修改 bug、修改老的代码、添加新的代码之类的工作。所谓“代码易维护”就是指，在不破坏原有代码设计、不引入新的 bug 的情况下，能够快速地修改或者添加代码。所谓“代码不易维护”就是指，修改或者添加代码需要冒着极大的引入新 bug 的风险，并且需要花费很长的时间才能完成。</li><li>可读性（readability），我们在编写代码的时候，时刻要考虑到代码是否易读、易理解。除此之外，代码的可读性在非常大程度上会影响代码的可维护性。毕竟，不管是修改 bug，还是修改添加功能代码，我们首先要做的事情就是读懂代码。代码读不大懂，就很有可能因为考虑不周全，而引入新的 bug。我们需要看代码是否符合编码规范、命名是否达意、注释是否详尽、函数是否长短合适、模块划分是否清晰、是否符合高内聚低耦合等等。你应该也能感觉到，从正面上，我们很难给出一个覆盖所有评价指标的列表。这也是我们无法量化可读性的原因。</li><li>可扩展性（extensibility），代码的可扩展性表示，我们在不修改或少量修改原有代码的情况下，通过扩展的方式添加新的功能代码。说直白点就是，代码预留了一些功能扩展点，你可以把新功能代码，直接插到扩展点上，而不需要因为要添加一个功能而大动干戈，改动大量的原始代码。</li><li>灵活性（flexibility），从刚刚举的场景来看，如果一段代码易扩展、易复用或者易用，我们都可以称这段代码写得比较灵活。所以，灵活这个词的含义非常宽泛，很多场景下都可以使用。</li><li>简洁性（simplicity），有一条非常著名的设计原则，你一定听过，那就是 KISS 原则：“Keep It Simple，Stupid”。这个原则说的意思就是，尽量保持代码简单。代码简单、逻辑清晰，也就意味着易读、易维护。我们在编写代码的时候，往往也会把简单、清晰放到首位。不过，很多编程经验不足的程序员会觉得，简单的代码没有技术含量，喜欢在项目中引入一些复杂的设计模式，觉得这样才能体现自己的技术水平。实际上，思从深而行从简，真正的高手能云淡风轻地用最简单的方法解决最复杂的问题。这也是一个编程老手跟编程新手的本质区别之一。</li><li>可复用性，代码的可复用性可以简单地理解为，尽量减少重复代码的编写，复用已有的代码。</li><li>可测试性，相对于前面六个评价标准，代码的可测试性是一个相对较少被提及，但又非常重要的代码质量评价标准。代码可测试性的好坏，能从侧面上非常准确地反应代码质量的好坏。代码的可测试性差，比较难写单元测试，那基本上就能说明代码设计得有问题。</li></ol><h3 id="最常用的评价标准有哪几个？"><a href="#最常用的评价标准有哪几个？" class="headerlink" title="最常用的评价标准有哪几个？"></a>最常用的评价标准有哪几个？</h3><p>最常用到几个评判代码质量的标准是：可维护性、可读性、可扩展性、灵活性、简洁性、可复用性、可测试性。其中，可维护性、可读性、可扩展性又是提到最多的、最重要的三个评价标准。</p><h3 id="面向对象、设计原则、设计模式、编程规范、重构，这五者有何关系？"><a href="#面向对象、设计原则、设计模式、编程规范、重构，这五者有何关系？" class="headerlink" title="面向对象、设计原则、设计模式、编程规范、重构，这五者有何关系？"></a>面向对象、设计原则、设计模式、编程规范、重构，这五者有何关系？</h3><ul><li>面向对象编程因为其具有丰富的特性（封装、抽象、继承、多态），可以实现很多复杂的设计思路，是很多设计原则、设计模式等编码实现的基础。</li><li>设计原则是指导我们代码设计的一些经验总结，对于某些场景下，是否应该应用某种设计模式，具有指导意义。比如，“开闭原则”是很多设计模式（策略、模板等）的指导原则。</li><li>设计模式是针对软件开发中经常遇到的一些设计问题，总结出来的一套解决方案或者设计思路。应用设计模式的主要目的是提高代码的可扩展性。从抽象程度上来讲，设计原则比设计模式更抽象。设计模式更加具体、更加可执行。</li><li>编程规范主要解决的是代码的可读性问题。编码规范相对于设计原则、设计模式，更加具体、更加偏重代码细节、更加能落地。持续的小重构依赖的理论基础主要就是编程规范。</li><li>重构作为保持代码质量不下降的有效手段，利用的就是面向对象、设计原则、设计模式、编码规范这些理论。</li></ul><p>实际上，面向对象、设计原则、设计模式、编程规范、代码重构，这五者都是保持或者提高代码质量的方法论，本质上都是服务于编写高质量代码这一件事的。当我们追本逐源，看清这个本质之后，很多事情怎么做就清楚了，很多选择怎么选也清楚了。比如，在某个场景下，该不该用这个设计模式，那就看能不能提高代码的可扩展性；要不要重构，那就看重代码是否存在可读、可维护问题等。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-aeff3710b8d147cd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a>面向对象</h2><h3 id="编程范式"><a href="#编程范式" class="headerlink" title="编程范式"></a>编程范式</h3><ol><li>面向过程</li><li>面向对象</li><li>函数式编程</li></ol><h3 id="什么是面向对象编程？"><a href="#什么是面向对象编程？" class="headerlink" title="什么是面向对象编程？"></a>什么是面向对象编程？</h3><p>面向对象编程的英文缩写是 OOP，全称是 Object Oriented Programming。对应地，面向对象编程语言的英文缩写是 OOPL，全称是 Object Oriented Programming Language。</p><p>面向对象编程中有两个非常重要、非常基础的概念，那就是类（class）和对象（object）。这两个概念最早出现在 1960 年，在 Simula 这种编程语言中第一次使用。而面向对象编程这个概念第一次被使用是在 Smalltalk 这种编程语言中。Smalltalk 被认为是第一个真正意义上的面向对象编程语言。</p><p>如果非得给出一个定义的话，我觉得可以用下面两句话来概括</p><ul><li>面向对象编程是一种编程范式或编程风格。它以类或对象作为组织代码的基本单元，并将封装、抽象、继承、多态四个特性，作为代码设计和实现的基石 。</li><li>面向对象编程语言是支持类或对象的语法机制，并有现成的语法机制，能方便地实现面向对象编程四大特性（封装、抽象、继承、多态）的编程语言。</li></ul><p>面向对象编程是一种编程范式或编程风格。它以类或对象作为组织代码的基本单元，并将封装、抽象、继承、多态四个特性，作为代码设计和实现的基石 。</p><h4 id="什么是面向对象编程语言？"><a href="#什么是面向对象编程语言？" class="headerlink" title="什么是面向对象编程语言？"></a>什么是面向对象编程语言？</h4><p>面向对象编程语言是支持类或对象的语法机制，并有现成的语法机制，能方便地实现面向对象编程四大特性（封装、抽象、继承、多态）的编程语言。</p><h4 id="如何判定一个编程语言是否是面向对象编程语言？"><a href="#如何判定一个编程语言是否是面向对象编程语言？" class="headerlink" title="如何判定一个编程语言是否是面向对象编程语言？"></a>如何判定一个编程语言是否是面向对象编程语言？</h4><p>如果按照严格的的定义，需要有现成的语法支持类、对象、四大特性才能叫作面向对象编程语言。如果放宽要求的话，只要某种编程语言支持类、对象语法机制，那基本上就可以说这种编程语言是面向对象编程语言了，不一定非得要求具有所有的四大特性。</p><h4 id="面向对象编程和面向对象编程语言之间有何关系？"><a href="#面向对象编程和面向对象编程语言之间有何关系？" class="headerlink" title="面向对象编程和面向对象编程语言之间有何关系？"></a>面向对象编程和面向对象编程语言之间有何关系？</h4><p>面向对象编程一般使用面向对象编程语言来进行，但是，不用面向对象编程语言，我们照样可以进行面向对象编程。反过来讲，即便我们使用面向对象编程语言，写出来的代码也不一定是面向对象编程风格的，也有可能是面向过程编程风格的。</p><h4 id="什么是面向对象分析和面向对象设计？"><a href="#什么是面向对象分析和面向对象设计？" class="headerlink" title="什么是面向对象分析和面向对象设计？"></a>什么是面向对象分析和面向对象设计？</h4><p>简单点讲，面向对象分析就是要搞清楚做什么，面向对象设计就是要搞清楚怎么做。两个阶段最终的产出是类的设计，包括程序被拆解为哪些类，每个类有哪些属性方法、类与类之间如何交互等等。</p><h3 id="面向对象四大特性"><a href="#面向对象四大特性" class="headerlink" title="面向对象四大特性"></a>面向对象四大特性</h3><h4 id="封装（Encapsulation）"><a href="#封装（Encapsulation）" class="headerlink" title="封装（Encapsulation）"></a>封装（Encapsulation）</h4><p>封装也叫作信息隐藏或者数据访问保护。类通过暴露有限的访问接口，授权外部仅能通过类提供的方式来访问内部信息或者数据。它需要编程语言提供权限访问控制语法来支持，例如 Java 中的 private、protected、public 关键字。封装特性存在的意义，一方面是保护数据不被随意修改，提高代码的可维护性；另一方面是仅暴露有限的必要接口，提高类的易用性。</p><h4 id="抽象（Abstraction）"><a href="#抽象（Abstraction）" class="headerlink" title="抽象（Abstraction）"></a>抽象（Abstraction）</h4><p>封装主要讲如何隐藏信息、保护数据，那抽象就是讲如何隐藏方法的具体实现，让使用者只需要关心方法提供了哪些功能，不需要知道这些功能是如何实现的。抽象可以通过接口类或者抽象类来实现，但也并不需要特殊的语法机制来支持。抽象存在的意义，一方面是提高代码的可扩展性、维护性，修改实现不需要改变定义，减少代码的改动范围；另一方面，它也是处理复杂系统的有效手段，能有效地过滤掉不必要关注的信息。</p><h4 id="继承（Inheritance）"><a href="#继承（Inheritance）" class="headerlink" title="继承（Inheritance）"></a>继承（Inheritance）</h4><p>继承是用来表示类之间的 is-a 关系，分为两种模式：单继承和多继承。单继承表示一个子类只继承一个父类，多继承表示一个子类可以继承多个父类。为了实现继承这个特性，编程语言需要提供特殊的语法机制来支持。继承主要是用来解决代码复用的问题。</p><h4 id="多态（Polymorphism）"><a href="#多态（Polymorphism）" class="headerlink" title="多态（Polymorphism）"></a>多态（Polymorphism）</h4><p>多态是指子类可以替换父类，在实际的代码运行过程中，调用子类的方法实现。多态这种特性也需要编程语言提供特殊的语法机制来实现，比如继承、接口类、duck-typing。多态可以提高代码的扩展性和复用性，是很多设计模式、设计原则、编程技巧的代码实现基础。</p><p>Clean Architecture 里面指出多态是函数指针的一种应用。并用getchar()举了例子。然后用多态的实现引出了“依赖反转”的例子。</p><h4 id="面向对象编程相比面向过程编程有哪些优势？"><a href="#面向对象编程相比面向过程编程有哪些优势？" class="headerlink" title="面向对象编程相比面向过程编程有哪些优势？"></a>面向对象编程相比面向过程编程有哪些优势？</h4><ul><li>对于大规模复杂程序的开发，程序的处理流程并非单一的一条主线，而是错综复杂的网状结构。面向对象编程比起面向过程编程，更能应对这种复杂类型的程序开发。</li><li>面向对象编程相比面向过程编程，具有更加丰富的特性（封装、抽象、继承、多态）。利用这些特性编写出来的代码，更加易扩展、易复用、易维护。</li><li>从编程语言跟机器打交道的方式的演进规律中，我们可以总结出：面向对象编程语言比起面向过程编程语言，更加人性化、更加高级、更加智能。</li></ul><h3 id="Constants、Utils-类设计问题"><a href="#Constants、Utils-类设计问题" class="headerlink" title="Constants、Utils 类设计问题"></a>Constants、Utils 类设计问题</h3><p>定义一个如此大而全的 Constants 类，并不是一种很好的设计思路。为什么这么说呢？原因主要有以下几点。</p><ul><li>首先，这样的设计会影响代码的可维护性。如果参与开发同一个项目的工程师有很多，在开发过程中，可能都要涉及修改这个类，比如往这个类里添加常量，那这个类就会变得越来越大，成百上千行都有可能，查找修改某个常量也会变得比较费时，而且还会增加提交代码冲突的概率。</li><li>其次，这样的设计还会增加代码的编译时间。当 Constants 类中包含很多常量定义的时候，依赖这个类的代码就会很多。那每次修改 Constants 类，都会导致依赖它的类文件重新编译，因此会浪费很多不必要的编译时间。不要小看编译花费的时间，对于一个非常大的工程项目来说，编译一次项目花费的时间可能是几分钟，甚至几十分钟。而我们在开发过程中，每次运行单元测试，都会触发一次编译的过程，这个编译时间就有可能会影响到我们的开发效率。</li><li>最后，这样的设计还会影响代码的复用性。如果我们要在另一个项目中，复用本项目开发的某个类，而这个类又依赖 Constants 类。即便这个类只依赖 Constants 类中的一小部分常量，我们仍然需要把整个 Constants 类也一并引入，也就引入了很多无关的常量到新的项目中。</li></ul><p>第一种是将 Constants 类拆解为功能更加单一的多个类，比如跟 MySQL 配置相关的常量，我们放到 MysqlConstants 类中；跟 Redis 配置相关的常量，我们放到 RedisConstants 类中。当然，还有一种我个人觉得更好的设计思路，那就是并不单独地设计 Constants 常量类，而是哪个类用到了某个常量，我们就把这个常量定义到这个类中。比如，RedisConfig 类用到了 Redis 配置相关的常量，那我们就直接将这些常量定义在 RedisConfig 中，这样也提高了类设计的内聚性和代码的复用性。</p><p>实际上，Utils 类的出现是基于这样一个问题背景：如果我们有两个类 A 和 B，它们要用到一块相同的功能逻辑，为了避免代码重复，我们不应该在两个类中，将这个相同的功能逻辑，重复地实现两遍。这个时候我们该怎么办呢？</p><p>我们在讲面向对象特性的时候，讲过继承可以实现代码复用。利用继承特性，我们把相同的属性和方法，抽取出来，定义到父类中。子类复用父类中的属性和方法，达到代码复用的目的。但是，有的时候，从业务含义上，A 类和 B 类并不一定具有继承关系，比如 Crawler 类和 PageAnalyzer 类，它们都用到了 URL 拼接和分割的功能，但并不具有继承关系（既不是父子关系，也不是兄弟关系）。仅仅为了代码复用，生硬地抽象出一个父类出来，会影响到代码的可读性。如果不熟悉背后设计思路的同事，发现 Crawler 类和 PageAnalyzer 类继承同一个父类，而父类中定义的却是 URL 相关的操作，会觉得这个代码写得莫名其妙，理解不了。</p><p>实际上，只包含静态方法不包含任何属性的 Utils 类，是彻彻底底的面向过程的编程风格。但这并不是说，我们就要杜绝使用 Utils 类了。实际上，从刚刚讲的 Utils 类存在的目的来看，它在软件开发中还是挺有用的，能解决代码复用问题。所以，这里并不是说完全不能用 Utils 类，而是说，要尽量避免滥用，不要不加思考地随意去定义 Utils 类。</p><p>在定义 Utils 类之前，你要问一下自己，你真的需要单独定义这样一个 Utils 类吗？是否可以把 Utils 类中的某些方法定义到其他类中呢？如果在回答完这些问题之后，你还是觉得确实有必要去定义这样一个 Utils 类，那就大胆地去定义它吧。因为即便在面向对象编程中，我们也并不是完全排斥面向过程风格的代码。只要它能为我们写出好的代码贡献力量，我们就可以适度地去使用。</p><p>除此之外，类比 Constants 类的设计，我们设计 Utils 类的时候，最好也能细化一下，针对不同的功能，设计不同的 Utils 类，比如 FileUtils、IOUtils、StringUtils、UrlUtils 等，不要设计一个过于大而全的 Utils 类。</p><h3 id="抽象类和接口"><a href="#抽象类和接口" class="headerlink" title="抽象类和接口"></a>抽象类和接口</h3><h4 id="抽象类"><a href="#抽象类" class="headerlink" title="抽象类"></a>抽象类</h4><ul><li>抽象类不允许被实例化，只能被继承。也就是说，你不能 new 一个抽象类的对象出来（Logger logger = new Logger(…); 会报编译错误）。</li><li>抽象类可以包含属性和方法。方法既可以包含代码实现（比如 Logger 中的 log() 方法），也可以不包含代码实现（比如 Logger 中的 doLog() 方法）。不包含代码实现的方法叫作抽象方法。</li><li>子类继承抽象类，必须实现抽象类中的所有抽象方法。对应到例子代码中就是，所有继承 Logger 抽象类的子类，都必须重写 doLog() 方法。</li></ul><h4 id="接口特性"><a href="#接口特性" class="headerlink" title="接口特性"></a>接口特性</h4><ul><li>接口不能包含属性（也就是成员变量）。</li><li>接口只能声明方法，方法不能包含代码实现。</li><li>类实现接口的时候，必须实现接口中声明的所有方法。</li></ul><p>前面我们讲了抽象类和接口的定义，以及各自的语法特性。从语法特性上对比，这两者有比较大的区别，比如抽象类中可以定义属性、方法的实现，而接口中不能定义属性，方法也不能包含代码实现等等。除了语法特性，从设计的角度，两者也有比较大的区别。</p><p>抽象类实际上就是类，只不过是一种特殊的类，这种类不能被实例化为对象，只能被子类继承。我们知道，继承关系是一种 is-a 的关系，那抽象类既然属于类，也表示一种 is-a 的关系。相对于抽象类的 is-a 关系来说，接口表示一种 has-a 关系，表示具有某些功能。对于接口，有一个更加形象的叫法，那就是协议（contract）。</p><h4 id="抽象类和接口能解决什么编程问题"><a href="#抽象类和接口能解决什么编程问题" class="headerlink" title="抽象类和接口能解决什么编程问题"></a>抽象类和接口能解决什么编程问题</h4><p>抽象类更多的是为了代码复用，而接口就更侧重于解耦。接口是对行为的一种抽象，<strong>相当于一组协议或者契约</strong>，你可以联想类比一下 API 接口。调用者只需要关注抽象的接口，不需要了解具体的实现，具体的实现代码对调用者透明。接口实现了约定和实现相分离，可以降低代码间的耦合性，提高代码的可扩展性。</p><p>抽象类是对成员变量和方法的抽象，是一种 is-a 关系，是为了解决代码复用问题。接口仅仅是对方法的抽象，是一种 has-a 关系，表示具有某一组行为特性，是为了解决解耦问题，隔离接口和具体的实现，提高代码的扩展性。</p><h4 id="抽象类和接口的应用场景区别"><a href="#抽象类和接口的应用场景区别" class="headerlink" title="抽象类和接口的应用场景区别"></a>抽象类和接口的应用场景区别</h4><p>什么时候该用抽象类？什么时候该用接口？实际上，判断的标准很简单。如果要表示一种 is-a 的关系，并且是为了解决代码复用问题，我们就用抽象类；如果要表示一种 has-a 关系，并且是为了解决抽象而非代码复用问题，那我们就用接口。</p><h4 id="基于接口而非实现编程"><a href="#基于接口而非实现编程" class="headerlink" title="基于接口而非实现编程"></a>基于接口而非实现编程</h4><ol><li>实际上，“基于接口而非实现编程”这条原则的另一个表述方式，是“基于抽象而非实现编程”。后者的表述方式其实更能体现这条原则的设计初衷。在软件开发中，最大的挑战之一就是需求的不断变化，这也是考验代码设计好坏的一个标准。<strong>越抽象、越顶层、越脱离具体某一实现的设计，越能提高代码的灵活性，越能应对未来的需求变化。好的代码设计，不仅能应对当下的需求，而且在将来需求发生变化的时候，仍然能够在不破坏原有代码设计的情况下灵活应对</strong>。而抽象就是提高代码扩展性、灵活性、可维护性最有效的手段之一。</li><li>我们在定义接口的时候，一方面，命名要足够通用，不能包含跟具体实现相关的字眼；另一方面，与特定实现有关的方法不要定义在接口中。</li><li>“基于接口而非实现编程”这条原则，不仅仅可以指导非常细节的编程开发，还能指导更加上层的架构设计、系统设计等。比如，服务端与客户端之间的“接口”设计、类库的“接口”设计。</li></ol><h4 id="是否需要为每个类定义接口？"><a href="#是否需要为每个类定义接口？" class="headerlink" title="是否需要为每个类定义接口？"></a>是否需要为每个类定义接口？</h4><p>做任何事情都要讲求一个“度”，过度使用这条原则，非得给每个类都定义接口，接口满天飞，也会导致不必要的开发负担。至于什么时候，该为某个类定义接口，实现基于接口的编程，什么时候不需要定义接口，直接使用实现类编程，我们做权衡的根本依据，还是要回归到设计原则诞生的初衷上来。只要搞清楚了这条原则是为了解决什么样的问题而产生的，你就会发现，很多之前模棱两可的问题，都会变得豁然开朗。</p><p>从这个设计初衷上来看，如果在我们的业务场景中，<strong>某个功能只有一种实现方式，未来也不可能被其他实现方式替换，那我们就没有必要为其设计接口，也没有必要基于接口编程，直接使用实现类就可以了</strong>。</p><p>除此之外，越是不稳定的系统，我们越是要在代码的扩展性、维护性上下功夫。相反，如果某个系统特别稳定，在开发完之后，基本上不需要做维护，那我们就没有必要为其扩展性，投入不必要的开发时间。</p><h3 id="为什么不推荐使用继承？"><a href="#为什么不推荐使用继承？" class="headerlink" title="为什么不推荐使用继承？"></a>为什么不推荐使用继承？</h3><p>在面向对象编程中，有一条非常经典的设计原则，那就是：组合优于继承，多用组合少用继承。为什么不推荐使用继承？组合相比继承有哪些优势？如何判断该用组合还是继承？今天，我们就围绕着这三个问题，来详细讲解一下这条设计原则。</p><p>继承是面向对象的四大特性之一，用来表示类之间的 is-a 关系，可以解决代码复用的问题。虽然继承有诸多作用，但继承层次过深、过复杂，也会影响到代码的可维护性。在这种情况下，我们应该尽量少用，甚至不用继承。</p><h3 id="组合相比继承有哪些优势？"><a href="#组合相比继承有哪些优势？" class="headerlink" title="组合相比继承有哪些优势？"></a>组合相比继承有哪些优势？</h3><p>我们知道继承主要有三个作用：表示 is-a 关系，支持多态特性，代码复用。而这三个作用都可以通过其他技术手段来达成。比如 is-a 关系，我们可以通过组合和接口的 has-a 关系来替代；多态特性我们可以利用接口来实现；代码复用我们可以通过组合和委托来实现。所以，从理论上讲，通过组合、接口、委托三个技术手段，我们完全可以替换掉继承，在项目中不用或者少用继承关系，特别是一些复杂的继承关系。</p><h3 id="如何判断该用组合还是继承？"><a href="#如何判断该用组合还是继承？" class="headerlink" title="如何判断该用组合还是继承？"></a>如何判断该用组合还是继承？</h3><p>尽管我们鼓励多用组合少用继承，但组合也并不是完美的，继承也并非一无是处。从上面的例子来看，继承改写成组合意味着要做更细粒度的类的拆分。这也就意味着，我们要定义更多的类和接口。类和接口的增多也就或多或少地增加代码的复杂程度和维护成本。所以，在实际的项目开发中，我们还是要根据具体的情况，来具体选择该用继承还是组合。</p><p>如果类之间的继承结构稳定（不会轻易改变），继承层次比较浅（比如，最多有两层继承关系），继承关系不复杂，我们就可以大胆地使用继承。反之，系统越不稳定，继承层次很深，继承关系复杂，我们就尽量使用组合来替代继承。</p><p>除此之外，还有一些设计模式会固定使用继承或者组合。比如，装饰者模式（decorator pattern）、策略模式（strategy pattern）、组合模式（composite pattern）等都使用了组合关系，而模板模式（template pattern）使用了继承关系。</p><p>尽管有些人说，要杜绝继承，100% 用组合代替继承，但是我的观点没那么极端！之所以“多用组合少用继承”这个口号喊得这么响，只是因为，长期以来，我们过度使用继承。还是那句话，组合并不完美，继承也不是一无是处。只要我们控制好它们的副作用、发挥它们各自的优势，在不同的场合下，恰当地选择使用继承还是组合，这才是我们所追求的境界。</p><h2 id="设计原则"><a href="#设计原则" class="headerlink" title="设计原则"></a>设计原则</h2><h3 id="SOLID-原则"><a href="#SOLID-原则" class="headerlink" title="SOLID 原则"></a>SOLID 原则</h3><h4 id="单一职责（Single-Responsibility-Principle）"><a href="#单一职责（Single-Responsibility-Principle）" class="headerlink" title="单一职责（Single Responsibility Principle）"></a>单一职责（Single Responsibility Principle）</h4><p>单一职责（SRP）：Single Responsibility Principle，一个类只负责完成一个职责或者功能。不要设计大而全的类，要设计粒度小、功能单一的类。单一职责原则是为了实现代码高内聚、低耦合，提高代码的复用性、可读性、可维护性。</p><pre><code>public class UserInfo {  private long userId;  private String username;  private String email;  private String telephone;  private long createTime;  private long lastLoginTime;  private String avatarUrl;  private String provinceOfAddress; // 省  private String cityOfAddress; // 市  private String regionOfAddress; // 区   private String detailedAddress; // 详细地址  // ...省略其他属性和方法...}</code></pre><p>对于这个问题，有两种不同的观点。一种观点是，UserInfo 类包含的都是跟用户相关的信息，所有的属性和方法都隶属于用户这样一个业务模型，满足单一职责原则；另一种观点是，地址信息在 UserInfo 类中，所占的比重比较高，可以继续拆分成独立的 UserAddress 类，UserInfo 只保留除 Address 之外的其他信息，拆分之后的两个类的职责更加单一。</p><p>哪种观点更对呢？实际上，要从中做出选择，我们不能脱离具体的应用场景。如果在这个社交产品中，用户的地址信息跟其他信息一样，只是单纯地用来展示，那 UserInfo 现在的设计就是合理的。但是，如果这个社交产品发展得比较好，之后又在产品中添加了电商的模块，用户的地址信息还会用在电商物流中，那我们最好将地址信息从 UserInfo 中拆分出来，独立成用户物流信息（或者叫地址信息、收货信息等）。</p><p>从刚刚这个例子，我们可以总结出，不同的应用场景、不同阶段的需求背景下，对同一个类的职责是否单一的判定，可能都是不一样的。在某种应用场景或者当下的需求背景下，一个类的设计可能已经满足单一职责原则了，但如果换个应用场景或着在未来的某个需求背景下，可能就不满足了，需要继续拆分成粒度更细的类。</p><p>综上所述，评价一个类的职责是否足够单一，我们并没有一个非常明确的、可以量化的标准，可以说，这是件非常主观、仁者见仁智者见智的事情。实际上，在真正的软件开发中，我们也没必要过于未雨绸缪，过度设计。所以，我们可以先写一个粗粒度的类，满足业务需求。随着业务的发展，如果粗粒度的类越来越庞大，代码越来越多，这个时候，我们就可以将这个粗粒度的类，拆分成几个更细粒度的类。这就是所谓的持续重构（后面的章节中我们会讲到）。</p><p>听到这里，你可能会说，这个原则如此含糊不清、模棱两可，到底该如何拿捏才好啊？我这里还有一些小技巧，能够很好地帮你，从侧面上判定一个类的职责是否够单一。而且，我个人觉得，下面这几条判断原则，比起很主观地去思考类是否职责单一，要更有指导意义、更具有可执行性：</p><ul><li>类中的代码行数、函数或属性过多，会影响代码的可读性和可维护性，我们就需要考虑对类进行拆分；</li><li>类依赖的其他类过多，或者依赖类的其他类过多，不符合高内聚、低耦合的设计思想，我们就需要考虑对类进行拆分；</li><li>私有方法过多，我们就要考虑能否将私有方法独立到新的类中，设置为 public 方法，供更多的类使用，从而提高代码的复用性；</li><li>比较难给类起一个合适名字，很难用一个业务名词概括，或者只能用一些笼统的 Manager、Context 之类的词语来命名，这就说明类的职责定义得可能不够清晰；</li><li>类中大量的方法都是集中操作类中的某几个属性，比如，在 UserInfo 例子中，如果一半的方法都是在操作 address 信息，那就可以考虑将这几个属性和对应的方法拆分出来。</li></ul><p><strong>类的职责是否设计得越单一越好？</strong></p><p>单一职责原则通过避免设计大而全的类，避免将不相关的功能耦合在一起，来提高类的内聚性。同时，类职责单一，类依赖的和被依赖的其他类也会变少，减少了代码的耦合性，以此来实现代码的高内聚、低耦合。但是，如果拆分得过细，实际上会适得其反，反倒会降低内聚性，也会影响代码的可维护性。</p><h4 id="开闭原则（Open-Closed-Principle）"><a href="#开闭原则（Open-Closed-Principle）" class="headerlink" title="开闭原则（Open Closed Principle）"></a>开闭原则（Open Closed Principle）</h4><p>开闭原则（OCP）：Open Closed Principle，对扩展开放，对修改关闭。添加一个新的功能，应该是通过在已有代码基础上扩展代码（新增模块、类、方法、属性等），而非修改已有代码（修改模块、类、方法、属性等）的方式来完成。关于定义，我们有两点要注意。第一点是，开闭原则并不是说完全杜绝修改，而是以最小的修改代码的代价来完成新功能的开发。第二点是，同样的代码改动，在粗代码粒度下，可能被认定为“修改”；在细代码粒度下，可能又被认定为“扩展”。</p><p>而且，我们要认识到，添加一个新功能，不可能任何模块、类、方法的代码都不“修改”，这个是做不到的。类需要创建、组装、并且做一些初始化操作，才能构建成可运行的的程序，这部分代码的修改是在所难免的。<strong>我们要做的是尽量让修改操作更集中、更少、更上层，尽量让最核心、最复杂的那部分逻辑代码满足开闭原则</strong>。</p><p>前面我们提到，写出支持“对扩展开放、对修改关闭”的代码的关键是预留扩展点。那问题是如何才能识别出所有可能的扩展点呢？</p><p>如果你开发的是一个业务导向的系统，比如金融系统、电商系统、物流系统等，要想识别出尽可能多的扩展点，就要对业务有足够的了解，能够知道当下以及未来可能要支持的业务需求。如果你开发的是跟业务无关的、通用的、偏底层的系统，比如，框架、组件、类库，你需要了解“它们会被如何使用？今后你打算添加哪些功能？使用者未来会有哪些更多的功能需求？”等问题。</p><p>不过，有一句话说得好，“唯一不变的只有变化本身”。即便我们对业务、对系统有足够的了解，那也不可能识别出所有的扩展点，即便你能识别出所有的扩展点，为这些地方都预留扩展点，这样做的成本也是不可接受的。我们没必要为一些遥远的、不一定发生的需求去提前买单，做过度设计。</p><p>最合理的做法是，对于一些比较确定的、短期内可能就会扩展，或者需求改动对代码结构影响比较大的情况，或者实现成本不高的扩展点，在编写代码的时候之后，我们就可以事先做些扩展性设计。但对于一些不确定未来是否要支持的需求，或者实现起来比较复杂的扩展点，我们可以等到有需求驱动的时候，再通过重构代码的方式来支持扩展的需求。</p><p>而且，开闭原则也并不是免费的。有些情况下，代码的扩展性会跟可读性相冲突。比如，我们之前举的 Alert 告警的例子。为了更好地支持扩展性，我们对代码进行了重构，重构之后的代码要比之前的代码复杂很多，理解起来也更加有难度。很多时候，我们都需要在扩展性和可读性之间做权衡。在某些场景下，代码的扩展性很重要，我们就可以适当地牺牲一些代码的可读性；在另一些场景下，代码的可读性更加重要，那我们就适当地牺牲一些代码的可扩展性。</p><ol><li>如何理解“对扩展开放、对修改关闭”？</li></ol><p>添加一个新的功能，应该是通过在已有代码基础上扩展代码（新增模块、类、方法、属性等），而非修改已有代码（修改模块、类、方法、属性等）的方式来完成。关于定义，我们有两点要注意。第一点是，开闭原则并不是说完全杜绝修改，而是以最小的修改代码的代价来完成新功能的开发。第二点是，同样的代码改动，在粗代码粒度下，可能被认定为“修改”；在细代码粒度下，可能又被认定为“扩展”。</p><ol start="2"><li>如何做到“对扩展开放、修改关闭”？</li></ol><p>我们要时刻具备扩展意识、抽象意识、封装意识。在写代码的时候，我们要多花点时间思考一下，这段代码未来可能有哪些需求变更，如何设计代码结构，事先留好扩展点，以便在未来需求变更的时候，在不改动代码整体结构、做到最小代码改动的情况下，将新的代码灵活地插入到扩展点上。</p><p>很多设计原则、设计思想、设计模式，都是以提高代码的扩展性为最终目的的。特别是 23 种经典设计模式，大部分都是为了解决代码的扩展性问题而总结出来的，都是以开闭原则为指导原则的。最常用来提高代码扩展性的方法有：多态、依赖注入、基于接口而非实现编程，以及大部分的设计模式（比如，装饰、策略、模板、职责链、状态）。</p><h4 id="里式替换（Liskov-Substitution-Principle）"><a href="#里式替换（Liskov-Substitution-Principle）" class="headerlink" title="里式替换（Liskov Substitution Principle）"></a>里式替换（Liskov Substitution Principle）</h4><p>里式替换（LSP）：子类对象（object of subtype/derived class）能够替换程序（program）中父类对象（object of base/parent class）出现的任何地方，并且保证原来程序的逻辑行为（behavior）不变及正确性不被破坏。举例： 是拿父类的单元测试去验证子类的代码。如果某些单元测试运行失败，就有可能说明，子类的设计实现没有完全地遵守父类的约定，子类有可能违背了里式替换原则。</p><p>里式替换原则是用来指导，继承关系中子类该如何设计的一个原则。理解里式替换原则，最核心的就是理解“design by contract，按照协议来设计”这几个字。父类定义了函数的“约定”（或者叫协议），那子类可以改变函数的内部实现逻辑，但不能改变函数原有的“约定”。这里的约定包括：函数声明要实现的功能；对输入、输出、异常的约定；甚至包括注释中所罗列的任何特殊说明。</p><p>理解这个原则，我们还要弄明白里式替换原则跟多态的区别。虽然从定义描述和代码实现上来看，多态和里式替换有点类似，但它们关注的角度是不一样的。多态是面向对象编程的一大特性，也是面向对象编程语言的一种语法。它是一种代码实现的思路。而里式替换是一种设计原则，用来指导继承关系中子类该如何设计，子类的设计要保证在替换父类的时候，不改变原有程序的逻辑及不破坏原有程序的正确性。</p><h4 id="接口隔离原则（Interface-Segregation-Principle）"><a href="#接口隔离原则（Interface-Segregation-Principle）" class="headerlink" title="接口隔离原则（Interface Segregation Principle）"></a>接口隔离原则（Interface Segregation Principle）</h4><p>接口隔离原则（ISP）：调用方不应该被强迫依赖它不需要的接口。举例： Config 接口拆分为Updater 和 Viewer 两个接口</p><pre><code>public interface Updater {  void update();}public interface Viewer {  String outputInPlainText();  Map&lt;String, String&gt; output();}public class RedisConfig implemets Updater, Viewer {  //...省略其他属性和方法...  @Override  public void update() { //... }  @Override  public String outputInPlainText() { //... }  @Override  public Map&lt;String, String&gt; output() { //...}}public class KafkaConfig implements Updater {  //...省略其他属性和方法...  @Override  public void update() { //... }}public class MysqlConfig implements Viewer {  //...省略其他属性和方法...  @Override  public String outputInPlainText() { //... }  @Override  public Map&lt;String, String&gt; output() { //...}}public class SimpleHttpServer {  private String host;  private int port;  private Map&lt;String, List&lt;Viewer&gt;&gt; viewers = new HashMap&lt;&gt;();    public SimpleHttpServer(String host, int port) {//...}    public void addViewers(String urlDirectory, Viewer viewer) {    if (!viewers.containsKey(urlDirectory)) {      viewers.put(urlDirectory, new ArrayList&lt;Viewer&gt;());    }    this.viewers.get(urlDirectory).add(viewer);  }    public void run() { //... }}public class Application {    ConfigSource configSource = new ZookeeperConfigSource();    public static final RedisConfig redisConfig = new RedisConfig(configSource);    public static final KafkaConfig kafkaConfig = new KakfaConfig(configSource);    public static final MySqlConfig mysqlConfig = new MySqlConfig(configSource);        public static void main(String[] args) {        ScheduledUpdater redisConfigUpdater =            new ScheduledUpdater(redisConfig, 300, 300);        redisConfigUpdater.run();                ScheduledUpdater kafkaConfigUpdater =            new ScheduledUpdater(kafkaConfig, 60, 60);        redisConfigUpdater.run();                SimpleHttpServer simpleHttpServer = new SimpleHttpServer(“127.0.0.1”, 2389);        simpleHttpServer.addViewer("/config", redisConfig);        simpleHttpServer.addViewer("/config", mysqlConfig);        simpleHttpServer.run();    }}</code></pre><h4 id="依赖反转原则（Dependency-Inversion-Principle）"><a href="#依赖反转原则（Dependency-Inversion-Principle）" class="headerlink" title="依赖反转原则（Dependency Inversion Principle）"></a>依赖反转原则（Dependency Inversion Principle）</h4><p>依赖反转原则（DIP）： Dependency Inversion Principle 高层模块（high-level modules）不要依赖低层模块（low-level）。高层模块和低层模块应该通过抽象（abstractions）来互相依赖。除此之外，抽象（abstractions）不要依赖具体实现细节（details），具体实现细节（details）依赖抽象（abstractions）。举例 Tomcat和Java WebApp，两者都依赖同一个“抽象”，也就是 Servlet 规范。</p><ol><li>控制反转</li></ol><p><strong>控制反转（IOC）</strong>: Inversion Of Control，框架提供了一个可扩展的代码骨架，用来组装对象、管理整个执行流程。程序员利用框架进行开发的时候，只需要往预留的扩展点上，添加跟自己业务相关的代码，就可以利用框架来驱动整个程序流程的执行。</p><p>实际上，控制反转是一个比较笼统的设计思想，并不是一种具体的实现方法，一般用来指导框架层面的设计。这里所说的“控制”指的是对程序执行流程的控制，而“反转”指的是在没有使用框架之前，程序员自己控制整个程序的执行。在使用框架之后，整个程序的执行流程通过框架来控制。流程的控制权从程序员“反转”给了框架。</p><ol start="2"><li>依赖注入</li></ol><p>依赖注入（DI）: Dependency Injection 依赖注入的方式来将依赖的类对象传递进来，这样就提高了代码的扩展性，我们可以灵活地替换依赖的类</p><p>依赖注入和控制反转恰恰相反，它是一种具体的编码技巧。我们不通过 new 的方式在类内部创建依赖类的对象，而是将依赖的类对象在外部创建好之后，通过构造函数、函数参数等方式传递（或注入）给类来使用。</p><ol start="3"><li>依赖注入框架</li></ol><p>我们通过依赖注入框架提供的扩展点，简单配置一下所有需要的类及其类与类之间依赖关系，就可以实现由框架来自动创建对象、管理对象的生命周期、依赖注入等原本需要程序员来做的事情。</p><ol start="4"><li>依赖反转原则</li></ol><p>依赖反转原则也叫作依赖倒置原则。这条原则跟控制反转有点类似，主要用来指导框架层面的设计。高层模块不依赖低层模块，它们共同依赖同一个抽象。抽象不要依赖具体实现细节，具体实现细节依赖抽象。</p><h3 id="KISS原则"><a href="#KISS原则" class="headerlink" title="KISS原则"></a>KISS原则</h3><p>KISS 原则是保持代码可读和可维护的重要手段。KISS 原则中的“简单”并不是以代码行数来考量的。代码行数越少并不代表代码越简单，我们还要考虑逻辑复杂度、实现难度、代码的可读性等。而且，本身就复杂的问题，用复杂的方法解决，并不违背 KISS 原则。除此之外，同样的代码，在某个业务场景下满足 KISS 原则，换一个应用场景可能就不满足了。</p><p>对于如何写出满足 KISS 原则的代码，我还总结了下面几条指导原则：</p><ol><li>不要使用同事可能不懂的技术来实现代码。比如前面例子中的正则表达式，还有一些编程语言中过于高级的语法等。</li><li>不要重复造轮子，要善于使用已经有的工具类库。经验证明，自己去实现这些类库，出 bug 的概率会更高，维护的成本也比较高。</li><li>不要过度优化。不要过度使用一些奇技淫巧（比如，位运算代替算术运算、复杂的条件语句代替 if-else、使用一些过于底层的函数等）来优化代码，牺牲代码的可读性。</li></ol><h3 id="YAGNI原则"><a href="#YAGNI原则" class="headerlink" title="YAGNI原则"></a>YAGNI原则</h3><p>YAGNI 原则的英文全称是：You Ain’t Gonna Need It。直译就是：你不会需要它。这条原则也算是万金油了。当用在软件开发中的时候，它的意思是：不要去设计当前用不到的功能；不要去编写当前用不到的代码。实际上，这条原则的核心思想就是：不要做过度设计。</p><p>再比如，我们不要在项目中提前引入不需要依赖的开发包。对于 Java 程序员来说，我们经常使用 Maven 或者 Gradle 来管理依赖的类库（library）。我发现，有些同事为了避免开发中 library 包缺失而频繁地修改 Maven 或者 Gradle 配置文件，提前往项目里引入大量常用的 library 包。实际上，这样的做法也是违背 YAGNI 原则的。</p><h3 id="DRY-原则（Don’t-Repeat-Yourself）"><a href="#DRY-原则（Don’t-Repeat-Yourself）" class="headerlink" title="DRY 原则（Don’t Repeat Yourself）"></a>DRY 原则（Don’t Repeat Yourself）</h3><p>我们今天讲了三种代码重复的情况：实现逻辑重复、功能语义重复、代码执行重复。实现逻辑重复，但功能语义不重复的代码，并不违反 DRY 原则。实现逻辑不重复，但功能语义重复的代码，也算是违反 DRY 原则。除此之外，代码执行重复也算是违反 DRY 原则。</p><h3 id="代码复用性"><a href="#代码复用性" class="headerlink" title="代码复用性"></a>代码复用性</h3><h4 id="减少代码耦合"><a href="#减少代码耦合" class="headerlink" title="减少代码耦合"></a>减少代码耦合</h4><p>对于高度耦合的代码，当我们希望复用其中的一个功能，想把这个功能的代码抽取出来成为一个独立的模块、类或者函数的时候，往往会发现牵一发而动全身。移动一点代码，就要牵连到很多其他相关的代码。所以，高度耦合的代码会影响到代码的复用性，我们要尽量减少代码耦合。</p><h4 id="满足单一职责原则"><a href="#满足单一职责原则" class="headerlink" title="满足单一职责原则"></a>满足单一职责原则</h4><p>我们前面讲过，如果职责不够单一，模块、类设计得大而全，那依赖它的代码或者它依赖的代码就会比较多，进而增加了代码的耦合。根据上一点，也就会影响到代码的复用性。相反，越细粒度的代码，代码的通用性会越好，越容易被复用。</p><h4 id="模块化"><a href="#模块化" class="headerlink" title="模块化"></a>模块化</h4><p>这里的“模块”，不单单指一组类构成的模块，还可以理解为单个类、函数。我们要善于将功能独立的代码，封装成模块。独立的模块就像一块一块的积木，更加容易复用，可以直接拿来搭建更加复杂的系统。</p><h4 id="业务与非业务逻辑分离"><a href="#业务与非业务逻辑分离" class="headerlink" title="业务与非业务逻辑分离"></a>业务与非业务逻辑分离</h4><p>越是跟业务无关的代码越是容易复用，越是针对特定业务的代码越难复用。所以，为了复用跟业务无关的代码，我们将业务和非业务逻辑代码分离，抽取成一些通用的框架、类库、组件等。</p><h4 id="通用代码下沉"><a href="#通用代码下沉" class="headerlink" title="通用代码下沉"></a>通用代码下沉</h4><p>从分层的角度来看，越底层的代码越通用、会被越多的模块调用，越应该设计得足够可复用。一般情况下，在代码分层之后，为了避免交叉调用导致调用关系混乱，我们只允许上层代码调用下层代码及同层代码之间的调用，杜绝下层代码调用上层代码。所以，通用的代码我们尽量下沉到更下层。</p><h4 id="继承、多态、抽象、封装"><a href="#继承、多态、抽象、封装" class="headerlink" title="继承、多态、抽象、封装"></a>继承、多态、抽象、封装</h4><p>在讲面向对象特性的时候，我们讲到，利用继承，可以将公共的代码抽取到父类，子类复用父类的属性和方法。利用多态，我们可以动态地替换一段代码的部分逻辑，让这段代码可复用。除此之外，抽象和封装，从更加广义的层面、而非狭义的面向对象特性的层面来理解的话，越抽象、越不依赖具体的实现，越容易复用。代码封装成模块，隐藏可变的细节、暴露不变的接口，就越容易复用。</p><h4 id="应用模板等设计模式"><a href="#应用模板等设计模式" class="headerlink" title="应用模板等设计模式"></a>应用模板等设计模式</h4><p>一些设计模式，也能提高代码的复用性。比如，模板模式利用了多态来实现，可以灵活地替换其中的部分代码，整个流程模板代码可复用。关于应用设计模式提高代码复用性这一部分，我们留在后面慢慢来讲解。</p><h3 id="迪米特法则-最小知识原则"><a href="#迪米特法则-最小知识原则" class="headerlink" title="迪米特法则/最小知识原则"></a>迪米特法则/最小知识原则</h3><p>迪米特法则的英文翻译是：Law of Demeter，缩写是 LOD。也叫作最小知识原则，英文翻译为：The Least Knowledge Principle。</p><p>不该有直接依赖关系的类之间，不要有依赖；有依赖关系的类之间，尽量只依赖必要的接口（也就是定义中的“有限知识”）。</p><h4 id="如何理解“高内聚、松耦合”？"><a href="#如何理解“高内聚、松耦合”？" class="headerlink" title="如何理解“高内聚、松耦合”？"></a>如何理解“高内聚、松耦合”？</h4><p>“高内聚、松耦合”是一个非常重要的设计思想，能够有效提高代码的可读性和可维护性，缩小功能改动导致的代码改动范围。“高内聚”用来指导类本身的设计，“松耦合”用来指导类与类之间依赖关系的设计。</p><p>所谓高内聚，就是指相近的功能应该放到同一个类中，不相近的功能不要放到同一类中。相近的功能往往会被同时修改，放到同一个类中，修改会比较集中。所谓松耦合指的是，在代码中，类与类之间的依赖关系简单清晰。即使两个类有依赖关系，一个类的代码改动也不会或者很少导致依赖类的代码改动。</p><h4 id="如何理解“迪米特法则”？"><a href="#如何理解“迪米特法则”？" class="headerlink" title="如何理解“迪米特法则”？"></a>如何理解“迪米特法则”？</h4><p>不该有直接依赖关系的类之间，不要有依赖；有依赖关系的类之间，尽量只依赖必要的接口。迪米特法则是希望减少类之间的耦合，让类越独立越好。每个类都应该少了解系统的其他部分。一旦发生变化，需要了解这一变化的类就会比较少。</p><h3 id="为什么要分-MVC-三层开发？"><a href="#为什么要分-MVC-三层开发？" class="headerlink" title="为什么要分 MVC 三层开发？"></a>为什么要分 MVC 三层开发？</h3><h4 id="分层能起到代码复用的作用"><a href="#分层能起到代码复用的作用" class="headerlink" title="分层能起到代码复用的作用"></a>分层能起到代码复用的作用</h4><p>同一个 Repository 可能会被多个 Service 来调用，同一个 Service 可能会被多个 Controller 调用。比如，UserService 中的 getUserById() 接口封装了通过 ID 获取用户信息的逻辑，这部分逻辑可能会被 UserController 和 AdminController 等多个 Controller 使用。如果没有 Service 层，每个 Controller 都要重复实现这部分逻辑，显然会违反 DRY 原则。</p><h4 id="分层能起到隔离变化的作用"><a href="#分层能起到隔离变化的作用" class="headerlink" title="分层能起到隔离变化的作用"></a>分层能起到隔离变化的作用</h4><p>分层体现了一种抽象和封装的设计思想。比如，Repository 层封装了对数据库访问的操作，提供了抽象的数据访问接口。基于接口而非实现编程的设计思想，Service 层使用 Repository 层提供的接口，并不关心其底层依赖的是哪种具体的数据库。当我们需要替换数据库的时候，比如从 MySQL 到 Oracle，从 Oracle 到 Redis，只需要改动 Repository 层的代码，Service 层的代码完全不需要修改。</p><p>除此之外，Controller、Service、Repository 三层代码的稳定程度不同、引起变化的原因不同，所以分成三层来组织代码，能有效地隔离变化。比如，Repository 层基于数据库表，而数据库表改动的可能性很小，所以 Repository 层的代码最稳定，而 Controller 层提供适配给外部使用的接口，代码经常会变动。分层之后，Controller 层中代码的频繁改动并不会影响到稳定的 Repository 层。</p><h4 id="分层能起到隔离关注点的作用"><a href="#分层能起到隔离关注点的作用" class="headerlink" title="分层能起到隔离关注点的作用"></a>分层能起到隔离关注点的作用</h4><p>Repository 层只关注数据的读写。Service 层只关注业务逻辑，不关注数据的来源。Controller 层只关注与外界打交道，数据校验、封装、格式转换，并不关心业务逻辑。三层之间的关注点不同，分层之后，职责分明，更加符合单一职责原则，代码的内聚性更好。</p><h4 id="分层能提高代码的可测试性"><a href="#分层能提高代码的可测试性" class="headerlink" title="分层能提高代码的可测试性"></a>分层能提高代码的可测试性</h4><p>后面讲单元测试的时候，我们会讲到，单元测试不依赖不可控的外部组件，比如数据库。分层之后，Repsitory 层的代码通过依赖注入的方式供 Service 层使用，当要测试包含核心业务逻辑的 Service 层代码的时候，我们可以用 mock 的数据源替代真实的数据库，注入到 Service 层代码中。代码的可测试性和单元测试我们后面会讲到，这里你稍微了解即可。</p><h4 id="分层能应对系统的复杂性"><a href="#分层能应对系统的复杂性" class="headerlink" title="分层能应对系统的复杂性"></a>分层能应对系统的复杂性</h4><p>所有的代码都放到一个类中，那这个类的代码就会因为需求的迭代而无限膨胀。我们知道，当一个类或一个函数的代码过多之后，可读性、可维护性就会变差。那我们就要想办法拆分。拆分有垂直和水平两个方向。水平方向基于业务来做拆分，就是模块化；垂直方向基于流程来做拆分，就是这里说的分层。</p><p>还是那句话，不管是分层、模块化，还是 OOP、DDD，以及各种设计模式、原则和思想，都是为了应对复杂系统，应对系统的复杂性。对于简单系统来说，其实是发挥不了作用的，就是俗话说的“杀鸡焉用牛刀”。</p><p>还是那句话，不管是分层、模块化，还是 OOP、DDD，以及各种设计模式、原则和思想，都是为了应对复杂系统，应对系统的复杂性。对于简单系统来说，其实是发挥不了作用的，就是俗话说的“杀鸡焉用牛刀”。</p><h3 id="BO、VO、Entity-存在的意义是什么？"><a href="#BO、VO、Entity-存在的意义是什么？" class="headerlink" title="BO、VO、Entity 存在的意义是什么？"></a>BO、VO、Entity 存在的意义是什么？</h3><p>在前面的章节中，我们提到，针对 Controller、Service、Repository 三层，每层都会定义相应的数据对象，它们分别是 VO（View Object）、BO（Business Object）、Entity，例如 UserVo、UserBo、UserEntity。在实际的开发中，VO、BO、Entity 可能存在大量的重复字段，甚至三者包含的字段完全一样。在开发的过程中，我们经常需要重复定义三个几乎一样的类，显然是一种重复劳动。</p><h4 id="相对于每层定义各自的数据对象来说，是不是定义一个公共的数据对象更好些呢？"><a href="#相对于每层定义各自的数据对象来说，是不是定义一个公共的数据对象更好些呢？" class="headerlink" title="相对于每层定义各自的数据对象来说，是不是定义一个公共的数据对象更好些呢？"></a>相对于每层定义各自的数据对象来说，是不是定义一个公共的数据对象更好些呢？</h4><p>实际上，我更加推荐每层都定义各自的数据对象这种设计思路，主要有以下 3 个方面的原因。</p><ul><li>VO、BO、Entity 并非完全一样。比如，我们可以在 UserEntity、UserBo 中定义 Password 字段，但显然不能在 UserVo 中定义 Password 字段，否则就会将用户的密码暴露出去。</li><li>VO、BO、Entity 三个类虽然代码重复，但功能语义不重复，从职责上讲是不一样的。所以，也并不能算违背 DRY 原则。在前面讲到 DRY 原则的时候，针对这种情况，如果合并为同一个类，那也会存在后期因为需求的变化而需要再拆分的问题。</li><li>为了尽量减少每层之间的耦合，把职责边界划分明确，每层都会维护自己的数据对象，层与层之间通过接口交互。数据从下一层传递到上一层的时候，将下一层的数据对象转化成上一层的数据对象，再继续处理。虽然这样的设计稍微有些繁琐，每层都需要定义各自的数据对象，需要做数据对象之间的转化，但是分层清晰。对于非常大的项目来说，结构清晰是第一位的！</li></ul><h4 id="既然-VO、BO、Entity-不能合并，那如何解决代码重复的问题呢？"><a href="#既然-VO、BO、Entity-不能合并，那如何解决代码重复的问题呢？" class="headerlink" title="既然 VO、BO、Entity 不能合并，那如何解决代码重复的问题呢？"></a>既然 VO、BO、Entity 不能合并，那如何解决代码重复的问题呢？</h4><p>从设计的角度来说，VO、BO、Entity 的设计思路并不违反 DRY 原则，为了分层清晰、减少耦合，多维护几个类的成本也并不是不能接受的。但是，如果你真的有代码洁癖，对于代码重复的问题，我们也有一些办法来解决。<br>我们前面讲到，继承可以解决代码重复问题。我们可以将公共的字段定义在父类中，让 VO、BO、Entity 都继承这个父类，各自只定义特有的字段。因为这里的继承层次很浅，也不复杂，所以使用继承并不会影响代码的可读性和可维护性。后期如果因为业务的需要，有些字段需要从父类移动到子类，或者从子类提取到父类，代码改起来也并不复杂。<br>前面在讲“多用组合，少用继承”设计思想的时候，我们提到，组合也可以解决代码重复的问题，所以，这里我们还可以将公共的字段抽取到公共的类中，VO、BO、Entity 通过组合关系来复用这个类的代码。</p><h4 id="代码重复问题解决了，那不同分层之间的数据对象该如何互相转化呢？"><a href="#代码重复问题解决了，那不同分层之间的数据对象该如何互相转化呢？" class="headerlink" title="代码重复问题解决了，那不同分层之间的数据对象该如何互相转化呢？"></a>代码重复问题解决了，那不同分层之间的数据对象该如何互相转化呢？</h4><p>当下一层的数据通过接口调用传递到上一层之后，我们需要将它转化成上一层对应的数据对象类型。比如，Service 层从 Repository 层获取的 Entity 之后，将其转化成 BO，再继续业务逻辑的处理。所以，整个开发的过程会涉及“Entity 到 BO”和“BO 到 VO”这两种转化。</p><p>VO、BO、Entity 都是基于贫血模型的，而且为了兼容框架或开发库（比如 MyBatis、Dozer、BeanUtils），我们还需要定义每个字段的 set 方法。这些都违背 OOP 的封装特性，会导致数据被随意修改。那到底该怎么办好呢？</p><p>前面我们也提到过，Entity 和 VO 的生命周期是有限的，都仅限在本层范围内。而对应的 Repository 层和 Controller 层也都不包含太多业务逻辑，所以也不会有太多代码随意修改数据，即便设计成贫血、定义每个字段的 set 方法，相对来说也是安全的。</p><p>不过，Service 层包含比较多的业务逻辑代码，所以 BO 就存在被任意修改的风险了。但是，设计的问题本身就没有最优解，只有权衡。为了使用方便，我们只能做一些妥协，放弃 BO 的封装特性，由程序员自己来负责这些数据对象的不被错误使用。</p><h2 id="规范与重构"><a href="#规范与重构" class="headerlink" title="规范与重构"></a>规范与重构</h2><h3 id="为什么要重构"><a href="#为什么要重构" class="headerlink" title="为什么要重构"></a>为什么要重构</h3><p>首先，重构是时刻保证代码质量的一个极其有效的手段，不至于让代码腐化到无可救药的地步。项目在演进，代码不停地在堆砌。如果没有人为代码的质量负责任，代码总是会往越来越混乱的方向演进。当混乱到一定程度之后，量变引起质变，项目的维护成本已经高过重新开发一套新代码的成本，想要再去重构，已经没有人能做到了。</p><p>其次，优秀的代码或架构不是一开始就能完全设计好的，就像优秀的公司和产品也都是迭代出来的。我们无法 100% 遇见未来的需求，也没有足够的精力、时间、资源为遥远的未来买单，所以，随着系统的演进，重构代码也是不可避免的。</p><p>重构是避免过度设计的有效手段。在我们维护代码的过程中，真正遇到问题的时候，再对代码进行重构，能有效避免前期投入太多时间做过度的设计，做到有的放矢。</p><p><strong>除此之外，重构对一个工程师本身技术的成长也有重要的意义。</strong></p><p>从前面我给出的重构的定义来看，重构实际上是对我们学习的经典设计思想、设计原则、设计模式、编程规范的一种应用。重构实际上就是将这些理论知识，应用到实践的一个很好的场景，能够锻炼我们熟练使用这些理论知识的能力。除此之外，平时堆砌业务逻辑，你可能总觉得没啥成长，而将一个比较烂的代码重构成一个比较好的代码，会让你很有成就感。</p><h3 id="重构的对象"><a href="#重构的对象" class="headerlink" title="重构的对象"></a>重构的对象</h3><p>根据重构的规模，我们可以笼统地分为大规模高层次重构（以下简称为“大型重构”）和小规模低层次的重构（以下简称为“小型重构”）。</p><p>大型重构指的是对顶层代码设计的重构，包括：系统、模块、代码结构、类与类之间的关系等的重构，重构的手段有：分层、模块化、解耦、抽象可复用组件等等。这类重构的工具就是我们学习过的那些设计思想、原则和模式。这类重构涉及的代码改动会比较多，影响面会比较大，所以难度也较大，耗时会比较长，引入 bug 的风险也会相对比较大。</p><p>小型重构指的是对代码细节的重构，主要是针对类、函数、变量等代码级别的重构，比如规范命名、规范注释、消除超大类或函数、提取重复代码等等。小型重构更多的是利用我们能后面要讲到的编码规范。这类重构要修改的地方比较集中，比较简单，可操作性较强，耗时会比较短，引入 bug 的风险相对来说也会比较小。你只需要熟练掌握各种编码规范，就可以做到得心应手。</p><h3 id="什么时候重构"><a href="#什么时候重构" class="headerlink" title="什么时候重构"></a>什么时候重构</h3><p>搞清楚了为什么重构，到底重构什么，我们再来看一下，什么时候重构？是代码烂到一定程度之后才去重构吗？当然不是。因为当代码真的烂到出现“开发效率低，招了很多人，天天加班，出活却不多，线上 bug 频发，领导发飙，中层束手无策，工程师抱怨不断，查找 bug 困难”的时候，基本上重构也无法解决问题了。</p><p>我个人比较反对，平时不注重代码质量，堆砌烂代码，实在维护不了了就大刀阔斧地重构、甚至重写的行为。有时候项目代码太多了，重构很难做得彻底，最后又搞出来一个“四不像的怪物”，这就更麻烦了！所以，寄希望于在代码烂到一定程度之后，集中重构解决所有问题是不现实的，我们必须探索一条可持续、可演进的方式。</p><p>所以，我特别提倡的重构策略是持续重构。这也是我在工作中特别喜欢干的事情。平时没有事情的时候，你可以看看项目中有哪些写得不够好的、可以优化的代码，主动去重构一下。或者，在修改、添加某个功能代码的时候，你也可以顺手把不符合编码规范、不好的设计重构一下。总之，就像把单元测试、Code Review 作为开发的一部分，我们如果能把持续重构也作为开发的一部分，成为一种开发习惯，对项目、对自己都会很有好处。</p><p>尽管我们说重构能力很重要，但持续重构意识更重要。我们要正确地看待代码质量和重构这件事情。技术在更新、需求在变化、人员在流动，代码质量总会在下降，代码总会存在不完美，重构就会持续在进行。时刻具有持续重构意识，才能避免开发初期就过度设计，避免代码维护的过程中质量的下降。而那些看到别人代码有点瑕疵就一顿乱骂，或者花尽心思去构思一个完美设计的人，往往都是因为没有树立正确的代码质量观，没有持续重构意识。</p><h3 id="如何重构"><a href="#如何重构" class="headerlink" title="如何重构"></a>如何重构</h3><p>前面我们讲到，按照重构的规模，重构可以笼统地分为大型重构和小型重构。对于这两种不同规模的重构，我们要区别对待。</p><p>对于大型重构来说，因为涉及的模块、代码会比较多，如果项目代码质量又比较差，耦合比较严重，往往会牵一发而动全身，本来觉得一天就能完成的重构，你会发现越改越多、越改越乱，没一两个礼拜都搞不定。而新的业务开发又与重构相冲突，最后只能半途而废，revert 掉所有的改动，很失落地又去堆砌烂代码了。</p><p>在进行大型重构的时候，我们要提前做好完善的重构计划，有条不紊地分阶段来进行。每个阶段完成一小部分代码的重构，然后提交、测试、运行，发现没有问题之后，再继续进行下一阶段的重构，保证代码仓库中的代码一直处于可运行、逻辑正确的状态。每个阶段，我们都要控制好重构影响到的代码范围，考虑好如何兼容老的代码逻辑，必要的时候还需要写一些兼容过渡代码。只有这样，我们才能让每一阶段的重构都不至于耗时太长（最好一天就能完成），不至于与新的功能开发相冲突。</p><p>大规模高层次的重构一定是有组织、有计划，并且非常谨慎的，需要有经验、熟悉业务的资深同事来主导。而小规模低层次的重构，因为影响范围小，改动耗时短，所以，只要你愿意并且有时间，随时都可以去做。实际上，除了人工去发现低层次的质量问题，我们还可以借助很多成熟的静态代码分析工具（比如 CheckStyle、FindBugs、PMD），来自动发现代码中的问题，然后针对性地进行重构优化。</p><p>对于重构这件事情，资深的工程师、项目 leader 要负起责任来，没事就重构一下代码，时刻保证代码质量处在一个良好的状态。否则，一旦出现“破窗效应”，一个人往里堆了一些烂代码，之后就会有更多的人往里堆更烂的代码。毕竟往项目里堆砌烂代码的成本太低了。不过，保持代码质量最好的方法还是打造一种好的技术氛围，以此来驱动大家主动去关注代码质量，持续重构代码。</p><h3 id="解耦为什么重要"><a href="#解耦为什么重要" class="headerlink" title="解耦为什么重要"></a>解耦为什么重要</h3><p>软件设计与开发最重要的工作之一就是应对复杂性。人处理复杂性的能力是有限的。过于复杂的代码往往在可读性、可维护性上都不友好。那如何来控制代码的复杂性呢？手段有很多，我个人认为，最关键的就是解耦，保证代码松耦合、高内聚。如果说重构是保证代码质量不至于腐化到无可救药地步的有效手段，那么利用解耦的方法对代码重构，就是保证代码不至于复杂到无法控制的有效手段。</p><h3 id="代码是否需要“解耦”？"><a href="#代码是否需要“解耦”？" class="headerlink" title="代码是否需要“解耦”？"></a>代码是否需要“解耦”？</h3><p>间接的衡量标准有很多，前面我们讲到了一些，比如，看修改代码会不会牵一发而动全身。除此之外，还有一个直接的衡量标准，也是我在阅读源码的时候经常会用到的，那就是把模块与模块之间、类与类之间的依赖关系画出来，根据依赖关系图的复杂性来判断是否需要解耦重构。</p><p>如果依赖关系复杂、混乱，那从代码结构上来讲，可读性和可维护性肯定不是太好，那我们就需要考虑是否可以通过解耦的方法，让依赖关系变得清晰、简单。当然，这种判断还是有比较强的主观色彩，但是可以作为一种参考和梳理依赖的手段，配合间接的衡量标准一块来使用。</p><h3 id="如何给代码解耦"><a href="#如何给代码解耦" class="headerlink" title="如何给代码解耦"></a>如何给代码解耦</h3><h4 id="封装与抽象"><a href="#封装与抽象" class="headerlink" title="封装与抽象"></a>封装与抽象</h4><p>封装和抽象作为两个非常通用的设计思想，可以应用在很多设计场景中，比如系统、模块、lib、组件、接口、类等等的设计。封装和抽象可以有效地隐藏实现的复杂性，隔离实现的易变性，给依赖的模块提供稳定且易用的抽象接口。</p><p>比如，Unix 系统提供的 open() 文件操作函数，我们用起来非常简单，但是底层实现却非常复杂，涉及权限控制、并发控制、物理存储等等。我们通过将其封装成一个抽象的 open() 函数，能够有效控制代码复杂性的蔓延，将复杂性封装在局部代码中。除此之外，因为 open() 函数基于抽象而非具体的实现来定义，所以我们在改动 open() 函数的底层实现的时候，并不需要改动依赖它的上层代码，也符合我们前面提到的“高内聚、松耦合”代码的评判标准。</p><h4 id="中间层"><a href="#中间层" class="headerlink" title="中间层"></a>中间层</h4><p>引入中间层能简化模块或类之间的依赖关系。下面这张图是引入中间层前后的依赖关系对比图。在引入数据存储中间层之前，A、B、C 三个模块都要依赖内存一级缓存、Redis 二级缓存、DB 持久化存储三个模块。在引入中间层之后，三个模块只需要依赖数据存储一个模块即可。从图上可以看出，中间层的引入明显地简化了依赖关系，让代码结构更加清晰。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f6b6293ea1c90320.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>除此之外，我们在进行重构的时候，引入中间层可以起到过渡的作用，能够让开发和重构同步进行，不互相干扰。比如，某个接口设计得有问题，我们需要修改它的定义，同时，所有调用这个接口的代码都要做相应的改动。如果新开发的代码也用到这个接口，那开发就跟重构冲突了。为了让重构能小步快跑，我们可以分下面四个阶段来完成接口的修改。</p><pre><code>第一阶段：引入一个中间层，包裹老的接口，提供新的接口定义。第二阶段：新开发的代码依赖中间层提供的新接口。第三阶段：将依赖老接口的代码改为调用新接口。第四阶段：确保所有的代码都调用新接口之后，删除掉老的接口。</code></pre><h4 id="模块化-1"><a href="#模块化-1" class="headerlink" title="模块化"></a>模块化</h4><p>模块化是构建复杂系统常用的手段。不仅在软件行业，在建筑、机械制造等行业，这个手段也非常有用。对于一个大型复杂系统来说，没有人能掌控所有的细节。之所以我们能搭建出如此复杂的系统，并且能维护得了，最主要的原因就是将系统划分成各个独立的模块，让不同的人负责不同的模块，这样即便在不了解全部细节的情况下，管理者也能协调各个模块，让整个系统有效运转。</p><p>聚焦到软件开发上面，很多大型软件（比如 Windows）之所以能做到几百、上千人有条不紊地协作开发，也归功于模块化做得好。不同的模块之间通过 API 来进行通信，每个模块之间耦合很小，每个小的团队聚焦于一个独立的高内聚模块来开发，最终像搭积木一样将各个模块组装起来，构建成一个超级复杂的系统。</p><p>我们再聚焦到代码层面。合理地划分模块能有效地解耦代码，提高代码的可读性和可维护性。所以，我们在开发代码的时候，一定要有模块化意识，将每个模块都当作一个独立的 lib 一样来开发，只提供封装了内部实现细节的接口给其他模块使用，这样可以减少不同模块之间的耦合度。</p><p>实际上，从刚刚的讲解中我们也可以发现，模块化的思想无处不在，像 SOA、微服务、lib 库、系统内模块划分，甚至是类、函数的设计，都体现了模块化思想。如果追本溯源，模块化思想更加本质的东西就是分而治之。</p><h4 id="其他设计思想和原则"><a href="#其他设计思想和原则" class="headerlink" title="其他设计思想和原则"></a>其他设计思想和原则</h4><p>“高内聚、松耦合”是一个非常重要的设计思想，能够有效提高代码的可读性和可维护性，缩小功能改动导致的代码改动范围。实际上，在前面的章节中，我们已经多次提到过这个设计思想。很多设计原则都以实现代码的“高内聚、松耦合”为目的。我们来一块总结回顾一下都有哪些原则。</p><ul><li>单一职责原则<br>我们前面提到，内聚性和耦合性并非独立的。高内聚会让代码更加松耦合，而实现高内聚的重要指导原则就是单一职责原则。模块或者类的职责设计得单一，而不是大而全，那依赖它的类和它依赖的类就会比较少，代码耦合也就相应的降低了。</li><li>基于接口而非实现编程<br>基于接口而非实现编程能通过接口这样一个中间层，隔离变化和具体的实现。这样做的好处是，在有依赖关系的两个模块或类之间，一个模块或者类的改动，不会影响到另一个模块或类。实际上，这就相当于将一种强依赖关系（强耦合）解耦为了弱依赖关系（弱耦合）。</li><li>依赖注入<br>跟基于接口而非实现编程思想类似，依赖注入也是将代码之间的强耦合变为弱耦合。尽管依赖注入无法将本应该有依赖关系的两个类，解耦为没有依赖关系，但可以让耦合关系没那么紧密，容易做到插拔替换。</li><li>多用组合少用继承<br>我们知道，继承是一种强依赖关系，父类与子类高度耦合，且这种耦合关系非常脆弱，牵一发而动全身，父类的每一次改动都会影响所有的子类。相反，组合关系是一种弱依赖关系，这种关系更加灵活，所以，对于继承结构比较复杂的代码，利用组合来替换继承，也是一种解耦的有效手段。</li><li>迪米特法则<br>迪米特法则讲的是，不该有直接依赖关系的类之间，不要有依赖；有依赖关系的类之间，尽量只依赖必要的接口。从定义上，我们明显可以看出，这条原则的目的就是为了实现代码的松耦合。</li></ul><p>除了上面讲到的这些设计思想和原则之外，还有一些设计模式也是为了解耦依赖，比如观察者模式，有关这一部分的内容，我们留在设计模式模块中慢慢讲解。</p><h3 id="快速改善代码质量的20条编程规范"><a href="#快速改善代码质量的20条编程规范" class="headerlink" title="快速改善代码质量的20条编程规范"></a>快速改善代码质量的20条编程规范</h3><h4 id="关于命名"><a href="#关于命名" class="headerlink" title="关于命名"></a>关于命名</h4><ol><li>命名的关键是能准确达意。对于不同作用域的命名，我们可以适当地选择不同的长度。</li><li>我们可以借助类的信息来简化属性、函数的命名，利用函数的信息来简化函数参数的命名。</li><li>命名要可读、可搜索。不要使用生僻的、不好读的英文单词来命名。命名要符合项目的统一规范，也不要用些反直觉的命名。</li><li>接口有两种命名方式：一种是在接口中带前缀“I”；另一种是在接口的实现类中带后缀“Impl”。对于抽象类的命名，也有两种方式，一种是带上前缀“Abstract”，一种是不带前缀。这两种命名方式都可以，关键是要在项目中统一。</li></ol><h4 id="关于注释"><a href="#关于注释" class="headerlink" title="关于注释"></a>关于注释</h4><ol><li>注释的内容主要包含这样三个方面：做什么、为什么、怎么做。对于一些复杂的类和接口，我们可能还需要写明“如何用”。</li><li>类和函数一定要写注释，而且要写得尽可能全面详细。函数内部的注释要相对少一些，一般都是靠好的命名、提炼函数、解释性变量、总结性注释来提高代码可读性。</li><li>关于代码风格</li><li>函数、类多大才合适？函数的代码行数不要超过一屏幕的大小，比如 50 行。类的大小限制比较难确定。</li><li>一行代码多长最合适？最好不要超过 IDE 的显示宽度。当然，也不能太小，否则会导致很多稍微长点的语句被折成两行，也会影响到代码的整洁，不利于阅读。</li><li>善用空行分割单元块。对于比较长的函数，为了让逻辑更加清晰，可以使用空行来分割各个代码块。</li><li>四格缩进还是两格缩进？我个人比较推荐使用两格缩进，这样可以节省空间，尤其是在代码嵌套层次比较深的情况下。不管是用两格缩进还是四格缩进，一定不要用 tab 键缩进。</li><li>大括号是否要另起一行？将大括号放到跟上一条语句同一行，可以节省代码行数。但是将大括号另起新的一行的方式，左右括号可以垂直对齐，哪些代码属于哪一个代码块，更加一目了然。</li><li>类中成员怎么排列？在 Google Java 编程规范中，依赖类按照字母序从小到大排列。类中先写成员变量后写函数。成员变量之间或函数之间，先写静态成员变量或函数，后写普通变量或函数，并且按照作用域大小依次排列。</li></ol><h4 id="关于编码技巧"><a href="#关于编码技巧" class="headerlink" title="关于编码技巧"></a>关于编码技巧</h4><ol><li>将复杂的逻辑提炼拆分成函数和类。</li><li>通过拆分成多个函数或将参数封装为对象的方式，来处理参数过多的情况。</li><li>函数中不要使用参数来做代码执行逻辑的控制。</li><li>函数设计要职责单一。</li><li>移除过深的嵌套层次，方法包括：去掉多余的 if 或 else 语句，使用 continue、break、return 关键字提前退出嵌套，调整执行顺序来减少嵌套，将部分嵌套逻辑抽象成函数。</li><li>用字面常量取代魔法数。</li><li>用解释性变量来解释复杂表达式，以此提高代码可读性。</li></ol><h4 id="统一编码规范"><a href="#统一编码规范" class="headerlink" title="统一编码规范"></a>统一编码规范</h4><ul><li>除了这三节讲到的比较细节的知识点之外，最后，还有一条非常重要的，那就是，项目、团队，甚至公司，一定要制定统一的编码规范，并且通过 Code Review 督促执行，这对提高代码质量有立竿见影的效果。</li></ul><h3 id="如何发现代码质量问题"><a href="#如何发现代码质量问题" class="headerlink" title="如何发现代码质量问题"></a>如何发现代码质量问题</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-04788b3c713fd591.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-86aa0ab18cec9edd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="关于重构思考"><a href="#关于重构思考" class="headerlink" title="关于重构思考"></a>关于重构思考</h3><ol><li>即便是非常简单的需求，不同水平的人写出来的代码，差别可能会很大。我们要对代码质量有所追求，不能只是凑活能用就好。花点心思写一段高质量的代码，比写 100 段凑活能用的代码，对你的代码能力提高更有帮助。</li><li>知其然知其所以然，了解优秀代码设计的演变过程，比学习优秀设计本身更有价值。知道为什么这么做，比单纯地知道怎么做更重要，这样可以避免你过度使用设计模式、思想和原则。</li><li>设计思想、原则、模式本身并没有太多“高大上”的东西，都是一些简单的道理，而且知识点也并不多，关键还是锻炼具体代码具体分析的能力，把知识点恰当地用在项目中。</li><li>我经常讲，高手之间的竞争都是在细节。大的架构设计、分层、分模块思路实际上都差不多。没有项目是靠一些不为人知的设计来取胜的，即便有，很快也能被学习过去。所以，关键还是看代码细节处理得够不够好。这些细节的差别累积起来，会让代码质量有质的差别。所以，要想提高代码质量，还是要在细节处下功夫。</li></ol><h3 id="函数出错应该返回啥？"><a href="#函数出错应该返回啥？" class="headerlink" title="函数出错应该返回啥？"></a>函数出错应该返回啥？</h3><ol><li>返回错误码C ：语言没有异常这样的语法机制，返回错误码便是最常用的出错处理方式。而 Java、Python 等比较新的编程语言中，大部分情况下，我们都用异常来处理函数出错的情况，极少会用到错误码。</li><li>返回 NULL 值：在多数编程语言中，我们用 NULL 来表示“不存在”这种语义。对于查找函数来说，数据不存在并非一种异常情况，是一种正常行为，所以返回表示不存在语义的 NULL 值比返回异常更加合理。</li><li>返回空对象返回： NULL 值有各种弊端，对此有一个比较经典的应对策略，那就是应用空对象设计模式。当函数返回的数据是字符串类型或者集合类型的时候，我们可以用空字符串或空集合替代 NULL 值，来表示不存在的情况。这样，我们在使用函数的时候，就可以不用做 NULL 值判断。</li><li>抛出异常对象：尽管前面讲了很多函数出错的返回数据类型，但是，最常用的函数出错处理方式是抛出异常。异常有两种类型：受检异常和非受检异常。</li></ol><p>总之，是否往上继续抛出，要看上层代码是否关心这个异常。关心就将它抛出，否则就直接吞掉。是否需要包装成新的异常抛出，看上层代码是否能理解这个异常、是否业务相关。如果能理解、业务相关就可以直接抛出，否则就封装成新的异常抛出。</p><h2 id="代码质量、面向对象、设计模式、规范与重构总结"><a href="#代码质量、面向对象、设计模式、规范与重构总结" class="headerlink" title="代码质量、面向对象、设计模式、规范与重构总结"></a>代码质量、面向对象、设计模式、规范与重构总结</h2><h3 id="代码质量评判标准"><a href="#代码质量评判标准" class="headerlink" title="代码质量评判标准"></a>代码质量评判标准</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-22a9b7ac55f0f80b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="面向对象-1"><a href="#面向对象-1" class="headerlink" title="面向对象"></a>面向对象</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7808c1408529028b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9f9f4b7eedf2e9fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="规范与重构-1"><a href="#规范与重构-1" class="headerlink" title="规范与重构"></a>规范与重构</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2c804e58bb5a0928.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="设计模式与范式：创建型"><a href="#设计模式与范式：创建型" class="headerlink" title="设计模式与范式：创建型"></a>设计模式与范式：创建型</h2><h3 id="单例设计模式（Singleton-Design-Pattern）"><a href="#单例设计模式（Singleton-Design-Pattern）" class="headerlink" title="单例设计模式（Singleton Design Pattern）"></a>单例设计模式（Singleton Design Pattern）</h3><h4 id="单例的定义"><a href="#单例的定义" class="headerlink" title="单例的定义"></a>单例的定义</h4><p>单例设计模式（Singleton Design Pattern）理解起来非常简单。一个类只允许创建一个对象（或者叫实例），那这个类就是一个单例类，这种设计模式就叫作单例设计模式，简称单例模式。</p><h4 id="单例的用处"><a href="#单例的用处" class="headerlink" title="单例的用处"></a>单例的用处</h4><p>从业务概念上，有些数据在系统中只应该保存一份，就比较适合设计为单例类。比如，系统的配置信息类。除此之外，我们还可以使用单例解决资源访问冲突的问题。</p><h4 id="单例存在哪些问题"><a href="#单例存在哪些问题" class="headerlink" title="单例存在哪些问题"></a>单例存在哪些问题</h4><ul><li>单例对 OOP 特性的支持不友好</li><li>单例会隐藏类之间的依赖关系</li><li>单例对代码的扩展性不友好</li><li>单例对代码的可测试性不友好</li><li>单例不支持有参数的构造函数</li></ul><h4 id="单例有什么替代解决方案？"><a href="#单例有什么替代解决方案？" class="headerlink" title="单例有什么替代解决方案？"></a>单例有什么替代解决方案？</h4><p>为了保证全局唯一，除了使用单例，我们还可以用静态方法来实现。不过，静态方法这种实现思路，并不能解决我们之前提到的问题。如果要完全解决这些问题，我们可能要从根上，寻找其他方式来实现全局唯一类了。比如，通过工厂模式、IOC 容器（比如 Spring IOC 容器）来保证，由程序员自己来保证（自己在编写代码的时候自己保证不要创建两个类对象）。</p><p>有人把单例当作反模式，主张杜绝在项目中使用。我个人觉得这有点极端。模式没有对错，关键看你怎么用。如果单例类并没有后续扩展的需求，并且不依赖外部系统，那设计成单例类就没有太大问题。对于一些全局的类，我们在其他地方 new 的话，还要在类之间传来传去，不如直接做成单例类，使用起来简洁方便。</p><h3 id="工厂模式（Factory-Design-Pattern）"><a href="#工厂模式（Factory-Design-Pattern）" class="headerlink" title="工厂模式（Factory Design Pattern）"></a>工厂模式（Factory Design Pattern）</h3><h4 id="简单工场（Simple-Factory）"><a href="#简单工场（Simple-Factory）" class="headerlink" title="简单工场（Simple Factory）"></a>简单工场（Simple Factory）</h4><p>实现一</p><pre><code>public class RuleConfigSource {  public RuleConfig load(String ruleConfigFilePath) {    String ruleConfigFileExtension = getFileExtension(ruleConfigFilePath);    IRuleConfigParser parser = RuleConfigParserFactory.createParser(ruleConfigFileExtension);    if (parser == null) {      throw new InvalidRuleConfigException(              "Rule config file format is not supported: " + ruleConfigFilePath);    }    String configText = "";    //从ruleConfigFilePath文件中读取配置文本到configText中    RuleConfig ruleConfig = parser.parse(configText);    return ruleConfig;  }  private String getFileExtension(String filePath) {    //...解析文件名获取扩展名，比如rule.json，返回json    return "json";  }}public class RuleConfigParserFactory {  public static IRuleConfigParser createParser(String configFormat) {    IRuleConfigParser parser = null;    if ("json".equalsIgnoreCase(configFormat)) {      parser = new JsonRuleConfigParser();    } else if ("xml".equalsIgnoreCase(configFormat)) {      parser = new XmlRuleConfigParser();    } else if ("yaml".equalsIgnoreCase(configFormat)) {      parser = new YamlRuleConfigParser();    } else if ("properties".equalsIgnoreCase(configFormat)) {      parser = new PropertiesRuleConfigParser();    }    return parser;  }}</code></pre><p>实现二</p><pre><code>public class RuleConfigParserFactory {  private static final Map&lt;String, RuleConfigParser&gt; cachedParsers = new HashMap&lt;&gt;();  static {    cachedParsers.put("json", new JsonRuleConfigParser());    cachedParsers.put("xml", new XmlRuleConfigParser());    cachedParsers.put("yaml", new YamlRuleConfigParser());    cachedParsers.put("properties", new PropertiesRuleConfigParser());  }  public static IRuleConfigParser createParser(String configFormat) {    if (configFormat == null || configFormat.isEmpty()) {      return null;//返回null还是IllegalArgumentException全凭你自己说了算    }    IRuleConfigParser parser = cachedParsers.get(configFormat.toLowerCase());    return parser;  }}</code></pre><h4 id="工厂方法模式"><a href="#工厂方法模式" class="headerlink" title="工厂方法模式"></a>工厂方法模式</h4><pre><code>public interface IRuleConfigParserFactory {  IRuleConfigParser createParser();}public class JsonRuleConfigParserFactory implements IRuleConfigParserFactory {  @Override  public IRuleConfigParser createParser() {    return new JsonRuleConfigParser();  }}public class XmlRuleConfigParserFactory implements IRuleConfigParserFactory {  @Override  public IRuleConfigParser createParser() {    return new XmlRuleConfigParser();  }}public class YamlRuleConfigParserFactory implements IRuleConfigParserFactory {  @Override  public IRuleConfigParser createParser() {    return new YamlRuleConfigParser();  }}public class PropertiesRuleConfigParserFactory implements IRuleConfigParserFactory {  @Override  public IRuleConfigParser createParser() {    return new PropertiesRuleConfigParser();  }}public class RuleConfigSource {  public RuleConfig load(String ruleConfigFilePath) {    String ruleConfigFileExtension = getFileExtension(ruleConfigFilePath);    IRuleConfigParserFactory parserFactory = RuleConfigParserFactoryMap.getParserFactory(ruleConfigFileExtension);    if (parserFactory == null) {      throw new InvalidRuleConfigException("Rule config file format is not supported: " + ruleConfigFilePath);    }    IRuleConfigParser parser = parserFactory.createParser();    String configText = "";    //从ruleConfigFilePath文件中读取配置文本到configText中    RuleConfig ruleConfig = parser.parse(configText);    return ruleConfig;  }  private String getFileExtension(String filePath) {    //...解析文件名获取扩展名，比如rule.json，返回json    return "json";  }}//因为工厂类只包含方法，不包含成员变量，完全可以复用，//不需要每次都创建新的工厂类对象，所以，简单工厂模式的第二种实现思路更加合适。public class RuleConfigParserFactoryMap { //工厂的工厂  private static final Map&lt;String, IRuleConfigParserFactory&gt; cachedFactories = new HashMap&lt;&gt;();那什么时候该用工厂方法模式，而非简单工厂模式呢？      static {    cachedFactories.put("json", new JsonRuleConfigParserFactory());    cachedFactories.put("xml", new XmlRuleConfigParserFactory());    cachedFactories.put("yaml", new YamlRuleConfigParserFactory());    cachedFactories.put("properties", new PropertiesRuleConfigParserFactory());  }  public static IRuleConfigParserFactory getParserFactory(String type) {    if (type == null || type.isEmpty()) {      return null;    }    IRuleConfigParserFactory parserFactory = cachedFactories.get(type.toLowerCase());    return parserFactory;  }}</code></pre><h4 id="那什么时候该用工厂方法模式，而非简单工厂模式呢？"><a href="#那什么时候该用工厂方法模式，而非简单工厂模式呢？" class="headerlink" title="那什么时候该用工厂方法模式，而非简单工厂模式呢？"></a>那什么时候该用工厂方法模式，而非简单工厂模式呢？</h4><p>我们前面提到，之所以将某个代码块剥离出来，独立为函数或者类，原因是这个代码块的逻辑过于复杂，剥离之后能让代码更加清晰，更加可读、可维护。但是，如果代码块本身并不复杂，就几行代码而已，我们完全没必要将它拆分成单独的函数或者类。</p><p><strong>基于这个设计思想，当对象的创建逻辑比较复杂，不只是简单的 new 一下就可以</strong>，而是要组合其他类对象，做各种初始化操作的时候，我们推荐使用工厂方法模式，将复杂的创建逻辑拆分到多个工厂类中，让每个工厂类都不至于过于复杂。而使用简单工厂模式，将所有的创建逻辑都放到一个工厂类中，会导致这个工厂类变得很复杂。</p><h4 id="抽象工厂（Abstract-Factory）"><a href="#抽象工厂（Abstract-Factory）" class="headerlink" title="抽象工厂（Abstract Factory）"></a>抽象工厂（Abstract Factory）</h4><p>在简单工厂和工厂方法中，类只有一种分类方式。比如，在规则配置解析那个例子中，解析器类只会根据配置文件格式（Json、Xml、Yaml……）来分类。但是，如果类有两种分类方式，比如，我们既可以按照配置文件格式来分类，也可以按照解析的对象（Rule 规则配置还是 System 系统配置）来分类，那就会对应下面这 8 个 parser 类。</p><pre><code>针对规则配置的解析器：基于接口IRuleConfigParserJsonRuleConfigParserXmlRuleConfigParserYamlRuleConfigParserPropertiesRuleConfigParser针对系统配置的解析器：基于接口ISystemConfigParserJsonSystemConfigParserXmlSystemConfigParserYamlSystemConfigParserPropertiesSystemConfigParser</code></pre><p>针对这种特殊的场景，如果还是继续用工厂方法来实现的话，我们要针对每个 parser 都编写一个工厂类，也就是要编写 8 个工厂类。如果我们未来还需要增加针对业务配置的解析器（比如 IBizConfigParser），那就要再对应地增加 4 个工厂类。而我们知道，过多的类也会让系统难维护。这个问题该怎么解决呢？</p><p>抽象工厂就是针对这种非常特殊的场景而诞生的。我们可以让一个工厂负责创建多个不同类型的对象（IRuleConfigParser、ISystemConfigParser 等），而不是只创建一种 parser 对象。这样就可以有效地减少工厂类的个数。具体的代码实现如下所示：</p><pre><code>public interface IConfigParserFactory {  IRuleConfigParser createRuleParser();  ISystemConfigParser createSystemParser();  //此处可以扩展新的parser类型，比如IBizConfigParser}public class JsonConfigParserFactory implements IConfigParserFactory {  @Override  public IRuleConfigParser createRuleParser() {    return new JsonRuleConfigParser();  }  @Override  public ISystemConfigParser createSystemParser() {    return new JsonSystemConfigParser();  }}public class XmlConfigParserFactory implements IConfigParserFactory {  @Override  public IRuleConfigParser createRuleParser() {    return new XmlRuleConfigParser();  }  @Override  public ISystemConfigParser createSystemParser() {    return new XmlSystemConfigParser();  }}// 省略YamlConfigParserFactory和PropertiesConfigParserFactory代码</code></pre><h4 id="工厂模式和-DI-容器有何区别？"><a href="#工厂模式和-DI-容器有何区别？" class="headerlink" title="工厂模式和 DI 容器有何区别？"></a>工厂模式和 DI 容器有何区别？</h4><p>实际上，DI 容器底层最基本的设计思路就是基于工厂模式的。DI 容器相当于一个大的工厂类，负责在程序启动的时候，根据配置（要创建哪些类对象，每个类对象的创建需要依赖哪些其他类对象）事先创建好对象。当应用程序需要使用某个类对象的时候，直接从容器中获取即可。正是因为它持有一堆对象，所以这个框架才被称为“容器”。</p><p>DI 容器相对于我们上节课讲的工厂模式的例子来说，它处理的是更大的对象创建工程。上节课讲的工厂模式中，一个工厂类只负责某个类对象或者某一组相关类对象（继承自同一抽象类或者接口的子类）的创建，而 DI 容器负责的是整个应用中所有类对象的创建。</p><p>除此之外，DI 容器负责的事情要比单纯的工厂模式要多。比如，它还包括配置的解析、对象生命周期的管理。接下来，我们就详细讲讲，一个简单的 DI 容器应该包含哪些核心功能。</p><h4 id="DI-容器的核心功能有哪些？"><a href="#DI-容器的核心功能有哪些？" class="headerlink" title="DI 容器的核心功能有哪些？"></a>DI 容器的核心功能有哪些？</h4><p>总结一下，一个简单的 DI 容器的核心功能一般有三个：配置解析、对象创建和对象生命周期管理。</p><h3 id="构建者模式（Builder-pattern）"><a href="#构建者模式（Builder-pattern）" class="headerlink" title="构建者模式（Builder pattern）"></a>构建者模式（Builder pattern）</h3><pre><code>public class ResourcePoolConfig {  private String name;  private int maxTotal;  private int maxIdle;  private int minIdle;  private ResourcePoolConfig(Builder builder) {    this.name = builder.name;    this.maxTotal = builder.maxTotal;    this.maxIdle = builder.maxIdle;    this.minIdle = builder.minIdle;  }  //...省略getter方法...  //我们将Builder类设计成了ResourcePoolConfig的内部类。  //我们也可以将Builder类设计成独立的非内部类ResourcePoolConfigBuilder。  public static class Builder {    private static final int DEFAULT_MAX_TOTAL = 8;    private static final int DEFAULT_MAX_IDLE = 8;    private static final int DEFAULT_MIN_IDLE = 0;    private String name;    private int maxTotal = DEFAULT_MAX_TOTAL;    private int maxIdle = DEFAULT_MAX_IDLE;    private int minIdle = DEFAULT_MIN_IDLE;    public ResourcePoolConfig build() {      // 校验逻辑放到这里来做，包括必填项校验、依赖关系校验、约束条件校验等      if (StringUtils.isBlank(name)) {        throw new IllegalArgumentException("...");      }      if (maxIdle &gt; maxTotal) {        throw new IllegalArgumentException("...");      }      if (minIdle &gt; maxTotal || minIdle &gt; maxIdle) {        throw new IllegalArgumentException("...");      }      return new ResourcePoolConfig(this);    }    public Builder setName(String name) {      if (StringUtils.isBlank(name)) {        throw new IllegalArgumentException("...");      }      this.name = name;      return this;    }    public Builder setMaxTotal(int maxTotal) {      if (maxTotal &lt;= 0) {        throw new IllegalArgumentException("...");      }      this.maxTotal = maxTotal;      return this;    }    public Builder setMaxIdle(int maxIdle) {      if (maxIdle &lt; 0) {        throw new IllegalArgumentException("...");      }      this.maxIdle = maxIdle;      return this;    }    public Builder setMinIdle(int minIdle) {      if (minIdle &lt; 0) {        throw new IllegalArgumentException("...");      }      this.minIdle = minIdle;      return this;    }  }}// 这段代码会抛出IllegalArgumentException，因为minIdle&gt;maxIdleResourcePoolConfig config = new ResourcePoolConfig.Builder()        .setName("dbconnectionpool")        .setMaxTotal(16)        .setMaxIdle(10)        .setMinIdle(12)        .build();                                </code></pre><h3 id="原型模式-Prototype-Pattern"><a href="#原型模式-Prototype-Pattern" class="headerlink" title="原型模式(Prototype Pattern)"></a>原型模式(Prototype Pattern)</h3><p><strong>什么是原型模式？</strong></p><p>如果对象的创建成本比较大，而同一个类的不同对象之间差别不大（大部分字段都相同），在这种情况下，我们可以利用对已有对象（原型）进行复制（或者叫拷贝）的方式，来创建新对象，以达到节省创建时间的目的。这种基于原型来创建对象的方式就叫作原型设计模式，简称原型模式。</p><p><strong>原型模式的两种实现方法</strong></p><p>原型模式有两种实现方法，深拷贝和浅拷贝。浅拷贝只会复制对象中基本数据类型数据和引用对象的内存地址，不会递归地复制引用对象，以及引用对象的引用对象……而深拷贝得到的是一份完完全全独立的对象。所以，深拷贝比起浅拷贝来说，更加耗时，更加耗内存空间。</p><p>如果要拷贝的对象是不可变对象，浅拷贝共享不可变对象是没问题的，但对于可变对象来说，浅拷贝得到的对象和原始对象会共享部分数据，就有可能出现数据被修改的风险，也就变得复杂多了。除非像我们今天实战中举的那个例子，需要从数据库中加载 10 万条数据并构建散列表索引，操作非常耗时，这种情况下比较推荐使用浅拷贝，否则，没有充分的理由，不要为了一点点的性能提升而使用浅拷贝。</p><h2 id="设计模式与范式：结构型"><a href="#设计模式与范式：结构型" class="headerlink" title="设计模式与范式：结构型"></a>设计模式与范式：结构型</h2><h3 id="代理模式（Proxy-Design-Pattern）"><a href="#代理模式（Proxy-Design-Pattern）" class="headerlink" title="代理模式（Proxy Design Pattern）"></a>代理模式（Proxy Design Pattern）</h3><p>代理模式（Proxy Design Pattern）的原理和代码实现都不难掌握。它在不改变原始类（或叫被代理类）代码的情况下，通过引入代理类来给原始类附加功能。我们通过一个简单的例子来解释一下这段话。</p><pre><code>public interface IUserController {  UserVo login(String telephone, String password);  UserVo register(String telephone, String password);}public class UserController implements IUserController {  //...省略其他属性和方法...  @Override  public UserVo login(String telephone, String password) {    //...省略login逻辑...    //...返回UserVo数据...  }  @Override  public UserVo register(String telephone, String password) {    //...省略register逻辑...    //...返回UserVo数据...  }}public class UserControllerProxy implements IUserController {  private MetricsCollector metricsCollector;  private UserController userController;  public UserControllerProxy(UserController userController) {    this.userController = userController;    this.metricsCollector = new MetricsCollector();  }  @Override  public UserVo login(String telephone, String password) {    long startTimestamp = System.currentTimeMillis();    // 委托    UserVo userVo = userController.login(telephone, password);    long endTimeStamp = System.currentTimeMillis();    long responseTime = endTimeStamp - startTimestamp;    RequestInfo requestInfo = new RequestInfo("login", responseTime, startTimestamp);    metricsCollector.recordRequest(requestInfo);    return userVo;  }  @Override  public UserVo register(String telephone, String password) {    long startTimestamp = System.currentTimeMillis();    UserVo userVo = userController.register(telephone, password);    long endTimeStamp = System.currentTimeMillis();    long responseTime = endTimeStamp - startTimestamp;    RequestInfo requestInfo = new RequestInfo("register", responseTime, startTimestamp);    metricsCollector.recordRequest(requestInfo);    return userVo;  }}//UserControllerProxy使用举例//因为原始类和代理类实现相同的接口，是基于接口而非实现编程//将UserController类对象替换为UserControllerProxy类对象，不需要改动太多代码IUserController userController = new UserControllerProxy(new UserController());</code></pre><h4 id="动态代理（Dynamic-Proxy）"><a href="#动态代理（Dynamic-Proxy）" class="headerlink" title="动态代理（Dynamic Proxy）"></a>动态代理（Dynamic Proxy）</h4><p>实际上，Spring AOP 底层的实现原理就是基于动态代理。用户配置好需要给哪些类创建代理，并定义好在执行原始类的业务代码前后执行哪些附加功能。Spring 为这些类创建动态代理对象，并在 JVM 中替代原始类对象。原本在代码中执行的原始类的方法，被换作执行代理类的方法，也就实现了给原始类添加附加功能的目的。</p><h4 id="代理模式的应用场景"><a href="#代理模式的应用场景" class="headerlink" title="代理模式的应用场景"></a>代理模式的应用场景</h4><ol><li>业务系统的非功能性需求开发</li></ol><p>代理模式最常用的一个应用场景就是，在业务系统中开发一些非功能性需求，比如：监控、统计、鉴权、限流、事务、幂等、日志。我们将这些附加功能与业务功能解耦，放到代理类中统一处理，让程序员只需要关注业务方面的开发。实际上，前面举的搜集接口请求信息的例子，就是这个应用场景的一个典型例子。</p><p>如果你熟悉 Java 语言和 Spring 开发框架，这部分工作都是可以在 Spring AOP 切面中完成的。前面我们也提到，Spring AOP 底层的实现原理就是基于动态代理。</p><ol start="2"><li>代理模式在 RPC、缓存中的应用</li></ol><p>实际上，RPC 框架也可以看作一种代理模式，GoF 的《设计模式》一书中把它称作远程代理。通过远程代理，将网络通信、数据编解码等细节隐藏起来。客户端在使用 RPC 服务的时候，就像使用本地函数一样，无需了解跟服务器交互的细节。除此之外，RPC 服务的开发者也只需要开发业务逻辑，就像开发本地使用的函数一样，不需要关注跟客户端的交互细节。</p><h3 id="桥接模式（Bridge-Design-Pattern）"><a href="#桥接模式（Bridge-Design-Pattern）" class="headerlink" title="桥接模式（Bridge Design Pattern）"></a>桥接模式（Bridge Design Pattern）</h3><p>今天，我们再学习另外一种结构型模式：桥接模式。桥接模式的代码实现非常简单，但是理解起来稍微有点难度，并且应用场景也比较局限，所以，相当于代理模式来说，桥接模式在实际的项目中并没有那么常用，你只需要简单了解，见到能认识就可以，并不是我们学习的重点。</p><p>桥接模式，也叫作桥梁模式，英文是 Bridge Design Pattern。这个模式可以说是 23 种设计模式中最难理解的模式之一了。我查阅了比较多的书籍和资料之后发现，对于这个模式有两种不同的理解方式。</p><p>当然，这其中“最纯正”的理解方式，当属 GoF 的《设计模式》一书中对桥接模式的定义。毕竟，这 23 种经典的设计模式，最初就是由这本书总结出来的。在 GoF 的《设计模式》一书中，桥接模式是这么定义的：“Decouple an abstraction from its implementation so that the two can vary independently。”翻译成中文就是：“将抽象和实现解耦，让它们可以独立变化。”</p><p>关于桥接模式，很多书籍、资料中，还有另外一种理解方式：“一个类存在两个（或多个）独立变化的维度，我们通过组合的方式，让这两个（或多个）维度可以独立进行扩展。”通过组合关系来替代继承关系，避免继承层次的指数级爆炸。这种理解方式非常类似于，我们之前讲过的“组合优于继承”设计原则，所以，这里我就不多解释了。我们重点看下 GoF 的理解方式。</p><pre><code>Class.forName("com.mysql.jdbc.Driver");//加载及注册JDBC驱动程序String url = "jdbc:mysql://localhost:3306/sample_db?user=root&amp;password=your_password";Connection con = DriverManager.getConnection(url);Statement stmt = con.createStatement()；String query = "select * from test";ResultSet rs=stmt.executeQuery(query);while(rs.next()) {  rs.getString(1);  rs.getInt(2);}</code></pre><p>如果我们想要把 MySQL 数据库换成 Oracle 数据库，只需要把第一行代码中的 com.mysql.jdbc.Driver 换成 oracle.jdbc.driver.OracleDriver 就可以了。当然，也有更灵活的实现方式，我们可以把需要加载的 Driver 类写到配置文件中，当程序启动的时候，自动从配置文件中加载，这样在切换数据库的时候，我们都不需要修改代码，只需要修改配置文件就可以了。</p><h3 id="装饰器模式-（Decorator-Pattern）"><a href="#装饰器模式-（Decorator-Pattern）" class="headerlink" title="装饰器模式 （Decorator Pattern）"></a>装饰器模式 （Decorator Pattern）</h3><p>装饰器模式主要解决继承关系过于复杂的问题，通过组合来替代继承。它主要的作用是给原始类添加增强功能。这也是判断是否该用装饰器模式的一个重要的依据。除此之外，装饰器模式还有一个特点，那就是可以对原始类嵌套使用多个装饰器。为了满足这个应用场景，在设计的时候，装饰器类需要跟原始类继承相同的抽象类或者接口。</p><p><strong>第一个比较特殊的地方是：装饰器类和原始类继承同样的父类，这样我们可以对原始类“嵌套”多个装饰器类</strong>。比如，下面这样一段代码，我们对 FileInputStream 嵌套了两个装饰器类：BufferedInputStream 和 DataInputStream，让它既支持缓存读取，又支持按照基本数据类型来读取数据。</p><pre><code>InputStream in = new FileInputStream("/user/wangzheng/test.txt");InputStream bin = new BufferedInputStream(in);DataInputStream din = new DataInputStream(bin);int data = din.readInt();</code></pre><p><strong>第二个比较特殊的地方是：装饰器类是对功能的增强，这也是装饰器模式应用场景的一个重要特点</strong>。实际上，符合“组合关系”这种代码结构的设计模式有很多，比如之前讲过的代理模式、桥接模式，还有现在的装饰器模式。尽管它们的代码结构很相似，但是每种设计模式的意图是不同的。就拿比较相似的代理模式和装饰器模式来说吧，代理模式中，代理类附加的是跟原始类无关的功能，而在装饰器模式中，装饰器类附加的是跟原始类相关的增强功能。</p><pre><code>// 代理模式的代码结构(下面的接口也可以替换成抽象类)public interface IA {  void f();}public class A impelements IA {  public void f() { //... }}public class AProxy implements IA {  private IA a;  public AProxy(IA a) {    this.a = a;  }    public void f() {    // 新添加的代理逻辑    a.f();    // 新添加的代理逻辑  }}// 装饰器模式的代码结构(下面的接口也可以替换成抽象类)public interface IA {  void f();}public class A implements IA {  public void f() { //... }}public class ADecorator implements IA {  private IA a;  public ADecorator(IA a) {    this.a = a;  }    public void f() {    // 功能增强代码    a.f();    // 功能增强代码  }}</code></pre><p><strong>Python的装饰器模式</strong></p><pre><code>import functoolsdef log_with_param(text):    def decorator(func):        @functools.wraps(func)        def wrapper(*args, **kwargs):            print('call %s():' % func.__name__)            print('args = {}'.format(*args))            print('log_param = {}'.format(text))            return func(*args, **kwargs)        return wrapper    return decorator    @log_with_param("param")def test_with_param(p):    print(test_with_param.__name__)                </code></pre><h3 id="适配器模式-（Adapter-Design-Pattern）"><a href="#适配器模式-（Adapter-Design-Pattern）" class="headerlink" title="适配器模式 （Adapter Design Pattern）"></a>适配器模式 （Adapter Design Pattern）</h3><p>适配器模式的英文翻译是 Adapter Design Pattern。顾名思义，这个模式就是用来做适配的，它将不兼容的接口转换为可兼容的接口，让原本由于接口不兼容而不能一起工作的类可以一起工作。对于这个模式，有一个经常被拿来解释它的例子，就是 USB 转接头充当适配器，把两种不兼容的接口，通过转接变得可以一起工作。</p><p>原理很简单，我们再来看下它的代码实现。适配器模式有两种实现方式：类适配器和对象适配器。其中，类适配器使用继承关系来实现，对象适配器使用组合关系来实现。具体的代码实现如下所示。其中，ITarget 表示要转化成的接口定义。Adaptee 是一组不兼容 ITarget 接口定义的接口，Adaptor 将 Adaptee 转化成一组符合 ITarget 接口定义的接口。</p><pre><code>// 类适配器: 基于继承public interface ITarget {  void f1();  void f2();  void fc();}public class Adaptee {  public void fa() { //... }  public void fb() { //... }  public void fc() { //... }}public class Adaptor extends Adaptee implements ITarget {  public void f1() {    super.fa();  }    public void f2() {    //...重新实现f2()...  }    // 这里fc()不需要实现，直接继承自Adaptee，这是跟对象适配器最大的不同点}// 对象适配器：基于组合public interface ITarget {  void f1();  void f2();  void fc();}public class Adaptee {  public void fa() { //... }  public void fb() { //... }  public void fc() { //... }}public class Adaptor implements ITarget {  private Adaptee adaptee;    public Adaptor(Adaptee adaptee) {    this.adaptee = adaptee;  }    public void f1() {    adaptee.fa(); //委托给Adaptee  }    public void f2() {    //...重新实现f2()...  }    public void fc() {    adaptee.fc();  }}</code></pre><p>针对这两种实现方式，在实际的开发中，到底该如何选择使用哪一种呢？判断的标准主要有两个，一个是 Adaptee 接口的个数，另一个是 Adaptee 和 ITarget 的契合程度。</p><ul><li>如果 Adaptee 接口并不多，那两种实现方式都可以。</li><li>如果 Adaptee 接口很多，而且 Adaptee 和 ITarget 接口定义大部分都相同，那我们推荐使用类适配器，因为 Adaptor 复用父类 Adaptee 的接口，比起对象适配器的实现方式，Adaptor 的代码量要少一些。</li><li>如果 Adaptee 接口很多，而且 Adaptee 和 ITarget 接口定义大部分都不相同，那我们推荐使用对象适配器，因为组合结构相对于继承更加灵活。</li></ul><h4 id="适配器模式应用场景总结"><a href="#适配器模式应用场景总结" class="headerlink" title="适配器模式应用场景总结"></a>适配器模式应用场景总结</h4><p>前面我们反复提到，适配器模式的应用场景是“接口不兼容”。那在实际的开发中，什么情况下才会出现接口不兼容呢？我建议你先自己思考一下这个问题，然后再来看我下面的总结 。</p><ol><li>封装有缺陷的接口设计</li></ol><p>假设我们依赖的外部系统在接口设计方面有缺陷（比如包含大量静态方法），引入之后会影响到我们自身代码的可测试性。为了隔离设计上的缺陷，我们希望对外部系统提供的接口进行二次封装，抽象出更好的接口设计，这个时候就可以使用适配器模式了。</p><pre><code>public class CD { //这个类来自外部sdk，我们无权修改它的代码  //...  public static void staticFunction1() { //... }    public void uglyNamingFunction2() { //... }  public void tooManyParamsFunction3(int paramA, int paramB, ...) { //... }     public void lowPerformanceFunction4() { //... }}// 使用适配器模式进行重构public class ITarget {  void function1();  void function2();  void fucntion3(ParamsWrapperDefinition paramsWrapper);  void function4();  //...}// 注意：适配器类的命名不一定非得末尾带Adaptorpublic class CDAdaptor extends CD implements ITarget {  //...  public void function1() {     super.staticFunction1();  }    public void function2() {    super.uglyNamingFucntion2();  }    public void function3(ParamsWrapperDefinition paramsWrapper) {     super.tooManyParamsFunction3(paramsWrapper.getParamA(), ...);  }    public void function4() {    //...reimplement it...  }}</code></pre><ol start="2"><li>统一多个类的接口设计</li></ol><p>某个功能的实现依赖多个外部系统（或者说类）。通过适配器模式，将它们的接口适配为统一的接口定义，然后我们就可以使用多态的特性来复用代码逻辑。具体我还是举个例子来解释一下。</p><p>假设我们的系统要对用户输入的文本内容做敏感词过滤，为了提高过滤的召回率，我们引入了多款第三方敏感词过滤系统，依次对用户输入的内容进行过滤，过滤掉尽可能多的敏感词。但是，每个系统提供的过滤接口都是不同的。这就意味着我们没法复用一套逻辑来调用各个系统。这个时候，我们就可以使用适配器模式，将所有系统的接口适配为统一的接口定义，这样我们可以复用调用敏感词过滤的代码。</p><pre><code>public class ASensitiveWordsFilter { // A敏感词过滤系统提供的接口  //text是原始文本，函数输出用***替换敏感词之后的文本  public String filterSexyWords(String text) {    // ...  }    public String filterPoliticalWords(String text) {    // ...  } }public class BSensitiveWordsFilter  { // B敏感词过滤系统提供的接口  public String filter(String text) {    //...  }}public class CSensitiveWordsFilter { // C敏感词过滤系统提供的接口  public String filter(String text, String mask) {    //...  }}// 未使用适配器模式之前的代码：代码的可测试性、扩展性不好public class RiskManagement {  private ASensitiveWordsFilter aFilter = new ASensitiveWordsFilter();  private BSensitiveWordsFilter bFilter = new BSensitiveWordsFilter();  private CSensitiveWordsFilter cFilter = new CSensitiveWordsFilter();    public String filterSensitiveWords(String text) {    String maskedText = aFilter.filterSexyWords(text);    maskedText = aFilter.filterPoliticalWords(maskedText);    maskedText = bFilter.filter(maskedText);    maskedText = cFilter.filter(maskedText, "***");    return maskedText;  }}// 使用适配器模式进行改造public interface ISensitiveWordsFilter { // 统一接口定义  String filter(String text);}public class ASensitiveWordsFilterAdaptor implements ISensitiveWordsFilter {  private ASensitiveWordsFilter aFilter;  public String filter(String text) {    String maskedText = aFilter.filterSexyWords(text);    maskedText = aFilter.filterPoliticalWords(maskedText);    return maskedText;  }}//...省略BSensitiveWordsFilterAdaptor、CSensitiveWordsFilterAdaptor...// 扩展性更好，更加符合开闭原则，如果添加一个新的敏感词过滤系统，// 这个类完全不需要改动；而且基于接口而非实现编程，代码的可测试性更好。public class RiskManagement {   private List&lt;ISensitiveWordsFilter&gt; filters = new ArrayList&lt;&gt;();   public void addSensitiveWordsFilter(ISensitiveWordsFilter filter) {    filters.add(filter);  }    public String filterSensitiveWords(String text) {    String maskedText = text;    for (ISensitiveWordsFilter filter : filters) {      maskedText = filter.filter(maskedText);    }    return maskedText;  }}</code></pre><ol start="3"><li>替换依赖的外部系统</li></ol><p>当我们把项目中依赖的一个外部系统替换为另一个外部系统的时候，利用适配器模式，可以减少对代码的改动。具体的代码示例如下所示：</p><pre><code>  // 外部系统A  public interface IA {    //...    void fa();  }  public class A implements IA {    //...    public void fa() { //... }  }  // 在我们的项目中，外部系统A的使用示例  public class Demo {    private IA a;    public Demo(IA a) {      this.a = a;    }    //...  }  Demo d = new Demo(new A());  // 将外部系统A替换成外部系统B  public class BAdaptor implemnts IA {    private B b;    public BAdaptor(B b) {      this.b= b;    }    public void fa() {      //...      b.fb();    }  }  // 借助BAdaptor，Demo的代码中，调用IA接口的地方都无需改动，  // 只需要将BAdaptor如下注入到Demo即可。  Demo d = new Demo(new BAdaptor(new B()));</code></pre><ol start="4"><li>兼容老版本接口</li></ol><p>在做版本升级的时候，对于一些要废弃的接口，我们不直接将其删除，而是暂时保留，并且标注为 deprecated，并将内部实现逻辑委托为新的接口实现。这样做的好处是，让使用它的项目有个过渡期，而不是强制进行代码修改。这也可以粗略地看作适配器模式的一个应用场景。同样，我还是通过一个例子，来进一步解释一下。</p><p>JDK1.0 中包含一个遍历集合容器的类 Enumeration。JDK2.0 对这个类进行了重构，将它改名为 Iterator 类，并且对它的代码实现做了优化。但是考虑到如果将 Enumeration 直接从 JDK2.0 中删除，那使用 JDK1.0 的项目如果切换到 JDK2.0，代码就会编译不通过。为了避免这种情况的发生，我们必须把项目中所有使用到 Enumeration 的地方，都修改为使用 Iterator 才行。</p><p>单独一个项目做 Enumeration 到 Iterator 的替换，勉强还能接受。但是，使用 Java 开发的项目太多了，一次 JDK 的升级，导致所有的项目不做代码修改就会编译报错，这显然是不合理的。这就是我们经常所说的不兼容升级。为了做到兼容使用低版本 JDK 的老代码，我们可以暂时保留 Enumeration 类，并将其实现替换为直接调用 Itertor。代码示例如下所示：</p><pre><code>  public class Collections {    public static Emueration emumeration(final Collection c) {      return new Enumeration() {        Iterator i = c.iterator();                public boolean hasMoreElments() {          return i.hashNext();        }                public Object nextElement() {          return i.next():        }      }    }  }</code></pre><ol start="5"><li>适配不同格式的数据</li></ol><p>前面我们讲到，适配器模式主要用于接口的适配，实际上，它还可以用在不同格式的数据之间的适配。比如，把从不同征信系统拉取的不同格式的征信数据，统一为相同的格式，以方便存储和使用。再比如，Java 中的 Arrays.asList() 也可以看作一种数据适配器，将数组类型的数据转化为集合容器类型。</p><p>   List<string> stooges = Arrays.asList(“Larry”, “Moe”, “Curly”);</string></p><h3 id="代理、桥接、装饰器、适配器-4-种设计模式的区别"><a href="#代理、桥接、装饰器、适配器-4-种设计模式的区别" class="headerlink" title="代理、桥接、装饰器、适配器 4 种设计模式的区别"></a>代理、桥接、装饰器、适配器 4 种设计模式的区别</h3><p>代理、桥接、装饰器、适配器，这 4 种模式是比较常用的结构型设计模式。它们的代码结构非常相似。笼统来说，它们都可以称为 Wrapper 模式，也就是通过 Wrapper 类二次封装原始类。</p><p>尽管代码结构相似，但这 4 种设计模式的用意完全不同，也就是说要解决的问题、应用场景不同，这也是它们的主要区别。这里我就简单说一下它们之间的区别。</p><p><strong>代理模式</strong>：代理模式在不改变原始类接口的条件下，为原始类定义一个代理类，主要目的是控制访问，而非加强功能，这是它跟装饰器模式最大的不同。</p><p><strong>桥接模式</strong>：桥接模式的目的是将接口部分和实现部分分离，从而让它们可以较为容易、也相对独立地加以改变。</p><p><strong>装饰器模式</strong>：装饰者模式在不改变原始类接口的情况下，对原始类功能进行增强，并且支持多个装饰器的嵌套使用。</p><p><strong>适配器模式</strong>：适配器模式是一种事后的补救策略。适配器提供跟原始类不同的接口，而代理模式、装饰器模式提供的都是跟原始类相同的接口。</p><h3 id="门面模式（Facade-Design-Pattern）"><a href="#门面模式（Facade-Design-Pattern）" class="headerlink" title="门面模式（Facade Design Pattern）"></a>门面模式（Facade Design Pattern）</h3><p>门面模式，也叫外观模式，英文全称是 Facade Design Pattern。在 GoF 的《设计模式》一书中，门面模式是这样定义的：</p><blockquote><p>Provide a unified interface to a set of interfaces in a subsystem. Facade Pattern defines a higher-level interface that makes the subsystem easier to use.</p></blockquote><p>翻译成中文就是：门面模式为子系统提供一组统一的接口，定义一组高层接口让子系统更易用。</p><p>我们知道，类、模块、系统之间的“通信”，一般都是通过接口调用来完成的。接口设计的好坏，直接影响到类、模块、系统是否好用。所以，我们要多花点心思在接口设计上。我经常说，完成接口设计，就相当于完成了一半的开发任务。只要接口设计得好，那代码就差不到哪里去。</p><p>接口粒度设计得太大，太小都不好。太大会导致接口不可复用，太小会导致接口不易用。在实际的开发中，接口的可复用性和易用性需要“微妙”的权衡。针对这个问题，我的一个基本的处理原则是，尽量保持接口的可复用性，但针对特殊情况，允许提供冗余的门面接口，来提供更易用的接口。</p><h3 id="门面模式的应用场景举例"><a href="#门面模式的应用场景举例" class="headerlink" title="门面模式的应用场景举例"></a>门面模式的应用场景举例</h3><h4 id="解决易用性问题"><a href="#解决易用性问题" class="headerlink" title="解决易用性问题"></a>解决易用性问题</h4><p>门面模式可以用来封装系统的底层实现，隐藏系统的复杂性，提供一组更加简单易用、更高层的接口。比如，Linux 系统调用函数就可以看作一种“门面”。它是 Linux 操作系统暴露给开发者的一组“特殊”的编程接口，它封装了底层更基础的 Linux 内核调用。再比如，Linux 的 Shell 命令，实际上也可以看作一种门面模式的应用。它继续封装系统调用，提供更加友好、简单的命令，让我们可以直接通过执行命令来跟操作系统交互。</p><p>我们前面也多次讲过，设计原则、思想、模式很多都是相通的，是同一个道理不同角度的表述。实际上，从隐藏实现复杂性，提供更易用接口这个意图来看，门面模式有点类似之前讲到的迪米特法则（最少知识原则）和接口隔离原则：两个有交互的系统，只暴露有限的必要的接口。除此之外，门面模式还有点类似之前提到封装、抽象的设计思想，提供更抽象的接口，封装底层实现细节。</p><h4 id="解决性能问题"><a href="#解决性能问题" class="headerlink" title="解决性能问题"></a>解决性能问题</h4><p>关于利用门面模式解决性能问题这一点，刚刚我们已经讲过了。我们通过将多个接口调用替换为一个门面接口调用，减少网络通信成本，提高 App 客户端的响应速度。所以，关于这点，我就不再举例说明了。我们来讨论一下这样一个问题：从代码实现的角度来看，该如何组织门面接口和非门面接口？</p><p>如果门面接口不多，我们完全可以将它跟非门面接口放到一块，也不需要特殊标记，当作普通接口来用即可。如果门面接口很多，我们可以在已有的接口之上，再重新抽象出一层，专门放置门面接口，从类、包的命名上跟原来的接口层做区分。如果门面接口特别多，并且很多都是跨多个子系统的，我们可以将门面接口放到一个新的子系统中。</p><h4 id="解决分布式事务问题"><a href="#解决分布式事务问题" class="headerlink" title="解决分布式事务问题"></a>解决分布式事务问题</h4><p>关于利用门面模式来解决分布式事务问题，我们通过一个例子来解释一下。</p><p>在一个金融系统中，有两个业务领域模型，用户和钱包。这两个业务领域模型都对外暴露了一系列接口，比如用户的增删改查接口、钱包的增删改查接口。假设有这样一个业务场景：在用户注册的时候，我们不仅会创建用户（在数据库 User 表中），还会给用户创建一个钱包（在数据库的 Wallet 表中）。</p><p>对于这样一个简单的业务需求，我们可以通过依次调用用户的创建接口和钱包的创建接口来完成。但是，用户注册需要支持事务，也就是说，创建用户和钱包的两个操作，要么都成功，要么都失败，不能一个成功、一个失败。</p><p>要支持两个接口调用在一个事务中执行，是比较难实现的，这涉及分布式事务问题。虽然我们可以通过引入分布式事务框架或者事后补偿的机制来解决，但代码实现都比较复杂。而最简单的解决方案是，利用数据库事务或者 Spring 框架提供的事务（如果是 Java 语言的话），在一个事务中，执行创建用户和创建钱包这两个 SQL 操作。这就要求两个 SQL 操作要在一个接口中完成，所以，我们可以借鉴门面模式的思想，再设计一个包裹这两个操作的新接口，让新接口在一个事务中执行两个 SQL 操作。</p><h3 id="组合模式（Composite-Design-Pattern）"><a href="#组合模式（Composite-Design-Pattern）" class="headerlink" title="组合模式（Composite Design Pattern）"></a>组合模式（Composite Design Pattern）</h3><p>组合模式跟我们之前讲的面向对象设计中的“组合关系（通过组合来组装两个类）”，完全是两码事。这里讲的“组合模式”，主要是用来处理树形结构数据。这里的“数据”，你可以简单理解为一组对象集合，待会我们会详细讲解。</p><p>正因为其应用场景的特殊性，数据必须能表示成树形结构，这也导致了这种模式在实际的项目开发中并不那么常用。但是，一旦数据满足树形结构，应用这种模式就能发挥很大的作用，能让代码变得非常简洁。</p><p>在 GoF 的《设计模式》一书中，组合模式是这样定义的：</p><blockquote><p>Compose objects into tree structure to represent part-whole hierarchies.Composite lets client treat individual objects and compositions of objects uniformly.</p></blockquote><p>翻译成中文就是：将一组对象组织（Compose）成树形结构，以表示一种“部分 - 整体”的层次结构。组合让客户端（在很多设计模式书籍中，“客户端”代指代码的使用者。）可以统一单个对象和组合对象的处理逻辑。</p><p>组合模式的设计思路，与其说是一种设计模式，<strong>倒不如说是对业务场景的一种数据结构和算法的抽象</strong>。其中，数据可以表示成树这种数据结构，业务需求可以通过在树上的递归遍历算法来实现。</p><p>说白了就是 一个树有很多节点，这个节点可以是多个类型的组合（比如文件系统，节点可以是目录，也可以是文件）。</p><h3 id="享元模式（Flyweight-Design-Pattern）"><a href="#享元模式（Flyweight-Design-Pattern）" class="headerlink" title="享元模式（Flyweight Design Pattern）"></a>享元模式（Flyweight Design Pattern）</h3><p>所谓“享元”，顾名思义就是被共享的单元。享元模式的意图是复用对象，节省内存，前提是享元对象是不可变对象。</p><p>具体来讲，当一个系统中存在大量重复对象的时候，如果这些重复的对象是不可变对象，我们就可以利用享元模式将对象设计成享元，在内存中只保留一份实例，供多处代码引用。这样可以减少内存中对象的数量，起到节省内存的目的。实际上，不仅仅相同对象可以设计成享元，对于相似对象，我们也可以将这些对象中相同的部分（字段）提取出来，设计成享元，让这些大量相似对象引用这些享元。</p><p>这里我稍微解释一下，定义中的“不可变对象”指的是，一旦通过构造函数初始化完成之后，它的状态（对象的成员变量或者属性）就不会再被修改了。所以，不可变对象不能暴露任何 set() 等修改内部状态的方法。之所以要求享元是不可变对象，那是因为它会被多处代码共享使用，避免一处代码对享元进行了修改，影响到其他使用它的代码。</p><p>在上面的代码实现中，我们利用工厂类来缓存 ChessPieceUnit 信息（也就是 id、text、color）。通过工厂类获取到的 ChessPieceUnit 就是享元。所有的 ChessBoard 对象共享这 30 个 ChessPieceUnit 对象（因为象棋中只有 30 个棋子）。在使用享元模式之前，记录 1 万个棋局，我们要创建 30 万（30*1 万）个棋子的 ChessPieceUnit 对象。利用享元模式，我们只需要创建 30 个享元对象供所有棋局共享使用即可，大大节省了内存。</p><p>那享元模式的原理讲完了，我们来总结一下它的代码结构。实际上，它的代码实现非常简单，主要是通过工厂模式，在工厂类中，通过一个 Map 来缓存已经创建过的享元对象，来达到复用的目的。</p><h3 id="享元模式-vs-单例、缓存、对象池"><a href="#享元模式-vs-单例、缓存、对象池" class="headerlink" title="享元模式 vs 单例、缓存、对象池"></a>享元模式 vs 单例、缓存、对象池</h3><h4 id="我们先来看享元模式跟单例的区别"><a href="#我们先来看享元模式跟单例的区别" class="headerlink" title="我们先来看享元模式跟单例的区别"></a>我们先来看享元模式跟单例的区别</h4><p>在单例模式中，一个类只能创建一个对象，而在享元模式中，一个类可以创建多个对象，每个对象被多处代码引用共享。实际上，享元模式有点类似于之前讲到的单例的变体：多例。</p><p>我们前面也多次提到，区别两种设计模式，不能光看代码实现，而是要看设计意图，也就是要解决的问题。尽管从代码实现上来看，享元模式和多例有很多相似之处，但从设计意图上来看，它们是完全不同的。应用享元模式是为了对象复用，节省内存，而应用多例模式是为了限制对象的个数。</p><h4 id="我们再来看享元模式跟缓存的区别"><a href="#我们再来看享元模式跟缓存的区别" class="headerlink" title="我们再来看享元模式跟缓存的区别"></a>我们再来看享元模式跟缓存的区别</h4><p>在享元模式的实现中，我们通过工厂类来“缓存”已经创建好的对象。这里的“缓存”实际上是“存储”的意思，跟我们平时所说的“数据库缓存”“CPU 缓存”“MemCache 缓存”是两回事。我们平时所讲的缓存，主要是为了提高访问效率，而非复用。</p><h4 id="最后我们来看享元模式跟对象池的区别"><a href="#最后我们来看享元模式跟对象池的区别" class="headerlink" title="最后我们来看享元模式跟对象池的区别"></a>最后我们来看享元模式跟对象池的区别</h4><p>对象池、连接池（比如数据库连接池）、线程池等也是为了复用，那它们跟享元模式有什么区别呢？</p><p>你可能对连接池、线程池比较熟悉，对对象池比较陌生，所以，这里我简单解释一下对象池。像 C++ 这样的编程语言，内存的管理是由程序员负责的。为了避免频繁地进行对象创建和释放导致内存碎片，我们可以预先申请一片连续的内存空间，也就是这里说的对象池。每次创建对象时，我们从对象池中直接取出一个空闲对象来使用，对象使用完成之后，再放回到对象池中以供后续复用，而非直接释放掉。</p><p>虽然对象池、连接池、线程池、享元模式都是为了复用，但是，如果我们再细致地抠一抠“复用”这个字眼的话，对象池、连接池、线程池等池化技术中的“复用”和享元模式中的“复用”实际上是不同的概念。</p><p>池化技术中的“复用”可以理解为“重复使用”，主要目的是节省时间（比如从数据库池中取一个连接，不需要重新创建）。在任意时刻，每一个对象、连接、线程，并不会被多处使用，而是被一个使用者独占，当使用完成之后，放回到池中，再由其他使用者重复利用。享元模式中的“复用”可以理解为“共享使用”，在整个生命周期中，都是被所有使用者共享的，主要目的是节省空间。</p><p>实际上，享元模式对 JVM 的垃圾回收并不友好。因为享元工厂类一直保存了对享元对象的引用，这就导致享元对象在没有任何代码使用的情况下，也并不会被 JVM 垃圾回收机制自动回收掉。因此，在某些情况下，如果对象的生命周期很短，也不会被密集使用，利用享元模式反倒可能会浪费更多的内存。所以，除非经过线上验证，利用享元模式真的可以大大节省内存，否则，就不要过度使用这个模式，为了一点点内存的节省而引入一个复杂的设计模式，得不偿失啊。</p><h2 id="设计模式与范式：行为型"><a href="#设计模式与范式：行为型" class="headerlink" title="设计模式与范式：行为型"></a>设计模式与范式：行为型</h2><p>我们常把 23 种经典的设计模式分为三类：创建型、结构型、行为型。前面我们已经学习了创建型和结构型，从今天起，我们开始学习行为型设计模式。我们知道，创建型设计模式主要解决“对象的创建”问题，结构型设计模式主要解决“类或对象的组合或组装”问题，那行为型设计模式主要解决的就是“类或对象之间的交互”问题。</p><p>前面我们已经学习了很多设计模式，不知道你有没有发现，实际上，设计模式要干的事情就是解耦。创建型模式是将创建和使用代码解耦，结构型模式是将不同功能代码解耦，行为型模式是将不同的行为代码解耦，具体到观察者模式，它是将观察者和被观察者代码解耦。借助设计模式，我们利用更好的代码结构，将一大坨代码拆分成职责更单一的小类，让其满足开闭原则、高内聚松耦合等特性，以此来控制和应对代码的复杂性，提高代码的可扩展性。</p><h3 id="观察者模式（Observer-Design-Pattern）"><a href="#观察者模式（Observer-Design-Pattern）" class="headerlink" title="观察者模式（Observer Design Pattern）"></a>观察者模式（Observer Design Pattern）</h3><p>观察者模式（Observer Design Pattern）也被称为发布订阅模式（Publish-Subscribe Design Pattern）。在 GoF 的《设计模式》一书中，它的定义是这样的：</p><blockquote><p>Define a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically.</p></blockquote><p>翻译成中文就是：在对象之间定义一个一对多的依赖，当一个对象状态改变的时候，所有依赖的对象都会自动收到通知。</p><p>一般情况下，被依赖的对象叫作被观察者（Observable），依赖的对象叫作观察者（Observer）。不过，在实际的项目开发中，这两种对象的称呼是比较灵活的，有各种不同的叫法，比如：Subject-Observer、Publisher-Subscriber、Producer-Consumer、EventEmitter-EventListener、Dispatcher-Listener。不管怎么称呼，只要应用场景符合刚刚给出的定义，都可以看作观察者模式。</p><pre><code>public interface Subject {  void registerObserver(Observer observer);  void removeObserver(Observer observer);  void notifyObservers(Message message);}public interface Observer {  void update(Message message);}public class ConcreteSubject implements Subject {  private List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;();  @Override  public void registerObserver(Observer observer) {    observers.add(observer);  }  @Override  public void removeObserver(Observer observer) {    observers.remove(observer);  }  @Override  public void notifyObservers(Message message) {    for (Observer observer : observers) {      observer.update(message);    }  }}public class ConcreteObserverOne implements Observer {  @Override  public void update(Message message) {    //TODO: 获取消息通知，执行自己的逻辑...    System.out.println("ConcreteObserverOne is notified.");  }}public class ConcreteObserverTwo implements Observer {  @Override  public void update(Message message) {    //TODO: 获取消息通知，执行自己的逻辑...    System.out.println("ConcreteObserverTwo is notified.");  }}public class Demo {  public static void main(String[] args) {    ConcreteSubject subject = new ConcreteSubject();    subject.registerObserver(new ConcreteObserverOne());    subject.registerObserver(new ConcreteObserverTwo());    subject.notifyObservers(new Message());  }}</code></pre><h4 id="观察者模式的应用场景"><a href="#观察者模式的应用场景" class="headerlink" title="观察者模式的应用场景"></a>观察者模式的应用场景</h4><p>观察者模式的应用场景非常广泛，小到代码层面的解耦，大到架构层面的系统解耦，再或者一些产品的设计思路，都有这种模式的影子，比如，邮件订阅、RSS Feeds，本质上都是观察者模式。不同的应用场景和需求下，这个模式也有截然不同的实现方式，有同步阻塞的实现方式，也有异步非阻塞的实现方式；有进程内的实现方式，也有跨进程的实现方式。</p><h3 id="模板模式"><a href="#模板模式" class="headerlink" title="模板模式"></a>模板模式</h3><p>模板代码，流程固定，动态的内容，用抽象方式给业务方自己去实现。</p><p>用处：复用、控制翻转IOC</p><p>模板模式，全称是模板方法设计模式，英文是 Template Method Design Pattern。在 GoF 的《设计模式》一书中，它是这么定义的：</p><blockquote><p>Define the skeleton of an algorithm in an operation, deferring some steps to subclasses. Template Method lets subclasses redefine certain steps of an algorithm without changing the algorithm’s structure.</p></blockquote><p>翻译成中文就是：模板方法模式在一个方法中定义一个算法骨架，并将某些步骤推迟到子类中实现。模板方法模式可以让子类在不改变算法整体结构的情况下，重新定义算法中的某些步骤。</p><p>这里的“算法”，我们可以理解为广义上的“业务逻辑”，并不特指数据结构和算法中的“算法”。这里的算法骨架就是“模板”，包含算法骨架的方法就是“模板方法”，这也是模板方法模式名字的由来。</p><h4 id="模板模式作用一：复用"><a href="#模板模式作用一：复用" class="headerlink" title="模板模式作用一：复用"></a>模板模式作用一：复用</h4><p>开篇的时候，我们讲到模板模式有两大作用：复用和扩展。我们先来看它的第一个作用：复用。</p><p>模板模式把一个算法中不变的流程抽象到父类的模板方法 templateMethod() 中，将可变的部分 method1()、method2() 留给子类 ContreteClass1 和 ContreteClass2 来实现。所有的子类都可以复用父类中模板方法定义的流程代码。我们通过两个小例子来更直观地体会一下。</p><p><strong>1.Java InputStream</strong></p><p>Java IO 类库中，有很多类的设计用到了模板模式，比如 InputStream、OutputStream、Reader、Writer。我们拿 InputStream 来举例说明一下。</p><p>我把 InputStream 部分相关代码贴在了下面。在代码中，read() 函数是一个模板方法，定义了读取数据的整个流程，并且暴露了一个可以由子类来定制的抽象方法。不过这个方法也被命名为了 read()，只是参数跟模板方法不同。</p><pre><code>public abstract class InputStream implements Closeable {  //...省略其他代码...    public int read(byte b[], int off, int len) throws IOException {    if (b == null) {      throw new NullPointerException();    } else if (off &lt; 0 || len &lt; 0 || len &gt; b.length - off) {      throw new IndexOutOfBoundsException();    } else if (len == 0) {      return 0;    }    int c = read();    if (c == -1) {      return -1;    }    b[off] = (byte)c;    int i = 1;    try {      for (; i &lt; len ; i++) {        c = read();        if (c == -1) {          break;        }        b[off + i] = (byte)c;      }    } catch (IOException ee) {    }    return i;  }    public abstract int read() throws IOException;}public class ByteArrayInputStream extends InputStream {  //...省略其他代码...    @Override  public synchronized int read() {    return (pos &lt; count) ? (buf[pos++] &amp; 0xff) : -1;  }}</code></pre><p><strong>2.Java AbstractList</strong></p><p>在 Java AbstractList 类中，addAll() 函数可以看作模板方法，add() 是子类需要重写的方法，尽管没有声明为 abstract 的，但函数实现直接抛出了 UnsupportedOperationException 异常。前提是，如果子类不重写是不能使用的。</p><pre><code>public boolean addAll(int index, Collection&lt;? extends E&gt; c) {    rangeCheckForAdd(index);    boolean modified = false;    for (E e : c) {        add(index++, e);        modified = true;    }    return modified;}public void add(int index, E element) {    throw new UnsupportedOperationException();}</code></pre><h4 id="模板模式作用二：扩展"><a href="#模板模式作用二：扩展" class="headerlink" title="模板模式作用二：扩展"></a>模板模式作用二：扩展</h4><p>模板模式的第二大作用的是扩展。这里所说的扩展，并不是指代码的扩展性，而是指框架的扩展性，有点类似我们之前讲到的控制反转，你可以结合第 19 节来一块理解。基于这个作用，模板模式常用在框架的开发中，让框架用户可以在不修改框架源码的情况下，定制化框架的功能。我们通过 Junit TestCase、Java Servlet 两个例子来解释一下。</p><p><strong>1.Java Servlet</strong></p><p>对于 Java Web 项目开发来说，常用的开发框架是 SpringMVC。利用它，我们只需要关注业务代码的编写，底层的原理几乎不会涉及。但是，如果我们抛开这些高级框架来开发 Web 项目，必然会用到 Servlet。实际上，使用比较底层的 Servlet 来开发 Web 项目也不难。我们只需要定义一个继承 HttpServlet 的类，并且重写其中的 doGet() 或 doPost() 方法，来分别处理 get 和 post 请求。</p><p><strong>2.JUnit TestCase</strong></p><p>跟 Java Servlet 类似，JUnit 框架也通过模板模式提供了一些功能扩展点（setUp()、tearDown() 等），让框架用户可以在这些扩展点上扩展功能。</p><p>在使用 JUnit 测试框架来编写单元测试的时候，我们编写的测试类都要继承框架提供的 TestCase 类。在 TestCase 类中，runBare() 函数是模板方法，它定义了执行测试用例的整体流程：先执行 setUp() 做些准备工作，然后执行 runTest() 运行真正的测试代码，最后执行 tearDown() 做扫尾工作。</p><h3 id="回调（Callback）"><a href="#回调（Callback）" class="headerlink" title="回调（Callback）"></a>回调（Callback）</h3><p>复用和扩展是模板模式的两大作用，实际上，还有另外一个技术概念，也能起到跟模板模式相同的作用，那就是回调（Callback）。今天我们今天就来看一下，回调的原理、实现和应用，以及它跟模板模式的区别和联系。</p><p>相对于普通的函数调用来说，回调是一种双向调用关系。A 类事先注册某个函数 F 到 B 类，A 类在调用 B 类的 P 函数的时候，B 类反过来调用 A 类注册给它的 F 函数。这里的 F 函数就是“回调函数”。A 调用 B，B 反过来又调用 A，这种调用机制就叫作“回调”。</p><p>A 类如何将回调函数传递给 B 类呢？不同的编程语言，有不同的实现方法。C 语言可以使用函数指针，Java 则需要使用包裹了回调函数的类对象，我们简称为回调对象。这里我用 Java 语言举例说明一下。代码如下所示：</p><pre><code>public interface ICallback {  void methodToCallback();}public class BClass {  public void process(ICallback callback) {    //...    callback.methodToCallback();    //...  }}public class AClass {  public static void main(String[] args) {    BClass b = new BClass();    b.process(new ICallback() { //回调对象      @Override      public void methodToCallback() {        System.out.println("Call back me.");      }    });  }}</code></pre><p>应用举例一：JdbcTemplate<br>应用举例二：setClickListener(）<br>应用举例三：addShutdownHook()</p><h3 id="模板模式-VS-回调"><a href="#模板模式-VS-回调" class="headerlink" title="模板模式 VS 回调"></a>模板模式 VS 回调</h3><p>从应用场景上来看，同步回调跟模板模式几乎一致。它们都是在一个大的算法骨架中，自由替换其中的某个步骤，起到代码复用和扩展的目的。而异步回调跟模板模式有较大差别，更像是观察者模式。</p><p>从代码实现上来看，回调和模板模式完全不同。回调基于组合关系来实现，把一个对象传递给另一个对象，是一种对象之间的关系；模板模式基于继承关系来实现，子类重写父类的抽象方法，是一种类之间的关系。</p><p>前面我们也讲到，组合优于继承。实际上，这里也不例外。在代码实现上，回调相对于模板模式会更加灵活，主要体现在下面几点。</p><ul><li>像 Java 这种只支持单继承的语言，基于模板模式编写的子类，已经继承了一个父类，不再具有继承的能力。</li><li>回调可以使用匿名类来创建回调对象，可以不用事先定义类；而模板模式针对不同的实现都要定义不同的子类。</li><li>如果某个类中定义了多个模板方法，每个方法都有对应的抽象方法，那即便我们只用到其中的一个模板方法，子类也必须实现所有的抽象方法。而回调就更加灵活，我们只需要往用到的模板方法中注入回调对象即可。</li></ul><h3 id="策略模式（Strategy-Design-Pattern）"><a href="#策略模式（Strategy-Design-Pattern）" class="headerlink" title="策略模式（Strategy Design Pattern）"></a>策略模式（Strategy Design Pattern）</h3><p>策略模式，英文全称是 Strategy Design Pattern。在 GoF 的《设计模式》一书中，它是这样定义的：</p><blockquote><p>Define a family of algorithms, encapsulate each one, and make them interchangeable. Strategy lets the algorithm vary independently from clients that use it.</p></blockquote><p>翻译成中文就是：定义一族算法类，将每个算法分别封装起来，让它们可以互相替换。策略模式可以使算法的变化独立于使用它们的客户端（这里的客户端代指使用算法的代码）。</p><p>我们知道，工厂模式是解耦对象的创建和使用，观察者模式是解耦观察者和被观察者。策略模式跟两者类似，也能起到解耦的作用，不过，它解耦的是策略的定义、创建、使用这三部分。接下来，我就详细讲讲一个完整的策略模式应该包含的这三个部分。</p><h4 id="策略的定义"><a href="#策略的定义" class="headerlink" title="策略的定义"></a>策略的定义</h4><p>策略类的定义比较简单，包含一个策略接口和一组实现这个接口的策略类。因为所有的策略类都实现相同的接口，所以，客户端代码基于接口而非实现编程，可以灵活地替换不同的策略。示例代码如下所示：</p><pre><code>public interface Strategy {  void algorithmInterface();}public class ConcreteStrategyA implements Strategy {  @Override  public void  algorithmInterface() {    //具体的算法...  }}public class ConcreteStrategyB implements Strategy {  @Override  public void  algorithmInterface() {    //具体的算法...  }}</code></pre><h4 id="策略的创建"><a href="#策略的创建" class="headerlink" title="策略的创建"></a>策略的创建</h4><p>因为策略模式会包含一组策略，在使用它们的时候，一般会通过类型（type）来判断创建哪个策略来使用。为了封装创建逻辑，我们需要对客户端代码屏蔽创建细节。我们可以把根据 type 创建策略的逻辑抽离出来，放到工厂类中。示例代码如下所示：</p><pre><code>public class StrategyFactory {  private static final Map&lt;String, Strategy&gt; strategies = new HashMap&lt;&gt;();  static {    strategies.put("A", new ConcreteStrategyA());    strategies.put("B", new ConcreteStrategyB());  }  public static Strategy getStrategy(String type) {    if (type == null || type.isEmpty()) {      throw new IllegalArgumentException("type should not be empty.");    }    return strategies.get(type);  }}</code></pre><p>一般来讲，如果策略类是无状态的，不包含成员变量，只是纯粹的算法实现，这样的策略对象是可以被共享使用的，不需要在每次调用 getStrategy() 的时候，都创建一个新的策略对象。针对这种情况，我们可以使用上面这种工厂类的实现方式，事先创建好每个策略对象，缓存到工厂类中，用的时候直接返回。</p><p>相反，如果策略类是有状态的，根据业务场景的需要，我们希望每次从工厂方法中，获得的都是新创建的策略对象，而不是缓存好可共享的策略对象，那我们就需要按照如下方式来实现策略工厂类。</p><pre><code>public class StrategyFactory {  public static Strategy getStrategy(String type) {    if (type == null || type.isEmpty()) {      throw new IllegalArgumentException("type should not be empty.");    }    if (type.equals("A")) {      return new ConcreteStrategyA();    } else if (type.equals("B")) {      return new ConcreteStrategyB();    }    return null;  }}</code></pre><h4 id="策略的使用"><a href="#策略的使用" class="headerlink" title="策略的使用"></a>策略的使用</h4><p>这里的“运行时动态”指的是，我们事先并不知道会使用哪个策略，而是在程序运行期间，根据配置、用户输入、计算结果等这些不确定因素，动态决定使用哪种策略。接下来，我们通过一个例子来解释一下。</p><pre><code>// 策略接口：EvictionStrategy// 策略类：LruEvictionStrategy、FifoEvictionStrategy、LfuEvictionStrategy...// 策略工厂：EvictionStrategyFactorypublic class UserCache {  private Map&lt;String, User&gt; cacheData = new HashMap&lt;&gt;();  private EvictionStrategy eviction;  public UserCache(EvictionStrategy eviction) {    this.eviction = eviction;  }  //...}// 运行时动态确定，根据配置文件的配置决定使用哪种策略public class Application {  public static void main(String[] args) throws Exception {    EvictionStrategy evictionStrategy = null;    Properties props = new Properties();    props.load(new FileInputStream("./config.properties"));    String type = props.getProperty("eviction_type");    evictionStrategy = EvictionStrategyFactory.getEvictionStrategy(type);    UserCache userCache = new UserCache(evictionStrategy);    //...  }}// 非运行时动态确定，在代码中指定使用哪种策略public class Application {  public static void main(String[] args) {    //...    EvictionStrategy evictionStrategy = new LruEvictionStrategy();    UserCache userCache = new UserCache(evictionStrategy);    //...  }}</code></pre><h3 id="职责链模式-（Chain-Of-Responsibility-Design-Pattern）"><a href="#职责链模式-（Chain-Of-Responsibility-Design-Pattern）" class="headerlink" title="职责链模式 （Chain Of Responsibility Design Pattern）"></a>职责链模式 （Chain Of Responsibility Design Pattern）</h3><p>感觉跟框架的中间件有点类似。</p><p>职责链模式的英文翻译是 Chain Of Responsibility Design Pattern。在 GoF 的《设计模式》中，它是这么定义的：</p><blockquote><p>Avoid coupling the sender of a request to its receiver by giving more than one object a chance to handle the request. Chain the receiving objects and pass the request along the chain until an object handles it.</p></blockquote><p>翻译成中文就是：将请求的发送和接收解耦，让多个接收对象都有机会处理这个请求。将这些接收对象串成一条链，并沿着这条链传递这个请求，直到链上的某个接收对象能够处理它为止。</p><p><strong>首先，我们来看，职责链模式如何应对代码的复杂性。</strong></p><p>将大块代码逻辑拆分成函数，将大类拆分成小类，是应对代码复杂性的常用方法。应用职责链模式，我们把各个敏感词过滤函数继续拆分出来，设计成独立的类，进一步简化了 SensitiveWordFilter 类，让 SensitiveWordFilter 类的代码不会过多，过复杂。</p><p><strong>其次，我们再来看，职责链模式如何让代码满足开闭原则，提高代码的扩展性。</strong></p><p>当我们要扩展新的过滤算法的时候，比如，我们还需要过滤特殊符号，按照非职责链模式的代码实现方式，我们需要修改 SensitiveWordFilter 的代码，违反开闭原则。不过，这样的修改还算比较集中，也是可以接受的。而职责链模式的实现方式更加优雅，只需要新添加一个 Filter 类，并且通过 addFilter() 函数将它添加到 FilterChain 中即可，其他代码完全不需要修改。</p><p>不过，你可能会说，即便使用职责链模式来实现，当添加新的过滤算法的时候，还是要修改客户端代码（ApplicationDemo），这样做也没有完全符合开闭原则。</p><p>实际上，细化一下的话，我们可以把上面的代码分成两类：框架代码和客户端代码。其中，ApplicationDemo 属于客户端代码，也就是使用框架的代码。除 ApplicationDemo 之外的代码属于敏感词过滤框架代码。</p><p>假设敏感词过滤框架并不是我们开发维护的，而是我们引入的一个第三方框架，我们要扩展一个新的过滤算法，不可能直接去修改框架的源码。这个时候，利用职责链模式就能达到开篇所说的，在不修改框架源码的情况下，基于职责链模式提供的扩展点，来扩展新的功能。换句话说，我们在框架这个代码范围内实现了开闭原则。</p><p>除此之外，利用职责链模式相对于不用职责链的实现方式，还有一个好处，那就是配置过滤算法更加灵活，可以只选择使用某几个过滤算法。</p><h3 id="状态模式（）"><a href="#状态模式（）" class="headerlink" title="状态模式（）"></a>状态模式（）</h3><h4 id="什么是有限状态机？"><a href="#什么是有限状态机？" class="headerlink" title="什么是有限状态机？"></a>什么是有限状态机？</h4><p>有限状态机，英文翻译是 Finite State Machine，缩写为 FSM，简称为状态机。状态机有 3 个组成部分：状态（State）、事件（Event）、动作（Action）。其中，事件也称为转移条件（Transition Condition）。事件触发状态的转移及动作的执行。不过，动作不是必须的，也可能只转移状态，不执行任何动作。</p><h4 id="状态机实现方式一：分支逻辑法"><a href="#状态机实现方式一：分支逻辑法" class="headerlink" title="状态机实现方式一：分支逻辑法"></a>状态机实现方式一：分支逻辑法</h4><pre><code>if (currentState.equals(State.SUPER)) {  this.currentState = State.SMALL;  this.score -= 100;  return;}</code></pre><h4 id="状态机实现方式二：查表法"><a href="#状态机实现方式二：查表法" class="headerlink" title="状态机实现方式二：查表法"></a>状态机实现方式二：查表法</h4><p>实际上，上面这种实现方法有点类似 hard code，对于复杂的状态机来说不适用，而状态机的第二种实现方式查表法，就更加合适了。接下来，我们就一块儿来看下，如何利用查表法来补全骨架代码。</p><p>实际上，除了用状态转移图来表示之外，状态机还可以用二维表来表示，如下所示。在这个二维表中，第一维表示当前状态，第二维表示事件，值表示当前状态经过事件之后，转移到的新状态及其执行的动作。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7856dd4dba54c077.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>相对于分支逻辑的实现方式，查表法的代码实现更加清晰，可读性和可维护性更好。当修改状态机时，我们只需要修改 transitionTable 和 actionTable 两个二维数组即可。实际上，如果我们把这两个二维数组存储在配置文件中，当需要修改状态机时，我们甚至可以不修改任何代码，只需要修改配置文件就可以了。</p><h4 id="状态机实现方式三：状态模式"><a href="#状态机实现方式三：状态模式" class="headerlink" title="状态机实现方式三：状态模式"></a>状态机实现方式三：状态模式</h4><p>在查表法的代码实现中，事件触发的动作只是简单的积分加减，所以，我们用一个 int 类型的二维数组 actionTable 就能表示，二维数组中的值表示积分的加减值。但是，如果要执行的动作并非这么简单，而是一系列复杂的逻辑操作（比如加减积分、写数据库，还有可能发送消息通知等等），我们就没法用如此简单的二维数组来表示了。这也就是说，查表法的实现方式有一定局限性。</p><p>虽然分支逻辑的实现方式不存在这个问题，但它又存在前面讲到的其他问题，比如分支判断逻辑较多，导致代码可读性和可维护性不好等。实际上，针对分支逻辑法存在的问题，我们可以使用状态模式来解决。</p><p>状态模式通过将事件触发的状态转移和动作执行，拆分到不同的状态类中，来避免分支判断逻辑。我们还是结合代码来理解这句话。</p><p>利用状态模式，我们来补全 MarioStateMachine 类，补全后的代码如下所示。</p><p>其中，IMario 是状态的接口，定义了所有的事件。SmallMario、SuperMario、CapeMario、FireMario 是 IMario 接口的实现类，分别对应状态机中的 4 个状态。原来所有的状态转移和动作执行的代码逻辑，都集中在 MarioStateMachine 类中，现在，这些代码逻辑被分散到了这 4 个状态类中。</p><pre><code>public interface IMario {  State getName();  void obtainMushRoom(MarioStateMachine stateMachine);  void obtainCape(MarioStateMachine stateMachine);  void obtainFireFlower(MarioStateMachine stateMachine);  void meetMonster(MarioStateMachine stateMachine);}public class SmallMario implements IMario {  private static final SmallMario instance = new SmallMario();  private SmallMario() {}  public static SmallMario getInstance() {    return instance;  }  @Override  public State getName() {    return State.SMALL;  }  @Override  public void obtainMushRoom(MarioStateMachine stateMachine) {    stateMachine.setCurrentState(SuperMario.getInstance());    stateMachine.setScore(stateMachine.getScore() + 100);  }  @Override  public void obtainCape(MarioStateMachine stateMachine) {    stateMachine.setCurrentState(CapeMario.getInstance());    stateMachine.setScore(stateMachine.getScore() + 200);  }  @Override  public void obtainFireFlower(MarioStateMachine stateMachine) {    stateMachine.setCurrentState(FireMario.getInstance());    stateMachine.setScore(stateMachine.getScore() + 300);  }  @Override  public void meetMonster(MarioStateMachine stateMachine) {    // do nothing...  }}// 省略SuperMario、CapeMario、FireMario类...public class MarioStateMachine {  private int score;  private IMario currentState;  public MarioStateMachine() {    this.score = 0;    this.currentState = SmallMario.getInstance();  }  public void obtainMushRoom() {    this.currentState.obtainMushRoom(this);  }  public void obtainCape() {    this.currentState.obtainCape(this);  }  public void obtainFireFlower() {    this.currentState.obtainFireFlower(this);  }  public void meetMonster() {    this.currentState.meetMonster(this);  }  public int getScore() {    return this.score;  }  public State getCurrentState() {    return this.currentState.getName();  }  public void setScore(int score) {    this.score = score;  }  public void setCurrentState(IMario currentState) {    this.currentState = currentState;  }}</code></pre><p>实际上，像游戏这种比较复杂的状态机，包含的状态比较多，我优先推荐使用查表法，而状态模式会引入非常多的状态类，会导致代码比较难维护。相反，像电商下单、外卖下单这种类型的状态机，它们的状态并不多，状态转移也比较简单，但事件触发执行的动作包含的业务逻辑可能会比较复杂，所以，更加推荐使用状态模式来实现。</p><h3 id="迭代器模式（Iterator-Design-Pattern）"><a href="#迭代器模式（Iterator-Design-Pattern）" class="headerlink" title="迭代器模式（Iterator Design Pattern）"></a>迭代器模式（Iterator Design Pattern）</h3><p>迭代器模式（Iterator Design Pattern），也叫作游标模式（Cursor Design Pattern）。</p><p>在开篇中我们讲到，它用来遍历集合对象。这里说的“集合对象”也可以叫“容器”“聚合对象”，实际上就是包含一组对象的对象，比如数组、链表、树、图、跳表。迭代器模式将集合对象的遍历操作从集合类中拆分出来，放到迭代器类中，让两者的职责更加单一。</p><p>迭代器是用来遍历容器的，所以，一个完整的迭代器模式一般会涉及容器和容器迭代器两部分内容。为了达到基于接口而非实现编程的目的，容器又包含容器接口、容器实现类，迭代器又包含迭代器接口、迭代器实现类。</p><pre><code>// 接口定义方式一public interface Iterator&lt;E&gt; {  boolean hasNext();  void next();  E currentItem();}</code></pre><h4 id="迭代时候增删数据怎么办？"><a href="#迭代时候增删数据怎么办？" class="headerlink" title="迭代时候增删数据怎么办？"></a>迭代时候增删数据怎么办？</h4><p>在通过迭代器来遍历集合元素的同时，增加或者删除集合中的元素，有可能会导致某个元素被重复遍历或遍历不到。不过，并不是所有情况下都会遍历出错，有的时候也可以正常遍历，所以，这种行为称为结果不可预期行为或者未决行为。实际上，“不可预期”比直接出错更加可怕，有的时候运行正确，有的时候运行错误，一些隐藏很深、很难 debug 的 bug 就是这么产生的。</p><p>有两种比较干脆利索的解决方案，来避免出现这种不可预期的运行结果。一种是遍历的时候不允许增删元素，另一种是增删元素之后让遍历报错。第一种解决方案比较难实现，因为很难确定迭代器使用结束的时间点。第二种解决方案更加合理。Java 语言就是采用的这种解决方案。增删元素之后，我们选择 fail-fast 解决方式，让遍历操作直接抛出运行时异常。</p><p>像 Java 语言，迭代器类中除了前面提到的几个最基本的方法之外，还定义了一个 remove() 方法，能够在遍历集合的同时，安全地删除集合中的元素。</p><h4 id="快照模式迭代器"><a href="#快照模式迭代器" class="headerlink" title="快照模式迭代器"></a>快照模式迭代器</h4><p>理解这个问题最关键的是理解“快照”两个字。所谓“快照”，指我们为容器创建迭代器的时候，相当于给容器拍了一张快照（Snapshot）。之后即便我们增删容器中的元素，快照中的元素并不会做相应的改动。而迭代器遍历的对象是快照而非容器，这样就避免了在使用迭代器遍历的过程中，增删容器中的元素，导致的不可预期的结果或者报错。</p><p><strong>解决方案一</strong></p><p>我们先来看最简单的一种解决办法。在迭代器类中定义一个成员变量 snapshot 来存储快照。每当创建迭代器的时候，都拷贝一份容器中的元素到快照中，后续的遍历操作都基于这个迭代器自己持有的快照来进行。</p><p><strong>解决方案二</strong></p><p>我们可以在容器中，为每个元素保存两个时间戳，一个是添加时间戳 addTimestamp，一个是删除时间戳 delTimestamp。当元素被加入到集合中的时候，我们将 addTimestamp 设置为当前时间，将 delTimestamp 设置成最大长整型值（Long.MAX_VALUE）。当元素被删除时，我们将 delTimestamp 更新为当前时间，表示已经被删除。</p><p>同时，每个迭代器也保存一个迭代器创建时间戳 snapshotTimestamp，也就是迭代器对应的快照的创建时间戳。当使用迭代器来遍历容器的时候，只有满足 addTimestamp</p><p>如果元素的 addTimestamp&gt;snapshotTimestamp，说明元素在创建了迭代器之后才加入的，不属于这个迭代器的快照；如果元素的 delTimestamp</p><h3 id="访问者模式-Visitor-Pattern"><a href="#访问者模式-Visitor-Pattern" class="headerlink" title="访问者模式(Visitor Pattern)"></a>访问者模式(Visitor Pattern)</h3><p>前面我们讲到，大部分设计模式的原理和实现都很简单，不过也有例外，比如今天要讲的访问者模式。它可以算是 23 种经典设计模式中最难理解的几个之一。因为它难理解、难实现，应用它会导致代码的可读性、可维护性变差，所以，访问者模式在实际的软件开发中很少被用到，在没有特别必要的情况下，建议你不要使用访问者模式。</p><pre><code>public abstract class ResourceFile {  protected String filePath;  public ResourceFile(String filePath) {    this.filePath = filePath;  }  abstract public void accept(Visitor vistor);}public class PdfFile extends ResourceFile {  public PdfFile(String filePath) {    super(filePath);  }  @Override  public void accept(Visitor visitor) {    visitor.visit(this);  }  //...}//...PPTFile、WordFile跟PdfFile类似，这里就省略了...public interface Visitor {  void visit(PdfFile pdfFile);  void visit(PPTFile pdfFile);  void visit(WordFile pdfFile);}public class Extractor implements Visitor {  @Override  public void visit(PPTFile pptFile) {    //...    System.out.println("Extract PPT.");  }  @Override  public void visit(PdfFile pdfFile) {    //...    System.out.println("Extract PDF.");  }  @Override  public void visit(WordFile wordFile) {    //...    System.out.println("Extract WORD.");  }}public class Compressor implements Visitor {  @Override  public void visit(PPTFile pptFile) {    //...    System.out.println("Compress PPT.");  }  @Override  public void visit(PdfFile pdfFile) {    //...    System.out.println("Compress PDF.");  }  @Override  public void visit(WordFile wordFile) {    //...    System.out.println("Compress WORD.");  }}public class ToolApplication {  public static void main(String[] args) {    Extractor extractor = new Extractor();    List&lt;ResourceFile&gt; resourceFiles = listAllResourceFiles(args[0]);    for (ResourceFile resourceFile : resourceFiles) {      resourceFile.accept(extractor);    }    Compressor compressor = new Compressor();    for(ResourceFile resourceFile : resourceFiles) {      resourceFile.accept(compressor);    }  }  private static List&lt;ResourceFile&gt; listAllResourceFiles(String resourceDirectory) {    List&lt;ResourceFile&gt; resourceFiles = new ArrayList&lt;&gt;();    //...根据后缀(pdf/ppt/word)由工厂方法创建不同的类对象(PdfFile/PPTFile/WordFile)    resourceFiles.add(new PdfFile("a.pdf"));    resourceFiles.add(new WordFile("b.word"));    resourceFiles.add(new PPTFile("c.ppt"));    return resourceFiles;  }}</code></pre><p>访问者模式允许一个或者多个操作应用到一组对象上，设计意图是解耦操作和对象本身，保持类职责单一、满足开闭原则以及应对代码的复杂性。</p><p>对于访问者模式，学习的主要难点在代码实现。而代码实现比较复杂的主要原因是，函数重载在大部分面向对象编程语言中是静态绑定的。也就是说，调用类的哪个重载函数，是在编译期间，由参数的声明类型决定的，而非运行时，根据参数的实际类型决定的。</p><p>正是因为代码实现难理解，所以，在项目中应用这种模式，会导致代码的可读性比较差。如果你的同事不了解这种设计模式，可能就会读不懂、维护不了你写的代码。所以，除非不得已，不要使用这种模式。</p><h4 id="Single-Dispatch-和-Double-Dispatch"><a href="#Single-Dispatch-和-Double-Dispatch" class="headerlink" title="Single Dispatch 和 Double Dispatch"></a>Single Dispatch 和 Double Dispatch</h4><p>如何理解“Dispatch”这个单词呢？ 在面向对象编程语言中，我们可以把方法调用理解为一种消息传递，也就是“Dispatch”。一个对象调用另一个对象的方法，就相当于给它发送一条消息。这条消息起码要包含对象名、方法名、方法参数。</p><p>所谓 Single Dispatch，指的是执行哪个对象的方法，根据对象的运行时类型来决定；执行对象的哪个方法，根据方法参数的编译时类型来决定。所谓 Double Dispatch，指的是执行哪个对象的方法，根据对象的运行时类型来决定；执行对象的哪个方法，根据方法参数的运行时类型来决定。</p><p>具体到编程语言的语法机制，Single Dispatch 和 Double Dispatch 跟多态和函数重载直接相关。当前主流的面向对象编程语言（比如，Java、C++、C#）都只支持 Single Dispatch，不支持 Double Dispatch。</p><pre><code>public class ParentClass {  public void f() {    System.out.println("I am ParentClass's f().");  }}public class ChildClass extends ParentClass {  public void f() {    System.out.println("I am ChildClass's f().");  }}public class SingleDispatchClass {  public void polymorphismFunction(ParentClass p) {    p.f();  }  public void overloadFunction(ParentClass p) {    System.out.println("I am overloadFunction(ParentClass p).");  }  public void overloadFunction(ChildClass c) {    System.out.println("I am overloadFunction(ChildClass c).");  }}public class DemoMain {  public static void main(String[] args) {    SingleDispatchClass demo = new SingleDispatchClass();    ParentClass p = new ChildClass();    demo.polymorphismFunction(p);//执行哪个对象的方法，由对象的实际类型决定    demo.overloadFunction(p);//执行对象的哪个方法，由参数对象的声明类型决定  }}//代码执行结果:I am ChildClass's f().I am overloadFunction(ParentClass p).</code></pre><p>Java 支持多态特性，代码可以在运行时获得对象的实际类型（也就是前面提到的运行时类型），然后根据实际类型决定调用哪个方法。尽管 Java 支持函数重载，但 Java 设计的函数重载的语法规则是，并不是在运行时，根据传递进函数的参数的实际类型，来决定调用哪个重载函数，而是在编译时，根据传递进函数的参数的声明类型（也就是前面提到的编译时类型），来决定调用哪个重载函数。也就是说，具体执行哪个对象的哪个方法，只跟对象的运行时类型有关，跟参数的运行时类型无关。所以，Java 语言只支持 Single Dispatch。</p><h3 id="备忘录模式（Memento-Design-Pattern）"><a href="#备忘录模式（Memento-Design-Pattern）" class="headerlink" title="备忘录模式（Memento Design Pattern）"></a>备忘录模式（Memento Design Pattern）</h3><p>备忘录模式，也叫快照（Snapshot）模式，英文翻译是 Memento Design Pattern。在 GoF 的《设计模式》一书中，备忘录模式是这么定义的：</p><blockquote><p>Captures and externalizes an object’s internal state so that it can be restored later, all without violating encapsulation.</p></blockquote><p>翻译成中文就是：在不违背封装原则的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，以便之后恢复对象为先前的状态。</p><p>备忘录模式也叫快照模式，具体来说，就是在不违背封装原则的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，以便之后恢复对象为先前的状态。这个模式的定义表达了两部分内容：一部分是，存储副本以便后期恢复；另一部分是，要在不违背封装原则的前提下，进行对象的备份和恢复。</p><p>备忘录模式的应用场景也比较明确和有限，主要是用来防丢失、撤销、恢复等。它跟平时我们常说的“备份”很相似。两者的主要区别在于，备忘录模式更侧重于代码的设计和实现，备份更侧重架构设计或产品设计。</p><p>对于大对象的备份来说，备份占用的存储空间会比较大，备份和恢复的耗时会比较长。针对这个问题，不同的业务场景有不同的处理方式。比如，只备份必要的恢复信息，结合最新的数据来恢复；再比如，全量备份和增量备份相结合，低频全量备份，高频增量备份，两者结合来做恢复。</p><h3 id="命令模式（Command-Design-Pattern）"><a href="#命令模式（Command-Design-Pattern）" class="headerlink" title="命令模式（Command Design Pattern）"></a>命令模式（Command Design Pattern）</h3><p>命令模式的英文翻译是 Command Design Pattern。在 GoF 的《设计模式》一书中，它是这么定义的：</p><blockquote><p>The command pattern encapsulates a request as an object, thereby letting us parameterize other objects with different requests, queue or log requests, and support undoable operations.</p></blockquote><p>命令模式将请求（命令）封装为一个对象，这样可以使用不同的请求参数化其他对象（将不同请求依赖注入到其他对象），并且能够支持请求（命令）的排队执行、记录日志、撤销等（附加控制）功能。</p><p>落实到编码实现，命令模式用的最核心的实现手段，是将函数封装成对象。我们知道，C 语言支持函数指针，我们可以把函数当作变量传递来传递去。但是，在大部分编程语言中，函数没法儿作为参数传递给其他函数，也没法儿赋值给变量。借助命令模式，我们可以将函数封装成对象。具体来说就是，设计一个包含这个函数的类，实例化一个对象传来传去，这样就可以实现把函数像对象一样使用。从实现的角度来说，它类似我们之前讲过的回调。</p><p>当我们把函数封装成对象之后，对象就可以存储下来，方便控制执行。所以，命令模式的主要作用和应用场景，是用来控制命令的执行，比如，异步、延迟、排队执行命令、撤销重做命令、存储命令、给命令记录日志等等，这才是命令模式能发挥独一无二作用的地方。</p><h4 id="命令模式-VS-策略模式"><a href="#命令模式-VS-策略模式" class="headerlink" title="命令模式 VS 策略模式"></a>命令模式 VS 策略模式</h4><p>实际上，每个设计模式都应该由两部分组成：第一部分是应用场景，即这个模式可以解决哪类问题；第二部分是解决方案，即这个模式的设计思路和具体的代码实现。不过，代码实现并不是模式必须包含的。如果你单纯地只关注解决方案这一部分，甚至只关注代码实现，就会产生大部分模式看起来都很相似的错觉。</p><p>之前讲策略模式的时候，我们有讲到，策略模式包含策略的定义、创建和使用三部分，从代码结构上来，它非常像工厂模式。它们的区别在于，策略模式侧重“策略”或“算法”这个特定的应用场景，用来解决根据运行时状态从一组策略中选择不同策略的问题，而工厂模式侧重封装对象的创建过程，这里的对象没有任何业务场景的限定，可以是策略，但也可以是其他东西。从设计意图上来，这两个模式完全是两回事儿。</p><p>在策略模式中，不同的策略具有相同的目的、不同的实现、互相之间可以替换。比如，BubbleSort、SelectionSort 都是为了实现排序的，只不过一个是用冒泡排序算法来实现的，另一个是用选择排序算法来实现的。而在命令模式中，不同的命令具有不同的目的，对应不同的处理逻辑，并且互相之间不可替换。</p><h3 id="解释器模式（Interpreter-Design-Pattern）"><a href="#解释器模式（Interpreter-Design-Pattern）" class="headerlink" title="解释器模式（Interpreter Design Pattern）"></a>解释器模式（Interpreter Design Pattern）</h3><p>解释器模式的英文翻译是 Interpreter Design Pattern。在 GoF 的《设计模式》一书中，它是这样定义的：</p><blockquote><p>Interpreter pattern is used to defines a grammatical representation for a language and provides an interpreter to deal with this grammar.</p></blockquote><p>翻译成中文就是：解释器模式为某个语言定义它的语法（或者叫文法）表示，并定义一个解释器用来处理这个语法。</p><p>解释器模式为某个语言定义它的语法（或者叫文法）表示，并定义一个解释器用来处理这个语法。实际上，这里的“语言”不仅仅指我们平时说的中、英、日、法等各种语言。从广义上来讲，只要是能承载信息的载体，我们都可以称之为“语言”，比如，古代的结绳记事、盲文、哑语、摩斯密码等。</p><p>解释器模式的代码实现比较灵活，没有固定的模板。我们前面说过，应用设计模式主要是应对代码的复杂性，解释器模式也不例外。它的代码实现的核心思想，就是将语法解析的工作拆分到各个小类中，以此来避免大而全的解析类。一般的做法是，将语法规则拆分一些小的独立的单元，然后对每个单元进行解析，最终合并为对整个语法规则的解析。</p><p><strong>解释器模式实战举例</strong></p><p>接下来，我们再来看一个更加接近实战的例子，也就是咱们今天标题中的问题：如何实现一个自定义接口告警规则功能？</p><p>在我们平时的项目开发中，监控系统非常重要，它可以时刻监控业务系统的运行情况，及时将异常报告给开发者。比如，如果每分钟接口出错数超过 100，监控系统就通过短信、微信、邮件等方式发送告警给开发者。</p><p>一般来讲，监控系统支持开发者自定义告警规则，比如我们可以用下面这样一个表达式，来表示一个告警规则，它表达的意思是：每分钟 API 总出错数超过 100 或者每分钟 API 总调用数超过 10000 就触发告警。</p><pre><code>public interface Expression {  boolean interpret(Map&lt;String, Long&gt; stats);}public class GreaterExpression implements Expression {  private String key;  private long value;  public GreaterExpression(String strExpression) {    String[] elements = strExpression.trim().split("\\s+");    if (elements.length != 3 || !elements[1].trim().equals("&gt;")) {      throw new RuntimeException("Expression is invalid: " + strExpression);    }    this.key = elements[0].trim();    this.value = Long.parseLong(elements[2].trim());  }  public GreaterExpression(String key, long value) {    this.key = key;    this.value = value;  }  @Override  public boolean interpret(Map&lt;String, Long&gt; stats) {    if (!stats.containsKey(key)) {      return false;    }    long statValue = stats.get(key);    return statValue &gt; value;  }}// LessExpression/EqualExpression跟GreaterExpression代码类似，这里就省略了public class AndExpression implements Expression {  private List&lt;Expression&gt; expressions = new ArrayList&lt;&gt;();  public AndExpression(String strAndExpression) {    String[] strExpressions = strAndExpression.split("&amp;&amp;");    for (String strExpr : strExpressions) {      if (strExpr.contains("&gt;")) {        expressions.add(new GreaterExpression(strExpr));      } else if (strExpr.contains("&lt;")) {        expressions.add(new LessExpression(strExpr));      } else if (strExpr.contains("==")) {        expressions.add(new EqualExpression(strExpr));      } else {        throw new RuntimeException("Expression is invalid: " + strAndExpression);      }    }  }  public AndExpression(List&lt;Expression&gt; expressions) {    this.expressions.addAll(expressions);  }  @Override  public boolean interpret(Map&lt;String, Long&gt; stats) {    for (Expression expr : expressions) {      if (!expr.interpret(stats)) {        return false;      }    }    return true;  }}public class OrExpression implements Expression {  private List&lt;Expression&gt; expressions = new ArrayList&lt;&gt;();  public OrExpression(String strOrExpression) {    String[] andExpressions = strOrExpression.split("\\|\\|");    for (String andExpr : andExpressions) {      expressions.add(new AndExpression(andExpr));    }  }  public OrExpression(List&lt;Expression&gt; expressions) {    this.expressions.addAll(expressions);  }  @Override  public boolean interpret(Map&lt;String, Long&gt; stats) {    for (Expression expr : expressions) {      if (expr.interpret(stats)) {        return true;      }    }    return false;  }}public class AlertRuleInterpreter {  private Expression expression;  public AlertRuleInterpreter(String ruleExpression) {    this.expression = new OrExpression(ruleExpression);  }  public boolean interpret(Map&lt;String, Long&gt; stats) {    return expression.interpret(stats);  }} </code></pre><h3 id="中介模式-Mediator-Design-Pattern"><a href="#中介模式-Mediator-Design-Pattern" class="headerlink" title="中介模式(Mediator Design Pattern)"></a>中介模式(Mediator Design Pattern)</h3><p>中介模式。跟前面刚刚讲过的命令模式、解释器模式类似，中介模式也属于不怎么常用的模式，应用场景比较特殊、有限，但是，跟它俩不同的是，中介模式理解起来并不难，代码实现也非常简单，学习难度要小很多。</p><p>中介模式的英文翻译是 Mediator Design Pattern。在 GoF 中的《设计模式》一书中，它是这样定义的：</p><blockquote><p>Mediator pattern defines a separate (mediator) object that encapsulates the interaction between a set of objects and the objects delegate their interaction to a mediator object instead of interacting with each other directly.</p></blockquote><p>翻译成中文就是：中介模式定义了一个单独的（中介）对象，来封装一组对象之间的交互。将这组对象之间的交互委派给与中介对象交互，来避免对象之间的直接交互。</p><p>实际上，中介模式的设计思想跟中间层很像，通过引入中介这个中间层，将一组对象之间的交互关系（或者说依赖关系）从多对多（网状关系）转换为一对多（星状关系）。原来一个对象要跟 n 个对象交互，现在只需要跟一个中介对象交互，从而最小化对象之间的交互关系，降低了代码的复杂度，提高了代码的可读性和可维护性。</p><p>这里我画了一张对象交互关系的对比图。其中，右边的交互图是利用中介模式对左边交互关系优化之后的结果，从图中我们可以很直观地看出，右边的交互关系更加清晰、简洁。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a2d06779fe61dcdd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>提到中介模式，有一个比较经典的例子不得不说，那就是航空管制。</p><p>为了让飞机在飞行的时候互不干扰，每架飞机都需要知道其他飞机每时每刻的位置，这就需要时刻跟其他飞机通信。飞机通信形成的通信网络就会无比复杂。这个时候，我们通过引入“塔台”这样一个中介，让每架飞机只跟塔台来通信，发送自己的位置给塔台，由塔台来负责每架飞机的航线调度。这样就大大简化了通信网络。</p><p>从代码中我们可以看出，原本业务逻辑会分散在各个控件中，现在都集中到了中介类中。实际上，这样做既有好处，也有坏处。好处是简化了控件之间的交互，坏处是中介类有可能会变成大而复杂的“上帝类”（God Class）。所以，在使用中介模式的时候，我们要根据实际的情况，平衡对象之间交互的复杂度和中介类本身的复杂度。</p><h3 id="中介模式-VS-观察者模式"><a href="#中介模式-VS-观察者模式" class="headerlink" title="中介模式 VS 观察者模式"></a>中介模式 VS 观察者模式</h3><p>前面讲观察者模式的时候，我们讲到，观察者模式有多种实现方式。虽然经典的实现方式没法彻底解耦观察者和被观察者，观察者需要注册到被观察者中，被观察者状态更新需要调用观察者的 update() 方法。但是，在跨进程的实现方式中，我们可以利用消息队列实现彻底解耦，观察者和被观察者都只需要跟消息队列交互，观察者完全不知道被观察者的存在，被观察者也完全不知道观察者的存在。</p><p>我们前面提到，中介模式也是为了解耦对象之间的交互，所有的参与者都只与中介进行交互。而观察者模式中的消息队列，就有点类似中介模式中的“中介”，观察者模式的中观察者和被观察者，就有点类似中介模式中的“参与者”。那问题来了：中介模式和观察者模式的区别在哪里呢？什么时候选择使用中介模式？什么时候选择使用观察者模式呢？</p><p>在观察者模式中，尽管一个参与者既可以是观察者，同时也可以是被观察者，但是，大部分情况下，交互关系往往都是单向的，一个参与者要么是观察者，要么是被观察者，不会兼具两种身份。也就是说，在观察者模式的应用场景中，参与者之间的交互关系比较有条理。</p><p>而中介模式正好相反。只有当参与者之间的交互关系错综复杂，维护成本很高的时候，我们才考虑使用中介模式。毕竟，中介模式的应用会带来一定的副作用，前面也讲到，它有可能会产生大而复杂的上帝类。除此之外，如果一个参与者状态的改变，其他参与者执行的操作有一定先后顺序的要求，这个时候，中介模式就可以利用中介类，通过先后调用不同参与者的方法，来实现顺序的控制，而观察者模式是无法实现这样的顺序要求的。</p><h2 id="Unix-设计模式应用"><a href="#Unix-设计模式应用" class="headerlink" title="Unix 设计模式应用"></a>Unix 设计模式应用</h2><h3 id="封装与抽象-1"><a href="#封装与抽象-1" class="headerlink" title="封装与抽象"></a>封装与抽象</h3><p>在 Unix、Linux 系统中，有一句经典的话，“Everything is a file”，翻译成中文就是“一切皆文件”。这句话的意思就是，在 Unix、Linux 系统中，很多东西都被抽象成“文件”这样一个概念，比如 Socket、驱动、硬盘、系统信息等。它们使用文件系统的路径作为统一的命名空间（namespace），使用统一的 read、write 标准函数来访问。</p><p>比如，我们要查看 CPU 的信息，在 Linux 系统中，我们只需要使用 Vim、Gedit 等编辑器或者 cat 命令，像打开其他文件一样，打开 /proc/cpuinfo，就能查看到相应的信息。除此之外，我们还可以通过查看 /proc/uptime 文件，了解系统运行了多久，查看 /proc/version 了解系统的内核版本等。</p><p>实际上，“一切皆文件”就体现了封装和抽象的设计思想。</p><p>封装了不同类型设备的访问细节，抽象为统一的文件访问方式，更高层的代码就能基于统一的访问方式，来访问底层不同类型的设备。这样做的好处是，隔离底层设备访问的复杂性。统一的访问方式能够简化上层代码的编写，并且代码更容易复用。</p><p>除此之外，抽象和封装还能有效控制代码复杂性的蔓延，将复杂性封装在局部代码中，隔离实现的易变性，提供简单、统一的访问接口，让其他模块来使用，其他模块基于抽象的接口而非具体的实现编程，代码会更加稳定。</p><h3 id="分层与模块化"><a href="#分层与模块化" class="headerlink" title="分层与模块化"></a>分层与模块化</h3><p>前面我们也提到，模块化是构建复杂系统的常用手段。</p><p>对于像 Unix 这样的复杂系统，没有人能掌控所有的细节。之所以我们能开发出如此复杂的系统，并且能维护得了，最主要的原因就是将系统划分成各个独立的模块，比如进程调度、进程通信、内存管理、虚拟文件系统、网络接口等模块。不同的模块之间通过接口来进行通信，模块之间耦合很小，每个小的团队聚焦于一个独立的高内聚模块来开发，最终像搭积木一样，将各个模块组装起来，构建成一个超级复杂的系统。</p><p>除此之外，Unix、Linux 等大型系统之所以能做到几百、上千人有条不紊地协作开发，也归功于模块化做得好。不同的团队负责不同的模块开发，这样即便在不了解全部细节的情况下，管理者也能协调各个模块，让整个系统有效运转。</p><p>我们常说，计算机领域的任何问题都可以通过增加一个间接的中间层来解决，这本身就体现了分层的重要性。比如，Unix 系统也是基于分层开发的，它可以大致上分为三层，分别是内核、系统调用、应用层。每一层都对上层封装实现细节，暴露抽象的接口来调用。而且，任意一层都可以被重新实现，不会影响到其他层的代码。</p><p>面对复杂系统的开发，我们要善于应用分层技术，把容易复用、跟具体业务关系不大的代码，尽量下沉到下层，把容易变动、跟具体业务强相关的代码，尽量上移到上层。</p><h3 id="基于接口通信"><a href="#基于接口通信" class="headerlink" title="基于接口通信"></a>基于接口通信</h3><p>刚刚我们讲了分层、模块化，那不同的层之间、不同的模块之间，是如何通信的呢？一般来讲都是通过接口调用。在设计模块（module）或者层（layer）要暴露的接口的时候，我们要学会隐藏实现，接口从命名到定义都要抽象一些，尽量少涉及具体的实现细节。</p><p>比如，Unix 系统提供的 open() 文件操作函数，底层实现非常复杂，涉及权限控制、并发控制、物理存储，但我们用起来却非常简单。除此之外，因为 open() 函数基于抽象而非具体的实现来定义，所以我们在改动 open() 函数的底层实现的时候，并不需要改动依赖它的上层代码。</p><h3 id="高内聚、松耦合"><a href="#高内聚、松耦合" class="headerlink" title="高内聚、松耦合"></a>高内聚、松耦合</h3><p>高内聚、松耦合是一个比较通用的设计思想，内聚性好、耦合少的代码，能让我们在修改或者阅读代码的时候，聚集到在一个小范围的模块或者类中，不需要了解太多其他模块或类的代码，让我们的焦点不至于太发散，也就降低了阅读和修改代码的难度。而且，因为依赖关系简单，耦合小，修改代码不会牵一发而动全身，代码改动比较集中，引入 bug 的风险也就减少了很多。</p><p>实际上，刚刚讲到的很多方法，比如封装、抽象、分层、模块化、基于接口通信，都能有效地实现代码的高内聚、松耦合。反过来，代码的高内聚、松耦合，也就意味着，抽象、封装做到比较到位、代码结构清晰、分层和模块化合理、依赖关系简单，那代码整体的质量就不会太差。即便某个具体的类或者模块设计得不怎么合理，代码质量不怎么高，影响的范围也是非常有限的。我们可以聚焦于这个模块或者类做相应的小型重构。而相对于代码结构的调整，这种改动范围比较集中的小型重构的难度就小多了。</p><h3 id="为扩展而设计"><a href="#为扩展而设计" class="headerlink" title="为扩展而设计"></a>为扩展而设计</h3><p>越是复杂项目，越要在前期设计上多花点时间。提前思考项目中未来可能会有哪些功能需要扩展，提前预留好扩展点，以便在未来需求变更的时候，在不改动代码整体结构的情况下，轻松地添加新功能。</p><p>做到代码可扩展，需要代码满足开闭原则。特别是像 Unix 这样的开源项目，有 n 多人参与开发，任何人都可以提交代码到代码库中。代码满足开闭原则，基于扩展而非修改来添加新功能，最小化、集中化代码改动，避免新代码影响到老代码，降低引入 bug 的风险。</p><p>除了满足开闭原则，做到代码可扩展，我们前面也提到很多方法，比如封装和抽象，基于接口编程等。识别出代码可变部分和不可变部分，将可变部分封装起来，隔离变化，提供抽象化的不可变接口，供上层系统使用。当具体的实现发生变化的时候，我们只需要基于相同的抽象接口，扩展一个新的实现，替换掉老的实现即可，上游系统的代码几乎不需要修改。</p><h3 id="KISS-首要原则"><a href="#KISS-首要原则" class="headerlink" title="KISS 首要原则"></a>KISS 首要原则</h3><p>简单清晰、可读性好，是任何大型软件开发要遵循的首要原则。只要可读性好，即便扩展性不好，顶多就是多花点时间、多改动几行代码的事情。但是，如果可读性不好，连看都看不懂，那就不是多花时间可以解决得了的了。如果你对现有代码的逻辑似懂非懂，抱着尝试的心态去修改代码，引入 bug 的可能性就会很大。</p><p>不管是自己还是团队，在参与大型项目开发的时候，要尽量避免过度设计、过早优化，在扩展性和可读性有冲突的时候，或者在两者之间权衡，模棱两可的时候，应该选择遵循 KISS 原则，首选可读性。</p><h3 id="最小惊奇原则"><a href="#最小惊奇原则" class="headerlink" title="最小惊奇原则"></a>最小惊奇原则</h3><p>《Unix 编程艺术》一书中提到一个 Unix 的经典设计原则，叫“最小惊奇原则”，英文是“The Least Surprise Principle”。实际上，这个原则等同于“遵守开发规范”，意思是，在做设计或者编码的时候要遵守统一的开发规范，避免反直觉的设计。实际上，关于这一点，我们在前面的编码规范部分也讲到过。</p><p>遵从统一的编码规范，所有的代码都像一个人写出来的，能有效地减少阅读干扰。在大型软件开发中，参与开发的人员很多，如果每个人都按照自己的编码习惯来写代码，那整个项目的代码风格就会千奇百怪，这个类是这种编码风格，另一个类又是另外一种风格。在阅读的时候，我们要不停地切换去适应不同的编码风格，可读性就变差了。所以，对于大型项目的开发来说，我们要特别重视遵守统一的开发规范。</p><h2 id="如何提高代码质量"><a href="#如何提高代码质量" class="headerlink" title="如何提高代码质量"></a>如何提高代码质量</h2><h3 id="吹毛求疵般地执行编码规范"><a href="#吹毛求疵般地执行编码规范" class="headerlink" title="吹毛求疵般地执行编码规范"></a>吹毛求疵般地执行编码规范</h3><p>严格执行代码规范，可以使一个项目乃至整个公司的代码具有完全统一的风格，就像同一个人编写的。而且，命名良好的变量、函数、类和注释，也可以提高代码的可读性。编码规范不难掌握，关键是要严格执行。在 Code Review 时，我们一定要严格要求，看到不符合规范的代码，一定要指出并要求修改。</p><p>但是，据我了解，实际情况往往事与愿违。虽然大家都知道优秀的代码规范是怎样的，但在具体写代码的过程中，执行得却不好。我觉得，这种情况产生的主要原因还是不够重视。很多人会觉得，一个变量或者函数命名成啥样，关系并不大。所以命名时不推敲，注释也不写，Code Review 的时候也都一副事不关己的心态，觉得没必要太抠细节。日积月累，项目代码就会变得越来越差。所以我这里还是要强调一下，细节决定成败，代码规范的严格执行极为关键。</p><h3 id="编写高质量的单元测试"><a href="#编写高质量的单元测试" class="headerlink" title="编写高质量的单元测试"></a>编写高质量的单元测试</h3><p>单元测试是最容易执行且对提高代码质量见效最快的方法之一。高质量的单元测试不仅仅要求测试覆盖率要高，还要求测试的全面性，除了测试正常逻辑的执行之外，还要重点、全面地测试异常下的执行情况。毕竟代码出问题的地方大部分都发生在异常、边界条件下。</p><p>对于大型复杂项目，集成测试、黑盒测试都很难测试全面，因为组合爆炸，穷举所有测试用例的成本很高，几乎是不可能的。单元测试就是很好的补充。它可以在类、函数这些细粒度的代码层面，保证代码运行无误。底层细粒度的代码 bug 少了，组合起来构建而成的整个系统的 bug 也就相应的减少了。</p><h3 id="不流于形式的-Code-Review"><a href="#不流于形式的-Code-Review" class="headerlink" title="不流于形式的 Code Review"></a>不流于形式的 Code Review</h3><p>如果说很多工程师对单元测试不怎么重视，那对 Code Review 就是不怎么接受。我之前跟一些同行聊起 Code Review 的时候，很多人的反应是，这玩意儿不可能很好地执行，形式大于效果，纯粹是浪费时间。是的，即便 Code Review 做得再流畅，也是要花时间的。所以，在业务开发任务繁重的时候，Code Review 往往会流于形式、虎头蛇尾，效果确实不怎么好。</p><p>但我们并不能因此就否定 Code Review 本身的价值。在 Google、Facebook 等外企中，Code Review 应用得非常成功，已经成为了开发流程中不可或缺的一部分。所以，要想真正发挥 Code Review 的作用，关键还是要执行到位，不能流于形式。</p><h3 id="开发未动、文档先行"><a href="#开发未动、文档先行" class="headerlink" title="开发未动、文档先行"></a>开发未动、文档先行</h3><p>对大部分工程师来说，编写技术文档是件挺让人“反感”的事情。一般来讲，在开发某个系统或者重要模块或者功能之前，我们应该先写技术文档，然后，发送给同组或者相关同事审查，在审查没有问题的情况下再开发。这样能够保证事先达成共识，开发出来的东西不至于走样。而且，当开发完成之后，进行 Code Review 的时候，代码审查者通过阅读开发文档，也可以快速理解代码。</p><p>除此之外，对于团队和公司来讲，文档是重要的财富。对新人熟悉代码或任务的交接等，技术文档很有帮助。而且，作为一个规范化的技术团队，技术文档是一种摒弃作坊式开发和个人英雄主义的有效方法，是保证团队有效协作的途径。</p><h3 id="持续重构、重构、重构"><a href="#持续重构、重构、重构" class="headerlink" title="持续重构、重构、重构"></a>持续重构、重构、重构</h3><p>我个人比较反对平时不注重代码质量，堆砌烂代码，实在维护不了了就大刀阔斧地重构甚至重写。有的时候，因为项目代码太多，重构很难做到彻底，最后又搞出来一个四不像的怪物，这就更麻烦了！</p><p>优秀的代码或架构不是一开始就能设计好的，就像优秀的公司或产品也都是迭代出来的。我们无法 100% 预见未来的需求，也没有足够的精力、时间、资源为遥远的未来买单。所以，随着系统的演进，重构是不可避免的。</p><p>虽然我刚刚说不支持大刀阔斧、推倒重来式的大重构，但持续的小重构我还是比较提倡的。它也是时刻保证代码质量、防止代码腐化的有效手段。换句话说，不要等到问题堆得太多了再去解决，要时刻有人对代码整体质量负责任，平时没事就改改代码。千万不要觉得重构代码就是浪费时间，不务正业！</p><p>特别是一些业务开发团队，有时候为了快速完成一个业务需求，只追求速度，到处 hard code，在完全不考虑非功能性需求、代码质量的情况下，堆砌烂代码。实际上，这种情况还是比较常见的。不过没关系，等你有时间了，一定要记着重构，不然烂代码越堆越多，总有一天代码会变得无法维护。</p><h3 id="对项目与团队进行拆分"><a href="#对项目与团队进行拆分" class="headerlink" title="对项目与团队进行拆分"></a>对项目与团队进行拆分</h3><p>我们知道，团队人比较少，比如十几个人的时候，代码量不多，不超过 10 万行，怎么开发、怎么管理都没问题，大家互相都比较了解彼此做的东西。即便代码质量太差了，我们大不了把它重写一遍。但是，对于一个大型项目来说，参与开发的人员会比较多，代码量很大，有几十万、甚至几百万行代码，有几十、甚至几百号人同时开发维护，那研发管理就变得极其重要。</p><p>面对大型复杂项目，我们不仅仅需要对代码进行拆分，还需要对研发团队进行拆分。上一节课我们讲到了一些代码拆分的方法，比如模块化、分层等。同理，我们也可以把大团队拆成几个小团队。每个小团队对应负责一个小的项目（模块、微服务等），这样每个团队负责的项目包含的代码都不至于很多，也不至于出现代码质量太差无法维护的情况。</p>]]></content>
      
      
      <categories>
          
          <category> Architecture </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Clean Code》</title>
      <link href="/2021/03/11/clean-code/"/>
      <url>/2021/03/11/clean-code/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是整洁代码"><a href="#什么是整洁代码" class="headerlink" title="什么是整洁代码"></a>什么是整洁代码</h2><ul><li>能通过所有测试；</li><li>没有重复代码；</li><li>体现系统中的全部设计理念；</li><li>包括尽量少的实体，比如类、方法、函数等。</li></ul><h3 id="童子军军规"><a href="#童子军军规" class="headerlink" title="童子军军规"></a>童子军军规</h3><p>光把代码写好可不够。必须时时保持代码整洁。我们都见过代码随时间流逝而腐坏。我们应当更积极地阻止腐坏的发生。</p><p>让营地比你来时更干净。</p><h2 id="有意义的命名"><a href="#有意义的命名" class="headerlink" title="有意义的命名"></a>有意义的命名</h2><h3 id="名副其实"><a href="#名副其实" class="headerlink" title="名副其实"></a>名副其实</h3><p>名副其实说起来简单。我们想要强调，这事很严肃。选个好名字要花时间，但省下来的时间比花掉的多。<strong>注意命名，而且一旦发现有更好的名称，就换掉旧的</strong>。这么做，读你代码的人（包括你自己）都会更开心。</p><p>变量、函数或类的名称应该已经答复了所有的大问题。它该告诉你，它为什么会存在，它做什么事，应该怎么用。<strong>如果名称需要注释来补充，那就不算是名副其实</strong>。</p><h3 id="避免误导"><a href="#避免误导" class="headerlink" title="避免误导"></a>避免误导</h3><p>程序员必须避免留下掩藏代码本意的错误线索。应当避免使用与本意相悖的词。例如，hp、aix 和 sco 都不该用做变量名，因为它们都是 UNIX 平台或类 UNIX 平台的专有名称。即便你是在编写三角计算程序，hp 看起来是个不错的缩写，但那也可能会提供错误信息。</p><p>别用 accountList 来指称一组账号，除非它真的是 List 类型。List 一词对程序员有特殊意义。如果包纳账号的容器并非真是个 List，就会引起错误的判断。所以，用 accountGroup或 bunchOfAccounts，甚至直接用 accounts 都会好一些。</p><p>提防使用不同之处较小的名称。想区分模块中某处的 XYZControllerForEfficientHandlingOfStrings 和另一处的 XYZControllerForEfficientStorageOfStrings，会花多长时间呢？这两个词外形实在太相似了。</p><h3 id="做有意义的区分"><a href="#做有意义的区分" class="headerlink" title="做有意义的区分"></a>做有意义的区分</h3><p>废话都是冗余。Variable 一词永远不应当出现在变量名中。Table 一词永远不应当出现在表名中。NameString 会比 Name 好吗？难道 Name 会是一个浮点数不成？如果是这样，就触犯了关于误导的规则。设想有个名为 Customer 的类，还有一个名为 CustomerObject 的类。区别何在呢？哪一个是表示客户历史支付情况的最佳途径？</p><p>如果缺少明确约定，变量 moneyAmount 就与 money 没区别，customerInfo 与 customer没区别，accountData 与 account 没区别，theMessage 也与 message 没区别。要区分名称，就要以读者能鉴别不同之处的方式来区分。</p><h3 id="使用读得出来的名称"><a href="#使用读得出来的名称" class="headerlink" title="使用读得出来的名称"></a>使用读得出来的名称</h3><p>人类长于记忆和使用单词。大脑的相当一部分就是用来容纳和处理单词的。单词能读得出来。人类进化到大脑中有那么大的一块地方用来处理言语，若不善加利用，实在是种耻辱。如果名称读不出来，讨论的时候就会像个傻鸟。“哎，这儿，鼻涕阿三喜摁踢（bee cee arr three cee enn tee）上头，有个皮挨死极翘（pee ess zee kyew）3整数，看见没？”这不是小事，因为编程本就是一种社会活动。</p><p>有家公司，程序里面写了个 genymdhms（生成日期，年、月、日、时、分、秒），他们一般读作“gen why emm dee aich emm ess”4。我有个见字照读的恶习，于是开口就念“gen-yah-mudda-hims”。后来好些设计师和分析师都有样学样，听起来傻乎乎的。我们知道典故，所以会觉得很搞笑。搞笑归搞笑，实际是在强忍糟糕的命名。</p><h3 id="使用可搜索的名称"><a href="#使用可搜索的名称" class="headerlink" title="使用可搜索的名称"></a>使用可搜索的名称</h3><p>同样，e 也不是个便于搜索的好变量名。它是英文中最常用的字母，在每个程序、每段代码中都有可能出现。由此而见，长名称胜于短名称，搜得到的名称胜于用自造编码代写就的名称。窃以为单字母名称仅用于短方法中的本地变量。<strong>名称长短应与其作用域大小相对应</strong>[N5]。若变量或常量可能在代码中多处使用，则应赋其以便于搜索的名称。再比较</p><h3 id="类名"><a href="#类名" class="headerlink" title="类名"></a>类名</h3><p>类名和对象名应该是名词或名词短语，如 Customer、WikiPage、Account 和 AddressParser。避免使用 Manager、Processor、Data 或 Info 这样的类名。<strong>类名不应当是动词</strong>。</p><h3 id="方法名"><a href="#方法名" class="headerlink" title="方法名"></a>方法名</h3><p>方法名应当是动词或动词短语，如 postPayment、deletePage 或 save。属性访问器、修改器和断言应该根据其值命名，并依 Javabean 标准 加上 get、set 和 is 前缀。</p><h3 id="添加有意义的语境"><a href="#添加有意义的语境" class="headerlink" title="添加有意义的语境"></a>添加有意义的语境</h3><p>设想你有名为 firstName、lastName、street、houseNumber、city、state 和 zipcode 的变量。当它们搁一块儿的时候，很明确是构成了一个地址。不过，假使只是在某个方法中看见孤零零一个 state 变量呢？你会理所当然推断那是某个地址的一部分吗？</p><p>可以添加前缀 addrFirstName、addrLastName、addrState 等，以此提供语境。至少，读者会明白这些变量是某个更大结构的一部分。当然，更好的方案是创建名为 Address 的类。这样，即便是编译器也会知道这些变量隶属某个更大的概念了。</p><h3 id="不要添加没用的语境"><a href="#不要添加没用的语境" class="headerlink" title="不要添加没用的语境"></a>不要添加没用的语境</h3><p>设若有一个名为“加油站豪华版”（Gas Station Deluxe）的应用，在其中给每个类添加GSD 前缀就不是什么好点子。说白了，你是在和自己在用的工具过不去。输入 G，按下自动完成键，结果会得到系统中全部类的列表，列表恨不得有一英里那么长。这样做聪明吗？为什么要搞得 IDE 没法帮助你？</p><p><strong>只要短名称足够清楚，就要比长名称好。别给名称添加不必要的语境。</strong></p><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><h3 id="短小"><a href="#短小" class="headerlink" title="短小"></a>短小</h3><p><strong>函数的第一规则是要短小。第二条规则是还要更短小</strong>。我无法证明这个断言。我给不出任何证实了小函数更好的研究结果。我能说的是，近 40 年来，我写过各种不同大小的函数。我写过令人憎恶的长达 3000 行的厌物，也写过许多 100 行到 300 行的函数，我还写过 20 行 到 30 行的。经过漫长的试错，经验告诉我，函数就该小。</p><h3 id="只做一件事"><a href="#只做一件事" class="headerlink" title="只做一件事"></a>只做一件事</h3><p>函数应该做一件事。做好这件事。只做这一件事。</p><h3 id="每个函数一个抽象层级"><a href="#每个函数一个抽象层级" class="headerlink" title="每个函数一个抽象层级"></a>每个函数一个抽象层级</h3><p>要确保函数只做一件事，函数中的语句都要在同一抽象层级上。一眼就能看出，代码清单 3-1 违反了这条规矩。那里面有 getHtml( )等位于较高抽象层的概念，也有 String pagePathName = PathParser.render(pagePath)等位于中间抽象层的概念，还有.append(“\n”)等位于相当低的抽象层的概念。</p><h4 id="自顶向下读代码：向下规则"><a href="#自顶向下读代码：向下规则" class="headerlink" title="自顶向下读代码：向下规则"></a>自顶向下读代码：向下规则</h4><p>我们想要让代码拥有自顶向下的阅读顺序。我们想要让每个函数后面都跟着位于下一抽象层级的函数，这样一来，在查看函数列表时，就能偱抽象层级向下阅读了。我把这叫做向下规则。</p><h3 id="switch"><a href="#switch" class="headerlink" title="switch"></a>switch</h3><p>写出短小的 switch 语句很难 。即便是只有两种条件的 switch 语句也要比我想要的单个代码块或函数大得多。写出只做一件事的 switch 语句也很难。Switch 天生要做 N 件事。不幸我们总无法避开 switch 语句，不过还是能够确保每个 switch 都埋藏在较低的抽象层级，而且永远不重复。当然，我们利用多态来实现这一点。</p><h3 id="函数参数"><a href="#函数参数" class="headerlink" title="函数参数"></a>函数参数</h3><p>最理想的参数数量是零（零参数函数），其次是一（单参数函数），再次是二（双参数函数），应尽量避免三（三参数函数）。有足够特殊的理由才能用三个以上参数（多参数函数）—所以无论如何也不要这么做。</p><h3 id="标识参数"><a href="#标识参数" class="headerlink" title="标识参数"></a>标识参数</h3><p>标识参数丑陋不堪。向函数传入布尔值简直就是骇人听闻的做法。这样做，方法签名立刻变得复杂起来，大声宣布本函数不止做一件事。如果标识为 true 将会这样做，标识为 false则会那样做！在代码清单 3-7 中，我们别无选择，因为调用者已经传入了那个标识，而我想把重构范围限制在该函数及该函数以下范围之内。方法调用 render(true)对于可怜的读者来说仍然摸不着头脑。卷动屏幕，看到 render(Boolean isSuite)，稍许有点帮助，不过仍然不够。应该把该函数一分为二：reanderForSuite( )和 renderForSingleTest( )。</p><h3 id="动词与关键字"><a href="#动词与关键字" class="headerlink" title="动词与关键字"></a>动词与关键字</h3><p>给函数取个好名字，能较好地解释函数的意图，以及参数的顺序和意图。对于一元函数，函数和参数应当形成一种非常良好的动词/名词对形式。例如，write(name)就相当令人认同。不管这个“name”是什么，都要被“write”。更好的名称大概是 writeField(name)，它告诉我们，“name”是一个“field”。</p><p>最后那个例子展示了函数名称的关键字（keyword）形式。使用这种形式，我们把参数的名称编码成了函数名。例如，assertEqual 改成 assertExpectedEqualsActual(expected, actual)可能会好些。这大大减轻了记忆参数顺序的负担。</p><h2 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h2><p>注释并不像辛德勒的名单。它们并不“纯然地好”。实际上，注释最多也就是一种必须的恶。若编程语言足够有表达力，或者我们长于用这些语言来表达意图，就不那么需要注释—也许根本不需要。</p><p>注释的恰当用法是弥补我们在用代码表达意图时遭遇的失败。注意，我用了“失败”一词。我是说真的。<strong>注释总是一种失败。我们总无法找到不用注释就能表达自我的方法，所以总要有注释，这并不值得庆贺</strong>。</p><p>如果你发现自己需要写注释，再想想看是否有办法翻盘，用代码来表达。每次用代码表达，你都该夸奖一下自己。每次写注释，你都该做个鬼脸，感受自己在表达能力上的失败。</p><p>我为什么要极力贬低注释？因为注释会撒谎。也不是说总是如此或有意如此，但出现得实在太频繁。注释存在的时间越久，就离其所描述的代码越远，越来越变得全然错误。原因很简单。程序员不能坚持维护注释。</p><h3 id="注释不能美化糟糕的代码"><a href="#注释不能美化糟糕的代码" class="headerlink" title="注释不能美化糟糕的代码"></a>注释不能美化糟糕的代码</h3><p>写注释的常见动机之一是糟糕的代码的存在。我们编写一个模块，发现它令人困扰、乱七八糟。我们知道，它烂透了。我们告诉自己：“喔，最好写点注释！”不！最好是把代码弄干净！</p><p>带有少量注释的整洁而有表达力的代码，要比带有大量注释的零碎而复杂的代码像样得多。与其花时间编写解释你搞出的糟糕的代码的注释，不如花时间清洁那堆糟糕的代码。</p><h3 id="好的注释场景"><a href="#好的注释场景" class="headerlink" title="好的注释场景"></a>好的注释场景</h3><ol><li>法律信息，有时，公司代码规范要求编写与法律有关的注释。例如，版权及著作权声明就是必须和有理由在每个源文件开头注释处放置的内容。</li><li>用注释来提供基本信息也有其用处。</li><li>对意图的解释,注释不仅提供了有关实现的有用信息，而且还提供了某个决定后面的意图。</li><li>阐释, 注释把某些晦涩难明的参数或返回值的意义翻译为某种可读形式，也会是有用的。通常，更好的方法是尽量让参数或返回值自身就足够清楚；但如果参数或返回值是某个标准库的一部分，或是你不能修改的代码，帮助阐释其含义的代码就会有用。</li><li>警示， 用于警告其他程序员会出现某种后果的注释也是有用的。</li><li>TODO 注释，有时，有理由用//TODO 形式在源代码中放置要做的工作列表。</li><li>放大，注释可以用来放大某种看来不合理之物的重要性。</li></ol><h3 id="坏注释场景"><a href="#坏注释场景" class="headerlink" title="坏注释场景"></a>坏注释场景</h3><ol><li>喃喃自语，如果只是因为你觉得应该或者因为过程需要就添加注释，那就是无谓之举。如果你决定写注释，就要花必要的时间确保写出最好的注释。</li><li>多余的注释，读这段注释花的时间没准比读代码花的时间还要长。</li><li>误导性注释，尽管初衷可嘉，程序员还是会写出不够精确的注释。</li><li>循规式注释，所谓每个函数都要有 Javadoc 或每个变量都要有注释的规矩全然是愚蠢可笑的。这类注释徒然让代码变得散乱，满口胡言，令人迷惑不解。</li><li>有人会在每次编辑代码时，在模块开始处添加一条注释。这类注释就像是一种记录每次修改的日志。我见过满篇尽是这类日志的代码模块。</li><li>废话注释，有时，你会看到纯然是废话的注释。它们对于显然之事喋喋不休，毫无新意。/** The day of the month. */ private int dayOfMonth;</li><li>可怕的废话，同上</li><li>能用函数或变量时就别用注释</li><li>位置标记，有时，程序员喜欢在源代码中标记某个特别位置。例如，最近我在程序中看到这样一行：// Actions //////////////////////////////////把特定函数趸放在这种标记栏下面，多数时候实属无理。鸡零狗碎，理当删除—特别是尾部那一长串无用的斜杠。</li><li>括号后面的注释，有时，程序员会在括号后面放置特殊的注释，如代码清单 4-6 所示。尽管这对于含有深度嵌套结构的长函数可能有意义，但只会给我们更愿意编写的短小、封装的函数带来混乱。如果你发现自己想标记右括号，其实应该做的是缩短函数。</li><li>归属与署名，源代码控制系统非常善于记住是谁在何时添加了什么。没必要用那些小小的签名搞脏代码。你也许会认为，这种注释大概有助于他人了解应该和谁讨论这段代码。不过，事实却是注释在那儿放了一年又一年，越来越不准确，越来越和原作者没关系。</li><li>注释掉的代码，直接把代码注释掉是讨厌的做法。别这么干！其他人不敢删除注释掉的代码。他们会想，代码依然放在那儿，一定有其原因，而且这段代码很重要，不能删除。注释掉的代码堆积在一起，就像破酒瓶底的渣滓一般。</li><li>HTML 注释，源代码注释中的 HTML 标记是一种厌物，如你在下面代码中所见。编辑器/IDE 中的代码本来易于阅读，却因为 HTML 注释的存在而变得难以卒读。如果注释将由某种工具（例如Javadoc）抽取出来，呈现到网页，那么该是工具而非程序员来负责给注释加上合适的 HTML标签。</li><li>非本地信息，假如你一定要写注释，请确保它描述了离它最近的代码。别在本地注释的上下文环境中给出系统级的信息。以下面的 Javadoc 注释为例，除了那可怕的冗余之外，它还给出了有关默认端口的信息。不过该函数完全没控制到那个所谓默认值。这个注释并未描述该函数，而是在描述系统中远在他方的其他函数。当然，也无法担保在包含那个默认值的代码修改之后，这里的注释也会跟着修改。</li><li>信息过多，别在注释中添加有趣的历史性话题或者无关的细节描述。下列注释来自某个用来测试base64 编解码函数的模块。除了 RFC 文档编号之外，注释中的其他细节信息对于读者完全没有必要。</li><li>不明显的联系，注释及其描述的代码之间的联系应该显而易见。如果你不嫌麻烦要写注释，至少让读者能看着注释和代码，并且理解注释所谈何物。</li><li>短函数不需要太多描述。为只做一件事的短函数选个好名字，通常要比写函数头注释要好。</li><li>非公共代码中的 Javadoc，虽然 Javadoc 对于公共 API 非常有用，但对于不打算作公共用途的代码就令人厌恶了。为系统中的类和函数生成 Javadoc 页并非总有用，而 Javadoc 注释额外的形式要求几乎等同于八股文章。</li></ol><h2 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h2><h3 id="垂直格式"><a href="#垂直格式" class="headerlink" title="垂直格式"></a>垂直格式</h3><p>对我们来说，这意味着什么？意味着有可能用大多数为 200 行、最长 500 行的单个文件<br>构造出色的系统（FitNesse 总长约 50000 行）。尽管这并非不可违背的原则，也应该乐于接受。<br>短文件通常比长文件易于理解</p><h3 id="向报纸学习"><a href="#向报纸学习" class="headerlink" title="向报纸学习"></a>向报纸学习</h3><p>源文件也要像报纸文章那样。名称应当简单且一目了然。名称本身应该足够告诉我们是否在正确的模块中。源文件最顶部应该给出高层次概念和算法。细节应该往下渐次展开，直至找到源文件中最底层的函数和细节。</p><h3 id="垂直距离"><a href="#垂直距离" class="headerlink" title="垂直距离"></a>垂直距离</h3><p>关系密切的概念应该互相靠近[G10]。显然，这条规则并不适用于分布在不同文件中的概念。除非有很好的理由，否则就不要把关系密切的概念放到不同的文件中。实际上，这也是避免使用 protected 变量的理由之一。</p><p>变量声明。变量声明应尽可能靠近其使用位置。因为函数很短，本地变量应该在函数的顶部出现，就像 Junit4.3.1 中这个稍长的函数中那样。</p><h3 id="垂直顺序"><a href="#垂直顺序" class="headerlink" title="垂直顺序"></a>垂直顺序</h3><p>一般而言，我们想自上向下展示函数调用依赖顺序。也就是说，被调用的函数应该放在执行调用的函数下面。这样就建立了一种自顶向下贯穿源代码模块的良好信息流。</p><h3 id="横向格式"><a href="#横向格式" class="headerlink" title="横向格式"></a>横向格式</h3><p>我一向遵循无需拖动滚动条到右边的原则。但近年来显示器越来越宽，而年轻程序员又能将显示字符缩小到如此程度，屏幕上甚至能容纳 200 个字符的宽度。别那么做。我个人的上限是 120 个字符。</p><h3 id="水平对齐"><a href="#水平对齐" class="headerlink" title="水平对齐"></a>水平对齐</h3><p>如今，我更喜欢用不对齐的声明和赋值，如下所示，因为它们指出了重点。如果有较长的列表需要做对齐处理，那问题就是在列表的长度上而不是对齐上。</p><h3 id="空范围"><a href="#空范围" class="headerlink" title="空范围"></a>空范围</h3><p>有时，while 或 for 语句的语句体为空，如下所示。我不喜欢这种结构，尽量不使用。如果无法避免，就确保空范围体的缩进，用括号包围起来。</p><h2 id="对象和数据结构"><a href="#对象和数据结构" class="headerlink" title="对象和数据结构"></a>对象和数据结构</h2><h3 id="得墨忒耳律"><a href="#得墨忒耳律" class="headerlink" title="得墨忒耳律"></a>得墨忒耳律</h3><p>著名的得墨忒耳律（The Law of Demeter）认为，模块不应了解它所操作对象的内部情形。如上节所见，对象隐藏数据，曝露操作。这意味着对象不应通过存取器曝露其内部结构，因为这样更像是曝露而非隐藏其内部结构。</p><h2 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h2><h3 id="TDD-三定律"><a href="#TDD-三定律" class="headerlink" title="TDD 三定律"></a>TDD 三定律</h3><p>谁都知道 TDD 要求我们在编写生产代码前先编写单元测试。但这条规则只是冰山之巅。看看下列三定律 ：</p><ul><li>定律一 在编写不能通过的单元测试前，不可编写生产代码。</li><li>定律二 只可编写刚好无法通过的单元测试，不能编译也算不通过。</li><li>定律三 只可编写刚好足以通过当前失败测试的生产代码。</li></ul><p>这三条定律将你限制在大概 30 秒一个的循环中。测试与生产代码一起编写，测试只比生产代码早写几秒钟。</p><h3 id="保持测试整洁"><a href="#保持测试整洁" class="headerlink" title="保持测试整洁"></a>保持测试整洁</h3><p>测试代码和生产代码一样重要。它可不是二等公民。它需要被思考、被设计和被照料。它该像生产代码一般保持整洁。</p><h3 id="整洁的测试"><a href="#整洁的测试" class="headerlink" title="整洁的测试"></a>整洁的测试</h3><p>整洁的测试有什么要素？有三个要素：可读性，可读性和可读性。在单元测试中，可读性甚至比在生产代码中还重要。测试如何才能做到可读？和其他代码中一样：明确，简洁，还有足够的表达力。在测试中，你要以尽可能少的文字表达大量内容。</p><h3 id="F-I-R-S-T"><a href="#F-I-R-S-T" class="headerlink" title="F.I.R.S.T."></a>F.I.R.S.T.</h3><p>整洁的测试还遵循以下 5 条规则，这 5 条规则的首字母构成了本节标题：</p><ul><li>快速（Fast） 测试应该够快。测试应该能快速运行。测试运行缓慢，你就不会想要频繁地运行它。如果你不频繁运行测试，就不能尽早发现问题，也无法轻易修正，从而也不能轻而易举地清理代码。最终，代码就会腐坏。</li><li>独立（Independent） 测试应该相互独立。某个测试不应为下一个测试设定条件。你应该可以单独运行每个测试，及以任何顺序运行测试。当测试互相依赖时，头一个没通过就会导致一连串的测试失败，使问题诊断变得困难，隐藏了下级错误。</li><li>可重复（Repeatable） 测试应当可在任何环境中重复通过。你应该能够在生产环境、质检环境中运行测试，也能够在无网络的列车上用笔记本电脑运行测试。如果测试不能在任意环境中重复，你就总会有个解释其失败的接口。当环境条件不具备时，你也会无法运行<br>测试。</li><li>自足验证（Self-Validating） 测试应该有布尔值输出。无论是通过或失败，你不应该查看日志文件来确认测试是否通过。你不应该手工对比两个不同文本文件来确认测试是否通过。如果测试不能自足验证，对失败的判断就会变得依赖主观，而运行测试也需要更长的手工操作时间。</li><li>及时（Timely） 测试应及时编写。单元测试应该恰好在使其通过的生产代码之前编写。如果在编写生产代码之后编写测试，你会发现生产代码难以测试。你可能会认为某些生产代码本身难以测试。你可能不会去设计可测试的代码。</li></ul><h2 id="类"><a href="#类" class="headerlink" title="类"></a>类</h2><h3 id="类的组织"><a href="#类的组织" class="headerlink" title="类的组织"></a>类的组织</h3><p>遵循标准的 Java 约定，类应该从一组变量列表开始。如果有公共静态常量，应该先出现。然后是私有静态变量，以及私有实体变量。很少会有公共变量。</p><p>公共函数应跟在变量列表之后。我们喜欢把由某个公共函数调用的私有工具函数紧随在该公共函数后面。这符合了自顶向下原则，让程序读起来就像一篇报纸文章。</p><h3 id="类应该短小"><a href="#类应该短小" class="headerlink" title="类应该短小"></a>类应该短小</h3><p>关于类的第一条规则是类应该短小。第二条规则是还要更短小。不，我们并不是要重弹“函数”一章的论调。就像函数一样，在设计类时，首要规条就是要更短小。</p><h3 id="单一权责原则"><a href="#单一权责原则" class="headerlink" title="单一权责原则"></a>单一权责原则</h3><p>认为，类或模块应有且只有一条加以修改的理由。该原则既给出了权责的定义，又是关于类的长度的指导方针。类只应有一个权责—只有一条修改的理由。</p><h3 id="内聚"><a href="#内聚" class="headerlink" title="内聚"></a>内聚</h3><p>类应该只有少量实体变量。类中的每个方法都应该操作一个或多个这种变量。通常而言，方法操作的变量越多，就越黏聚到类上。如果一个类中的每个变量都被每个方法所使用，则该类具有最大的内聚性。</p><h3 id="保持内聚性就会得到许多短小的类"><a href="#保持内聚性就会得到许多短小的类" class="headerlink" title="保持内聚性就会得到许多短小的类"></a>保持内聚性就会得到许多短小的类</h3><p>对于多数系统，修改将一直持续。每处修改都让我们冒着系统其他部分不能如期望般工作的风险。在整洁的系统中，我们对类加以组织，以降低修改的风险。</p><h3 id="隔离修改"><a href="#隔离修改" class="headerlink" title="隔离修改"></a>隔离修改</h3><p>需求会改变，所以代码也会改变。我们学习到，具体类包含实现细节（代码），而抽象类则只呈现概念。依赖于具体细节的客户类，当细节改变时，就会有风险。我们可以借助接口和抽象类来隔离这些细节带来的影响。</p><h2 id="系统"><a href="#系统" class="headerlink" title="系统"></a>系统</h2><h3 id="将系统的构造与使用分开"><a href="#将系统的构造与使用分开" class="headerlink" title="将系统的构造与使用分开"></a>将系统的构造与使用分开</h3><p>软件系统应将启始过程和启始过程之后的运行时逻辑分离开，在启始过程中构建应用对象，也会存在互相缠结的依赖关系。</p><h3 id="工厂"><a href="#工厂" class="headerlink" title="工厂"></a>工厂</h3><p>使用工厂分离构造过程</p><h3 id="依赖注入"><a href="#依赖注入" class="headerlink" title="依赖注入"></a>依赖注入</h3><p>有一种强大的机制可以实现分离构造与使用，那就是依赖注入（Dependency Injection，DI），控制反转（Inversion of Control，IoC）在依赖管理中的一种应用手段。控制反转将第二权责从对象中拿出来，转移到另一个专注于此的对象中，从而遵循了单一权责原则。在依赖管理情景中，对象不应负责实体化对自身的依赖。反之，它应当将这份权责移交给其他“有权力”的机制，从而实现控制的反转。因为初始设置是一种全局问题，这种授权机制通常要么是 main 例程，要么是有特定目的的容器。</p><h2 id="并发编程"><a href="#并发编程" class="headerlink" title="并发编程"></a>并发编程</h2><h3 id="为什么要并发"><a href="#为什么要并发" class="headerlink" title="为什么要并发"></a>为什么要并发</h3><p>并发是一种解耦策略。它帮助我们把做什么（目的）和何时（时机）做分解开。在单线程应用中，目的与时机紧密耦合，很多时候只要查看堆栈追踪即可断定应用程序的状态。调试这种系统的程序员可以设定断点或者断点序列，通过查看到达哪个断点来了解系统状态。</p><p>解耦目的与时机能明显地改进应用程序的吞吐量和结构。从结构的角度来看，应用程序看起来更像是许多台协同工作的计算机，而不是一个大循环。系统因此会更易于被理解，给出了许多切分关注面的有力手段。</p><h3 id="保持同步区域微小"><a href="#保持同步区域微小" class="headerlink" title="保持同步区域微小"></a>保持同步区域微小</h3><p>关键字 synchronized 制造了锁。同一个锁维护的所有代码区域在任一时刻保证只有一个线程执行。锁是昂贵的，因为它们带来了延迟和额外开销。所以我们不愿将代码扔给 synchronized语句了事。另一方面，临界区应该被保护起来。所以，应该尽可能少地设计临界区。</p><h2 id="味道与启发"><a href="#味道与启发" class="headerlink" title="味道与启发"></a>味道与启发</h2><h3 id="注释-1"><a href="#注释-1" class="headerlink" title="注释"></a>注释</h3><h4 id="不恰当的信息"><a href="#不恰当的信息" class="headerlink" title="不恰当的信息"></a>不恰当的信息</h4><p>让注释传达本该更好地在源代码控制系统、问题追踪系统或任何其他记录系统中保存的信息，是不恰当的。例如，修改历史记录只会用大量过时而无趣的文本搞乱源代码文件。通常，作者、最后修改时间、SPR 数等元数据不该在注释中出现。注释只应该描述有关代码和设计的技术性信息。</p><h4 id="废弃的注释"><a href="#废弃的注释" class="headerlink" title="废弃的注释"></a>废弃的注释</h4><p>过时、无关或不正确的注释就是废弃的注释。注释会很快过时。最好别编写将被废弃的注释。如果发现废弃的注释，最好尽快更新或删除掉。废弃的注释会远离它们曾经描述的代码，变成代码中无关和误导的浮岛。</p><h4 id="冗余注释"><a href="#冗余注释" class="headerlink" title="冗余注释"></a>冗余注释</h4><p>如果注释描述的是某种充分自我描述了的东西，那么注释就是多余的。例如：</p><pre><code>i++; // increment i</code></pre><p>另一个例子是除函数签名之外什么也没多说（或少说）的 Javadoc</p><pre><code>/*** @param sellRequest* @return* @throws ManagedComponentException*/public SellResponse beginSellItem(SellRequest sellRequest) throws ManagedComponentException</code></pre><h4 id="糟糕的注释"><a href="#糟糕的注释" class="headerlink" title="糟糕的注释"></a>糟糕的注释</h4><p>值得编写的注释，也值得好好写。如果要编写一条注释，就花时间保证写出最好的注释。字斟句酌。使用正确的语法和拼写。别闲扯，别画蛇添足，保持简洁。</p><h4 id="注释掉的代码"><a href="#注释掉的代码" class="headerlink" title="注释掉的代码"></a>注释掉的代码</h4><p>看到被注释掉的代码会令我抓狂。谁知道它有多旧？谁知道它有没有意义？没人会删除它，因为大家都假设别人需要它或是有进一步计划。</p><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><h4 id="需要多步才能实现的构建"><a href="#需要多步才能实现的构建" class="headerlink" title="需要多步才能实现的构建"></a>需要多步才能实现的构建</h4><p>构建系统应该是单步的小操作。不应该从源代码控制系统中一小点一小点签出代码。不应该需要一系列神秘指令或环境依赖脚本来构建单个元素。不应该四处寻找额外的小JAR、XML 文件和其他系统所需的杂物。你应当能够用单个命令签出系统，并用单个指令构建它。</p><pre><code>svn get mySystem cd mySystemant all</code></pre><h4 id="需要多步才能做到的测试"><a href="#需要多步才能做到的测试" class="headerlink" title="需要多步才能做到的测试"></a>需要多步才能做到的测试</h4><p>你应当能够发出单个指令就可以运行全部单元测试。能够运行全部测试是如此基础和重要，应该快速、轻易和直截了当地做到。</p><h3 id="函数-1"><a href="#函数-1" class="headerlink" title="函数"></a>函数</h3><h4 id="过多的参数"><a href="#过多的参数" class="headerlink" title="过多的参数"></a>过多的参数</h4><p>函数的参数量应该少。没参数最好，一个次之，两个、三个再次之。三个以上的参数非常值得质疑，应坚决避免。</p><h4 id="输出参数"><a href="#输出参数" class="headerlink" title="输出参数"></a>输出参数</h4><p>输出参数违反直觉。读者期望参数用于输入而非输出。如果函数非要修改什么东西的状态不可，就修改它所在对象的状态好了。</p><h4 id="标识参数-1"><a href="#标识参数-1" class="headerlink" title="标识参数"></a>标识参数</h4><p>布尔值参数大声宣告函数做了不止一件事。它们令人迷惑，应该消灭掉。</p><h4 id="死函数"><a href="#死函数" class="headerlink" title="死函数"></a>死函数</h4><p>永不被调用的方法应该丢弃。保留死代码纯属浪费。别害怕删除函数。记住，源代码控制系统还会记得它。</p><h3 id="一般性问题"><a href="#一般性问题" class="headerlink" title="一般性问题"></a>一般性问题</h3><h4 id="一个源文件中存在多种语言"><a href="#一个源文件中存在多种语言" class="headerlink" title="一个源文件中存在多种语言"></a>一个源文件中存在多种语言</h4><p>当今的现代编程环境允许在单个源文件中存在多种不同语言。例如，Java 源文件可能还包括 XML、HTML、YAML、JavaDoc、英文、JavaScript 等语言。另例，JSP 文件可能还包括 HTML、Java、标签库语法、英文注释、Javadoc、XML、JavaScript 等。往好处说是令人迷惑，往坏处说就是粗心大意、驳杂不精。</p><p>理想的源文件包括且只包括一种语言。现实上，我们可能会不得不使用多于一种语言。但应该尽力减少源文件中额外语言的数量和范围。</p><h4 id="明显的行为未被实现"><a href="#明显的行为未被实现" class="headerlink" title="明显的行为未被实现"></a>明显的行为未被实现</h4><p>遵循“最小惊异原则”（The Principle of Least Surprise），函数或类应该实现其他程序员有理由期待的行为。例如，考虑一个将日期名称翻译为表示该日期的枚举的函数。</p><pre><code>Day day = DayDate.StringToDay(String dayName);</code></pre><p>我们期望字符串 Monday 翻译为 Day.MONDAY。我们也期望常用缩写形式也能被翻译出来，我们还期待函数忽略大小写。<br>如果明显的行为未被实现，读者和用户就不能再依靠他们对函数名称的直觉。他们不再信任原作者，不得不阅读代码细节。</p><h4 id="不正确的边界行为"><a href="#不正确的边界行为" class="headerlink" title="不正确的边界行为"></a>不正确的边界行为</h4><p>代码应该有正确行为，这话看似明白。问题是我们很少能明白正确行为有多复杂。开发者常常写出他们以为能工作的函数，信赖自己的直觉，而不是努力去证明代码在所有的角落和边界情形下真能工作。</p><p>没什么可以替代谨小慎微。每种边界条件、每种极端情形、每个异常都代表了某种可能搞乱优雅而直白的算法的东西。别依赖直觉。追索每种边界条件，并编写测试。</p><h4 id="忽视安全"><a href="#忽视安全" class="headerlink" title="忽视安全"></a>忽视安全</h4><p>切尔诺贝利核电站崩塌了，因为电厂经理一条又一条地忽视了安全机制。遵守安全就不便于做试验。结果就是试验未能运行，全世界都目睹首个民用核电站大灾难。忽</p><p>视安全相当危险。手工控制 serialVersionUID 可能有必要，但总会有风险。关闭某些编译器警告（或者全部警告！）可能有助于构建成功，但也存在陷于无穷无尽的调试的风险。关闭失败测试、告诉自己过后再处理，这和假装刷信用卡不用还钱一样坏。</p><h4 id="重复"><a href="#重复" class="headerlink" title="重复"></a>重复</h4><p>有一条本书提到的最重要的规则之一，你应该非常严肃地对待。实际上，每位编写有关软件设计的作者都提到这条规则。Dave Thomas 和 Andy Hunt 称之为 DRY 原则（Don’t Repeat Yourself，别重复自己）1。Kent Beck 将它列为极限编程核心原则之一，并称之为“一次，也只一次” 。Ron Jeffries 将这条规则列在第二位，地位只低于通过所有测试。</p><p>每次看到重复代码，都代表遗漏了抽象。重复的代码可能成为子程序或干脆是另一个类。将重复代码叠放进类似的抽象，增加了你的设计语言的词汇量。其他程序员可以用到你创建的抽象设施。编码变得越来越快，错误越来越少，因为你提升了抽象层级。</p><p>重复最明显的形态是你不断看到明显一样的代码，就像是某位程序员疯狂地用鼠标不断复制粘贴代码。可以用单一方法来替代之。</p><p>较隐蔽的形态是在不同模块中不断重复出现、检测同一组条件的 switch/case 或 if/else 链。<br>可以用多态来替代之。</p><p>更隐蔽的形态是采用类似算法但具体代码行不同的模块。这也是一种重复，可以使用模板方法模式或策略模式来修正。</p><p>的确，过去 15 年内出现的多数设计模式都是消除重复的有名手段。考德范式（Codd Normal Forms）是消除数据库规划中的重复的策略。OO 自身也是组织模块和消除重复的策略。毫不出奇，结构化编程也是。</p><p>重点已经在那里了。尽可能找到并消除重复。</p><h4 id="在错误的抽象层级上的代码"><a href="#在错误的抽象层级上的代码" class="headerlink" title="在错误的抽象层级上的代码"></a>在错误的抽象层级上的代码</h4><p>创建分离较高层级一般性概念与较低层级细节概念的抽象模型，这很重要。有时，我们创建抽象类来容纳较高层级概念，创建派生类来容纳较低层次概念。这样做的时候，需要确保分离完整。所有较低层级概念放在派生类中，所有较高层级概念放在基类中。</p><p>例如，只与细节实现有关的常量、变量或工具函数不应该在基类中出现。基类应该对这些东西一无所知。</p><p>这条规则对于源文件、组件和模块也适用。良好的软件设计要求分离位于不同层级的概念，将它们放到不同容器中。有时，这些容器是基类或派生类，有时是源文件、模块或组件。无论哪种情况，分离都要完整。较低层级概念和较高层级概念不应混杂在一起。看看下面的代码：</p><pre><code>public interface Stack {    Object pop() throws EmptyException;    void push(Object o) throws FullException;    double percentFull();    class EmptyException extends Exception {}    class FullException extends Exception {}}</code></pre><h4 id="基类依赖于派生类"><a href="#基类依赖于派生类" class="headerlink" title="基类依赖于派生类"></a>基类依赖于派生类</h4><p>将概念分解到基类和派生类的最普遍的原因是较高层级基类概念可以不依赖于较低层级派生类概念。这样，如果看到基类提到派生类名称，就可能发现了问题。通常来说，基类对派生类应该一无所知。</p><h4 id="信息过多"><a href="#信息过多" class="headerlink" title="信息过多"></a>信息过多</h4><p>设计良好的模块有着非常小的接口，让你能事半功倍。设计低劣的模块有着广阔、深入的接口，你不得不事倍功半。设计良好的接口并不提供许多需要依靠的函数，所以耦合度也较低。设计低劣的借口提供大量你必须调用的函数，耦合度较高。</p><h4 id="死代码"><a href="#死代码" class="headerlink" title="死代码"></a>死代码</h4><p>死代码就是不执行的代码。可以在检查不会发生的条件的 if 语句体中找到。可以在从不抛出异常的 try 语句的 catch 块中找到。可以在从不被调用的小工具方法中找到，也可以在永不会发生的 switch/case 条件中找到。</p><p>死代码的问题是过不久它就会发出臭味。时间越久，味道就越酸臭。这是因为，在设计改变时，死代码不会随之更新。它还能通过编译，但并不会遵循较新的约定或规则。它编写的时候，系统是另一番模样。如果你找到死代码，就体面地埋葬它，将它从系统中删除掉。</p><h4 id="垂直分隔"><a href="#垂直分隔" class="headerlink" title="垂直分隔"></a>垂直分隔</h4><p>变量和函数应该在靠近被使用的地方定义。本地变量应该正好在其首次被使用的位置上面声明，垂直距离要短。本地变量不该在其被使用之处几百行以外声明。</p><p>私有函数应该刚好在其首次被使用的位置下面定义。私有函数属于整个类，但我们还是要限制调用和定义之间的垂直距离。找个私有函数，应该只是从其首次被使用处往下看一点那么简单。</p><h4 id="前后不一致"><a href="#前后不一致" class="headerlink" title="前后不一致"></a>前后不一致</h4><p>从一而终。这可以追溯到最小惊异原则。小心选择约定，一旦选中，就小心持续遵循。</p><p>如果在特定函数中用名为 response 的变量来持有 HttpServletResponse 对象，则在其他用到 HttpServletResponse 对象的函数中也用同样的变量名。 如果将某个方法命名为processVerificationRequest，则给处理其他请求类型的方法取类似的名字，例如 processDeletionRequest。</p><p>如此简单的前后一致，一旦坚决贯彻，就能让代码更加易于阅读和修改。</p><h4 id="混淆视听"><a href="#混淆视听" class="headerlink" title="混淆视听"></a>混淆视听</h4><p>没有实现的默认构造器有何用处呢？它只会用无意义的杂碎搞乱对代码的理解。没有用到的变量，从不调用的函数，没有信息量的注释，等等，这些都是应该移除的废物。保持源文件整洁，良好地组织，不被搞乱。</p><h4 id="人为耦合"><a href="#人为耦合" class="headerlink" title="人为耦合"></a>人为耦合</h4><p>不互相依赖的东西不该耦合。例如，普通的 enum 不应在特殊类中包括，因为这样一来应用程序就要了解这些更为特殊的类。对于在特殊类中声明一般目的的 static 函数也是如此。</p><p>一般来说，人为耦合是指两个没有直接目的之间的模块的耦合。其根源是将变量、常量或函数不恰当地放在临时方便的位置。这是种漫不经心的偷懒行为。</p><p>花点时间研究应该在什么地方声明函数、常量和变量。不要为了方便随手放置，然后置之不理。</p><h4 id="特性依恋"><a href="#特性依恋" class="headerlink" title="特性依恋"></a>特性依恋</h4><p>这是 Martin Fowler 提出的代码味道之一 。类的方法只应对其所属类中的变量和函数感兴趣，不该垂青其他类中的变量和函数。当方法通过某个其他对象的访问器和修改器来操作该对象内部数据，则它就依恋于该对象所属类的范围。它期望自己在那个类里面，这样就能直接访问它操作的变量。</p><pre><code>public class HourlyPayCalculator {    public Money calculateWeeklyPay(HourlyEmployee e) {        int tenthRate = e.getTenthRate().getPennies();        int tenthsWorked = e.getTenthsWorked();        int straightTime = Math.min(400, tenthsWorked);        int overTime = Math.max(0, tenthsWorked - straightTime);        int straightPay = straightTime * tenthRate;        int overtimePay = (int)Math.round(overTime*tenthRate*1.5);         return new Money(straightPay + overtimePay);    } }</code></pre><p>方法 calculateWeeklyPay 伸手到 HourlyEmployee 对象，获取要操作的数据。方法calculateWeeklyPay 依恋于 HourlyEmployee 的作用范围。它“期望”自己在 HourlyEmployee 中。</p><h4 id="选择算子参数"><a href="#选择算子参数" class="headerlink" title="选择算子参数"></a>选择算子参数</h4><p>没有什么比在函数调用末尾遇到一个 false 参数更为可憎的事情了。那个 false 是什么意思？如果它是 true，会有什么变化吗？不仅是一个选择算子（selector）参数的目的难以记住，每个选择算子参数将多个函数绑到了一起。选择算子参数只是一种避免把大函数切分为多个小函数的偷懒做法。</p><h4 id="晦涩的意图"><a href="#晦涩的意图" class="headerlink" title="晦涩的意图"></a>晦涩的意图</h4><p>代码要尽可能具有表达力。联排表达式、匈牙利语标记法和魔术数都遮蔽了作者的意图。例如，下面是 overTimePay 函数可能的一种表现形式：</p><pre><code>public int m_otCalc() {    return iThsWkd * iThsRte +    (int) Math.round(0.5 * iThsRte *    Math.max(0, iThsWkd - 400)    );}</code></pre><h4 id="位置错误的权责"><a href="#位置错误的权责" class="headerlink" title="位置错误的权责"></a>位置错误的权责</h4><p>软件开发者做出的最重要决定之一就是在哪里放代码。例如，PI 常量放在何处？是该在Math 类中吗？或者应该属于 Trigonometry 类？还是在 Circle 类？</p><p>最小惊异原则在这里起作用了。代码应该放在读者自然而然期待它所在的地方。PI 常量应该在出现在声明三角函数的地方。OVERTIME_RATE 常量应该在 HourlyPayCalculator 类中声明。</p><p>有时，我们“聪明”地知道在何处放置功能代码。我们会放在自己方便而读者不能随直觉找到的地方。例如，也许我们需要打印出某个雇员的总工作时间的报表。我们可以在打印报表的代码中做工作时间统计，或者我们可以在接受工作时间卡的代码中保留一份工作时间记录。</p><h4 id="不恰当的静态方法"><a href="#不恰当的静态方法" class="headerlink" title="不恰当的静态方法"></a>不恰当的静态方法</h4><p>Math.max(double a, double)是个良好的静态方法。它并不在单个实体上操作；的确，不得不写 new Math( ).max(a,b)甚至 a.max(b)实在愚蠢。那个 max 用到的全部数据来自其两个参数，而不是来自“所属”对象。而且，我们也没机会用到 Math.max 的多态特征</p><h4 id="使用解释性变量"><a href="#使用解释性变量" class="headerlink" title="使用解释性变量"></a>使用解释性变量</h4><p>Kent Beck在其巨著Smalltalk Best Practice Patterns和另一部巨著Implementation Patterns<br>（中译版 《实现模式》）中都写到这个。让程序可读的最有力方法之一就是将计算过程打散成在用有意义的单词命名的变量中放置的中间值。</p><h4 id="函数名称应该表达其行为"><a href="#函数名称应该表达其行为" class="headerlink" title="函数名称应该表达其行为"></a>函数名称应该表达其行为</h4><p>看看这行代码：</p><pre><code>Date newDate = date.add(5);</code></pre><p>你会期望它向日期添加5天吗？或者是5个星期？5个小时？该date实体会变化吗？或者该函数只是返回一个新的Date实体，并不改动旧的？从函数调用中看不出函数的行为。</p><p>如果函数向日期添加5天并且修改该日期，就该命名为addDaysTo或increaseByDays。如果函数返回一个表示5天后的日期，而不修改日期实体，就该叫做daysLater或daysSince。</p><p>如果你必须查看函数的实现（或文档）才知道它是做什么的，就该换个更好的函数名，或者重新安排功能代码，放到有较好名称的函数中。</p><h4 id="理解算法"><a href="#理解算法" class="headerlink" title="理解算法"></a>理解算法</h4><p>好多可笑代码的出现，是因为人们没花时间去理解算法。他们硬塞进足够多的if语句和标识，从不真正停下来考虑发生了什么，勉强让系统能工作。</p><p>编程常常是一种探险。你以为自己知道某事的正确算法，然后就卷起袖子瞎干一气，搞到“可以工作”为止。你怎么知道它“可以工作”？因为它通过了你能想到的单元测试。这种做法没错。实际上，这也是让函数按你设想的方式执行的唯一途径。不过，“可以工作”周围的引号可不能一直保留。</p><p>在你认为自己完成某个函数之前，确认自己理解了它是怎么工作的。通过全部测试还不够好。你必须知道[10]解决方案是正确的。</p><p>获得这种知识和理解的最好途径，往往是重构函数，得到某种整洁而足具表达力、清楚呈示如何工作的东西。</p><h4 id="把逻辑依赖改为物理依赖"><a href="#把逻辑依赖改为物理依赖" class="headerlink" title="把逻辑依赖改为物理依赖"></a>把逻辑依赖改为物理依赖</h4><p>如果某个模块依赖于另一个模块，依赖就该是物理上的而不是逻辑上的。依赖者模块不应对被依赖者模块有假定（换言之，逻辑依赖）。它应当明确地询问后者全部信息。</p><p>例如， 想像你在编写一个打印出雇员工作时长的纯文本报表的函数。有个名为HourlyReporter的类把数据收集为某种方便的形式，传递到HourlyReportFormatter中，再打印出来。（如代码清单17-1所示。）</p><p>代码清单17-1　HourlyReporter.java</p><pre><code>public class HourlyReporter {　private HourlyReportFormatter formatter;　private List&lt;LineItem&gt; page;　private final int PAGE_SIZE = 55;　public HourlyReporter(HourlyReportFormatter formatter) {　　this.formatter = formatter;　　page = new ArrayList&lt;LineItem&gt;();　}　public void generateReport(List&lt;HourlyEmployee&gt; employees) {　　for (HourlyEmployee e : employees) {　　　addLineItemToPage(e);　　　if (page.size() == PAGE_SIZE)　　　　printAndClearItemList();　　}　　if (page.size() &gt; 0)　　　printAndClearItemList();　}　private void printAndClearItemList() {　　formatter.format(page);　　page.clear();　}　private void addLineItemToPage(HourlyEmployee e) {　　LineItem item = new LineItem();　　item.name = e.getName();　　item.hours = e.getTenthsWorked() / 10;　　item.tenths = e.getTenthsWorked() % 10;　　page.add(item);　}　public class LineItem {　　public String name;　　public int hours;　　public int tenths;　}}</code></pre><p>这段代码有尚未物理化的逻辑依赖。你能指出来吗？那就是常量PAGE_SIZE。HourlyReporter为什么要知道页面尺寸？页面尺寸只该是HourlyReportFormatter的权责。</p><p>PAGE_SIZE在HourlyReporter中声明，代表了一种位置错误的权责[G17]，导致HourlyReporter假定它知道页面尺寸。这类假设是一种逻辑依赖。HourlyReporter依赖于HourlyReportFormatter能应付55的页面尺寸。如果HourlyReportFormatter的某些实现不能处理这样的尺寸，就会出错。</p><p>可以通过创建HourlyReport中名为getMaxPageSize()的新方法来物理化这种依赖。HourlyReporter将调用这个方法，而不是使用PAGE_SIZE常量。</p><h4 id="用多态替代-If-Else-或-Switch-Case"><a href="#用多态替代-If-Else-或-Switch-Case" class="headerlink" title="用多态替代 If/Else 或 Switch/Case"></a>用多态替代 If/Else 或 Switch/Case</h4><p>有了第6章谈及的主题，这条建议看似奇怪。在那章中，我提出在添加新函数甚于添加新类型的系统中，switch语句是恰当的。</p><p>首先，多数人使用switch语句，因为它是最直截了当又有力的方案，而不是因为它适合当前情形。这给我们的启发是在使用switch之前，先考虑使用多态。</p><p>其次，函数变化甚于类型变化的情形相对罕见。每个switch语句都值得怀疑。</p><p>我使用所谓“单个switch”规则：对于给定的选择类型，不应有多于一个switch语句。在那个switch语句中的多个case，必须创建多态对象，取代系统中其他类似switch语句。</p><h4 id="遵循标准约定"><a href="#遵循标准约定" class="headerlink" title="遵循标准约定"></a>遵循标准约定</h4><p>每个团队都应遵循基于通用行业规范的一套编码标准。编码标准应指定诸如在何处声明实体变量，如何命名类，方法和变量，在何处放置括号，等等。团队不应用文档描述这些约定，因为代码本身提供了范例。</p><p>团队中的每个成员都应遵循这些约定。这意味着每个团队成员必须成熟到能了解只要全体同意在何处放置括号，那么在哪里放置都无关紧要。</p><p>如果你想知道我遵循哪些约定，可以查看代码清单B-7～B-14中重构之后的代码。</p><h4 id="用命名常量替代魔术数"><a href="#用命名常量替代魔术数" class="headerlink" title="用命名常量替代魔术数"></a>用命名常量替代魔术数</h4><p>这大概是软件开发中最古老的规则之一了。我记得，在20世纪60年代介绍COBOL、FORTRAN和PL/1的手册中就读到过。在代码中出现原始形态数字通常来说是坏现象。应该用良好命名的常量来隐藏它。</p><p>例如，数字86400应当藏在常量SECONDS_PER_DAY后面。如果每页打印55行，则常数55应该藏在常量LINES_PER_PAGE后面。</p><p>有些常量与非常具有自我解释能力的代码协同工作时，如此易于识别，也就不必总是需要命名常量来隐藏了。例如：</p><pre><code>double milesWalked = feetWalked/5280.0;int dailyPay = hourlyRate * 8;double circumference = radius * Math.PI * 2;</code></pre><p>在上例中，我们真需要常量FEET_PER_MILE、WORK_HOURS_PER_DAY和TWO吗？显然，最后那个很可笑。有些情况下，常量直接写作原始形态数字会更好。你可能会质疑WORK_HOURS_PER_ DAY，因为约定规则可能会改变。另一方面，在这里直接用数字8读起来很舒服，也就没必要非用17个额外的字母来加重读者负担不可。对于FEET_PER_MILE，数字5280众人皆知，意义独特，即便没有上下文环境，读者也能识别它。</p><p>3.141592653589793之类常数也众所周知，很容易识别。不过，如果直接使用原始形式，却很有可能出错。每次有人看到3.141592653589793，都会知道那是π值，从而不会去仔细查看。（你发现那个错误的数字了吗？）我们不想要人们使用3.14、3.14159或3.142等。所以，为我们定义好Math.PI是件好事。</p><p>术语“魔术数”不仅是说数字。它泛指任何不能自我描述的符号。例如：</p><p>assertEquals(7777, Employee.find(“John Doe”).employeeNumber());</p><p>上列断言中有两个魔术数。第一个显然是777，它的意义并不明确。第二个魔术数是John Doe，因为其意图不明显。<br>John Doe是开发团队创建的测试数据中编号为#7777的雇员。团队中每个成员都知道，当连接到数据库时，里面已经有数个雇员信息，其值和属性都是大家熟知的。所以，这个测试应该读作：</p><pre><code>assertEquals(　HOURLY_EMPLOYEE_ID,　Employee.find(HOURLY_EMPLOYEE_NAME).employeeNumber());　</code></pre><h4 id="准确"><a href="#准确" class="headerlink" title="准确"></a>准确</h4><p>期望某个查询的第一次匹配就是唯一匹配可能过于天真。用浮点数表示货币几近于犯罪。因为你不想做并发更新就避免使用锁和/或事务管理往好处说也是一种懒惰行为。在可以用List的时候非要把变量声明为ArrayList就过分拘束了。把所有变量设置为protected却不够自律。</p><p>在代码中做决定时，确认自己足够准确。明确自己为何要这么做，如果遇到异常情况如何处理。别懒得理会决定的准确性。如果你打算调用可能返回null的函数，确认自己检查了null值。如果查询你认为是数据库中唯一的记录，确保代码检查不存在其他记录。如果要处理货币数据，使用整数[11]，并恰当地处理四舍五入。如果可能有并发更新，确认你实现了某种锁定机制。</p><p>代码中的含糊和不准确要么是意见不同的结果，要么源于懒惰。无论原因是什么，都要消除。</p><h4 id="结构甚于约定"><a href="#结构甚于约定" class="headerlink" title="结构甚于约定"></a>结构甚于约定</h4><p>坚守结构甚于约定的设计决策。命名约定很好，但却次于强制性的结构。例如，用到良好命名的枚举的switch/case要弱于拥有抽象方法的基类。没人会被强迫每次都以同样方式实现switch/case语句，但基类却让具体类必须实现所有抽象方法。</p><h4 id="封装条件"><a href="#封装条件" class="headerlink" title="封装条件"></a>封装条件</h4><p>如果没有if或while语句的上下文，布尔逻辑就难以理解。应该把解释了条件意图的函数抽离出来。<br>例如：</p><pre><code>if (shouldBeDeleted(timer))</code></pre><p>要好于</p><pre><code>if　(timer.hasExpired()　&amp;&amp;　!timer.isRecurrent())</code></pre><h4 id="避免否定性条件"><a href="#避免否定性条件" class="headerlink" title="避免否定性条件"></a>避免否定性条件</h4><p>否定式要比肯定式难明白一些。所以，尽可能将条件表示为肯定形式。例如：</p><pre><code>if　(buffer.shouldCompact())</code></pre><p>要好于</p><pre><code>if　(!buffer.shouldNotCompact())</code></pre><h4 id="函数只该做一件事"><a href="#函数只该做一件事" class="headerlink" title="函数只该做一件事"></a>函数只该做一件事</h4><p>编写执行一系列操作的包括多段代码的函数常常是诱人的。这类函数做了不只一件事，应该转换为多个更小的函数，每个只做一件事。</p><h4 id="掩蔽时序耦合"><a href="#掩蔽时序耦合" class="headerlink" title="掩蔽时序耦合"></a>掩蔽时序耦合</h4><p>常常有必要使用时序耦合，但你不应该掩蔽它。排列函数参数，好让它们被调用的次序显而易见。看下列代码：</p><pre><code>public class MoogDiver {　Gradient gradient;　List&lt;Spline&gt; splines;　public void dive(String reason) {　　saturateGradient();　　reticulateSplines();　　diveForMoog(reason);　}　...}</code></pre><p>三个函数的次序很重要。捕鱼之前先织网，织网之前先编绳。不幸的是，代码并没有强制这种时序耦合。其他程序员可以在调用saturateGradient之前调用reticulateSplines，从而导致抛出UnsaturatedGradientException异常。更好的方式是：</p><pre><code>public class MoogDiver {　Gradient gradient;　List&lt;Spline&gt; splines;　public void dive(String reason) {　　Gradient gradient = saturateGradient();　　List&lt;Spline&gt; splines = reticulateSplines(gradient);　　diveForMoog(splines, reason);　}　...}</code></pre><p>这样就通过创建顺序队列暴露了时序耦合。每个函数都产生出下一个函数所需的结果，这样一来就没理由不按顺序调用了。</p><p>你可能会抱怨着增加了函数的复杂度，没错，不过这点额外的复杂度却曝露了该种情况真正的时序复杂性。</p><p>注意我保留了那些实体变量。我假设类中的私有方法可能会用到它们。即便如此，我还是希望参数能让时序耦合变得可见。</p><h4 id="别随意"><a href="#别随意" class="headerlink" title="别随意"></a>别随意</h4><p>构建代码需要理由，而且理由应与代码结构相契合。如果结构显得太随意，其他人就会想修改它。如果结构自始至终保持一致，其他人就会使用它，并且遵循其约定。例如，我最近对FitNesse做合并修改，发现有位贡献者这么做：</p><pre><code>public class AliasLinkWidget extends ParentWidget{　public static class VariableExpandingWidgetRoot {　　...　...}</code></pre><p>问题在于，VariableExpandingWidgetRoot没必要在AliasLinkWidget作用范围之内。而且，其他无关的类也用到AliasLinkWidget.VariableExpandingWidgetRoot。这些类没必要了解AliasLinkWidget。</p><p>或许那位程序员只是循例把VariableExpandingWidgetRoot放到AliasWidget里面，或者他真认为这么做是对的。不管原因是什么，结果都显得随心所欲。不作为类工具的公共类，不应该放到其他类里面。惯例是将它置为public，并且放在代码包的顶部。</p><h4 id="封装边界条件"><a href="#封装边界条件" class="headerlink" title="封装边界条件"></a>封装边界条件</h4><p>边界条件难以追踪。把处理边界条件的代码集中到一处，不要散落于代码中。我们不想见到四处散见的+1和−1字样。看看这个来自FIT的简单例子：</p><pre><code>if(level + 1 &lt; tags.length){　parts = new Parse(body, tags, level + 1, offset + endTag);　body = null;}</code></pre><p>注意，level + 1出现了两次。这是个应该封装到名为nextLevel之类的变量中的边界条件。</p><pre><code>int nextLevel = level + 1;if(nextLevel &lt; tags.length){　parts = new Parse(body, tags, nextLevel, offset + endTag);　body = null;}</code></pre><h4 id="函数应该只在一个抽象层级上"><a href="#函数应该只在一个抽象层级上" class="headerlink" title="函数应该只在一个抽象层级上"></a>函数应该只在一个抽象层级上</h4><p>函数中的语句应该在同一抽象层级上，该层级应该是函数名所示操作的下一层。这可能是最难理解和遵循的启发。尽管概念足够直白，人们还是很容易混淆抽象层级。例如，请看下面来自FitNesse的例子：</p><pre><code>public String render() throws Exception{　StringBuffer html = new StringBuffer("&lt;hr");　if(size &gt; 0)　　html.append(" size=\"").append(size + 1).append("\"");　html.append("&gt;");　return html.toString();}</code></pre><p>稍微研究一下，你就会看到发生了什么。该函数构建了绘制横贯页面线条的HTML标记。线条高度在size变量中指定。<br>再看一遍。方法混杂了至少两个抽象层级。第一个是横线有尺寸这个概念。第二个是hr标记自身的语法。这段代码来自FitNesse的HruleWidget模块。该模块检测一行4个或更多个破折号，并将其转换为恰当的hr标记。破折号越多，尺寸越大。</p><p>我重构了这段代码。注意，我修改了size字段的名称，反映其真正目的。它表示额外破折号的数量。</p><pre><code>public String render() throws Exception{　HtmlTag hr = new HtmlTag("hr");　if (extraDashes &gt; 0)　　hr.addAttribute("size", hrSize(extraDashes));　return hr.html();}private String hrSize(int height){　int hrSize = height + 1;　return String.format("%d", hrSize);}</code></pre><p>这次修改很好地拆开了两个抽象层级。函数render只构造一个hr标记，不去管该标记的HTML语法。而HtmlTag模块则照管所有这些肮脏的语法问题。</p><p>做出修改时，我发现了一处微小的错误。原始代码没有加上hr标记的结束斜线符，而XHTML标准要求这样做。（换言之，代码使用了<code>&lt;hr</code>&gt;而不是<code>&lt;hr /&gt;。</code>）HtmlTag模块很早就改造成符合XHTML标准了。</p><p>拆分不同抽象层级是重构的最重要功能之一，也是最难做的一个。以下面的代码为例。这是我第一次尝试拆分HruleWidget.rendermethod中的抽象层级的结果。</p><pre><code>public String render() throws Exception{　HtmlTag hr = new HtmlTag("hr");　if (size &gt; 0) {　　hr.addAttribute("size", ""+(size+1));　}　return hr.html();}</code></pre><p>此时，我的目的是做必要的拆分，并让测试通过。我轻易达到了这一目的，但结果是该函数仍然混杂了多个抽象层级。此时，混杂的层级是hr标记的构建，以及size变量的翻译和格式化。这说明当你偱抽象界线拆解函数时，经常会挖出原本被之前的结构所掩蔽的新抽象界线。</p><h4 id="在较高层级放置可配置数据"><a href="#在较高层级放置可配置数据" class="headerlink" title="在较高层级放置可配置数据"></a>在较高层级放置可配置数据</h4><p>如果你有个已知并该在较高抽象层级的默认常量或配置值，不要将它埋藏到较低层级的函数中。把它作为较高层级函数调用较低层级函数时的一个参数。看看以下来自FItNesse的代码：</p><pre><code>　public static void main(String[] args) throws Exception　{　　Arguments arguments = parseCommandLine(args);　　...　}public class Arguments{　public static final String DEFAULT_PATH = ".";　public static final String DEFAULT_ROOT = "FitNesseRoot";　public static final int DEFAULT_PORT = 80;　public static final int DEFAULT_VERSION_DAYS = 14;　...}</code></pre><p>命令行参数在FitNesse中的第一行可执行代码得到解析。这些参数的默认值在Argument类的顶部指定。你不必到系统的较低层级去查看类似的语句：</p><p>if (arguments.port == 0) // use 80 by default</p><p>位于较高层级的配置性常量易于修改。它们向下贯穿应用程序。应用程序的较低层级并不拥有这些常量的值。</p><h4 id="避免传递浏览"><a href="#避免传递浏览" class="headerlink" title="避免传递浏览"></a>避免传递浏览</h4><p>通常我们不想让某个模块了解太多其协作者的信息。更具体地说，如果A与B协作，B与C协作，我们不想让使用A的模块了解C的信息。（例如，我们不想写类似a.getB( ).getC( ).doSomething( )的代码。）</p><p>这就是所谓得墨忒耳律。The Pragmatic Programmers（中译版《程序员修炼之道》）称之为“编写害羞代码”[12]。两者都归结为确保模块只了解其直接协作者，不了解整个系统的游览图。</p><p>如果有多个模块使用类似a.getB( ).getC( )这样的语句形式，就难以修改设计和架构，在B和C之间插进一个Q。你得找到a.getB( ).getC( )出现的所有地方，并将其改为a.getB( ).getQ( ).getC( )。系统就此变得缺乏柔韧性。太多的模块了解了太多有关架构的信息。</p><p>正确的做法是让直接协作者提供所需的全部服务。不必逛遍系统的对象全图，搜寻我们要调用的方法。只要简单地说：<br>myCollaborator.doSomething().</p><h3 id="Java"><a href="#Java" class="headerlink" title="Java"></a>Java</h3><h4 id="采用描述性名称"><a href="#采用描述性名称" class="headerlink" title="采用描述性名称"></a>采用描述性名称</h4><p>不要太快取名。确认名称具有描述性。记住，事物的意义随着软件的演化而变化，所以，要经常性地重新估量名称是否恰当。<br>这不仅是一条“感觉良好式”建议。软件中的名称对于软件可读性有90%的作用。你要花时间明智地取名，保持名称有关。名称太重要了，不可随意对待。</p><p>看看以下代码。这段代码是做什么的？用了好名称的代码一目了然，而这样的代码却是符号和魔术数的大杂烩。</p><pre><code>public int x() {　int q = 0;　int z = 0;　for (int kk = 0; kk &lt; 10; kk++) {　　if (l[z] == 10)　　{　　　q += 10 + (l[z + 1] + l[z + 2]);　　　z += 1;　　}　　else if (l[z] + l[z + 1] == 10)　　{　　　q += 10 + l[z + 2];　　　z += 2;　　} else {　　　q += l[z] + l[z + 1];　　　z += 2;　　}　}　return q;}</code></pre><p>下面是这段代码应该写成的样子。代码片段实际上不如上段完整。但你还是能马上推断出它要做什么，而且很有可能依据推断出的意思写出遗漏的函数。魔术数不复神秘，算法的结构也足具描述性。</p><pre><code>public int score() {　int score = 0;　int frame = 0;　for (int frameNumber = 0; frameNumber &lt; 10; frameNumber++) {　　if (isStrike(frame)) {　　　score += 10 + nextTwoBallsForStrike(frame);　　　frame += 1;　　} else if (isSpare(frame)) {　　　score += 10 + nextBallForSpare(frame);　　　frame += 2;　　} else {　　　score += twoBallsInFrame(frame);　　　frame += 2;　　}　}　return score;}</code></pre><p>仔细取好的名称的威力在于，它用描述性信息覆盖了代码。这种信息覆盖设定了读者对于模块中其他函数行为的期待。看看上面的代码，你就能推断出isStrike( )的实现。读到isStrick方法时，它“深合你意”[13]。</p><pre><code>private boolean isStrike(int frame) {　return rolls[frame] == 10;}</code></pre><h4 id="名称应与抽象层级相"><a href="#名称应与抽象层级相" class="headerlink" title="名称应与抽象层级相"></a>名称应与抽象层级相</h4><p>不要取沟通实现的名称；取反映类或函数抽象层级的名称。这样做不容易。人们擅长于混杂抽象层级。每次浏览代码，你总会发现有些变量的名称层级太低。你应当趁机为之改名。要让代码可读，需要持续不断的改进。看看下面的Modem接口：</p><pre><code>public interface Modem {　boolean dial(String phoneNumber);　boolean disconnect();　boolean send(char c);　char recv();　String getConnectedPhoneNumber();}</code></pre><p>粗看还行。函数看来都很合适，对于多数应用程序来说是这样。不过，想想看某个应用中有些调制解调器并不用拨号连接的情形。有些用线缆直连（就像如今为多数家庭提供Internet连接的线缆解调器）的情形。有些通过向USB口发送端口信息连接。显然，有关电话号码的信息就是位于错误的抽象层级了。对于这种情形，更好的命名策略可能是：</p><pre><code>public interface Modem {　boolean connect(String connectionLocator);　boolean disconnect();　boolean send(char c);　char recv();　String getConnectedLocator();}</code></pre><p>现在名称再不与电话号码有关系。还是可以用于用电话号码的情形，也可以用于其他连接策略。</p><h4 id="尽可能使用标准命名法"><a href="#尽可能使用标准命名法" class="headerlink" title="尽可能使用标准命名法"></a>尽可能使用标准命名法</h4><p>如果名称基于既存约定或用法，就比较易于理解。例如，如果你采用油漆工模式，就该在给油漆类命名时用上Decorator字样。例如，AutoHangupModemDecorator可能是某个给Modem类刷上在会话结束时自动挂机的能力的类的名称。</p><p>模式只是标准的一种。例如，在Java中，将对象转换为字符串的函数通常命名为toString。最好是遵循这些约定，而不是自己创造命名法。</p><p>对于特定项目，开发团队常常发明自己的命名标准系统。Eric Evans称之为项目的共同语言[14]。代码应该使用来自这种语言的术语。简言之，具有与项目有关的特定意义的名称用得越多，读者就越容易明白你的代码是做什么的。</p><h4 id="无歧义的名称"><a href="#无歧义的名称" class="headerlink" title="无歧义的名称"></a>无歧义的名称</h4><p>选用不会混淆函数或变量意义的名称。看看来自FitNesse的这个例子：</p><pre><code>private String doRename() throws Exception{　if(refactorReferences)　　renameReferences();　renamePage();　pathToRename.removeNameFromEnd();　pathToRename.addNameToEnd(newName);　return PathParser.render(pathToRename);}</code></pre><p>该函数的名称含混不清，没有说明函数的作用。由于在doRename函数里面还有个名为renamePage的函数，这就更不明白了！这些名称有没有说明两个函数之间的区别呢？没有。</p><p>该函数的更好名称应该是renamePageAndOptionallyAllReferences。看似太长，的确也很长，不过它只在模块中的一处被调用，所以其解释性的好处大过了长度的坏处。</p><h4 id="为较大作用范围选用较长名称"><a href="#为较大作用范围选用较长名称" class="headerlink" title="为较大作用范围选用较长名称"></a>为较大作用范围选用较长名称</h4><p>名称的长度应与作用范围的广泛度相关。对于较小的作用范围，可以用很短的名称，而对于较大作用范围就该用较长的名称。<br>类似i和j之类的变量名对于作用范围在5行之内的情形没问题。看看以下来自老“标准保龄球游戏”的代码片段：</p><pre><code>private void rollMany(int n, int pins){　for (int i=0; i&lt;n; i++)　　g.roll(pins);}</code></pre><p>这段代码很明白，如果用rollCount之类烦人的名称代替变量i，反而是徒增混乱。另一方面，在较长距离上，使用短名称的变量和函数会丧失其含义。名称的作用范围越大，名称就该越长、越准确。</p><h4 id="避免编码"><a href="#避免编码" class="headerlink" title="避免编码"></a>避免编码</h4><p>不应在名称中包括类型或作用范围信息。在如今的开发环境中，m_或f之类前缀完全无用。类似vis_（表示图形系统）之类的项目或子系统名称也属多余。当今的开发环境不用纠缠于名称也能提供这些信息。不要用匈牙利语命名法污染你的名称。</p><h4 id="名称应该说明副作用"><a href="#名称应该说明副作用" class="headerlink" title="名称应该说明副作用"></a>名称应该说明副作用</h4><p>名称应该说明函数、变量或类的一切信息。不要用名称掩蔽副作用。不要用简单的动词来描述做了不止一个简单动作的函数。例如，请看以下来自TestNG的代码：</p><pre><code>public ObjectOutputStream getOos() throws IOException {　if (m_oos == null) {　　m_oos = new ObjectOutputStream(m_socket.getOutputStream());　}　return m_oos;}</code></pre><p>该函数不只是获取一个oos，如果oos不存在，还会创建一个。所以，更好的名称大概是createOrReturnOos。</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><h4 id="测试不足"><a href="#测试不足" class="headerlink" title="测试不足"></a>测试不足</h4><p>一套测试中应该有多少个测试？不幸的是，许多程序员的衡量标准是“看起来够了”。一套测试应该测到所有可能失败的东西。只要还有没被测试探测过的条件，或是还有没被验证过的计算，测试就还不够。</p><h4 id="使用覆盖率工具"><a href="#使用覆盖率工具" class="headerlink" title="使用覆盖率工具"></a>使用覆盖率工具</h4><p>覆盖率工具能汇报你测试策略中的缺口。使用覆盖率工具能更容易地找到测试不足的模块、类和函数。多数IDE都给出直观的指示，用绿色标记测试覆盖了的代码行，而未覆盖的代码行则是红色。这样就能又快又容易地找到尚未检测过的if或catch语句。</p><h4 id="别略过小测试"><a href="#别略过小测试" class="headerlink" title="别略过小测试"></a>别略过小测试</h4><p>小测试易于编写，其文档上的价值高于编写成本。</p><h4 id="被忽略的测试就是对不确定事物的疑问"><a href="#被忽略的测试就是对不确定事物的疑问" class="headerlink" title="被忽略的测试就是对不确定事物的疑问"></a>被忽略的测试就是对不确定事物的疑问</h4><p>有时，我们会因为需求不明而不能确定某个行为细节。可以用注释掉的测试或者用@Ignore标记的测试来表达我们对于需求的疑问。使用哪种方式，取决于该不确定性所关涉代码是否要编译。</p><h4 id="测试边界条件"><a href="#测试边界条件" class="headerlink" title="测试边界条件"></a>测试边界条件</h4><p>特别注意测试边界条件。算法的中间部分正确但边界判断错误的情形很常见。</p><h4 id="全面测试相近的缺陷"><a href="#全面测试相近的缺陷" class="headerlink" title="全面测试相近的缺陷"></a>全面测试相近的缺陷</h4><p>缺陷趋向于扎堆。在某个函数中发现一个缺陷时，最好全面测试那个函数。你可能会发现缺陷不止一个。</p><h4 id="测试失败的模式有启发性"><a href="#测试失败的模式有启发性" class="headerlink" title="测试失败的模式有启发性"></a>测试失败的模式有启发性</h4><p>有时，你可以通过找到测试用例失败的模式来诊断问题所在。这也是尽可能编写足够完整的测试用例的理由之一。完整的测试用例，按合理的顺序排列，能暴露出模式。</p><p>简单举例，假设你注意到所有长于5个字符的输入都会导致测试失败，或者向函数的第二个参数传入负数都会导致测试失败。有时，只要看看测试报告的红绿模式，就足以绽放出那句带来解决方法的“啊哈！”回头看看第16章“重构SerialDate”中的有趣例子吧。</p><h4 id="测试覆盖率的模式有启发性"><a href="#测试覆盖率的模式有启发性" class="headerlink" title="测试覆盖率的模式有启发性"></a>测试覆盖率的模式有启发性</h4><p>查看被或未被已通过的测试执行的代码，往往能发现失败的测试为何失败的线索。</p><h4 id="测试应该快速"><a href="#测试应该快速" class="headerlink" title="测试应该快速"></a>测试应该快速</h4><p>慢速的测试是不会被运行的测试。时间一紧，较慢的测试就会被摘掉。所以，竭尽所能让测试够快。</p>]]></content>
      
      
      <categories>
          
          <category> Architecture </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《30天自制操作系统》</title>
      <link href="/2021/03/10/note/30day-os/"/>
      <url>/2021/03/10/note/30day-os/</url>
      
        <content type="html"><![CDATA[<h1 id="30天自制操作系统"><a href="#30天自制操作系统" class="headerlink" title="30天自制操作系统"></a>30天自制操作系统</h1><h3 id="启动区"><a href="#启动区" class="headerlink" title="启动区"></a>启动区</h3><p>（boot sector）软盘第一个的扇区称为启动区。那么什么是扇区呢？计算机读写软<br>盘的时候，并不是一个字节一个字节地读写的，而是以512字节为一个单位进行读<br>写。因此,软盘的512字节就称为一个扇区。一张软盘的空间共有1440KB，也就是<br>1474560字节，除以512得2880，这也就是说一张软盘共有2880个扇区。那为什么<br>第一个扇区称为启动区呢？那是因为计算机首先从最初一个扇区开始读软盘，然<br>后去检查这个扇区最后2个字节的内容。<br>如果这最后2个字节不是0x55 AA，计算机会认为这张盘上没有所需的启动程序，就<br>会报一个不能启动的错误。（也许有人会问为什么一定是0x55 AA呢？那是当初的<br>设计者随便定的，笔者也没法解释）。如果计算机确认了第一个扇区的最后两个字<br>节正好是0x55 AA，那它就认为这个扇区的开头是启动程序，并开始执行这个程序。</p><h3 id="IPL"><a href="#IPL" class="headerlink" title="IPL"></a>IPL</h3><p>initial program loader的缩写。启动程序加载器。启动区只有区区512字节，实际的<br>操作系统不像hello-os这么小，根本装不进去。所以几乎所有的操作系统，都是把<br>加载操作系统本身的程序放在启动区里的。有鉴于此，有时也将启动区称为IPL。 但hello-os没有加载程序的功能，所以HELLOIPL这个名字不太顺理成章。如果有<br>人正义感特别强，觉得“这是撒谎造假，万万不能容忍！”，那也可以改成其他的<br>名字。但是必须起一个8字节的名字，如果名字长度不到8字节的话，需要在最后<br>补上空格。</p><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><p>（boot）boot这个词本是长靴（boots）的单数形式。它与计算机的启动有什么关系<br>呢？一般应该将启动称为start的。实际上，boot这个词是bootstrap的缩写，原指靴<br>子上附带的便于拿取的靴带。但自从有了《吹牛大王历险记》（德国）这个故事<br>以后，bootstrap这个词就有了“自力更生完成任务”这种意思（大家如果对详情感<br>兴趣，可以在Google上查找，也可以在帮助和支持网页<a href="http://hrb.osask.jp上提问)./">http://hrb.osask.jp上提问）。</a><br>而且，磁盘上明明装有操作系统，还要说读入操作系统的程序（即IPL）也放在磁<br>盘里，这就像打开宝物箱的钥匙就在宝物箱里一样，是一种矛盾的说法。这种矛<br>盾的操作系统自动启动机制，被称为bootstrap方式。boot这个说法就来源于此。如<br>果是笔者来命名的话，肯定不会用bootstrap 这么奇怪的名字，笔者大概会叫它“多<br>级火箭式”吧。</p><h3 id="寄存器"><a href="#寄存器" class="headerlink" title="寄存器"></a>寄存器</h3><p>AX——accumulator，累加寄存器<br>CX——counter，计数寄存器<br>DX——data，数据寄存器<br>BX——base，基址寄存器<br>SP——stack pointer，栈指针寄存器<br>BP——base pointer，基址指针寄存器<br>SI——source index，源变址寄存器<br>DI——destination index，目的变址寄存器</p><p>大家所用的电脑里配置的，大概都是64MB，甚至512MB这样非常大的内存。那是不是这些<br>内存我们想怎么用就能怎么用呢？也不是这样的。比如说，内存的0号地址，也就是最开始的部<br>分，是BIOS程序用来实现各种不同功能的地方，如果我们随便使用的话，就会与BIOS发生冲突，<br>结果不只是BIOS会出错，而且我们的程序也肯定会问题百出。另外，在内存的0xf0000号地址附<br>近，还存放着BIOS程序本身，那里我们也不能使用。</p><p>0x00007c00-0x00007dff ：启动区内容的装载地址</p><p>看到这，大家可能会问：“为什么是0x7c00呢？ 0x7000不是更简单、好记吗？”其实笔者也<br>是这么想的，不过没办法，当初规定的就是0x7c00。做出这个规定的应该是IBM的大叔们，不过<br>估计他们现在都成爷爷了。</p><h3 id=""><a href="#" class="headerlink" title=""></a></h3><p>通过以上的尝试，最终证明，不管是CPU还是内存，它们根本就不关心所处理的电信号<br>到底代表什么意思。这么一来，说不定我们拿数码相机拍一幅风景照，把它作为磁盘映像文<br>件保存到磁盘里，就能成为世界上最优秀的操作系统！这看似荒谬的情况也是有可能发生的。<br>但从常识来看，这样做成的东西肯定会故障百出。反之，我们把做出的可执行文件作为一幅<br>画来看，也没准能成为世界上最高水准的艺术品。不过可以想象的是，要么文件格式有错，<br>要么显示出来的图是乱七八糟的。</p><h3 id="INT-13"><a href="#INT-13" class="headerlink" title="INT 13"></a>INT 13</h3><pre><code> MOV AX,0x0820  MOV ES,AX  MOV CH,0 ; 柱面0  MOV DH,0 ; 磁头0  MOV CL,2 ; 扇区2  MOV AH,0x02 ; AH=0x02 : 读盘 MOV AL,1 ; 1个扇区 MOV BX,0  MOV DL,0x00 ; A驱动器 INT 0x13 ; 调用磁盘BIOS  JC error </code></pre><p>说明</p><pre><code>AH=0x02;（读盘）AH=0x03;（写盘）AH=0x04;（校验）AH=0x0c;（寻道）AL=处理对象的扇区数;（只能同时处理连续的扇区）CH=柱面号 &amp;0xff; CL=扇区号（0-5位）|（柱面号&amp;0x300）&gt;&gt;2; DH=磁头号; DL=驱动器号；ES:BX=缓冲地址；(校验及寻道时不使用) 返回值：FLACS.CF==0：没有错误，AH==0 FLAGS.CF==1：有错误，错误号码存入AH内（与重置（reset）功能一样）</code></pre><p>以前我们用的“MOV CX,[1234]”，其实是“MOV CX,[DS:1234]”的意思。“MOV AL,[SI]”，<br>也就是“MOV AL,[DS:SI]”的意思。在汇编语言中，如果每回都这样写就太麻烦了，所以可以<br>省略默认的段寄存器DS。</p><p>上面虽然写着486用，但并不是说会出现仅能在486中执行的机器语言，这只是单纯的词语解<br>释的问题。所以486用的模式下，如果只使用16位寄存器，也能成为在8086中亦可执行的机器语言。<br>“纸娃娃操作系统”也支持386，所以虽然这里指定的是486，但并不是386中就不能用。可能会有<br>人问，这里的386，486都是什么意思啊？我们来简单介绍一下电脑的CPU（英特尔系列）家谱</p><p>8086→80186→286→386→486→Pentium→PentiumPro→PentiumII→PentiumIII→Pentium4→…</p><h3 id="中断"><a href="#中断" class="headerlink" title="中断"></a>中断</h3><p>首先是CLI和STI。所谓CLI，是将中断标志（interrupt flag）置为0的指令（clear interrupt flag）。<br>STI是要将这个中断标志置为1的指令（set interrupt flag）。而标志，是指像以前曾出现过的进位标<br>志一样的各种标志，也就是说在CPU中有多种多样的标志。更改中断标志有什么好处呢？正如其<br>名所示，它与CPU的中断处理有关系。当CPU遇到中断请求时，是立即处理中断请求（中断标志为1），还是忽略中断请求（中断标志为0），就由这个中断标志位来设定。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e8593f95c9ab224b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="全局段号记录表"><a href="#全局段号记录表" class="headerlink" title="全局段号记录表"></a>全局段号记录表</h3><p>GDT是“global（segment）descriptor table”的缩写，意思是全局段号记录表。将这些数据整<br>齐地排列在内存的某个地方，然后将内存的起始地址和有效设定个数放在CPU内被称作GDTR①的<br>特殊寄存器中，设定就完成了。</p><h3 id="中断记录表"><a href="#中断记录表" class="headerlink" title="中断记录表"></a>中断记录表</h3><p>IDT是“interrupt descriptor table”的缩写，直译过来就是“中断记录表”。当CPU遇到<br>外部状况变化，或者是内部偶然发生某些错误时，会临时切换过去处理这种突发事件。这就是中<br>断功能。</p><p>我们拿电脑的键盘来举个例子。以CPU的速度来看，键盘特别慢，只是偶尔动一动。就算是<br>重复按同一个键，一秒钟也很难输入50个字符。而CPU在1/50秒的时间内，能执行200万条指令<br>（CPU主频100MHz时）。CPU每执行200万条指令，查询一次键盘的状况就已经足够了。如果查询<br>得太慢，用户输入一个字符时电脑就会半天没反应。</p><p>要是设备只有键盘，用“查询”这种处理方法还好。但事实上还有鼠标、软驱、硬盘、光驱、<br>网卡、声卡等很多需要定期查看状态的设备。其中，网卡还需要CPU快速响应。响应不及时的话，<br>数据就可能接受失败，而不得不再传送一次。如果因为害怕处理不及时而靠查询的方法轮流查看<br>各个设备状态的话，CPU就会穷于应付，不能完成正常的处理</p><p>正是为解决以上问题，才有了中断机制。各个设备有变化时就产生中断，中断发生后，CPU<br>暂时停止正在处理的任务，并做好接下来能够继续处理的准备，转而执行中断程序。中断程序执<br>行完以后，再调用事先设定好的函数，返回处理中的任务。正是得益于中断机制，CPU可以不用<br>一直查询键盘，鼠标，网卡等设备的状态，将精力集中在处理任务上。</p><h3 id="PIC"><a href="#PIC" class="headerlink" title="PIC"></a>PIC</h3><p>所谓PIC是“programmable interrupt controller”的缩写，意思是“可编程中断控制器”。PIC<br>与中断的关系可是很密切的哟。它到底是什么呢？在设计上，CPU单独只能处理一个中断，这不<br>够用，所以IBM的大叔们在设计电脑时，就在主板上增设了几个辅助芯片。现如今它们已经被集<br>成在一个芯片组里了。</p><p>PIC是将8个中断信号①集合成一个中断信号的装置。PIC监视着输入管脚的8个中断信号，只<br>要有一个中断信号进来，就将唯一的输出管脚信号变成ON，并通知给CPU。IBM的大叔们想要<br>通过增加PIC来处理更多的中断信号，他们认为电脑会有8个以上的外部设备，所以就把中断信号<br>设计成了15个，并为此增设了2个PIC。</p><p>与CPU直接相连的PIC称为主PIC（master PIC），与主PIC相连的PIC称为从PIC（slave PIC）。<br>主PIC负责处理第0到第7号中断信号，从PIC负责处理第8到第15号中断信号。master意为主人，<br>slave意为奴隶，笔者搞不清楚这两个词的由来，但现在结果是不论从PIC如何地拼命努力，如果<br>主PIC不通知给CPU，从PIC的意思也就不能传达给CPU。或许是从这种关系上考虑，而把它们一<br>个称为主人，一个称为奴隶。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-90aa904fb15258ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="中断屏蔽寄存器"><a href="#中断屏蔽寄存器" class="headerlink" title="中断屏蔽寄存器"></a>中断屏蔽寄存器</h3><p>现在简单介绍一下PIC的寄存器。首先，它们都是8位寄存器。IMR是“interrupt mask register”<br>的缩写，意思是“中断屏蔽寄存器”。8位分别对应8路IRQ信号。如果某一位的值是1，则该位所<br>对应的IRQ信号被屏蔽，PIC就忽视该路信号。这主要是因为，正在对中断设定进行更改时，如<br>果再接受别的中断会引起混乱，为了防止这种情况的发生，就必须屏蔽中断。还有，如果某个IRQ<br>没有连接任何设备的话，静电干扰等也可能会引起反应，导致操作系统混乱，所以也要屏蔽掉这<br>类干扰。</p><p>ICW是“initial control word”的缩写，意为“初始化控制数据”。因为这里写着word，所以<br>我们会想，“是不是16位”？不过，只有在电脑的CPU里，word这个词才是16位的意思，在别的<br>设备上，有时指8位，有时也会指32位。PIC不是仅为电脑的CPU而设计的控制芯片，其他种类的<br>CPU也能使用，所以这里word的意思也并不是我们觉得理所当然的16位。</p><p>ICW有4个，分别编号为1~4，共有4个字节的数据。ICW1和ICW4与PIC主板配线方式、中断<br>信号的电气特性等有关，所以就不详细说明了。电脑上设定的是上述程序所示的固定值，不会设<br>定其他的值。如果故意改成别的什么值的话，早期的电脑说不定会烧断保险丝，或者器件冒<br>烟①；最近的电脑，对这种设定起反应的电路本身被省略了，所以不会有任何反应。</p><p>ICW3是有关主—从连接的设定，对主PIC而言，第几号IRQ与从PIC相连，是用8位来设定的。<br>如果把这些位全部设为1，那么主PIC就能驱动8个从PIC（那样的话，最大就可能有64个IRQ），<br>但我们所用的电脑并不是这样的，所以就设定成00000100。另外，对从PIC来说，该从PIC与主PIC<br>的第几号相连，用3位来设定。因为硬件上已经不可能更改了，如果软件上设定不一致的话，只<br>会发生错误，所以只能维持现有设定不变。</p><p>因此不同的操作系统可以进行独特设定的就只有ICW2了。这个ICW2，决定了IRQ以哪一号<br>中断通知CPU。“哎？怎么有这种事？刚才不是说中断信号的管脚只有1根吗？”嗯，话是那么说，<br>但PIC还有个挺有意思的小窍门，利用它就可以由PIC来设定中断号了</p><p>大家可能会对此有兴趣，所以再详细介绍一下。中断发生以后，如果CPU可以受理这个<br>中断，CPU就会命令PIC发送2个字节的数据。这2个字节是怎么传送的呢？CPU与PIC用IN<br>或OUT进行数据传送时，有数据信号线连在一起。PIC就是利用这个信号线发送这2个字节数<br>据的。送过来的数据是“0xcd 0x??”这两个字节。由于电路设计的原因，这两个字节的数据<br>在CPU看来，与从内存读进来的程序是完全一样的，所以CPU就把送过来的“0xcd 0x??”作<br>为机器语言执行。这恰恰就是把数据当作程序来执行的情况。这里的0xcd就是调用BIOS时<br>使用的那个INT指令。我们在程序里写的“INT 0x10”，最后就被编译成了“0xcd 0x10”。所<br>以，CPU上了PIC的当，按照PIC所希望的中断号执行了INT指令</p><pre><code>下面要讲的内容可能有点偏离主题，但笔者还是想介绍一下“纸娃娃系统”的内存分布图。0x00000000 - 0x000fffff : 虽然在启动中会多次使用，但之后就变空。（1MB）0x00100000 - 0x00267fff : 用于保存软盘的内容。（1440KB）0x00268000 - 0x0026f7ff : 空（30KB）0x0026f800 - 0x0026ffff : IDT （2KB）0x00270000 - 0x0027ffff : GDT （64KB）0x00280000 - 0x002fffff : bootpack.hrb（512KB）0x00300000 - 0x003fffff : 栈及其他（1MB）0x00400000 - : 空这个内存分布图当然是笔者所做出来的。为什么要做成这呢？其实也没有什么特别的理由，觉得这样还行，跟着感觉走就决定了。另外，虽然没有明写，但在最初的1MB范围内，还有BIOS，VRAM等内容，也就是说并不是1MB全都空着。</code></pre><h3 id="禁止高速缓存"><a href="#禁止高速缓存" class="headerlink" title="禁止高速缓存"></a>禁止高速缓存</h3><p>为了禁止缓存，需要对CR0寄存器的某一标志位进行操作。对哪里操作，怎么操作，大家一<br>看程序就能明白。这时，需要用到函数load_cr0和store_cr0，与之前的情况一样，这两个函数不<br>能用C语言写，只能用汇编语言来写，存在naskfunc.nas里。</p><h3 id="可编程的间隔型定时器"><a href="#可编程的间隔型定时器" class="headerlink" title="可编程的间隔型定时器"></a>可编程的间隔型定时器</h3><p>要在电脑中管理定时器，只需对PIT进行设定就可以了。PIT是“ Programmable Interval Timer”<br>的缩写，翻译过来就是“可编程的间隔型定时器”。我们可以通过设定PIT，让定时器每隔多少秒<br>就产生一次中断。因为在电脑中PIT连接着IRQ（interrupt request，参考第6章）的0号，所以只要<br>设定了PIT就可以设定IRQ0的中断间隔。……在旧机种上PIT是作为一个独立的芯片安装在主板<br>上的，而现在已经和PIC（programmable interrupt controller，参考第6章）一样被集成到别的芯片<br>里了。</p><h3 id="分辨率"><a href="#分辨率" class="headerlink" title="分辨率"></a>分辨率</h3><p>其实说起来也很简单。给AX赋值0x4f02，给BX赋值画面模式号码，这样就可以切换到高分<br>辨率画面模式了。为什么呢？这个笔者也答不上来，原本就是这样的。这次我们只是正好使用到<br>了这个功能。以前画面是320×200的时候，我们用的是“AH=0； AL=画面模式号码；”。现在切换<br>到新画面时就使用“AX = 0x4f02；”。</p><p>有鉴于此，多家显卡公司经过协商，成立了VESA协会（Video Electronics Standards<br>Association，视频电子标准协会）。此后，这个协会制定了虽然不能说完全兼容、但几乎可以通<br>用的设定方法，制作了专用的BIOS。这个追加的BIOS被称作“VESA BIOS extension”（VESA-BIOS<br>扩展，简略为VBE）。利用它，就可以使用显卡的高分辨率功能了。</p>]]></content>
      
      
      <categories>
          
          <category> System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Clean Architecture》</title>
      <link href="/2021/02/26/clean-architecture/"/>
      <url>/2021/02/26/clean-architecture/</url>
      
        <content type="html"><![CDATA[<h2 id="零、概述"><a href="#零、概述" class="headerlink" title="零、概述"></a>零、概述</h2><p>软件架构设计是一件非常困难的事情，这通常需要大多数程序员所不具备的经验和技能。同时，也不是所有人都愿意花时间来学习和钻研这个方向。做一个好的软件架构师所需要的自律和专注程度可能会让大部分程序员始料未及，更别提软件架构师这个职业本身的社会认同感与人们投身其中的热情了。</p><p>采用好的软件架构可以大大节省软件项目构建与维护的人力成本。让每次变更都短小简单，易于实施，并且避免缺陷，用最小的成本，最大程度地满足功能性和灵活性的要求。</p><h3 id="0-1-设计与架构究竟是什么？"><a href="#0-1-设计与架构究竟是什么？" class="headerlink" title="0.1 设计与架构究竟是什么？"></a>0.1 设计与架构究竟是什么？</h3><p>一直以来，设计（Design）与架构（Architecture）这两个概念让大多数人十分迷惑——什么是设计？什么是架构？二者究竟有什么区别？</p><p>本书的一个重要目标就是要清晰、明确地对二者进行定义。首先我要明确地说，二者没有任何区别。一丁点区别都没有！</p><p>“架构”这个词往往使用于“高层级”的讨论中。这类讨论一般都把“底层”的实现细节排除在外。而“设计”一词，往往用来指代具体的系统底层组织结构和实现的细节。但是，从一个真正的系统架构师的日常工作来看，这样的区分是根本不成立的。</p><h4 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h4><p>所有这些决策的终极目标是什么呢？一个好的软件设计的终极目标是什么呢？</p><p><strong>软件架构的终极目标是，用最小的人力成本来满足构建和维护该系统的需求。</strong></p><p>一个软件架构的优劣，可以用它满足用户需求所需要的成本来衡量。如果该成本很低，并且在系统的整个生命周期内一直都能维持这样的低成本，那么这个系统的设计就是优良的。如果该系统的每次发布都会提升下一次变更的成本，那么这个设计就是不好的。就这么简单。 </p><h3 id="0-2-两个纬度的价值"><a href="#0-2-两个纬度的价值" class="headerlink" title="0.2 两个纬度的价值"></a>0.2 两个纬度的价值</h3><p>对于每个软件系统，我们都可以通过行为和架构两个维度来体现它的实际价值。软件研发人员应该确保自己的系统在这两个维度上的实际价值都能长时间维持在很高的状态。不幸的是，他们往往只关注一个维度，而忽视了另外一个维度。更不幸的是，他们常常关注的还是错误的维度，这导致了系统的价值最终趋降为零。</p><h4 id="行为价值"><a href="#行为价值" class="headerlink" title="行为价值"></a>行为价值</h4><p>软件系统的行为是其最直观的价值维度。程序员的工作就是让机器按照某种指定方式运转，给系统的使用者创造或者提高利润。程序员们为了达到这个目的，往往需要帮助系统使用者编写一个对系统功能的定义，也就是需求文档。然后，程序员们再把需求文档转化为实际的代码。</p><h4 id="架构价值"><a href="#架构价值" class="headerlink" title="架构价值"></a>架构价值</h4><p>软件系统的第二个价值维度，就体现在软件这个英文单词上：software。“ware”的意思是“产品”，而“soft”的意思，不言而喻，是指软件的灵活性。</p><p>软件系统必须保持灵活。软件发明的目的，就是让我们可以以一种灵活的方式来改变机器的工作行为。对机器上那些很难改变的工作行为，我们通常称之为硬件（hardware）。</p><p>为了达到软件的本来目的，软件系统必须够“软”——也就是说，软件应该容易被修改。当需求方改变需求的时候，随之所需的软件变更必须可以简单而方便地实现。变更实施的难度应该和变更的范畴（scope）成等比关系，而与变更的具体形状（shape）无关。</p><p>需求变更的范畴与形状，是决定对应软件变更实施成本高低的关键。这就是为什么有的代码变更的成本与其实现的功能改变不成比例。这也是为什么第二年的研发成本比第一年的高很多，第三年又比第二年更高。</p><p>从系统相关方（Stakeholder）的角度来看，他们所提出的一系列的变更需求的范畴都是类似的，因此成本也应该是固定的。但是从研发者角度来看，系统用户持续不断的变更需求就像是要求他们不停地用一堆不同形状的拼图块，拼成一个新的形状。整个拼图的过程越来越困难，因为现有系统的形状永远和需求的形状不一致。</p><p>我们在这里使用了“形状”这个词，这可能不是该词的标准用法，但是其寓意应该很明确。毕竟，软件工程师们经常会觉得自己的工作就是把方螺丝拧到圆螺丝孔里面。</p><p>问题的实际根源当然就是系统的架构设计。如果系统的架构设计偏向某种特定的“形状”，那么新的变更就会越来越难以实施。所以，好的系统架构设计应该尽可能做到与“形状”无关。</p><h2 id="一、编程范式"><a href="#一、编程范式" class="headerlink" title="一、编程范式"></a>一、编程范式</h2><h3 id="1-1-结构化编程（Structured-Programming）"><a href="#1-1-结构化编程（Structured-Programming）" class="headerlink" title="1.1 结构化编程（Structured Programming）"></a>1.1 结构化编程（Structured Programming）</h3><p><strong>结构化编程是对程序控制权的直接转移的限制。</strong></p><p>结构化编程范式促使我们先将一段程序递归降解为一系列可证明的小函数，然后再编写相关的测试来试图证明这些函数是错误的。如果这些测试无法证伪这些函数，那么我们就可以认为这些函数是足够正确的，进而推导整个程序是正确的。</p><h3 id="1-2-面向对象编程（Object-Oriented-Programming）"><a href="#1-2-面向对象编程（Object-Oriented-Programming）" class="headerlink" title="1.2 面向对象编程（Object Oriented Programming）"></a>1.2 面向对象编程（Object Oriented Programming）</h3><p><strong>面向对象编程是对程序控制权的间接转移的限制。</strong></p><ul><li><p>封装（Encapsulation），封装也叫作信息隐藏或者数据访问保护。类通过暴露有限的访问接口，授权外部仅能通过类提供的方式来访问内部信息或者数据。它需要编程语言提供权限访问控制语法来支持，例如 Java 中的 private、protected、public 关键字。封装特性存在的意义，一方面是保护数据不被随意修改，提高代码的可维护性；另一方面是仅暴露有限的必要接口，提高类的易用性。</p></li><li><p>抽象（Abstraction），封装主要讲如何隐藏信息、保护数据，那抽象就是讲如何隐藏方法的具体实现，让使用者只需要关心方法提供了哪些功能，不需要知道这些功能是如何实现的。抽象可以通过接口类或者抽象类来实现，但也并不需要特殊的语法机制来支持。抽象存在的意义，一方面是提高代码的可扩展性、维护性，修改实现不需要改变定义，减少代码的改动范围；另一方面，它也是处理复杂系统的有效手段，能有效地过滤掉不必要关注的信息。</p></li><li><p>继承（Inheritance），继承是用来表示类之间的 is-a 关系，比如猫是一种哺乳动物。从继承关系上来讲，继承可以分为两种模式，单继承和多继承。单继承表示一个子类只继承一个父类，多继承表示一个子类可以继承多个父类，比如猫既是哺乳动物，又是爬行动物。为了实现继承这个特性，编程语言需要提供特殊的语法机制来支持。继承主要是用来解决代码复用的问题。</p></li><li><p>多态（Polymorphism），多态是指子类可以替换父类，在实际的代码运行过程中，调用子类的方法实现。多态这种特性也需要编程语言提供特殊的语法机制来实现，比如继承、接口类、duck-typing。多态可以提高代码的扩展性和复用性，是很多设计模式、设计原则、编程技巧的代码实现基础。</p></li><li><p>Clean Architecture 里面指出<strong>多态是函数指针的一种应用</strong>。并用getchar()举了例子。然后用多态的实现引出了“依赖反转”的例子。</p></li></ul><p>面向对象编程就是以多态为手段来对源代码中的依赖关系进行控制的能力，这种能力让软件架构师可以构建出某种插件式架构，让高层策略性组件与底层实现性组件相分离，底层组件可以被编译成插件，实现独立于高层组件的开发和部署。</p><h3 id="1-3-函数式编程（Functional-Programming）"><a href="#1-3-函数式编程（Functional-Programming）" class="headerlink" title="1.3 函数式编程（Functional Programming）"></a>1.3 函数式编程（Functional Programming）</h3><p><strong>函数式编程是对程序中赋值操作的限制</strong></p><ul><li>本质：函数式编程中的函数这个术语不是指计算机中的函数（实际上是Subroutine），而是指数学中的函数，即自变量的映射。也就是说一个函数的值仅决定于函数参数的值，不依赖其他状态。比如sqrt(x)函数计算x的平方根，只要x不变，不论什么时候调用，调用几次，值都是不变的。</li><li>在函数式语言中，函数作为一等公民，可以在任何地方定义，在函数内或函数外，可以作为函数的参数和返回值，可以对函数进行组合。</li><li>纯函数式编程语言中的变量也不是命令式编程语言中的变量，即存储状态的单元，而是代数中的变量，即一个值的名称。变量的值是不可变的（immutable），也就是说不允许像命令式编程语言中那样多次给一个变量赋值。比如说在命令式编程语言我们写“x = x + 1”，这依赖可变状态的事实，拿给程序员看说是对的，但拿给数学家看，却被认为这个等式为假。</li><li>由于命令式编程语言也可以通过类似函数指针的方式来实现高阶函数，函数式的最主要的好处主要是不可变性带来的。没有可变的状态，函数就是引用透明（Referential transparency）的和没有副作用（No Side Effect）。</li><li>一个好处是，函数即不依赖外部的状态也不修改外部的状态，函数调用的结果不依赖调用的时间和位置，这样写的代码容易进行推理，不容易出错。这使得单元测试和调试都更容易。</li><li>不变性带来的另一个好处是：由于（多个线程之间）不共享状态，不会造成资源争用(Race condition)，也就不需要用锁来保护可变状态，也就不会出现死锁，这样可以更好地并发起来，尤其是在对称多处理器（SMP）架构下能够更好地利用多个处理器（核）提供的并行处理能力。</li></ul><h3 id="1-4-总结"><a href="#1-4-总结" class="headerlink" title="1.4 总结"></a>1.4 总结</h3><p>这三个编程范式都对程序员提出了新的限制。每个范式都约束了某种编写代码的方式，没有一个编程范式是在增加新能力。<br>也就是说，我们过去50年学到的东西主要是——什么不应该做。</p><p>我们必须面对这种不友好的现实：软件构建并不是一个迅速前进的技术。今天构建软件的规则和1946年阿兰·图灵写下电子计算机的第一行代码时是一样的。尽管工具变化了，硬件变化了，但是软件编程的核心没有变。<br>总而言之，软件，或者说计算机程序无一例外是由顺序结构、分支结构、循环结构和间接转移这几种行为组合而成的，无可增加，也缺一不可。</p><h2 id="二、设计原则-SOLID-原则"><a href="#二、设计原则-SOLID-原则" class="headerlink" title="二、设计原则 - SOLID 原则"></a>二、设计原则 - SOLID 原则</h2><h3 id="2-1-单一职责（SRP）"><a href="#2-1-单一职责（SRP）" class="headerlink" title="2.1 单一职责（SRP）"></a>2.1 单一职责（SRP）</h3><p>Single Responsibility Principle，一个类只负责完成一个职责或者功能。不要设计大而全的类，要设计粒度小、功能单一的类。单一职责原则是为了实现代码高内聚、低耦合，提高代码的复用性、可读性、可维护性。</p><p>Clean Architecture 中用一个 “工资管理程序中的 Employee 类”举例，这类里面分别有三个函数  </p><ul><li>calculatePay （）函数是由财务部门制定的，他们负责向 CFO 汇报。</li><li>reportHours （）函数是由人力资源部门制定并使用的，他们负责向 coo汇报。</li><li>save （）函数是由 DBA 制定的，他们负责向 CTO 汇报。</li></ul><p>这三个函数由三个部门负责，然后修改其中一个就会影响其他两个，这个类很显然违背了单一职责。</p><p>总结为：<strong>“任何一个软件模块都应该只对某一类行为者负责”</strong></p><p>单一职责原则主要讨论的是函数和类之间的关系——但是它在两个讨论层面上会以不同的形式出现。在组件层面，我们可以将其称为共同闭包原则（Common Closure Principle），在软件架构层面，它则是用于奠定架构边界的变更轴心（Axis of Change）。</p><p>facade 模式：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-736908d53218edd8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="facade_mode.png"></p><h3 id="2-2-开闭原则（OCP）"><a href="#2-2-开闭原则（OCP）" class="headerlink" title="2.2 开闭原则（OCP）"></a>2.2 开闭原则（OCP）</h3><p>Open Closed Principle，对扩展开放，对修改关闭。添加一个新的功能，应该是通过在已有代码基础上扩展代码（新增模块、类、方法、属性等），而非修改已有代码（修改模块、类、方法、属性等）的方式来完成。关于定义，我们有两点要注意。第一点是，开闭原则并不是说完全杜绝修改，而是以最小的修改代码的代价来完成新功能的开发。第二点是，同样的代码改动，在粗代码粒度下，可能被认定为“修改”；在细代码粒度下，可能又被认定为“扩展”。</p><ul><li><p>一个设计良好的计算机系统应该在不需要修改的前提下就可以轻易被扩展。</p></li><li><p>一个好的软件架构设计师会努力将旧代码的修改需求量降至最小，甚至为0。</p></li></ul><h3 id="2-3-里式替换（LSP）"><a href="#2-3-里式替换（LSP）" class="headerlink" title="2.3 里式替换（LSP）"></a>2.3 里式替换（LSP）</h3><p>Liskov Substitution Principle 子类对象（object of subtype/derived class）能够替换程序（program）中父类对象（object of base/parent class）出现的任何地方，并且保证原来程序的逻辑行为（behavior）不变及正确性不被破坏。举例： 是拿父类的单元测试去验证子类的代码。如果某些单元测试运行失败，就有可能说明，子类的设计实现没有完全地遵守父类的约定，子类有可能违背了里式替换原则。</p><h3 id="2-4-接口隔离原则（ISP"><a href="#2-4-接口隔离原则（ISP" class="headerlink" title="2.4 接口隔离原则（ISP)"></a>2.4 接口隔离原则（ISP)</h3><p>Interface Segregation Principle 调用方不应该被强迫依赖它不需要的接口。</p><p>回顾一下ISP最初的成因：在一般情况下，任何层次的软件设计如果依赖于不需要的东西，都会是有害的。从源代码层次来说，<strong>这样的依赖关系会导致不必要的重新编译和重新部署</strong>，对更高层次的软件架构设计来说，问题也是类似的。</p><p>本章所讨论的设计原则告诉我们：任何层次的软件设计如果依赖了它并不需要的东西，就会带来意料之外的麻烦。</p><p><strong>Bad Case:</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2c2ed731a470f6cd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="isp1.png"></p><p><strong>Good Case:</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fb32dba139193d2e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="isp2.png"></p><h3 id="2-5-依赖反转原则（DIP"><a href="#2-5-依赖反转原则（DIP" class="headerlink" title="2.5 依赖反转原则（DIP)"></a>2.5 依赖反转原则（DIP)</h3><p>Dependency Inversion Principle 高层模块（high-level modules）不要依赖低层模块（low-level）。高层模块和低层模块应该通过抽象（abstractions）来互相依赖。除此之外，抽象（abstractions）不要依赖具体实现细节（details），具体实现细节（details）依赖抽象（abstractions）。举例 Tomcat和Java WebApp，两者都依赖同一个“抽象”，也就是 Servlet 规范。</p><p><strong>依赖反转原则（DIP）主要想告诉我们的是，如果想要设计一个灵活的系统，在源代码层次的依赖关系中就应该多引用抽象类型，而非具体实现。</strong></p><p>同理，在应用DIP时，我们也不必考虑稳定的操作系统或者平台设施，因为这些系统接口很少会有变动。我们主要应该关注的是软件系统内部那些会经常变动的（volatile）具体实现模块，这些模块是不停开发的，也就会经常出现变更。</p><p>我们每次修改抽象接口的时候，一定也会去修改对应的具体实现。但反过来，当我们修改具体实现时，却很少需要去修改相应的抽象接口。所以我们可以认为<strong>接口比实现更稳定</strong>。</p><p>依赖反转原则归结为具体编码守则：</p><ul><li><p>应在代码中多使用抽象接口，尽量避免使用那些多变的具体实现类。这条守则适用于所有编程语言，无论静态类型语言还是动态类型语言。同时，对象的创建过程也应该受到严格限制，对此，我们通常会选择用抽象工厂（abstract factory）这个设计模式。</p></li><li><p>不要在具体实现类上创建衍生类。上一条守则虽然也隐含了这层意思，但它还是值得被单独拿出来做一次详细声明。在静态类型的编程语言中，<strong>继承关系是所有一切源代码依赖关系中最强的、最难被修改的，所以我们对继承的使用应该格外小心</strong>。即使是在稍微便于修改的动态类型语言中，这条守则也应该被认真考虑。(我理解这里说的意思不是说不能使用继承，而已谨慎使用继承，防止杂乱无章的继承)</p></li><li><p>不要覆盖（override）包含具体实现的函数。调用包含具体实现的函数通常就意味着引入了源代码级别的依赖。即使覆盖了这些函数，我们也无法消除这其中的依赖——这些函数继承了那些依赖关系。在这里，控制依赖关系的唯一办法，就是创建一个抽象函数，然后再为该函数提供多种具体实现。</p></li><li><p><strong>应避免在代码中写入与任何具体实现相关的名字，或者是其他容易变动的事物的名字</strong>。这基本上是DIP原则的另外一个表达方式。比如mysql存储、redis存储。</p></li></ul><p>控制反转（IOC）: Inversion Of Control，框架提供了一个可扩展的代码骨架，用来组装对象、管理整个执行流程。程序员利用框架进行开发的时候，只需要往预留的扩展点上，添加跟自己业务相关的代码，就可以利用框架来驱动整个程序流程的执行。</p><p>依赖注入（DI）: Dependency Injection 依赖注入的方式来将依赖的类对象传递进来，这样就提高了代码的扩展性，我们可以灵活地替换依赖的类</p><h2 id="三、组件"><a href="#三、组件" class="headerlink" title="三、组件"></a>三、组件</h2><h3 id="3-1-什么是组件"><a href="#3-1-什么是组件" class="headerlink" title="3.1 什么是组件"></a>3.1 什么是组件</h3><p>组件是软件的部署单元，是整个软 系统在部署过程中可以独立完成部署的最小实体。例如，对于 Java 来说 它的组件是 jar 文件。而在 Rub 它们是 ge。在.Net 中，它们 DLL 文件。总而言之 在编译运行语言中，组件是二进制文件的集合。而在解释运行语言中 组件则是 组源代码文件的集合。无采用什么编程语言来开发软件，组件都是该软件在部署过程中的最小单元。</p><p>大型软件系统的构建过程与建筑物修建很类似，都是由一个个小组件组成的。所以，<strong>如果说SOLID原则是用于指导我们如何将砖块砌成墙与房间的，那么组件构建原则就是用来指导我们如何将这些房间组合成房子的</strong>。</p><h3 id="3-2-组件的聚合"><a href="#3-2-组件的聚合" class="headerlink" title="3.2 组件的聚合"></a>3.2 组件的聚合</h3><h4 id="3-2-1-复用-发布等同原则（REP原则-Release-Reuse-Equivalency-Principle）"><a href="#3-2-1-复用-发布等同原则（REP原则-Release-Reuse-Equivalency-Principle）" class="headerlink" title="3.2.1 复用/发布等同原则（REP原则 - Release Reuse Equivalency Principle）"></a>3.2.1 复用/发布等同原则（REP原则 - Release Reuse Equivalency Principle）</h4><p><strong>软件复用的最粒度应等同于其发布的最小粒度</strong></p><p>从软件设计和架构设计的角度来看，REP原则就是指组件中的类与模块必须是彼此紧密相关的，也就是说，一个组件不能由毫无关联类和模块组成，它们之间应该有一个共同的主题或者大方向。</p><p>但从另外一个视角来看，这个原则就没那么简单了 因为根据该原则，一个组件中包含的类与模块还应该是可以同时发布的这意味着它 享相同的版本号与版本跟踪，并且包含在相同的发行文档中，这些都应该同时对该组件的作者和用户有意义。</p><p>这层建议听起来就比较薄弱了，毕竟说某项事情的安排应该“合理”的确有点假大空，不着实该建议薄弱的原因是它没有清晰地定义出到底应该如何将类与模块组合成组件 但即使这样， REP 原则的重要性也是毋庸置疑的，因为违反这个原则的后果事实上很明显定会有人抱怨你的安排“不合理”，并进而对你的软件架构能力产生怀疑</p><h4 id="3-2-2-共同闭包原则-（CCP原则-the-Common-Closure-Principle）"><a href="#3-2-2-共同闭包原则-（CCP原则-the-Common-Closure-Principle）" class="headerlink" title="3.2.2 共同闭包原则 （CCP原则 - the Common Closure Principle）"></a>3.2.2 共同闭包原则 （CCP原则 - the Common Closure Principle）</h4><p><strong>我们应该将那些会同时修改，并且为相同目的而修改的类放到同一个组件中，而将不会同时修改，并且不会为了相同目的而修改的那些类放到不同的组件中。</strong></p><p>对大部分应用程序来说，可维护性的重要性要远远高于可复用性。如果某程序中的代码必须要进行某些变更，那么这些变更最好都体现在同一个组件中，而不是分布于很多个组件中[4]。因为如果这些变更都集中在同一个组件中，我们就只需要重新部署该组件，其他组件则不需要被重新验证、重新部署了。</p><p>总而言之，CCP的主要作用就是提示我们要将所有可能会被一起修改的类集中在一处。也就是说，如果两个类紧密相关，不管是源代码层面还是抽象理念层面，永远都会一起被修改，那么它们就应该被归属为同一个组件。通过遵守这个原则，我们就可以有效地降低因软件发布、验证及部署所带来的工作压力。</p><p>另外，CCP原则和开闭原则（OCP）也是紧密相关的。CCP讨论的就是OCP中所指的“闭包”。OCP原则认为一个类应该便于扩展，而抗拒修改。由于100%的闭包是不可能的，所以我们只能战略性地选择闭包范围。在设计类的时候，我们需要根据历史经验和预测能力，尽可能地将需要被一同变更的那些点聚合在一起。</p><p>对于CCP，我们还可以在此基础上做进一步的延伸，即可以将某一类变更所涉及的所有类尽量聚合在一处。这样当此类变更出现时，我们就可以最大限度地做到使该类变更只影响到有限的相关组件。</p><h5 id="CCP与SRP原则的相似点"><a href="#CCP与SRP原则的相似点" class="headerlink" title="CCP与SRP原则的相似点:"></a>CCP与SRP原则的相似点:</h5><p>如前所述，CCP原则实际上就是SRP原则的组件版。在SRP原则的指导下，我们将会把变更原因不同的函数放入不同的类中。而CCP原则指导我们应该将变更原因不同的类放入不同的组件中。简而言之，这两个原则都可以用以下一句简短的话来概括：<br>**”将由于相同原因而修改，并且需要同时修改的东西放在一起。将由于不同原因而修改，并且不同时修改的东西分开”**。</p><h4 id="3-2-3-共同复用原则（CRP原则-Composite-Reuse-Principle）"><a href="#3-2-3-共同复用原则（CRP原则-Composite-Reuse-Principle）" class="headerlink" title="3.2.3 共同复用原则（CRP原则 - Composite Reuse Principle）"></a>3.2.3 共同复用原则（CRP原则 - Composite Reuse Principle）</h4><p><strong>不要强迫一个组件的用户依赖他们不需要的东西。</strong></p><p>共同复用原则（CRP）是另外一个帮助我们决策类和模块归属于哪一个组件的原则。该原则建议我们将经常共同复用的类和模块放在同一个组件中。<br>通常情况下，类很少会被单独复用。更常见的情况是多个类同时作为某个可复用的抽象定义被共同复用。CRP原则指导我们将这些类放在同一个组件中，而在这样的组件中，我们应该预见到会存在着许多互相依赖的类。</p><p>一个简单的例子就是容器类与其相关的遍历器类，这些类之间通常是紧密相关的，一般会被共同复用，因此应该被放置在同一个组件中。<br>但是CRP的作用不仅是告诉我们应该将哪些类放在一起，更重要的是要告诉我们应该将哪些类分开。因为每当一个组件引用了另一个组件时，就等于增加了一条依赖关系。虽然这个引用关系仅涉及被引用组件中的一个类，但它所带来的依赖关系丝毫没有减弱。也就是说，引用组件已然依赖于被引用组件了。</p><p>由于这种依赖关系的存在，每当被引用组件发生变更时，引用它的组件一般也需要做出相应的变更。即使它们不需要进行代码级的变更，一般也免不了需要被重新编译、验证和部署。哪怕引用组件根本不关心被引用组件中的变更，也要如此。</p><p>因此，当我们决定要依赖某个组件时，最好是实际需要依赖该组件中的每个类。换句话说，我们希望组件中的所有类是不能拆分的，即不应该出现别人只需要依赖它的某几个类而不需要其他类的情况。否则，我们后续就会浪费不少时间与精力来做不必要的组件部署。</p><p>因此在CRP原则中，关于哪些类不应该被放在一起的建议是其更为重要的内容。简而言之，CRP原则实际上是在指导我们：不是紧密相连的类不应该被放在同一个组件里。</p><h5 id="与ISP原则的关系"><a href="#与ISP原则的关系" class="headerlink" title="与ISP原则的关系"></a>与ISP原则的关系</h5><p>CRP原则实际上是ISP原则的一个普适版。ISP原则是建议我们不要依赖带有不需要的函数的类，而CRP原则则是建议我们不要依赖带有不需要的类的组件。上述两条建议实际上都可以用下面一句话来概括：**”不要依赖不需要用到的东西。”**</p><h4 id="3-2-4-三个规则的Treadeoff"><a href="#3-2-4-三个规则的Treadeoff" class="headerlink" title="3.2.4 三个规则的Treadeoff"></a>3.2.4 三个规则的Treadeoff</h4><p>下面我们来看下图。这是一张组件聚合三大原则的张力图，图的边线所描述的是忽视对应原则的后果。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c38b621f4aee1208.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="tradeoff.jpg"></p><p>￼<br>简而言之，只关注REP和CRP的软件架构师会发现，即使是简单的变更也会同时影响到许多组件。相反，如果软件架构师过于关注CCP和REP，则会导致很多不必要的发布。</p><p>优秀的软件架构师应该能在上述三角张力区域中定位一个最适合目前研发团队状态的位置，同时也会根据时间不停调整。例如在项目早期，CCP原则会比REP原则更重要，因为在这一阶段研发速度比复用性更重要。</p><p>一般来说，一个软件项目的重心会从该三角区域的右侧开始，先期主要牺牲的是复用性。然后，随着项目逐渐成熟，其他项目会逐渐开始对其产生依赖，项目重心就会逐渐向该三角区域的左侧滑动。换句话说，一个项目在组件结构设计上的重心是根据该项目的开发时间和成熟度不断变动的，我们对组件结构的安排主要与项目开发的进度和它被使用的方式有关，与项目本身功能的关系其实很小。</p><h3 id="3-3-组件耦合"><a href="#3-3-组件耦合" class="headerlink" title="3.3 组件耦合"></a>3.3 组件耦合</h3><h4 id="3-3-1-无依赖环原则（ADP-Acyclic-Dependency-Principle）"><a href="#3-3-1-无依赖环原则（ADP-Acyclic-Dependency-Principle）" class="headerlink" title="3.3.1 无依赖环原则（ADP - Acyclic Dependency Principle）"></a>3.3.1 无依赖环原则（ADP - Acyclic Dependency Principle）</h4><p><strong>组件依赖关系图中不应该出现环。</strong></p><h4 id="3-3-2-自上而下的设计"><a href="#3-3-2-自上而下的设计" class="headerlink" title="3.3.2 自上而下的设计"></a>3.3.2 自上而下的设计</h4><p>组件结构图是不可能自上而下被设计出来的。它必须随着软件系统的变化而变化和扩张，而不可能在系统构建的最初就被完美设计出来。</p><p>事实上，组件依赖结构图并不是用来描述应用程序功能的，它更像是应用程序在构建性与维护性方面的一张地图。这就是组件的依赖结构图不能在项目的开始阶段被设计出来的原因——当时该项目还没有任何被构建和维护的需要，自然也就不需要一张地图来指引。然而，随着早期被设计并实现出来的模块越来越多，项目中就逐渐出现了要对组件依赖关系进行管理的需求，以此来预防“一觉醒来综合征”的爆发。除此之外，我们还希望将项目变更所影响的范围被限制得越小越好，因此需要应用单一职责原则（SRP）和共同闭包原则（CCP）来将经常同时被变更的类聚合在一起。</p><p>组件结构图中的一个重要目标是指导如何隔离频繁的变更。我们不希望那些频繁变更的组件影响到其他本来应该很稳定的组件，例如，我们通常不会希望无关紧要的GUI变更影响到业务逻辑组件；我们也不希望对报表的增删操作影响到其高阶策略。出于这样的考虑，软件架构师们才有必要设计并且铸造出一套组件依赖关系图来，以便将稳定的高价值组件与常变的组件隔离开，从而起到保护作用。</p><p>另外，随着应用程序的增长，创建可重用组件的需要也会逐渐重要起来。这时CRP又会开始影响组件的组成。最后当循环依赖出现时，随着无循环依赖原则（ADP）的应用，组件依赖关系会产生相应的抖动和扩张。</p><p>如果我们在设计具体类之前就来设计组件依赖关系，那么几乎是必然要失败的。因为在当下，我们对项目中的共同闭包一无所知，也不可能知道哪些组件可以复用，这样几乎一定会创造出循环依赖的组件。因此，<strong>组件依赖关系是必须要随着项目的逻辑设计一起扩张和演进的。</strong></p><h4 id="3-3-3-稳定依赖原则-（SDP-Stabilization-Dependency-Principle）"><a href="#3-3-3-稳定依赖原则-（SDP-Stabilization-Dependency-Principle）" class="headerlink" title="3.3.3 稳定依赖原则 （SDP - Stabilization Dependency Principle）"></a>3.3.3 稳定依赖原则 （SDP - Stabilization Dependency Principle）</h4><p><strong>依赖关系必须要指向更稳定的方向。</strong> 例如：我们Service Mesh 的控制面氛围<code>cp_http</code>、<code>cp_core</code> 两个模块，<code>cp_core</code> 是比较核心比较稳定的模块，改动也会比较少。<code>cp_http</code>是非核心模块，改动了个会比较多。</p><p>设计这件事不可能是完全静止的，如果我们要让一个设计是可维护的，那么其中某些部分就必须是可变的。通过遵守共同闭包原则（CCP），我们可以创造出对某些变更敏感，对其他变更不敏感的组件。这其中的一些组件在设计上就已经是考虑了易变性，预期它们会经常发生变更的。</p><p>任何一个我们预期会经常变更的组件都不应该被一个难于修改的组件所依赖，否则这个多变的组件也将会变得非常难以被修改。</p><p>这就是软件开发的困难之处，我们精心设计的一个容易被修改的组件很可能会由于别人的一条简单依赖而变得非常难以被修改。即使该模块中没有一行代码需要被修改，但是整个模块在被修改时所面临的挑战性也已经存在了。而通过遵守稳定依赖原则（SDP），我们就可以确保自己设计中那些容易变更的模块不会被那些难于修改的组件所依赖。<br>稳定性</p><p>我们该如何定义“稳定性”呢？譬如说将一个硬币立起来放，你认为它会处于一个稳定的位置吗？当然不会。然而，除非受到外界因素干扰，否则硬币本身可以在这个位置保持相当长的一段时间。因此稳定性应该与变更的频繁度没有直接关系。但问题是硬币并没有倒，为什么我们却并不认为它是稳定的呢？</p><p>下面来看看Webster在线字典中的描述：稳定指的是“很难移动”。所以稳定性应该与变更所需的工作量有关。例如，硬币是不稳定的，因为只需要很小的动作就可以推倒它，而桌子则是非常稳定的，因为将它掀翻需要很大的动作。</p><p>但如果将这套理论关联到软件开发的问题上呢？软件组件的变更困难度与很多因素有关，例如代码的体量大小、复杂度、清晰度等。我们在这里会忽略这些因素，只集中讨论一个特别的因素——让软件组件难于修改的一个最直接的办法就是让很多其他组件依赖于它。带有许多入向依赖关系的组件是非常稳定的，因为它的任何变更都需要应用到所有依赖它的组件上。</p><h4 id="3-3-4-稳定抽象原则（SAP-Stable-Abstractions-Principle）"><a href="#3-3-4-稳定抽象原则（SAP-Stable-Abstractions-Principle）" class="headerlink" title="3.3.4 稳定抽象原则（SAP - Stable Abstractions Principle）"></a>3.3.4 稳定抽象原则（SAP - Stable Abstractions Principle）</h4><p><strong>一个组件的抽象化程度应该与其稳定性保持一致。</strong></p><h5 id="3-3-5-高阶策略应该放在哪里？"><a href="#3-3-5-高阶策略应该放在哪里？" class="headerlink" title="3.3.5 高阶策略应该放在哪里？"></a>3.3.5 高阶策略应该放在哪里？</h5><p>在一个软件系统中，总有些部分是不应该经常发生变更的。这些部分通常用于表现该系统的高阶架构设计及一些策略相关的高阶决策。我们不想让这些业务决策和架构设计经常发生变更，因此这些代表了系统高阶策略的组件应该被放到稳定组件（I=0）中，而不稳定的组件（I=1）中应该只包含那些我们想要快速和方便修改的部分。</p><p>然而，如果我们将高阶策略放入稳定组件中，那么用于描述那些策略的源代码就很难被修改了。这可能会导致整个系统的架构设计难于被修改。如何才能让一个无限稳定的组件（I=0）接受变更呢？开闭原则（OCP）为我们提供了答案。这个原则告诉我们：创造一个足够灵活、能够被扩展，而且不需要修改的类是可能的，而这正是我们所需要的。哪一种类符合这个原则呢？答案是抽象类。</p><h5 id="3-3-6-稳定抽象原则简介"><a href="#3-3-6-稳定抽象原则简介" class="headerlink" title="3.3.6 稳定抽象原则简介"></a>3.3.6 稳定抽象原则简介</h5><p>稳定抽象原则（SAP）为组件的稳定性与它的抽象化程度建立了一种关联。一方面，该原则要求稳定的组件同时应该是抽象的，这样它的稳定性就不会影响到扩展性。另一方面，该原则也要求一个不稳定的组件应该包含具体的实现代码，这样它的不稳定性就可以通过具体的代码被轻易修改。</p><p>因此，如果一个组件想要成为稳定组件，那么它就应该由接口和抽象类组成，以便将来做扩展。如此，这些既稳定又便于扩展的组件可以被组合成既灵活又不会受到过度限制的架构。</p><p>将SAP与SDP这两个原则结合起来，就等于组件层次上的DIP。因为SDP要求的是让依赖关系指向更稳定的方向，而SAP则告诉我们稳定性本身就隐含了对抽象化的要求，即依赖关系应该指向更抽象的方向。</p><p>然而，DIP毕竟是与类这个层次有关的原则——对类来说，设计是没有灰色地带的。一个类要么是抽象类，要么就不是。SDP与SAP这对原则是应用在组件层面上的，我们要允许一个组件部分抽象，部分稳定。</p><h2 id="四、软件架构"><a href="#四、软件架构" class="headerlink" title="四、软件架构"></a>四、软件架构</h2><h3 id="4-1-什么是软件架构？软件架构师工作内容是什么？"><a href="#4-1-什么是软件架构？软件架构师工作内容是什么？" class="headerlink" title="4.1 什么是软件架构？软件架构师工作内容是什么？"></a>4.1 什么是软件架构？软件架构师工作内容是什么？</h3><p>首先，<strong>软件架构师自身需要是程序员</strong>，并且必须一直坚持做一线程序员，绝对不要听从那些说应该让软件架构师从代码中解放出来以专心解决高阶问题的伪建议。不是这样的！软件架构师其实应该是能力最强的一群程序员，他们通常会在自身承接编程任务的同时，逐渐引导整个团队向一个<strong>能够最大化生产力的系统设计方向前进</strong>。也许软件架构师生产的代码量不是最多的，但是他们必须不停地承接编程任务。如果不亲身承受因系统设计而带来的麻烦，就体会不到设计不佳所带来的痛苦，接着就会逐渐迷失正确的设计方向。</p><p>软件系统的架构质量是由它的构建者所决定的，<strong>软件架构这项工作的实质就是规划如何将系统切分成组件，并安排好组件之间的排列关系，以及组件之间互相通信的方式</strong>。</p><p>而设计软件架构的目的，<strong>就是为了在工作中更好地对这些组件进行研发、部署、运行以及维护</strong>。</p><p>软件架构设计的主要目标是支撑软件系统的全生命周期，设计良好的架构可以让系统便于理解、易于修改、方便维护，并且能轻松部署。软件架构的终极目标就是最大化程序员的生产力，同时最小化系统的总运营成本。</p><h4 id="4-1-1-开发（development）"><a href="#4-1-1-开发（development）" class="headerlink" title="4.1.1 开发（development）"></a>4.1.1 开发（development）</h4><p>一个开发起来很困难的软件系统一般不太可能会有一个长久、健康的生命周期，所以系统架构的作用就是要方便其开发团队对它的开发。</p><p>这意味着，不同的团队结构应该采用不同的架构设计。一方面，对于一个只有五个开发人员的小团队来说，他们完全可以非常高效地共同开发一个没有明确定义组件和接口的单体系统（monolithic system）。事实上，<strong>这样的团队可能会发现软件架构在早期开发中反而是一种障碍</strong>。这可能就是为什么许多系统都没有设计一个良好架构的原因，因为它们的开发团队起初都很小，<strong>不需要设计一些上层建筑来限制某些事情</strong>。</p><h4 id="4-1-2-部署（Deployment）"><a href="#4-1-2-部署（Deployment）" class="headerlink" title="4.1.2 部署（Deployment）"></a>4.1.2 部署（Deployment）</h4><p>为了让开发成为有效的工作，软件系统就必须是可部署的。在通常情况下，一个系统的部署成本越高，可用性就越低。因此，实现一键式的轻松部署应该是我们设计软件架构的一个目标。</p><p>但很不幸，我们在系统的早期开发中很少会考虑部署策略方面的事情，这常常会导致一些易于开发、难于部署的系统架构。</p><p>例如，在系统的早期开发中，开发人员可能会决定采用某种“微服务架构”。这种架构的组件边界清晰，接口稳定，非常利于开发。但当我们实际部署这种系统时，就会发现其微服务的数量已经大到令人望而生畏，而配置这些微服务之间的连接以及启动时间都会成为系统出错的主要来源。</p><p>如果软件架构师早先就考虑到这些部署问题，可能就会有意地减少微服务的数量，采用进程内部组件与外部服务混合的架构，以及更加集成式的连接管理方式。</p><h4 id="4-1-3-维护（Maintenance）"><a href="#4-1-3-维护（Maintenance）" class="headerlink" title="4.1.3 维护（Maintenance）"></a>4.1.3 维护（Maintenance）</h4><p>在软件系统的所有方面中，维护所需的成本是最高的。满足永不停歇的新功能需求，以及修改层出不穷的系统缺陷这些工作将会占去绝大部分的人力资源。</p><p>系统维护的主要成本集中在“探秘”和“风险”这两件事上。其中，“探秘（spelunking）”的成本主要来自我们对于现有软件系统的挖掘，目的是确定新增功能或被修复问题的最佳位置和最佳方式。而“风险（risk）”，则是指当我们进行上述修改时，总是有可能衍生出新的问题，这种可能性就是风险成本。</p><p>我们可以通过精雕细琢的架构设计极大地降低这两项成本。通过将系统切分为组件，并使用稳定的接口将组件隔离，我们可以将未来新功能的添加方式明确出来，并大幅度地降低在修改过程中对系统其他部分造成伤害的可能性。</p><h4 id="4-1-3-保持可选项"><a href="#4-1-3-保持可选项" class="headerlink" title="4.1.3 保持可选项"></a>4.1.3 保持可选项</h4><p>正如我们在之前章节中所说的，软件有行为价值与架构价值两种价值。这其中的第二种价值又比第一种更重要，因为它正是软件之所以“软”的原因。<br>软件被发明出来就是因为我们需要一种灵活和便捷的方式来改变机器的行为。而软件的灵活性则取决于系统的整体状况、组件的布置以及组件之间的连接方式。<br>我们让软件维持“软”性的方法就是尽可能长时间地保留尽可能多的可选项。那么到底哪些选项是我们应该保留的？它们就是那些无关紧要的细节设计。<br>基本上，所有的软件系统都可以降解为策略与细节这两种主要元素。策略体现的是软件中所有的业务规则与操作过程，因此它是系统真正的价值所在。<br>而细节则是指那些让操作该系统的人、其他系统以及程序员们与策略进行交互，但是又不会影响到策略本身的行为。它们包括I/O设备、数据库、Web系统、服务器、框架、交互协议等。<br>软件架构师的目标是创建一种系统形态，该形态会以策略为最基本的元素，并让细节与策略脱离关系，以允许在具体决策过程中推迟或延迟与细节相关的内容。<br>如果在开发高层策略时有意地让自己摆脱具体细节的纠缠，我们就可以将与具体实现相关的细节决策推迟或延后，因为越到项目的后期，<strong>我们就拥有越多的信息来做出合理的决策</strong>。<br><strong>一个优秀的软件架构师应该致力于最大化可选项数量。</strong></p><h3 id="4-2-独立性"><a href="#4-2-独立性" class="headerlink" title="4.2 独立性"></a>4.2 独立性</h3><h4 id="4-2-1-用例"><a href="#4-2-1-用例" class="headerlink" title="4.2.1 用例"></a>4.2.1 用例</h4><p>我们先来看第一个支持目标：用例。我们认为一个系统的架构必须能支持其自身的设计意图。也就是说，如果某系统是一个购物车应用，那么该系统的架构就必须非常直观地支持这类应用可能会涉及的所有用例。事实上，这本来就是架构师们首先要关注的问题，也是架构设计过程中的首要工作。<strong>软件的架构必须为其用例提供支持</strong>。</p><p>然而，正如我们前面所讨论的，一个系统的架构对其行为并没有太大的影响。虽然架构也可以限制一些行为选项，但这种影响所涉及的范围并不大。<strong>一个设计良好的架构在行为上对系统最重要的作用就是明确和显式地反映系统设计意图的行为，使其在架构层面上可见。</strong></p><h4 id="4-2-2-运行"><a href="#4-2-2-运行" class="headerlink" title="4.2.2 运行"></a>4.2.2 运行</h4><p>架构在支持系统运行方面扮演着更实际的角色。如果某个系统每秒要处理100000个用户，该系统的架构就必须能支持这种级别的吞吐量和响应时间。同样的，如果某个系统要在毫秒级的时间内完成对大数据仓库的查询，那么该系统的架构也必须能支持这类操作。</p><h4 id="4-2-3-开发"><a href="#4-2-3-开发" class="headerlink" title="4.2.3 开发"></a>4.2.3 开发</h4><p>系统的架构在支持开发环境方面当然扮演着重要的角色，我们在这里可以引述一下康威定律：</p><blockquote><p>任何一个组织在设计系统时，往往都会复制出一个与该组织内沟通结构相同的系统。</p></blockquote><p>一个由多个不同目标的团队协作开发的系统必须具有相应的软件架构。这样，这些团队才可以各自独立地完成工作，不会彼此干扰。这就需要恰当地将系统切分为一系列隔离良好、可独立开发的组件。然后才能将这些组件分配给不同的团队，各自独立开发。</p><h4 id="4-2-4-部署"><a href="#4-2-4-部署" class="headerlink" title="4.2.4 部署"></a>4.2.4 部署</h4><p>一个系统的架构在其部署的便捷性方面起到的作用也是非常大的。设计目标一定是实现“立刻部署”。一个设计良好的架构通常不会依赖于成堆的脚本与配置文件，也不需要用户手动创建一堆“有严格要求”的目录与文件。总而言之，一个设计良好的软件架构可以让系统在构建完成之后立刻就能部署。</p><p>同样的，这些也需要通过正确地划分、隔离系统组件来实现，这其中包括开发一些主组件，让它们将整个系统黏合在一起，正确地启动、连接并监控每个组件。同样的，这些也需要通过正确地划分、隔离系统组件来实现，这其中包括开发一些主组件，让它们将整个系统黏合在一起，正确地启动、连接并监控每个组件。</p><h4 id="4-2-5-保留可选项"><a href="#4-2-5-保留可选项" class="headerlink" title="4.2.5 保留可选项"></a>4.2.5 保留可选项</h4><p>一个设计良好的架构应该充分地权衡以上所述的所有关注点，然后尽可能地形成一个可以同时满足所有需求的组件结构。这说起来还挺容易的，不是吗？</p><p>事实上，要实现这种平衡是很困难的。主要问题是，我们在大部分时间里是无法预知系统的所有用例的，而且我们也无法提前预知系统的运行条件、开发团队的结构，或者系统的部署需求。更糟糕的是，就算我们能提前了解这些需求，随着系统生命周期的演进，这些需求也会不可避免地发生变化。总而言之，事实上我们想要达到的目标本身就是模糊多变的。真实的世界就这样。</p><p>然而，我们还是可以通过采用一些实现成本较低的架构原则来做一些事情的。虽然我们没有清晰的目标，但采用一些原则总是有助于提前解决一些平衡问题。通过遵守这些原则可以帮助我们正确地将系统划分为一些隔离良好的组件，以便尽可能长时间地为我们的未来保留尽可能多的可选项。</p><p><strong>一个设计良好的架构应该通过保留可选项的方式，让系统在任何情况下都能方便地做出必要的变更。</strong></p><h4 id="4-2-6-按层解耦"><a href="#4-2-6-按层解耦" class="headerlink" title="4.2.6 按层解耦"></a>4.2.6 按层解耦</h4><p>一个系统可以被解耦成若干个水平分层——UI界面、应用独有的业务逻辑、领域普适的业务逻辑、数据库等。</p><h4 id="4-2-7-用例的解耦"><a href="#4-2-7-用例的解耦" class="headerlink" title="4.2.7 用例的解耦"></a>4.2.7 用例的解耦</h4><p>如果我们按照变更原因的不同对系统进行解耦，就可以持续地向系统内添加新的用例，而不会影响旧有的用例。如果我们同时对支持这些用例的UI和数据库也进行了分组，那么每个用例使用的就是不同面向的UI与数据库，因此增加新用例就更不太可能会影响旧有的用例了。</p><h4 id="4-2-8-结构模式"><a href="#4-2-8-结构模式" class="headerlink" title="4.2.8 结构模式"></a>4.2.8 结构模式</h4><p>SOA还是但单体？一个设计良好的架构总是要为将来多留一些可选项，这里所讨论的解耦模式也是这样的可选项之一。</p><h4 id="4-2-9-开发独立性"><a href="#4-2-9-开发独立性" class="headerlink" title="4.2.9 开发独立性"></a>4.2.9 开发独立性</h4><p>我们进行架构设计的第三个目标是支持系统的开发。很显然，当系统组件之间被高度解耦之后，开发团队之间的干扰就大大减少了。譬如说，如果系统的业务逻辑与其UI无关，那么专注于UI开发的团队就不会对专注于业务逻辑开发的团队造成多大的影响。同样的，如果系统的各个用例之间相互隔离，那么专注于addOrder用例的团队就不太可能干扰到负责deleteOrder用例的团队。</p><h4 id="4-2-10-部署独立性"><a href="#4-2-10-部署独立性" class="headerlink" title="4.2.10 部署独立性"></a>4.2.10 部署独立性</h4><p>这种按用例和水平分层的解耦也会给系统的部署带来极大的灵活性。实际上，如果解耦工作做得好，我们甚至可以在系统运行过程中热切换（hot-swap）其各个分层实现和具体用例。在这种情况下，我们增加新用例就只需要在系统中添加一些新的jar文件，或启动一些服务即可，其他部分将完全不受影响。</p><h4 id="4-2-11-重复"><a href="#4-2-11-重复" class="headerlink" title="4.2.11 重复"></a>4.2.11 重复</h4><p>架构师们经常会钻进一个牛角尖——害怕重复。</p><p>当然，重复在软件行业里一般来说都是坏事。我们不喜欢重复的代码，当代码真的出现重复时，我们经常会感到作为一个专业人士，自己是有责任减少或消除这种重复的。</p><p>但是重复也存在着很多种情况。其中有些是真正的重复，在这种情况下，每个实例上发生的每项变更都必须同时应用到其所有的副本上。重复的情况中也有一些是假的，或者说这种重复只是表面性的。如果有两段看起来重复的代码，它们走的是不同的演进路径，也就是说它们有着不同的变更速率和变更缘由，那么这两段代码就不是真正的重复。等我们几年后再回过头来看，可能就会发现这两段代码是非常不一样的了。</p><h4 id="4-2-12-再谈解耦模式"><a href="#4-2-12-再谈解耦模式" class="headerlink" title="4.2.12 再谈解耦模式"></a>4.2.12 再谈解耦模式</h4><p>让我们再回到解耦模式的问题上来。按水平分层和用例解耦一个系统有很多种方式。例如，我们可以在源码层次上解耦、二进制层次上解耦（部署），也可以在执行单元层次上解耦（服务）。</p><ul><li>源码层次：我们可以控制源代码模块之间的依赖关系，以此来实现一个模块的变更不会导致其他模块也需要变更或重新编译（例如Ruby Gem）。在这种解耦模式下，系统所有的组件都会在同一个地址空间内执行，它们会通过简单的函数调用来进行彼此的交互。这类系统在运行时是作为一个执行文件被统一加载到计算机内存中的。人们经常把这种模式叫作单体结构。</li><li>部署层次：我们可以控制部署单元（譬如jar文件、DLL、共享库等）之间的依赖关系，以此来实现一个模块的变更不会导致其他模块的重新构建和部署。在这种模式下，大部分组件可能还是依然运行在同一个地址空间内，通过彼此的函数调用通信。但有一些别的组件可能会运行在同一个处理器下的其他进程内，使用跨进程通信，或者通过socket或共享内存进行通信。这里最重要的是，这些组件的解耦产生出许多可独立部署的单元，例如jar文件、Gem文件和DLL等。</li><li>服务层次：我们可以将组件间的依赖关系降低到数据结构级别，然后仅通过网络数据包来进行通信。这样系统的每个执行单元在源码层和二进制层都会是一个独立的个体，它们的变更不会影响其他地方（例如，常见的服务或微服务就都是如此的）。</li></ul><h3 id="4-3-划分边界"><a href="#4-3-划分边界" class="headerlink" title="4.3 划分边界"></a>4.3 划分边界</h3><p>软件架构设计本身就是一门划分边界的艺术。边界的作用是将软件分割成各种元素，以便约束边界两侧之间的依赖关系。其中有一些边界是在项目初期——甚至在编写代码之前——就已经划分好，而其他的边界则是后来才划分的。在项目初期划分这些边界的目的是方便我们尽量将一些决策延后进行，并且确保未来这些决策不会对系统的核心业务逻辑产生干扰。</p><p>正如我们之前所说，架构师们所追求的目标是最大限度地降低构建和维护一个系统所需的人力资源。那么我们就需要了解一个系统最消耗人力资源的是什么？答案是系统中存在的耦合——尤其是那些过早做出的、不成熟的决策所导致的耦合。</p><p>那么，怎样的决策会被认为是过早且不成熟的呢？答案是那些决策与系统的业务需求（也就是用例）无关。这部分决策包括我们要采用的框架、数据库、Web服务器、工具库、依赖注入等。在一个设计良好的系统架构中，这些细节性的决策都应该是辅助性的，可以被推迟的。一个设计良好的系统架构不应该依赖于这些细节，而应该尽可能地推迟这些细节性的决策，并致力于将这种推迟所产生的影响降到最低。</p><h4 id="4-3-1-插件式架构"><a href="#4-3-1-插件式架构" class="headerlink" title="4.3.1 插件式架构"></a>4.3.1 插件式架构</h4><p>综上所述，我们似乎可以基于数据库和GUI这两个为例来建立一种向系统添加其他组件的模式。这种模式与支持第三方插件的系统模式是一样的。</p><p>事实上，软件开发技术发展的历史就是一个如何想方设法方便地增加插件，从而构建一个可扩展、可维护的系统架构的故事。系统的核心业务逻辑必须和其他组件隔离，保持独立，而这些其他组件要么是可以去掉的，要么是有多种实现的。</p><p>由于用户界面在这个设计中是以插件形式存在的，所以我们可以用插拔的方式切换很多不同类型的用户界面。可以是基于Web模式的、基于客户端/服务器端模式的、基于SOA模式的、基于命令行模式的或者基于其他任何类型的用户界面技术的。</p><p>数据库也类似。因为我们现在是将数据库作为插件来对待的，所以它就可以被替换成不同类型的SQL数据库、NoSQL数据库，甚至基于文件系统的数据库，以及未来任何一种我们认为有必要发展的数据库技术。<br>当然，这些替换工作可能并不轻松，如果我们的系统一开始是按照Web方式部署的，那么为它写一个客户端/服务器端模型的UI插件就可能会比较困难一些。很可能业务逻辑与新UI之间的交互方式也要重新修改。但即使这样，插件式架构也至少为我们提供了这种实现的可能性。</p><h4 id="4-3-3-插件式架构好处"><a href="#4-3-3-插件式架构好处" class="headerlink" title="4.3.3 插件式架构好处"></a>4.3.3 插件式架构好处</h4><p>将系统设计为插件式架构，就等于构建起了一面变更无法逾越的防火墙。换句话说，只要GUI是以插件形式插入系统的业务逻辑中的，那么GUI这边所发生的变更就不会影响系统的业务逻辑。</p><p>所以，边界线也应该沿着系统的变更轴来画。也就是说，位于边界线两侧的组件应该以不同原因、不同速率变化着。</p><p>一个系统的GUI与业务逻辑的变更原因、变更速率显然是不同的，所以二者中间应该有一条边界线。同样的，一个系统的业务逻辑与依赖注入框架之间的变更原因和变更速度也会不同，它们之间也应该画边界线。</p><p><strong>这其实就是单一职责原则（SRP）的具体实现，SRP的作用就是告诉我们应该在哪里画边界线。</strong></p><h3 id="4-4-边界剖析"><a href="#4-4-边界剖析" class="headerlink" title="4.4 边界剖析"></a>4.4 边界剖析</h3><h4 id="4-4-1-跨边界调用"><a href="#4-4-1-跨边界调用" class="headerlink" title="4.4.1 跨边界调用"></a>4.4.1 跨边界调用</h4><p>在运行时，跨边界调用指的是边界线一侧的函数调用另一侧的函数，并同时传递数据的行为。构造合理的跨边界调用需要我们对源码中的依赖关系进行合理管控。</p><p>为什么需要管控源码中的依赖关系呢？因为当一个模块的源码发生变更时，其他模块的源码也可能会随之发生变更或重新编译，并需要重新部署。所谓划分边界，就是指在这些模块之间建立这种针对变更的防火墙。</p><h4 id="4-4-2-本地进程"><a href="#4-4-2-本地进程" class="headerlink" title="4.4.2 本地进程"></a>4.4.2 本地进程</h4><p>系统架构还有一个更明显的物理边界形式，那就是本地进程。本地进程一般是由命令行启动或其他等价的系统调用产生的。本地进程往往运行于单个处理器或多核系统的同一组处理器上，但它们拥有各自不同的地址空间。一般来说，现有的内存保护机制会使这些进程无法共享其内存，但它们通常可以用某种独立的内存区域来实现共享。</p><p>最常见的情况是，这些本地进程会用socket来实现彼此的通信。当然，它们也可以通过一些操作系统提供的方式来通信，例如共享邮件或消息队列。</p><h4 id="4-4-3-服务"><a href="#4-4-3-服务" class="headerlink" title="4.4.3 服务"></a>4.4.3 服务</h4><p>系统架构中最强的边界形式就是服务。一个服务就是一个进程，它们通常由命令行环境或其他等价的系统调用来产生。服务并不依赖于具体的运行位置，两个互相通信的服务既可以处于单一物理处理器或多核系统的同一组处理器上，也可以彼此位于不同的处理器上。服务会始终假设它们之间的通信将全部通过网络进行。</p><p>服务之间的跨边界通信相对于函数调用来说，速度是非常缓慢的，其往返时间可以从几十毫秒到几秒不等。因此我们在划分架构边界时，一定要尽可能地控制通信次数。在这个层次上通信必须能够适应高延时情况。</p><p>除此之外，我们可以在服务层次上使用与本地进程相同的规则。也就是让较低层次服务成为较高层次服务的“插件”。为此，我们要确保高层服务的源码中没有包含任何与低层服务相关的物理信息（例如URI）。</p><h3 id="4-5-策略和层次"><a href="#4-5-策略和层次" class="headerlink" title="4.5 策略和层次"></a>4.5 策略和层次</h3><p>本质上，所有的软件系统都是一组策略语句的集合。是的，可以说计算机程序不过就是一组仔细描述如何将输入转化为输出的策略语句的集合。<br>在大多数非小型系统（nontrivial system）中，整体业务策略通常都可以被拆解为多组更小的策略语句。一部分策略语句专门用于描述计算部分的业务逻辑，另一部分策略语句则负责描述计算报告的格式。除此之外，可能还会有一些用于描述如何校验输入数据的策略。<br>软件架构设计的工作重点之一就是，将这些策略彼此分离，然后将它们按照变更的方式进行重新分组。其中变更原因、时间和层次相同的策略应该被分到同一个组件中。反之，变更原因、时间和层次不同的策略则应该分属于不同的组件。</p><h4 id="4-5-1-层次"><a href="#4-5-1-层次" class="headerlink" title="4.5.1 层次"></a>4.5.1 层次</h4><p>我们对“层次”是严格按照“输入与输出之间的距离”来定义的。也就是说，一条策略距离系统的输入/输出越远，它所属的层次就越高。而直接管理输入/输出的策略在系统中的层次是最低的。</p><h3 id="4-6-业务逻辑"><a href="#4-6-业务逻辑" class="headerlink" title="4.6 业务逻辑"></a>4.6 业务逻辑</h3><p>严格地讲，<strong>业务逻辑就是程序中那些真正用于赚钱或省钱的业务逻辑与过程</strong>。更严格地讲，无论这些业务逻辑是在计算机上实现的，还是人工执行的，它们在省钱/赚钱上的作用都是一样的。</p><h4 id="4-6-1-业务实体"><a href="#4-6-1-业务实体" class="headerlink" title="4.6.1 业务实体"></a>4.6.1 业务实体</h4><p>业务实体实际上就是计算机系统中的一种对象，这种对象中包含了一系列用于操作关键数据的业务逻辑。这些实体对象要么直接包含关键业务数据，要么可以很容易地访问这些数据。业务实体的接口层则是由那些实现关键业务逻辑、操作关键业务数据的函数组成的。</p><h4 id="4-6-2-用例"><a href="#4-6-2-用例" class="headerlink" title="4.6.2 用例"></a>4.6.2 用例</h4><p>并不是所有的业务逻辑都是一个纯粹的业务实体。例如，有些业务逻辑是通过定义或限制自动化系统的运行方式来实现赚钱或省钱的业务的。这些业务逻辑就不能靠人工来执行，它们只有在作为自动化系统的一部分时才有意义。</p><h3 id="4-7-尖叫的软件架构"><a href="#4-7-尖叫的软件架构" class="headerlink" title="4.7 尖叫的软件架构"></a>4.7 尖叫的软件架构</h3><h4 id="4-7-1-架构设计的主题"><a href="#4-7-1-架构设计的主题" class="headerlink" title="4.7.1 架构设计的主题"></a>4.7.1 架构设计的主题</h4><p>软件的系统架构应该为该系统的用例提供支持。这就像住宅和图书馆的建筑计划满篇都在非常明显地凸显这些建筑的用例一样，软件系统的架构设计图也应该非常明确地凸显该应用程序会有哪些用例。</p><p>架构设计不是（或者说不应该是）与框架相关的，这件事不应该是基于框架来完成的。对于我们来说，框架只是一个可用的工具和手段，而不是一个架构所规范的内容。如果我们的架构是基于框架来设计的，它就不能基于我们的用例来设计了。</p><h4 id="4-7-2-架构设计的核心目标"><a href="#4-7-2-架构设计的核心目标" class="headerlink" title="4.7.2 架构设计的核心目标"></a>4.7.2 架构设计的核心目标</h4><p>一个良好的架构设计应该围绕着用例来展开，这样的架构设计可以在脱离框架、工具以及使用环境的情况下完整地描述用例。这就好像一个住宅建筑设计的首要目标应该是满足住宅的使用需求，而不是确保一定要用砖来构建这个房子。架构师应该花费很多精力来确保该架构的设计在满足用例需要的情况下，尽可能地允许用户能自由地选择建筑材料（砖头、石料或者木材）。</p><p>而且，良好的架构设计应该尽可能地允许用户推迟和延后决定采用什么框架、数据库、Web服务以及其他与环境相关的工具。框架应该是一个可选项，良好的架构设计应该允许用户在项目后期再决定是否采用Rails、Spring、Hibernate、Tomcat、MySQL这些工具。同时，良好的架构设计还应该让我们很容易改变这些决定。总之，良好的架构设计应该只关注用例，并能将它们与其他的周边因素隔离。</p><h4 id="4-7-3-框架是工具不是生活信条"><a href="#4-7-3-框架是工具不是生活信条" class="headerlink" title="4.7.3 框架是工具不是生活信条"></a>4.7.3 框架是工具不是生活信条</h4><p>当然，框架通常可以是非常强大、非常有用的。但框架作者往往对自己写出的框架有着极深的信念，他们所写出来的使用手册一般都是从如何成为该框架的虔诚信徒的角度来描绘如何使用这个框架的。甚至这些框架的使用者所写的教程也会出现这种传教士模式。他们会告诉你某个框架是能包揽一切、超越一切、解决一切问题的存在。</p><p>这不应该成为你的观点。</p><p>我们一定要带着怀疑的态度审视每一个框架。是的，采用框架可能会很有帮助，但采用它们的成本呢？我们一定要懂得权衡如何使用一个框架，如何保护自己。无论如何，<strong>我们需要仔细考虑如何能保持对系统用例的关注，避免让框架主导我们的架构设计</strong>。</p><h4 id="4-7-4-可测试的框架设计"><a href="#4-7-4-可测试的框架设计" class="headerlink" title="4.7.4 可测试的框架设计"></a>4.7.4 可测试的框架设计</h4><p>如果系统架构的所有设计都是围绕着用例来展开的，并且在使用框架的问题上保持谨慎的态度，那么我们就应该可以在不依赖任何框架的情况下针对这些用例进行单元测试。另外，我们在运行测试的时候不应该运行Web服务，也不应该需要连接数据库。我们测试的应该只是一个简单的业务实体对象，没有任何与框架、数据库相关的依赖关系。总而言之，我们应该通过用例对象来调度业务实体对象，确保所有的测试都不需要依赖框架。<br>本章小结</p><h3 id="4-8-整洁架构"><a href="#4-8-整洁架构" class="headerlink" title="4.8 整洁架构"></a>4.8 整洁架构</h3><p>在过去的几十年中，我们曾见证过一系列关于系统架构的想法被提出，列举如下。</p><ul><li>六边形架构（Hexagonal Architecture）（也称为端口与适配器架构，Ports and Adpaters）：该架构由Alistair Cockburn首先提出。Steve Freeman和Nat Pryce在他们合写的著作Growing Object Oriented Software with Tests一书中对该架构做了隆重的推荐。</li><li>DCI架构：由James Coplien和Trygve Reenskaug首先提出。</li><li>BCE架构：由Ivar Jacobson在他的Object Oriented Software Engineering:A Use-Case Driven Approach一书中首先提出。</li></ul><p>虽然这些架构在细节上各有不同，但总体来说是非常相似的。它们都具有同一个设计目标：按照不同关注点对软件进行切割。也就是说，这些架构都会将软件切割成不同的层，至少有一层是只包含该软件的业务逻辑的，而用户接口、系统接口则属于其他层。</p><p>按照这些架构设计出来的系统，通常都具有以下特点。</p><ul><li>独立于框架：这些系统的架构并不依赖某个功能丰富的框架之中的某个函数。框架可以被当成工具来使用，但不需要让系统来适应框架。</li><li>可被测试：这些系统的业务逻辑可以脱离UI、数据库、Web服务以及其他的外部元素来进行测试。</li><li>独立于UI：这些系统的UI变更起来很容易，不需要修改其他的系统部分。例如，我们可以在不修改业务逻辑的前提下将一个系统的UI由Web界面替换成命令行界面。</li><li>独立于数据库：我们可以轻易将这些系统使用的Oracle、SQL Server替换成Mongo、BigTable、CouchDB之类的数据库。因为业务逻辑与数据库之间已经完成了解耦。</li><li>独立于任何外部机构：这些系统的业务逻辑并不需要知道任何其他外部接口的存在。</li></ul><p>下面我们要通过下图将<strong>上述所有架构的设计理念综合成为一个独立的理念：整洁架构</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-129e4afc2e66c860.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="clean.png"></p><h4 id="4-8-1-依赖关系规则"><a href="#4-8-1-依赖关系规则" class="headerlink" title="4.8.1  依赖关系规则"></a>4.8.1  依赖关系规则</h4><p>图中的同心圆分别代表了软件系统中的不同层次，通常越靠近中心，其所在的软件层次就越高。基本上，外层圆代表的是机制，内层圆代表的是策略。</p><p>当然这其中有一条贯穿整个架构设计的规则，即它的依赖关系规则：</p><p><strong>“源码中的依赖关系必须只指向同心圆的内层，即由低层机制指向高层策略。”</strong></p><p>换句话说，就是任何属于内层圆中的代码都不应该牵涉外层圆中的代码，尤其是内层圆中的代码不应该引用外层圆中代码所声明的名字，包括函数、类、变量以及一切其他有命名的软件实体。</p><p>同样的道理，外层圆中使用的数据格式也不应该被内层圆中的代码所使用，尤其是当数据格式是由外层圆的框架所生成时。总之，我们不应该让外层圆中发生的任何变更影响到内层圆的代码。</p><h4 id="4-8-2-一个场景应用场景"><a href="#4-8-2-一个场景应用场景" class="headerlink" title="4.8.2 一个场景应用场景"></a>4.8.2 一个场景应用场景</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ad1cd828a469b39d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sence.png"></p><h3 id="4-9-展示器和谦卑对象"><a href="#4-9-展示器和谦卑对象" class="headerlink" title="4.9 展示器和谦卑对象"></a>4.9 展示器和谦卑对象</h3><p>我们引入了展示器（presenter）的概念，展示器实际上是采用谦卑对象（humble object）模式的一种形式，这种设计模式可以很好地帮助识别和保护系统架构的边界。事实上，第22章所介绍的整洁架构中就充满了大量谦卑对象的实现体。</p><h4 id="4-9-1-谦卑对象模式"><a href="#4-9-1-谦卑对象模式" class="headerlink" title="4.9.1 谦卑对象模式"></a>4.9.1 谦卑对象模式</h4><p>谦卑对象模式[11]最初的设计目的是帮助单元测试的编写者区分容易测试的行为与难以测试的行为，并将它们隔离。其设计思路非常简单，就是将这两类行为拆分成两组模块或类。其中一组模块被称为谦卑（Humble）组，包含了系统中所有难以测试的行为，而这些行为已经被简化到不能再简化了。另一组模块则包含了所有不属于谦卑对象的行为。</p><p>例如，GUI通常是很难进行单元测试的，因为让计算机自行检视屏幕内容，并检查指定元素是否出现是非常难的事情。然而，GUI中的大部分行为实际上是很容易被测试的。这时候，我们就可以利用谦卑对象模式将GUI的这两种行为拆分成展示器与视图两部分。</p><h4 id="4-9-2-展示器与视图"><a href="#4-9-2-展示器与视图" class="headerlink" title="4.9.2  展示器与视图"></a>4.9.2  展示器与视图</h4><p>视图部分属于难以测试的谦卑对象。这种对象的代码通常应该越简单越好，它只应负责将数据填充到GUI上，而不应该对数据进行任何处理。</p><p>展示器则是可测试的对象。展示器的工作是负责从应用程序中接收数据，然后按视图的需要将这些数据格式化，以便视图将其呈现在屏幕上。例如，如果应用程序需要在屏幕上展示一个日期，那么它传递给展示器的应该是一个Date对象。然后展示器会将该对象格式化成所需的字符串形式，并将其填充到视图模型中。</p><p>如果应用程序需要在屏幕上展示金额，那么它应该将Currency对象传递给展示器。展示器随后会将这个对象按所需的小数位数进行格式化，并加上对应的货币标识符，形成一个字符串存放在视图模型中。如果需要将负数金额显示成红色，那么该视图模型中就应该有一个简单的布尔值被恰当地设置。</p><p>另外，应用程序在屏幕上的每个按钮都应该有其对应的名称，这些名称也是由展示器在视图模型中设置的。如果某个按钮需要变灰，展示器就应该将相应的开关变量设置成对应的布尔值。同样，菜单中每个菜单项所显示的值，也应该是一个个由展示器加载到视图模型中的字符串。应用程序在屏幕上显示的每个单选项、多选项以及文本框的名字也都如此，在视图模型中都有相应的字符串和布尔值可供展示器做对应的设置。即使屏幕上要加载的是一个数值表，展示器也应该负责把这些数值格式化成具有表格属性的字符串，以供视图使用。</p><p>总而言之，应用程序所能控制的、要在屏幕上显示的一切东西，都应该在视图模型中以字符串、布尔值或枚举值的形式存在。然后，视图部分除了加载视图模型所需要的值，不应该再做任何其他事情。因此，我们才能说视图是谦卑对象。</p><h4 id="4-9-3-测试与架构"><a href="#4-9-3-测试与架构" class="headerlink" title="4.9.3 测试与架构"></a>4.9.3 测试与架构</h4><p>众所周知，强大的可测试性是一个架构的设计是否优秀的显著衡量标准之一。谦卑对象模式就是这方面的一个非常好的例子。我们将系统行为分割成可测试和不可测试两部分的过程常常就也定义了系统的架构边界。展示器与视图之间的边界只是多种架构边界中的一种，另外还有许多其他边界。</p><h4 id="4-9-4-数据库网关"><a href="#4-9-4-数据库网关" class="headerlink" title="4.9.4 数据库网关"></a>4.9.4 数据库网关</h4><p>对于用例交互器（interactor）与数据库中间的组件，我们通常称之为数据库网关[13]。这些数据库网关本身是一个多态接口，包含了应用程序在数据库上所要执行的创建、读取、更新、删除等所有操作。例如，如果应用程序需要知道所有昨天登录系统的用户的姓，那么 UserGateway 接口就应该包含一个 getLastNamesOfUsers WhoLoggedInAfter方法，接收一个Date参数，并返回一个包含姓的列表。<br>另外，我们之前说过，SQL不应该出现在用例层的代码中，所以这部分的功能就需要通过网关接口来提供，而这些接口的实现则要由数据库层的类来负责。显然，这些实现也应该都属于谦卑对象，它们应该只利用SQL或其他数据库提供的接口来访问所需要的数据。与之相反，交互器则不属于谦卑对象，因为它们封装的是特定应用场景下的业务逻辑。不过，交互器尽管不属于谦卑对象，却是可测试的，因为数据库网关通常可以被替换成对应的测试桩和测试替身类。</p><h4 id="4-9-5-数据映射器"><a href="#4-9-5-数据映射器" class="headerlink" title="4.9.5 数据映射器"></a>4.9.5 数据映射器</h4><p>让我们继续数据库方面的话题，现在我们来思考一下Hibernate这类的ORM框架应该属于系统架构中的哪一层呢？<br>首先，我们要弄清楚一件事：对象关系映射器（ORM）事实上是压根就不存在的。道理很简单，对象不是数据结构。至少从用户的角度来说，对象内部的数据应该都是私有的，不可见的，用户在通常情况下只能看到对象的公有函数。因此从用户角度来说，对象是一些操作的集合，而不是简单的数据结构体。</p><p>与之相反，数据结构体则是一组公开的数据变量，其中不包含任何行为信息。所以ORM更应该被称为“数据映射器”，因为它们只是将数据从关系型数据库加载到了对应的数据结构中。</p><p>那么，这样的ORM系统应该属于系统架构中的哪一层呢？当然是数据库层。ORM其实就是在数据库和数据库网关接口之间构建了另一种谦卑对象的边界。</p><h4 id="4-9-6-服务监听器"><a href="#4-9-6-服务监听器" class="headerlink" title="4.9.6 服务监听器"></a>4.9.6 服务监听器</h4><p>如果我们的应用程序需要与其他服务进行某种交互，或者该应用本身要提供某一套服务，我们在相关服务的边界处也会看到谦卑对象模式吗？</p><p>答案是肯定的。我们的应用程序会将数据加载到简单的数据结构中，并将这些数据结构跨边界传输给那些能够将其格式化并传递其他外部服务的模块。在输入端，服务监听器会负责从服务接口中接收数据，并将其格式化成该应用程序易用的格式。总而言之，上述数据结构可以进行跨服务边界的传输。</p><h4 id="4-9-7-不完全边界"><a href="#4-9-7-不完全边界" class="headerlink" title="4.9.7 不完全边界"></a>4.9.7 不完全边界</h4><p>构建完整的架构边界是一件很耗费成本的事。在这个过程中，需要为系统设计双向的多态边界接口，用于输入和输出的数据结构，以及所有相关的依赖关系管理，以便将系统分割成可独立编译与部署的组件。这里会涉及大量的前期工作，以及大量的后期维护工作。</p><p>在很多情况下，一位优秀的架构师都会认为设计架构边界的成本太高了——但为了应对将来可能的需要，通常还是希望预留一个边界。</p><p>但这种预防性设计在敏捷社区里是饱受诟病的，因为它显然违背了YAGNI原则（“You Aren’t Going to Need It”，意即“不要预测未来的需要”）。然而，架构师的工作本身就是要做这样的预见性设计，这时候，我们就需要引入不完全边界（partial boundary）的概念了。</p><h4 id="4-9-8-单向边界"><a href="#4-9-8-单向边界" class="headerlink" title="4.9.8 单向边界"></a>4.9.8 单向边界</h4><p>在设计一套完整的系统架构边界时，往往需要用反向接口来维护边界两侧组件的隔离性。而且，维护这种双向的隔离性，通常不会是一次性的工作，它需要我们持续地长期投入资源维护下去。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-bcdaa537e032f923.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="danxiang.png"></p><p>在图中，你会看到一个临时占位的，将来可被替换成完整架构边界的更简单的结构。这个结构采用了传统的策略模式（strategy pattern）。如你所见，其Client使用的是一个由ServiceImpl类实现的ServiceBoundary接口。<br>￼</p><p>很明显，上述设计为未来构建完整的系统架构边界打下了坚实基础。为了未来将Client与ServiceImpl隔离，必要的依赖反转已经做完了。同时，我们也能清楚地看到，图中的虚线箭头代表了未来有可能很快就会出现的隔离问题。由于没有采用双向反向接口，这部分就只能依赖开发者和架构师的自律性来保证组件持久隔离了。</p><h4 id="4-9-9-门户模式"><a href="#4-9-9-门户模式" class="headerlink" title="4.9.9 门户模式"></a>4.9.9 门户模式</h4><p>下面，我们再来看一个更简单的架构边界设计：采用门户模式（facade pattern），其架构如图所示。在这种模式下，我们连依赖反转的工作都可以省了。这里的边界将只能由Facade类来定义，这个类的背后是一份包含了所有服务函数的列表，它会负责将Client的调用传递给对Client不可见的服务函数。<br>￼<br><img src="https://upload-images.jianshu.io/upload_images/12321605-c9993498ceb02d3e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="menhu.png"></p><p>但需要注意的是，在该设计中，Client会传递性地依赖于所有的Service类。在静态类型语言中，这就意味着对Service类的源码所做的任何修改都会导致Client的重新编译。另外，我们应该也能想象得到为这种结构建立反向通道是多容易的事。</p><h3 id="4-10-层次与边界"><a href="#4-10-层次与边界" class="headerlink" title="4.10 层次与边界"></a>4.10 层次与边界</h3><p>为什么要将一个极为简单的、在Kornshell中只需200行代码就能写完的小程序扩展成具有这些系统架构边界的复杂程序？</p><p>我们设计这个例子的目的就是为了证明架构边界可以存在于任何地方。作为架构师，我们必须要小心审视究竟在什么地方才需要设计架构边界。另外，我们还必须弄清楚完全实现这些边界将会带来多大的成本。</p><p>同时，我们也必须要了解如果事先忽略了这些边界，后续再添加会有多么困难——哪怕有覆盖广泛的测试，严加小心的重构也于事无补。</p><p>所以作为架构师，我们应该怎么办？这个问题恐怕没有答案。一方面，就像一些很聪明的人多年来一直告诉我们的那样，不应该将未来的需求抽象化。这就是YAGNI原则：“You aren’t going to need it”，臆想中的需求事实上往往是不存在的。这是一句饱含智慧的建议，因为过度的工程设计往往比工程设计不足还要糟糕。但另一方面，如果我们发现自己在某个位置确实需要设置一个架构边界，却又没有事先准备的时候，再添加边界所需要的成本和风险往往是很高的。</p><p>现实就是这样。作为软件架构师，我们必须有一点未卜先知的能力。有时候要依靠猜测——当然还要用点脑子。软件架构师必须仔细权衡成本，决定哪里需要设计架构边界，以及这些地方需要的是完整的边界，还是不完全的边界，还是可以忽略的边界。</p><p>而且，这不是一次性的决定。我们不能在项目开始时就决定好哪里需要设计边界，哪里不需要。相反，架构师必须持续观察系统的演进，时刻注意哪里可能需要设计边界，然后仔细观察这些地方会由于不存在边界而出现哪些问题。</p><p>当出现问题时，我们还需要权衡一下实现这个边界的成本，并拿它与不实现这个边界的成本对比——这种对比经常需要反复地进行。我们的目标是找到设置边界的优势超过其成本的拐点，那就是实现该边界的最佳时机。</p><p>持之以恒，一刻也不能放松。</p><h3 id="4-11-Main组件"><a href="#4-11-Main组件" class="headerlink" title="4.11 Main组件"></a>4.11 Main组件</h3><p>Main组件也可以被视为应用程序的一个插件——这个插件负责设置起始状态、配置信息、加载外部资源，最后将控制权转交给应用程序的其他高层组件。另外，由于Main组件能以插件形式存在于系统中，因此我们可以为一个系统设计多个Main组件，让它们各自对应于不同的配置。</p><p>例如，我们既可以设计专门针对开发环境的Main组件，也可以设计专门针对测试的或者生产环境的Main组件。除此之外，我们还可以针对要部署的国家、地区甚至客户设计不同的Main组件。</p><p>当我们将Main组件视为一种插件时，用架构边界将它与系统其他部分隔离开这件事，在系统的配置上是不是就变得更容易了呢？</p><h3 id="4-12-宏观和微观"><a href="#4-12-宏观和微观" class="headerlink" title="4.12 宏观和微观"></a>4.12 宏观和微观</h3><p>虽然服务化可能有助于提升系统的可扩展性和可研发性，但服务本身却并不能代表整个系统的架构设计。系统的架构是由系统内部的架构边界，以及边界之间的依赖关系所定义的，与系统中各组件之间的调用和通信方式无关。</p><p>一个服务可能是一个独立组件，以系统架构边界的形式隔开。一个服务也可能由几个组件组成，其中的组件以架构边界的形式互相隔离。在极端情况下[19]，客户端和服务端甚至可能会由于耦合得过于紧密而不具备系统架构意义上的隔离性。</p><h3 id="4-13-测试边界"><a href="#4-13-测试边界" class="headerlink" title="4.13 测试边界"></a>4.13 测试边界</h3><p>测试并不是独立于整个系统之外的，恰恰相反，它们是系统的一个重要组成部分。我们需要精心设计这些测试，才能让它们发挥验证系统稳定性和预防问题复发的作用。没有按系统组成部分来设计的测试代码，往往是非常脆弱且难以维护的。这种测试最后常常会被抛弃，因为它们终究会出问题。</p><h3 id="4-14-整洁的嵌入式架构"><a href="#4-14-整洁的嵌入式架构" class="headerlink" title="4.14 整洁的嵌入式架构"></a>4.14 整洁的嵌入式架构</h3><p>嵌入式编程人员应该多学习一些非嵌入式系统的编程经验。如果你从事的是嵌入式编程工作，相信你一定会从本章的建议中得到很多启发。</p><p>为了让我们的产品能长期地保持健康，请别让你的代码都变成固件。如果一个系统的代码只能在目标硬件上测试，那么它的开发过程会变得非常艰难。总之，为产品的长期健康着想而采用一套整洁的嵌入式架构是很有必要的。</p><h2 id="五、实现细节"><a href="#五、实现细节" class="headerlink" title="五、实现细节"></a>五、实现细节</h2><h3 id="5-1-数据库只是细节"><a href="#5-1-数据库只是细节" class="headerlink" title="5.1 数据库只是细节"></a>5.1 数据库只是细节</h3><p>从系统架构的角度来看，数据库并不重要——它只是一个实现细节，在系统架构中并不占据重要角色。如果就数据库与整个系统架构的关系打个比方，它们之间就好比是门把手和整个房屋架构的关系。</p><p>这个比喻肯定会招来非议。相信我，这种架我吵过很多次了。所以我在这里要把话说得清楚一点：这里讨论的不是数据模型。为应用程序中的数据设计结构，对于系统架构来说当然是很重要的，但是数据库并不是数据模型。数据库只是一款软件，是用来存取数据的工具。从系统架构的角度来看，工具通常是无关紧要的——因为这只是一个底层的实现细节，一种达成目标的手段。一个优秀的架构师是不会让实现细节污染整个系统架构的。</p><h4 id="5-1-1-假设磁盘不存在会怎么样？"><a href="#5-1-1-假设磁盘不存在会怎么样？" class="headerlink" title="5.1.1 假设磁盘不存在会怎么样？"></a>5.1.1 假设磁盘不存在会怎么样？</h4><p>虽然硬盘现在还是很常见，但其实已经在走下坡路了。很快它们就会和磁带、软盘、CD一样成为历史，RAM正在替代一切。</p><p>现在，我们要来考虑一下：如果所有的数据都存在内存中，应该如何组织它们呢？需要按表格存储并且用SQL查询吗？需要用文件形式存储，然后按目录查找吗？</p><p>当然不，我们会将数据存储为链表、树、哈希表、堆栈、队列等各种各样的数据结构，然后用指针或者引用来访问这些数据——因为这对程序员来说是最自然的方式。</p><p>事实上，如果你再仔细想想，就会发现我们已经在这样做了。即使数据保存在数据库或者文件系统中，我们最终也会将其读取到内存中，并按照最方便的形式将其组织成列表、集合、堆栈、队列、树等各种数据结构，继续按文件和表格的形式来操作数据是非常少见的</p><h4 id="5-1-2-实现细节"><a href="#5-1-2-实现细节" class="headerlink" title="5.1.2 实现细节"></a>5.1.2 实现细节</h4><p>上面所说的，就是为什么我们认为数据库只是一种实现细节的原因。数据库终究只是在硬盘与内存之间相互传输数据的一种手段而已，它真的可以被认为只是一个长期存储数据的、装满字节的大桶。我们通常并不会真的以这种形式来使用数据。</p><p>因此，从系统架构的视角来看，真的不应该关心数据在旋转的磁盘表面上以什么样的格式存在。实际上，系统架构应该对磁盘本身的存在完全不关心。</p><p>数据的组织结构，数据的模型，都是系统架构中的重要部分，但是从磁盘上存储/读取数据的机制和手段却没那么重要。关系型数据库强制我们将数据存储成表格并且以SQL访问，主要是为了后者。<strong>总而言之，数据本身很重要，但数据库系统仅仅是一个实现细节</strong>。</p><h3 id="5-2-web只是细节"><a href="#5-2-web只是细节" class="headerlink" title="5.2 web只是细节"></a>5.2 web只是细节</h3><p>GUI只是一个实现细节。而Web则是GUI的一种，所以也是一个实现细节。作为一名软件架构师，我们需要将这类细节与核心业务逻辑隔离开来。</p><p>其实我们可以这样考虑这个问题：Web只是一种I/O设备。早在20世纪60年代，我们就已经了解编写设备无关应用程序的重要性。这种独立性的重要性至今仍然没有变化，Web也不例外。</p><p>是这样的吗？有人可能会辩称Web这样的GUI是非常特殊的，它能力强大，强大到让我们追求设备无关的架构变得毫无意义。当我们考虑到JavaScript数据校验的复杂程度、可拖拽的Ajax调用，以及无数可以轻松引入的设计组件时，很容易认为追求设备无关性是不现实的。</p><p>从某种程度上来说，的确如此。应用程序和GUI之间的频繁交互的确是与GUI的类型密切相关的。浏览器与Web应用之间的交互模式也的确与桌面客户端/服务器之间的交互模式区别很大。想要让浏览器上的Web操作模仿我们在UNIX中对I/O设备那样的操作，将其抽象成界面交互模型几乎是不可能的。</p><p>但我们其实可以从UI和应用程序之间的另一条边界出发来进行抽象化。因为业务逻辑可以被视为是一组用例的集合。而每个用例都是以用户的身份来执行某种操作的，所以它们都可以用输入数据、处理过程以及输出数据这个流程来描述。</p><p>也就是说，在UI和应用程序之间的某一点上，输入数据会被认为达到了一个完整状态，然后用例就被允许进入执行阶段了。在用例执行完之后，其生成的返回数据又继续在UI与应用程序之间传递。</p><p>这样一来，完整的输入数据，以及完整的输出数据就可以被标准化为数据结构，并提供给执行用例的进程了。通过这种方法，我们就可以认为用例都是以设备无关的方式在操作I/O设备。</p><h3 id="5-3-应用程序框架只是细节"><a href="#5-3-应用程序框架只是细节" class="headerlink" title="5.3 应用程序框架只是细节"></a>5.3 应用程序框架只是细节</h3><p>应用程序框架现在非常流行，这在通常情况下是一件好事。许多框架都非常有效，非常有用，而且是免费的。<br>但框架并不等同于系统架构——尽管有些框架确实以此为目标。</p><h4 id="5-3-1-单向婚姻"><a href="#5-3-1-单向婚姻" class="headerlink" title="5.3.1 单向婚姻"></a>5.3.1 单向婚姻</h4><p>我们与框架作者之间的关系是非常不对等的。我们要采用某个框架就意味着自己要遵守一大堆约定，但框架作者却完全不需要为我们遵守什么约定。</p><p>请仔细想想这一关系，当我们决定采用一个框架时，就需要完整地阅读框架作者提供的文档。在这个文档中，框架作者和框架其他用户对我们提出进行应用整合的一些建议。一般来说，这些建议就是在要求我们围绕着该框架来设计自己的系统架构。譬如，框架作者会建议我们基于框架中的基类来创建一些派生类，并在业务对象中引入一些框架的工具。框架作者还会不停地催促我们将应用与框架结合得越紧密越好。</p><p>对框架作者来说，应用程序与自己的框架耦合是没有风险的。毕竟作为作者，他们对框架有绝对的控制权，强耦合是应该的。</p><p>与此同时，作者当然是非常希望让我们的应用与其框架紧密结合的，因为这意味着脱离框架会很困难。作为框架作者来说，没有什么比让一堆用户心甘情愿地基于他的框架基类来构建派生类更自豪的事情了。</p><p>换句话说，框架作者想让我们与框架订终身——这相当于我们要对他们的框架做一个巨大而长期的承诺，而在任何情况下框架作者都不会对我们做出同样的承诺。这种婚姻是单向的。我们要承担所有的风险，而框架作者则没有任何风险。</p><h4 id="5-3-2-风险"><a href="#5-3-2-风险" class="headerlink" title="5.3.2 风险"></a>5.3.2 风险</h4><p>那么我们要承担的风险究竟有哪些呢？我们可以想到的至少有以下这几项：</p><ul><li><p>框架自身的架构设计很多时候并不是特别正确的。框架本身可能经常违反依赖关系原则。譬如，框架可能会要求我们将代码引入到业务对象中——甚至是业务实体中。框架可能会想要我们将框架耦合在最内圈代码中。而我们一旦引入，就再也不会离开该框架了，这就像戴上结婚戒指一样，从此一生不离不弃了。</p></li><li><p>框架可能会帮助我们实现一些应用程序的早期功能，但随着产品的成熟，功能要求很可能超出框架所能提供的范围。而且随着时间的推移，我们也会发现在应用的开发过程中，自己与框架斗争的时间要比框架帮助我们的时间长得多。</p></li><li><p>框架本身可能朝着我们不需要的方向演进。也许我们会被迫升级到一个并不需要的新版本，甚至会发现自己之前所使用的旧功能突然消失了，或悄悄改变了行为。</p></li><li><p>未来我们可能会想要切换到一个更新、更好的框架上。</p></li></ul><h4 id="5-3-3-解决方案"><a href="#5-3-3-解决方案" class="headerlink" title="5.3.3 解决方案"></a>5.3.3 解决方案</h4><p>解决方案是什么呢？</p><p><strong>请不要嫁给框架！</strong></p><p>我们可以使用框架——但要时刻警惕，别被它拖住。我们应该将框架作为架构最外圈的一个实现细节来使用，不要让它们进入内圈。</p><p>如果框架要求我们根据它们的基类来创建派生类，就请不要这样做！我们可以创造一些代理类，同时把这些代理类当作业务逻辑的插件来管理。</p><p>另外，不要让框架污染我们的核心代码，应该依据依赖关系原则，将它们当作核心代码的插件来管理。</p><p>以Spring为例，它作为一个依赖注入框架是不错的，也许我们会需要用Spring来自动连接应用程序中的各种依赖关系。这不要紧，但是千万别在业务对象里到处写@autowired注解。业务对象应该对Spring完全不知情才对。</p><p>反之，我们也可以利用Spring将依赖关系注入到Main组件中，毕竟Main组件作为系统架构中最低层、依赖最多的组件，它依赖于Spring并不是问题。</p><h4 id="5-3-4-总结"><a href="#5-3-4-总结" class="headerlink" title="5.3.4 总结"></a>5.3.4 总结</h4><p>总而言之。当我们面临框架选择时，尽量不要草率地做出决定。在全身心投入之前，应该首先看看是否可以部分地采用以增加了解。另外，请尽可能长时间地将框架留在架构边界之外，越久越好。因为谁知道呢，也许你可以不用买奶牛也能喝到牛奶</p><h2 id="六、代码结构组织的几种方式"><a href="#六、代码结构组织的几种方式" class="headerlink" title="六、代码结构组织的几种方式"></a>六、代码结构组织的几种方式</h2><h3 id="6-1-按层封装"><a href="#6-1-按层封装" class="headerlink" title="6.1 按层封装"></a>6.1 按层封装</h3><p>我们首先想到的，也可能是最简单的设计方式，就是传统的水平分层架构。在这个架构里，我们将代码从技术角度进行分类。这通常被称为“按层封装”。图34.1用UML类图展示了这种设计。</p><p>在这种常见的分层架构中，Web代码分为一层，业务逻辑分为一层，持久化是另外一层。换句话说，我们对代码进行了水平分层，相同类型的代码在一层。在“严格的分层架构”中，每一层只能对相邻的下层有依赖关系。在Java中，分层的概念通常是用包来表示的。如图34.1所示，所有的分层（包）之间的依赖关系都是指向下的。这里包括了以下Java类。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-933587110d2dea90.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="ceng.png"></p><h3 id="6-2-按功能封装"><a href="#6-2-按功能封装" class="headerlink" title="6.2 按功能封装"></a>6.2 按功能封装</h3><p>另外一种组织代码的形式是“按功能封装”，即垂直切分，根据相关的功能、业务概念或者聚合根（领域驱动设计原则中的术语）来切分。在常见的实现中，所有的类型都会放在一个相同的包中，以业务概念来命名。</p><p>图34.2展示了这种方式，类和接口与之前类似，但是相比之前，这次它们都被放到了同一个Java包中。相比“按层封装”，这只是一个小变化，但是现在顶层代码结构至少与业务领域有点相关了。我们可以看到这段代码是与订单有关的，而不是只能看到Web、服务及数据访问。另外一个好处是，如果需要修改“查看订单”这个业务用例，比较容易找到相关代码，毕竟它们都在一个包中，而不是分散在各处。</p><p>软件研发团队常常一开始采用水平分层方式（即“按层封装”），遇到困难后再切换到垂直分层方式（即“按功能封装”）。我认为，两种方式都很不好。看完本书，你应该意识到还有更好的分类方式——没错。<br>￼<br><img src="https://upload-images.jianshu.io/upload_images/12321605-f37a25d4a07fbfc3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="gongneng.png"></p><h3 id="6-3-端口和适配"><a href="#6-3-端口和适配" class="headerlink" title="6.3 端口和适配"></a>6.3 端口和适配</h3><p>如Bob大叔所说，通过采用“端口和适配器”“六边形架构”“边界、控制器、实体”等，我们可以创造出一个业务领域代码与具体实现细节（数据库、框架等）隔离的架构。总结下来，如图34.3所示，我们可以区分出代码中的内部代码（领域，Domain）与外部代码（基础设施，Infrastructure）。<br>￼<br><img src="https://upload-images.jianshu.io/upload_images/12321605-f9f764b06cb78736.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="domain1.png"></p><p>内部区域包含了所有的领域概念，而外部区域则包含了与外界交互的部分（例如UI、数据库、第三方集成等）。这里主要的规则是，只有外部代码能依赖内部代码，反之则不能。图34.4展示了“查看订单”这个业务用例是如何用这种方式实现的。</p><p>这里com.mycompnay.myapp.domain包是内部代码，另外一个包是外部代码。注意这里的依赖关系是由外向内的。眼尖的读者可以注意到之前的OrderRepository类现在被改名为Orders。这个概念基于领域驱动设计理念，其中要求内部代码都应该用独特的领域语言来描述。换句话说，我们在业务领域里面讨论的应该是“Orders”，而不是“OrdersRepository”。<br>￼<br><img src="https://upload-images.jianshu.io/upload_images/12321605-886e80f583106502.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="domain2.png"></p><h3 id="6-4-宽松的分层架构"><a href="#6-4-宽松的分层架构" class="headerlink" title="6.4 宽松的分层架构"></a>6.4 宽松的分层架构</h3><p>虽然我对本书中的SOLID、REP、CCP、CRP以及其他大部分建议完全认同，我想提出对代码组织方式的一个不同看法——“按组件封装”。一些背景信息：在我的职业生涯中，我基于Java构建了大量不同领域的企业软件，这些软件系统要求各异。大部分系统都是基于Web的，也有一些是CS架构[5]，或者是分布式架构的、基于消息的，或者其他的。虽然具体采用的技术不同，但大部分系统都是基于传统的分层架构的。</p><p>我已经给出一些分层架构不好的理由，但这还不是全部。分层架构设计的目的是将功能相似的代码进行分组。处理Web的代码应该与处理业务逻辑的代码分开，同时也与处理数据访问的代码分开。正如我们在UML类图中所见，从实现角度讲，层就是代表了Java包。从代码可访问性角度来讲，如果需要OrdersController依赖OrderService接口，那么这个接口必须设置为public，因为它们在不同的包中。同样的，OrdersRepository接口也需要设置为public，这样才能被包外的类OrdersServiceImple使用。</p><p>在严格分层的架构中，依赖指向的箭头应该永远向下，每一层只能依赖相邻的下一层。通过引入一些代码互相依赖的规则，我们就形成了一个干净、漂亮的单向依赖图。这里有一个大问题——只要通过引入一些不应该有的依赖来作弊，依然可以形成漂亮的单向依赖图。</p><p>假设新员工加入了团队，你给新人安排了一个订单相关的业务用例的实现任务。由于这个人刚刚入职，他想好好表现，尽快完成这项功能。粗略看过代码之后，新人发现了OrdersController这个类，于是他将新的订单相关的Web代码都塞了进去。但是这段代码需要从数据库查找一些订单数据。这时候这个新人灵机一动：“代码已经有了一个OrdersRepository接口，只需要将它用依赖注入框架引入控制器就行，我真机智！”几分钟之后，功能已经正常了，但是UML结构图变成了图34.5这样。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c6c26c5e139690f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="kuansong.png"></p><p>依赖关系箭头依然向下，但是现在OrdersController在某些情况下绕过了OrderService类。这种组织形式被称为宽松的分层架构，允许某些层跳过直接相邻的邻居。在有些情况下，这是意料之中的——例如，如果我们在遵循CQRS设计模式[6]，这是合理的。但是更多的情况下，绕过业务逻辑层是不合理的，尤其是在业务逻辑层要控制权限的情况下。</p><p>虽然新的业务用例可以正常工作，但是它可能不是按照合理方式实现的。作为咨询师，我曾经见过很多团队出现这种情况，只有他们开始仔细观察自己的代码结构图时才会发现。</p><p>这里我们有的其实只是一个规范——一个架构设计原则——内容是“Web控制器永远不应该直接访问数据层”。这里的核心问题当然是如何强制执行。我遇见的很多团队仅仅通过采用“自律”或者“代码评审”方式来执行，“我相信我的程序员”。有这种自信当然很好，但是我们都知道当预算缩减、工期临近的时候会发生什么事情。</p><p>有一小部分团队告诉我，他们会采用静态分析工具（例如Ndepend、Structure101、Checkstyle）来在构建阶段自动检查违反架构设计规则的代码。估计你见过这种代码，一般来说就是一段正则表达式，例如“包 xx/web下面的类型不允许访问 xx/data下面的类型”，这些检查在编译步骤之后执行。</p><p>这种方式虽然简单粗暴，但是确实能起效果，可以锁定违反了团队定义的系统架构设计原则的情况，并且（理想情况下）导致构建失败。这两种方法的共同问题是容易出错，同时反馈循环时间太长了。如果不精心维护，整个代码库可能很快就变成“一团泥巴”[7]。我个人更倾向选择能够让编译器执法的做法。</p><h3 id="6-5-按组件封装"><a href="#6-5-按组件封装" class="headerlink" title="6.5 按组件封装"></a>6.5 按组件封装</h3><p>那么，看一下“按组件封装”的做法。这种做法混合了我们之前讲的所有的方法，目标是将一个粗粒度组件相关的所有类放入一个Java包中。这就像是以一种面向服务的视角来构建软件系统，与微服务架构类似。这里，就像端口和适配器模式将Web视为一种交付手段一样，“按组件封装”将UI与粗粒度组件分离。图34.6展示了“查看订单”这个用例的设计图。<br>￼<br><img src="https://upload-images.jianshu.io/upload_images/12321605-c2f341fcfadd98ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="zujian.png"></p><p>总的来说，这种方式将“业务逻辑”与“持久化代码”合并在一起，称为“组件”,Bob大叔在本书中对“组件”的定义如下：</p><p>组件是部署单元。组件是系统中能够部署的最小单位，对应在Java里就是jar文件。</p><p>我对组件的定义稍有不同：“在一个执行环境（应用程序）中的、一个干净、良好的接口背后的一系列相关功能的集合”。这个定义来自我的“C4软件架构模型”[8]。这个模型以一种层级模型讨论软件系统的静态结构，其中的概念包括容器、组件、类。这个模型认为，系统由一个或者多个容器组成（例如Web应用、移动App、独立应用、数据库、文件系统），每个容器包含一个或多个组件，每个组件由一个或多个类组成。每个组件具体存在于哪个jar文件中则是另外一个维度的事情。</p><p>这种“按组件封装”的方式的一个好处是，如果我们需要编写和订单有关的代码，只有一个位置需要修改——OrdersComponet。在这个组件中，仍然应该关注重点隔离原则，但这是组件内部问题，使用者不需要关心。这就有点像采用微服务架构，或者是面向服务架构的结果——独立的OrderService会将所有订单相关的东西封装起来。这里关键的区别是解耦的方式。我们可以认为，单体程序中的一个良好定义的组件，是微服务化架构的一个前提条件。</p><h3 id="6-6-总结"><a href="#6-6-总结" class="headerlink" title="6.6 总结"></a>6.6 总结</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e2d9b8ee5284084e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="summary.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> Architecture </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL Insert 死锁问题研究</title>
      <link href="/2021/02/11/mysql-insert-lock/"/>
      <url>/2021/02/11/mysql-insert-lock/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>不想看废话的，建议直接去最后看死锁的本质原因。</p><h3 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h3><p>线上一个很简单结构的表，报<code>insert</code>死锁，这个表基本上只有<code>insert</code>操作，所以引出一个问题<code>insert</code> 和<code>insert</code>之间为什么会死锁？</p><p><strong>顺便说下我们线上库的隔离级别都是RC，日志格式是ROW，我下面所有测试都是在RC下</strong>。</p><pre><code>*** (1) TRANSACTION:TRANSACTION 2404187192, ACTIVE 0 sec insertingmysql tables in use 1, locked 1LOCK WAIT 8 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 118913019, OS thread handle 140411115681536, query id 8752700587 xx.xx.xx.147 message__u updateINSERT  INTO `message_entity` (`id`,`message_id`,`chat_id`,`entity`) VALUES (6921593523158564868,6921593523158564868,6579445153033879811,_binary'')*** (1) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 289 page no 2697984 n bits 80 index PRIMARY of table `lark_message_shard_xxx`.`message_entity` trx id 2404187192 lock_mode X locks gap before rec insert intention waiting*** (2) TRANSACTION:TRANSACTION 2404186956, ACTIVE 0 sec inserting, thread declared inside InnoDB 1mysql tables in use 1, locked 18 lock struct(s), heap size 1136, 3 row lock(s)MySQL thread id 118913470, OS thread handle 140410161960704, query id 8752703155 xx.xx.xx.25 message__u updateINSERT INTO `message_entity` (`id`,`message_id`,`chat_id`,`entity`) VALUES (6921593568842792988,6921593568842792988,6807310568442118145,_binary'')*** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 289 page no 2697984 n bits 80 index PRIMARY of table `lark_message_shard_xxx`.`message_entity` trx id 2404186956 lock mode S locks gap before rec*** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 289 page no 2697984 n bits 80 index PRIMARY of table `lark_message_shard_xxx`.`message_entity` trx id 2404186956 lock_mode X locks gap before rec insert intention waiting*** WE ROLL BACK TRANSACTION (2)------------TRANSACTIONS------------</code></pre><p>上面死锁日志重点如下： </p><ul><li>事务一，<code>WAITING FOR THIS LOCK TO BE GRANTED:  lock_mode X locks gap before rec insert intention waiting</code></li><li>事务二， <code>HOLDS THE LOCK(S): lock mode S locks gap before rec </code></li><li>事务二，<code>WAITING FOR THIS LOCK TO BE GRANTED: lock_mode X locks gap before rec insert intention waiting</code></li></ul><p><strong>因为死锁日志并没有完整的死锁现场</strong>，光看事两个务发生的语句，我们这里很难分析出具体死锁原因，真正原因我们下面复现死锁场景的时候再说。</p><p>顺便说下第一次看到这个死锁日志，我有两个反应。</p><ol><li>这两个<code>Insert</code>数据没有任何冲突，为什么会死锁？（其实这个是因为日志没有完整的现场，后面会复现这个现场）</li><li><code>locks gap before rec</code> RC下为什么会有GAP锁？（下面场景2，时间线9中我证明了，RC下的确有Gap Lock）</li></ol><h3 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h3><h4 id="0-死锁日志含义"><a href="#0-死锁日志含义" class="headerlink" title="0. 死锁日志含义"></a>0. 死锁日志含义</h4><p>在此之前，我们能要先了解下日志里面各种锁对应的相关描述：</p><ul><li>记录锁（<code>LOCK_REC_NOT_GAP</code>）: <code>locks rec but not gap</code></li><li>间隙锁（<code>LOCK_GAP</code>）: <code>locks gap before rec</code></li><li>Next-key 锁（<code>LOCK_ORNIDARY</code>）: <code>lock_mode X</code></li><li>插入意向锁（<code>LOCK_INSERT_INTENTION</code>）: <code>locks gap before rec insert intention</code></li></ul><p>这里有一点要注意的是，并不是在日志里看到 <code>lock_mode X</code> 就认为这是 <code>Next-key</code> 锁，因为还有一个例外：如果在 <code>supremum record</code> 上加锁，<code>locks gap before rec</code> 会省略掉，间隙锁会显示成 <code>lock_mode X</code>，插入意向锁会显示成 <code>lock_mode X insert intention</code>。</p><h4 id="1-READ-COMMITTED-下是否有间隙锁-Gap-Lock"><a href="#1-READ-COMMITTED-下是否有间隙锁-Gap-Lock" class="headerlink" title="1. READ COMMITTED 下是否有间隙锁(Gap Lock)?"></a>1. READ COMMITTED 下是否有间隙锁(Gap Lock)?</h4><p>因为上面死锁日志里面有 <code>lock mode S locks gap before rec</code>，我们<code>MySQL</code>隔离级别都是<code>RC</code>，大家都说<code>RC</code>隔离级别下<code>Gap Lock</code>会失效，那<code>RC</code>下面到底有没有<code>Gap Lock</code>。</p><p>其实答案很明显，我们可以根据结果来推导，死锁日志里面报了插入意向锁的死锁，我们知道插入意向锁只跟间隙锁冲突，说明<code>RC</code>下面肯定是存在<code>Gap</code>锁的，不然插入意向锁也不会造成死锁。</p><p>口说无凭，撸下<code>MySQL</code>官方文档，关于 <a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-locking.html">Gap Lock</a> 描述如下。</p><blockquote><p>Gap locking can be disabled explicitly. This occurs if you change the transaction isolation level to READ COMMITTED or enable the innodb_locks_unsafe_for_binlog system variable (which is now deprecated). Under these circumstances, gap locking is disabled for searches and index scans and is used only for foreign-key constraint checking and duplicate-key checking.</p></blockquote><p>说的很明白，在 RC 下<code>searches and index scans</code> 时候 <code>Gap</code> 是失效的，但是<code>duplicate-key checking</code>时候还是会有间隙锁。</p><p><strong>所以结论是，RC隔离级别下，某些场景还是会有Gap Lock。</strong></p><h4 id="2-Insert-的时候-MySQL-到底会加哪些锁？"><a href="#2-Insert-的时候-MySQL-到底会加哪些锁？" class="headerlink" title="2. Insert 的时候 MySQL 到底会加哪些锁？"></a>2. Insert 的时候 MySQL 到底会加哪些锁？</h4><p>继续撸下 <code>MySQL</code> 官方文档，关于 <a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-locks-set.html">insert锁有如下描述</a></p><blockquote><p>INSERT sets an exclusive lock on the inserted row. This lock is an index-record lock, not a next-key lock (that is, there is no gap lock) and does not prevent other sessions from inserting into the gap before the inserted row.</p><p>Prior to inserting the row, a type of gap lock called an insert intention gap lock is set. This lock signals the intent to insert in such a way that multiple transactions inserting into the same index gap need not wait for each other if they are not inserting at the same position within the gap. Suppose that there are index records with values of 4 and 7. Separate transactions that attempt to insert values of 5 and 6 each lock the gap between 4 and 7 with insert intention locks prior to obtaining the exclusive lock on the inserted row, but do not block each other because the rows are nonconflicting.</p><p>If a duplicate-key error occurs, a shared lock on the duplicate index record is set. This use of a shared lock can result in deadlock should there be multiple sessions trying to insert the same row if another session already has an exclusive lock. This can occur if another session deletes the row.</p></blockquote><p>具体翻译如下：</p><ol><li><p><code>insert</code>会对插入成功的行加上排它锁，这个排它锁是个记录锁，而非<code>next-key</code>锁（当然更不是<code>gap</code>锁了），不会阻止其他并发的事务往这条记录之前插入记录。</p></li><li><p>在插入之前，会先在插入记录所在的间隙加上一个插入意向锁（<code>Insert intenion Lock</code>），并发的事务可以对同一个<code>Gap</code>加插入意向锁。插入意向锁和插入意向锁不会互相阻塞。</p></li><li><p>如果<code>insert</code>的事务出现了<code>duplicate-key error</code> ，事务会对<code>duplicate index record</code>加共享锁。这个共享锁在并发的情况下是会产生死锁的，比如有两个并发的<code>insert</code>都对要对同一条记录加共享锁，而此时这条记录又被其他事务加上了排它锁，<strong>排它锁的事务者回滚后，两个并发的insert操作是会发生死锁的</strong>。  </p></li></ol><p>这个只是官方文档说明的。实际上：</p><ol><li>执行 <code>insert</code> 之后，如果没有任何冲突，在 <code>show engine innodb status</code> 命令中是看不到任何锁的，这是因为<code>insert</code>加的是隐式锁。什么是隐式锁？隐式锁的意思就是没有锁。</li><li><code>InnoDB</code>在插入记录时，是不加锁的。如果事务<code>A</code>插入记录且未提交，这时事务<code>B</code>尝试对这条记录加锁，事务<code>B</code>会先去判断记录上保存的事务<code>id</code>是否活跃，如果活跃的话，那么就帮助事务<code>A</code>去建立一个锁对象，然后自身进入等待事务<code>A</code>状态，这就是所谓的隐式锁转换为显式锁。</li></ol><p>具体可以参考<a href="https://www.aneasystone.com/archives/2018/06/insert-locks-via-mysql-source-code.html">读 MySQL 源码再看 INSERT 加锁流程</a></p><h4 id="3-InnoDb锁的基本常识"><a href="#3-InnoDb锁的基本常识" class="headerlink" title="3. InnoDb锁的基本常识"></a>3. InnoDb锁的基本常识</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b2525df3e681e976.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="mysql-lock.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2f71882756c45aa4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="conflict.jpg"></p><h2 id="死锁场景复现"><a href="#死锁场景复现" class="headerlink" title="死锁场景复现"></a>死锁场景复现</h2><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><pre><code>mysql&gt; select version();+-----------+| version() |+-----------+| 5.6.41    |+-----------+1 row in set (0.05 sec)mysql&gt; select @@tx_isolation;+----------------+| @@tx_isolation |+----------------+| READ-COMMITTED |+----------------+1 row in set (0.05 sec)</code></pre><p>表结构</p><pre><code>DROP TABLE IF EXISTS `message_entity`;CREATE TABLE `message_entity` (  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,  `chat_id` bigint(20) unsigned NOT NULL,  PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 ROW_FORMAT=DYNAMIC;</code></pre><h3 id="场景一"><a href="#场景一" class="headerlink" title="场景一"></a>场景一</h3><p>这个也是<a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-locks-set.html">官方文档里面给出的insert死锁场景</a></p><table><thead><tr><th>时间线</th><th>Session1</th><th>Session2</th><th>Session3</th></tr></thead><tbody><tr><td>1</td><td>BEGIN</td><td>BEGIN</td><td>BEGIN</td></tr><tr><td>2</td><td>INSERT  INTO <code>message_entity</code>(<code>id</code>,<code>chat_id</code>) VALUES (1,1)</td><td></td><td></td></tr><tr><td>3</td><td></td><td>INSERT  INTO <code>message_entity</code>(<code>id</code>,<code>chat_id</code>) VALUES (1,1)</td><td></td></tr><tr><td>4</td><td></td><td></td><td>INSERT  INTO <code>message_entity</code>(<code>id</code>,<code>chat_id</code>) VALUES (1,1)</td></tr><tr><td>5</td><td>ROLLBACK</td><td></td><td></td></tr></tbody></table><p><strong>时间线2</strong>，<code>Session1</code> 插入成功，查询下<code>MySQL</code>锁状态，发现当前没有阻塞的锁。</p><pre><code>mysql&gt; select * from information_schema.innodb_locks;Empty set (0.05 sec)</code></pre><p><strong>时间线3</strong>，<code>Session2</code> 插入语句会被 <code>block</code>，查询锁状态发现，<code>Session1</code> 持有了<code>X</code>锁 (事务还没提交所以一直持有)，<code>Session2</code>请求持有<code>S</code>锁，但是被<code>Session1</code>的持有的<code>X</code>锁<code>block</code>住了。</p><pre><code>mysql&gt; select * from information_schema.innodb_locks;+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+| lock_id        | lock_trx_id | lock_mode | lock_type | lock_table                 | lock_index | lock_space | lock_page | lock_rec | lock_data |+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+| 167515:252:3:2 | 167515      | S         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        2 | 1         || 167514:252:3:2 | 167514      | X         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        2 | 1         |+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+2 rows in set (0.19 sec)</code></pre><p><strong>时间线4</strong>， <code>Session3</code> 插入语句阻塞，<code>Session3</code> 跟 <code>Session2</code>一样，都是请求<code>S</code>锁，被 <code>Session1</code>的持有的<code>X</code>锁<code>block</code>住了。</p><pre><code>mysql&gt; select * from information_schema.innodb_locks;+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+| lock_id        | lock_trx_id | lock_mode | lock_type | lock_table                 | lock_index | lock_space | lock_page | lock_rec | lock_data |+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+| 167516:252:3:2 | 167516      | S         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        2 | 1         || 167514:252:3:2 | 167514      | X         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        2 | 1         || 167515:252:3:2 | 167515      | S         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        2 | 1         |+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+3 rows in set (0.10 sec)</code></pre><p><strong>时间线5</strong>，<code>Session1</code> 回滚以后，<code>Session2</code> 和 <code>Session3</code> 都成功拿到了<code>S</code>锁，可以<code>show engine innodb status</code>，看下死锁日志如下</p><pre><code>------------------------LATEST DETECTED DEADLOCK------------------------2021-02-09 15:26:42 7efe3d7a6700*** (1) TRANSACTION:TRANSACTION 167515, ACTIVE 76 sec insertingmysql tables in use 1, locked 1LOCK WAIT 4 lock struct(s), heap size 1184, 2 row lock(s)MySQL thread id 13393, OS thread handle 0x7efe3d7e8700, query id 394645 123.58.117.233 root updateINSERT  INTO `message_entity` (`id`,`message_id`,`chat_id`,`entity`) VALUES (1,1,1,_binary'')*** (1) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 244 page no 3 n bits 72 index `PRIMARY` of table `test_db`.`message_entity` trx id 167515 lock_mode X insert intention waitingRecord lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;;*** (2) TRANSACTION:TRANSACTION 167516, ACTIVE 28 sec insertingmysql tables in use 1, locked 14 lock struct(s), heap size 1184, 2 row lock(s)MySQL thread id 13395, OS thread handle 0x7efe3d7a6700, query id 394651 123.58.117.233 root updateINSERT  INTO `message_entity` (`id`,`message_id`,`chat_id`,`entity`) VALUES (1,1,1,_binary'')*** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 244 page no 3 n bits 72 index `PRIMARY` of table `test_db`.`message_entity` trx id 167516 lock mode SRecord lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;;*** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 244 page no 3 n bits 72 index `PRIMARY` of table `test_db`.`message_entity` trx id 167516 lock_mode X insert intention waitingRecord lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;;*** WE ROLL BACK TRANSACTION (2)</code></pre><p>从日志我们可以看出：</p><ol><li>事务167515（<code>Session2</code>）<code>lock_mode X insert intention waiting Record lock</code>插入意向排他锁在等待记录锁。</li><li>事务167516 （<code>Session3</code>）<code>lock mode S Record lock</code> 持有 S 记录锁。</li><li>事务167516 （<code>Session3</code>）<code>lock_mode X insert intention waiting Record lock</code> 插入意向排他锁等待记录锁。</li><li><code>WE ROLL BACK TRANSACTION (2)</code> , 最终 <code>MySQL roll back</code>了<code>Session3</code> , 执行了<code>Session2</code>.</li></ol><h4 id="场景一死锁总结"><a href="#场景一死锁总结" class="headerlink" title="场景一死锁总结"></a>场景一死锁总结</h4><p><strong>死锁日志的信息记录并不全</strong>，其实在<strong>时间线4</strong>的时候，我们可以看到<code>Session2</code>和<code>Session3</code>在申请 <code>S</code>记录锁。Session1 回滚了以后，<code>Session2</code>和<code>Session3</code>都持有<code>S</code>锁，然后都请求<code>X</code>锁，互相等待对方释放<code>S</code>锁，所以导致死锁。</p><p>具体流程图类似下图</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0587c2779c17321f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="dead_lock1.png"></p><p><strong>但是这个报错信息跟我们线上死锁场景不一样，我们线上是两个毫不想干的数据互相死锁。我们继续看下场景二。</strong></p><h3 id="场景二"><a href="#场景二" class="headerlink" title="场景二"></a>场景二</h3><pre><code>INSERT  INTO `message_entity`(`id`,`chat_id`) VALUES (100,100)</code></pre><p>先插入一个边界数据，主要是为了跟线上现场的死锁日志保持一致。上面说了如果在 <code>supremum record</code> 上加锁，<code>locks gap before rec</code> 会省略掉，间隙锁会显示成 <code>lock_mode X</code>，插入意向锁会显示成 <code>lock_mode X insert intention</code>。</p><p>死锁场景复现如下：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8fc5699bf54101d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>时间线2、3、4</strong> 都是正常插入，没有阻塞的锁。</p><pre><code>mysql&gt; select * from information_schema.innodb_locks;Empty set (0.10 sec)</code></pre><p><strong><a href="./5.txt">时间线5</a></strong> <code>Session1.2</code>执行<code>Insert</code>被阻塞，查询锁的情况，可以看到<code>Session1.1</code> 对有<code>10</code>有<code>X</code>锁。<br><code>Session1.2</code>等待<code>S</code>锁。</p><pre><code>mysql&gt; select * from information_schema.innodb_locks;+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+| lock_id        | lock_trx_id | lock_mode | lock_type | lock_table                 | lock_index | lock_space | lock_page | lock_rec | lock_data |+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+| 167566:252:3:4 | 167566      | S         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        4 | 10        || 167559:252:3:4 | 167559      | X         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        4 | 10        |+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+2 rows in set (0.09 sec)</code></pre><p><strong><a href="./6.txt">时间线6</a></strong> 跟上面一样，<code>Session2.1</code> 对有<code>20</code>有<code>X</code>锁，<code>Session2.2</code>等待<code>S</code>锁。</p><pre><code>mysql&gt; select * from information_schema.innodb_locks;+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+| lock_id        | lock_trx_id | lock_mode | lock_type | lock_table                 | lock_index | lock_space | lock_page | lock_rec | lock_data |+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+| 167567:252:3:3 | 167567      | S         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        3 | 20        || 167560:252:3:3 | 167560      | X         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        3 | 20        || 167566:252:3:4 | 167566      | S         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        4 | 10        || 167559:252:3:4 | 167559      | X         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        4 | 10        |+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+4 rows in set (0.12 sec)</code></pre><p><strong><a href="./7.txt">时间线7</a></strong> 跟上面一样，<code>Session3.1</code>对有<code>30</code>有<code>X</code>锁，<code>Session3.2</code>等待<code>S</code>锁。 </p><pre><code>mysql&gt; select * from information_schema.innodb_locks;+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+| lock_id        | lock_trx_id | lock_mode | lock_type | lock_table                 | lock_index | lock_space | lock_page | lock_rec | lock_data |+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+| 167568:252:3:5 | 167568      | S         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        5 | 30        || 167565:252:3:5 | 167565      | X         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        5 | 30        || 167567:252:3:3 | 167567      | S         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        3 | 20        || 167560:252:3:3 | 167560      | X         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        3 | 20        || 167566:252:3:4 | 167566      | S         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        4 | 10        || 167559:252:3:4 | 167559      | X         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        4 | 10        |+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+6 rows in set (0.13 sec)</code></pre><p><strong><a href="./8.txt">时间线8</a></strong> 这里我们<code>Rollback</code>了<code>Session1.1</code>的操作，这个时候<code>Session1.2</code>不在阻塞，数据正常插入。</p><pre><code>mysql&gt; select * from information_schema.innodb_locks;+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+| lock_id        | lock_trx_id | lock_mode | lock_type | lock_table                 | lock_index | lock_space | lock_page | lock_rec | lock_data |+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+| 167568:252:3:5 | 167568      | S         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        5 | 30        || 167565:252:3:5 | 167565      | X         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        5 | 30        || 167567:252:3:3 | 167567      | S         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        3 | 20        || 167560:252:3:3 | 167560      | X         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        3 | 20        |+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+4 rows in set (0.10 sec)</code></pre><p>因为<code>Session1.1</code> (<code>167559</code>)会滚了，只看<code>innodb_locks</code>表，  看不到 <code>Session1.2</code>(<code>167566</code>) 的锁状态了，我们<code>show engine innodb status</code>看下 <code>Session1.2</code>(<code>167566</code>) 的状态，显示 <code>Session1.2</code>(<code>167566</code>) 这个时候持有了数据<code>20</code>共享<code>Record Lock</code>和<code>(10，20)</code>的 <code>Gap Lock</code>。我测试了下，尝试再开个事务尝试插入（<code>11，19</code>）数据，都被阻塞了，证明这就是<code>Gap Lock</code>。</p><pre><code>---TRANSACTION 167566, ACTIVE 217 sec3 lock struct(s), heap size 360, 2 row lock(s), undo log entries 1MySQL thread id 13469, OS thread handle 0x7efe42afe700, query id 396301 111.225.144.149 rootTABLE LOCK table `test_db`.`message_entity` trx id 167566 lock mode IXRECORD LOCKS space id 252 page no 3 n bits 72 index `PRIMARY` of table `test_db`.`message_entity` trx id 167566 lock mode S locks rec but not gapRECORD LOCKS space id 252 page no 3 n bits 72 index `PRIMARY` of table `test_db`.`message_entity` trx id 167566 lock mode S locks gap before recRecord lock, heap no 3 PHYSICAL RECORD（表示20）: n_fields 4; compact format; info bits 0</code></pre><p><strong><a href="./9.txt">时间线9</a></strong> 这里我们 <code>Rollback</code> 了 <code>Session2.1</code>(<code>167560</code>) 的操作，因为我们上面知道 <code>Session1.2</code>(<code>167566</code>) 持有了数据<code>20</code>的共享的<code>Gap Lock</code>和<code>Record Lock</code>，所以 <code>Session2.2</code> 的数据依然<code>Block</code>了。查询<code>innodb_lock_waits</code> 发现，的确 <code>Session1.2</code>(<code>167566</code>) 阻塞了 <code>Session2.2</code>(<code>167567</code>) 的操作。</p><pre><code>mysql&gt; select * from information_schema.innodb_lock_waits;+-------------------+-------------------+-----------------+------------------+| requesting_trx_id | requested_lock_id | blocking_trx_id | blocking_lock_id |+-------------------+-------------------+-----------------+------------------+| 167568            | 167568:252:3:5    | 167565          | 167565:252:3:5   || 167567            | 167567:252:3:5    | 167566          | 167566:252:3:5   |+-------------------+-------------------+-----------------+------------------+2 rows in set (0.14 sec)</code></pre><p>再查下<code>innodb_locks</code>阻塞锁的状态，因为<code>RollBack</code>了<code>20</code>这条数据，现在<code>Session.1.2</code>(<code>167566</code>) 和 <code>Session2.2</code>(<code>167567</code>)，发现这时候<code>Gap Lock</code>的数据都加到了<code>30</code>上面去了。</p><pre><code>mysql&gt; select * from information_schema.innodb_locks;+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+| lock_id        | lock_trx_id | lock_mode | lock_type | lock_table                 | lock_index | lock_space | lock_page | lock_rec | lock_data |+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+| 167568:252:3:5 | 167568      | S         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        5 | 30        || 167565:252:3:5 | 167565      | X         | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        5 | 30        || 167567:252:3:5 | 167567      | X,GAP     | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        5 | 30        || 167566:252:3:5 | 167566      | S,GAP     | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        5 | 30        |+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+4 rows in set (0.10 sec)</code></pre><p><code>show engine innodb status</code>，看下锁详细信息 <code>Session.1.2</code>(<code>167566</code>) 持有数据<code>10</code>的共享<code>Record Lock</code>和<code>(10,30)</code>的<code>Gap Lock</code>。</p><pre><code>---TRANSACTION 167566, ACTIVE 270 sec4 lock struct(s), heap size 1184, 2 row lock(s), undo log entries 1MySQL thread id 13469, OS thread handle 0x7efe42afe700, query id 396301 111.225.144.149 rootTABLE LOCK table `test_db`.`message_entity` trx id 167566 lock mode IXRECORD LOCKS space id 252 page no 3 n bits 72 index `PRIMARY` of table `test_db`.`message_entity` trx id 167566 lock mode S locks rec but not gapRECORD LOCKS space id 252 page no 3 n bits 72 index `PRIMARY` of table `test_db`.`message_entity` trx id 167566 lock mode S locks gap before recRecord lock, heap no 4 PHYSICAL RECORD（表示10）: n_fields 4; compact format; info bits 0RECORD LOCKS space id 252 page no 3 n bits 72 index `PRIMARY` of table `test_db`.`message_entity` trx id 167566 lock mode S locks gap before recRecord lock, heap no 5 PHYSICAL RECORD（表示30）: n_fields 4; compact format; info bits 0</code></pre><p>继续看下<code>Session2.2</code>(<code>167567</code>)的锁详细信息，<code>Session2.2</code>(<code>167567</code>)持有了数据<code>30</code>的的共享<code>Record Lock</code>和<code>Gap Lock</code>，等待对<code>(10,30)</code>上加上插入间隙锁。</p><pre><code>TABLE LOCK table `test_db`.`message_entity` trx id 167567 lock mode IXRECORD LOCKS space id 252 page no 3 n bits 72 index `PRIMARY` of table `test_db`.`message_entity` trx id 167567 lock mode S locks rec but not gapRECORD LOCKS space id 252 page no 3 n bits 72 index `PRIMARY` of table `test_db`.`message_entity` trx id 167567 lock mode S locks gap before recRecord lock, heap no 5 PHYSICAL RECORD（表示30）: n_fields 4; compact format; info bits 0 0: len 8; hex 000000000000001e; asc         ;; 1: len 6; hex 000000028e8d; asc       ;; 2: len 7; hex d6000001b90110; asc        ;; 3: len 8; hex 000000000000001e; asc         ;;RECORD LOCKS space id 252 page no 3 n bits 72 index `PRIMARY` of table `test_db`.`message_entity` trx id 167567 lock_mode X locks gap before rec insert intention waitingRecord lock, heap no 5 PHYSICAL RECORD（表示30）: n_fields 4; compact format; info bits 0</code></pre><p><strong><a href="./10.txt">时间线10</a></strong> 执行 <code>Session3.1</code>(<code>167565</code>) <code>Rollback</code>，<code>Session3.2</code> 立即报死锁，然后回滚了。<code>Session2.2</code>，还在被<code>Session1.2</code> 阻塞了。</p><pre><code>mysql&gt; select * from information_schema.innodb_locks;+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+| lock_id        | lock_trx_id | lock_mode | lock_type | lock_table                 | lock_index | lock_space | lock_page | lock_rec | lock_data |+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+| 167567:252:3:2 | 167567      | X,GAP     | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        2 | 100       || 167566:252:3:2 | 167566      | S,GAP     | RECORD    | `test_db`.`message_entity` | PRIMARY    |        252 |         3 |        2 | 100       |+----------------+-------------+-----------+-----------+----------------------------+------------+------------+-----------+----------+-----------+2 rows in set (0.08 sec)</code></pre><p><strong><a href="./11.txt">时间线11</a></strong> 回滚了<code>Session1.2</code>(<code>167566</code>)以后，没有<code>（10，30）</code>的<code>Gap Lock</code>，<code>Session2.2</code>(<code>167567</code>)数据就能正常插入了。</p><h4 id="场景二死锁总结"><a href="#场景二死锁总结" class="headerlink" title="场景二死锁总结"></a>场景二死锁总结</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-088d32e9056bd2e4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="time7.jpg"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-05f1586574d7c5f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="time8.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-918745a083db22ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="time9.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-61239a0c71180437.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="time10-1.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9dedea5e0e5aa779.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="time10-2.png"></p><h2 id="总结-本质问题"><a href="#总结-本质问题" class="headerlink" title="总结-本质问题"></a>总结-本质问题</h2><p>一句话总结就是，假设两个事务插入的数据有<code>duplicate</code>冲突，如果回滚了第一个事务，第二个事务插入成功以后，<code>MySQL</code>会给这个记录加上间隙锁（<strong>RC下也会有间隙锁</strong>），如果多个事务，都因为<code>duplicate</code>阻塞，又都有<code>Rollback</code>，导致多个事务对同一个区间加上间隙锁，然后又都想像这个区间插入数据，所以就会死锁（例如上面场景2）。</p><p>验证间隙锁如下：</p><table><thead><tr><th>时间线</th><th>Session1</th><th>Session2</th><th>Session3</th></tr></thead><tbody><tr><td>1</td><td>BEGIN</td><td>BEGIN</td><td>BEGIN</td></tr><tr><td>2</td><td>INSERT  INTO <code>message_entity</code>(<code>id</code>,<code>chat_id</code>) VALUES (1,1)</td><td></td><td></td></tr><tr><td>3</td><td></td><td>INSERT  INTO <code>message_entity</code>(<code>id</code>,<code>chat_id</code>) VALUES (1,1)</td><td></td></tr><tr><td>4</td><td>ROLLBACK</td><td>插入成功</td><td></td></tr><tr><td>5</td><td></td><td></td><td>INSERT  INTO <code>message_entity</code>(<code>id</code>,<code>chat_id</code>) VALUES (10,10) 会被阻塞，因为(1 +∞）已经加上间隙锁</td></tr></tbody></table><h3 id="如何避免"><a href="#如何避免" class="headerlink" title="如何避免"></a>如何避免</h3><ol><li>我们知道，发生上面死锁主要有两个原因，唯一索引冲突。</li><li>两个事务同时插入一个唯一索引冲突的数据，然后第一个事务<code>Rollback</code>。</li></ol><p>针对1，我们如果不指定主键<code>id</code>，或者放弃插入失败重试操作，能一定概率避免死锁。</p><p>针对2，<code>GORM</code>作者金柱之前说过<code>GORM</code>有默认事务，会显示的为<code>Create</code>语句加上 <code>begin</code>、<code>commit</code>（主要是给<code>hook</code>功能用的，如果不需要<code>hook</code>可以关闭）。我们这里关闭显示事务能一定程度上降低死锁概率。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2e8f15b8d29eba77.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="gorm.jpeg"></p><pre><code>// 全局关闭dbProxy, err := gorm.POpenWithConfig("bytedmysql", "XXXX_DSN", gorm.Config{  SkipDefaultTransaction: true,})</code></pre>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Redis核心技术与实战》</title>
      <link href="/2021/02/10/reids-action/"/>
      <url>/2021/02/10/reids-action/</url>
      
        <content type="html"><![CDATA[<h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><h3 id="Redis数据结构"><a href="#Redis数据结构" class="headerlink" title="Redis数据结构"></a>Redis数据结构</h3><p>简单来说，底层数据结构一共有 6 种，分别是简单动态字符串、双向链表、压缩列表、哈希表、跳表和整数数组。它们和数据类型的对应关系如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e61bfa91dc198b1b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="全局哈希表"><a href="#全局哈希表" class="headerlink" title="全局哈希表"></a>全局哈希表</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8bd16b21ce7e3cb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因为这个哈希表保存了所有的键值对，所以，我也把它称为全局哈希表。哈希表的最大好处很明显，就是让我们可以用 O(1) 的时间复杂度来快速查找到键值对——我们只需要计算键的哈希值，就可以知道它所对应的哈希桶位置，然后就可以访问相应的 entry 元素。</p><h3 id="渐进式-rehash"><a href="#渐进式-rehash" class="headerlink" title="渐进式 rehash"></a>渐进式 rehash</h3><p>简单来说就是在第二步拷贝数据时，Redis 仍然正常处理客户端请求，每处理一个请求时，从哈希表 1 中的第一个索引位置开始，顺带着将这个索引位置上的所有 entries 拷贝到哈希表 2 中；等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的 entries。如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-32a124b6b3d63095.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="查找复杂度"><a href="#查找复杂度" class="headerlink" title="查找复杂度"></a>查找复杂度</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5fa2f6205c0b216a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="Redis-IO-模型"><a href="#Redis-IO-模型" class="headerlink" title="Redis IO 模型"></a>Redis IO 模型</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d4a960937b69b70b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="AOF和RDB"><a href="#AOF和RDB" class="headerlink" title="AOF和RDB"></a>AOF和RDB</h2><h3 id="AOF-重写"><a href="#AOF-重写" class="headerlink" title="AOF 重写"></a>AOF 重写</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-edb422acc9565202.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="RDB"><a href="#RDB" class="headerlink" title="RDB"></a>RDB</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b006d8a5dcf08c91.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="RBD是不是可以每秒做一次快照？"><a href="#RBD是不是可以每秒做一次快照？" class="headerlink" title="RBD是不是可以每秒做一次快照？"></a>RBD是不是可以每秒做一次快照？</h3><p>对于快照来说，所谓“连拍”就是指连续地做快照。这样一来，快照的间隔时间变得很短，即使某一时刻发生宕机了，因为上一时刻快照刚执行，丢失的数据也不会太多。但是，这其中的快照间隔时间就很关键了。</p><p>这种想法其实是错误的。虽然 bgsave 执行时不阻塞主线程，但是，如果频繁地执行全量快照，也会带来两方面的开销。</p><p>一方面，频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环。</p><p>另一方面，bgsave 子进程需要通过 fork 操作从主线程创建出来。虽然，子进程在创建后不会再阻塞主线程，但是，fork 这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。如果频繁 fork 出 bgsave 子进程，这就会频繁阻塞主线程了（所以，在 Redis 中如果有一个 bgsave 在运行，就不会再启动第二个 bgsave 子进程）。那么，有什么其他好方法吗？</p><p>在第一次做完全量快照后，T1 和 T2 时刻如果再做快照，我们只需要将被修改的数据写入快照文件就行。但是，这么做的前提是，我们需要记住哪些数据被修改了。你可不要小瞧这个“记住”功能，它需要我们使用额外的元数据信息去记录哪些数据被修改了，这会带来额外的空间开销问题。如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f31c6ef2d97bbe72.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果我们对每一个键值对的修改，都做个记录，那么，如果有 1 万个被修改的键值对，我们就需要有 1 万条额外的记录。而且，有的时候，键值对非常小，比如只有 32 字节，而记录它被修改的元数据信息，可能就需要 8 字节，这样的画，为了“记住”修改，引入的额外空间开销比较大。这对于内存资源宝贵的 Redis 来说，有些得不偿失。</p><p>Redis 4.0 中提出了一个混合使用 AOF 日志和内存快照的方法。简单来说，内存快照以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作。</p><p>如下图所示，T1 和 T2 时刻的修改，用 AOF 日志记录，等到第二次做全量快照时，就可以清空 AOF 日志，因为此时的修改都已经记录到快照中了，恢复时就不再用日志了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9a05c848340da60b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个方法既能享受到 RDB 文件快速恢复的好处，又能享受到 AOF 只记录操作命令的简单优势，颇有点“鱼和熊掌可以兼得”的感觉，建议你在实践中用起来。</p><h2 id="高可用"><a href="#高可用" class="headerlink" title="高可用"></a>高可用</h2><h3 id="主从"><a href="#主从" class="headerlink" title="主从"></a>主从</h3><h4 id="主从库第一次同步的流程"><a href="#主从库第一次同步的流程" class="headerlink" title="主从库第一次同步的流程"></a>主从库第一次同步的流程</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e75e5aff7114c3c1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>全量复制虽然耗时，但是对于从库来说，如果是第一次同步，全量复制是无法避免的，所以，我给你一个小建议：一个 Redis 实例的数据库不要太大，一个实例大小在几 GB 级别比较合适，这样可以减少 RDB 文件生成、传输和重新加载的开销。另外，为了避免多个从库同时和主库进行全量复制，给主库过大的同步压力，我们也可以采用“主 - 从 - 从”这一级联模式，来缓解主库的压力。</p><p>长连接复制是主从库正常运行后的常规同步阶段。在这个阶段中，主从库之间通过命令传播实现同步。不过，这期间如果遇到了网络断连，增量复制就派上用场了。我特别建议你留意一下 repl_backlog_size 这个配置参数。如果它配置得过小，在增量复制阶段，可能会导致从库的复制进度赶不上主库，进而导致从库重新进行全量复制。所以，通过调大这个参数，可以减少从库在网络断连时全量复制的风险。</p><h3 id="哨兵"><a href="#哨兵" class="headerlink" title="哨兵"></a>哨兵</h3><h4 id="哨兵机制的基本流程"><a href="#哨兵机制的基本流程" class="headerlink" title="哨兵机制的基本流程"></a>哨兵机制的基本流程</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6ded434e112a5e10.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="主观下线和客观下线"><a href="#主观下线和客观下线" class="headerlink" title="主观下线和客观下线"></a>主观下线和客观下线</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-32899718bb7831f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="如何选定新主库？"><a href="#如何选定新主库？" class="headerlink" title="如何选定新主库？"></a>如何选定新主库？</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ead329f8b53859e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>所以，在选主时，除了要检查从库的当前在线状态，还要判断它之前的网络连接状态。如果从库总是和主库断连，而且断连次数超出了一定的阈值，我们就有理由相信，这个从库的网络状况并不是太好，就可以把这个从库筛掉了。</p><p>具体怎么判断呢？你使用配置项 down-after-milliseconds * 10。其中，down-after-milliseconds 是我们认定主从库断连的最大连接超时时间。如果在 down-after-milliseconds 毫秒内，主从节点都没有通过网络联系上，我们就可以认为主从节点断连了。如果发生断连的次数超过了 10 次，就说明这个从库的网络状况不好，不适合作为新主库。</p><p>接下来就要给剩余的从库打分了。我们可以分别按照三个规则依次进行三轮打分，这三个规则分别是从库优先级、从库复制进度以及从库 ID 号。只要在某一轮中，有从库得分最高，那么它就是主库了，选主过程到此结束。如果没有出现得分最高的从库，那么就继续进行下一轮。</p><p>第一轮：优先级最高的从库得分高。<br>用户可以通过 slave-priority 配置项，给不同的从库设置不同优先级。比如，你有两个从库，它们的内存大小不一样，你可以手动给内存大的实例设置一个高优先级。在选主时，哨兵会给优先级高的从库打高分，如果有一个从库优先级最高，那么它就是新主库了。如果从库的优先级都一样，那么哨兵开始第二轮打分。</p><p>第二轮：和旧主库同步程度最接近的从库得分高。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2dbae6ae77988f1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>当然，如果有两个从库的 slave_repl_offset 值大小是一样的（例如，从库 1 和从库 2 的 slave_repl_offset 值都是 990），我们就需要给它们进行第三轮打分了。</p><p>第三轮：ID 号小的从库得分高。</p><p>每个实例都会有一个 ID，这个 ID 就类似于这里的从库的编号。目前，Redis 在选主库时，有一个默认的规定：在优先级和复制进度都相同的情况下，ID 号最小的从库得分最高，会被选为新主库。</p><h4 id="哨兵在操作主从切换的过程中，客户端能否正常地进行请求操作？"><a href="#哨兵在操作主从切换的过程中，客户端能否正常地进行请求操作？" class="headerlink" title="哨兵在操作主从切换的过程中，客户端能否正常地进行请求操作？"></a>哨兵在操作主从切换的过程中，客户端能否正常地进行请求操作？</h4><p>哨兵在操作主从切换的过程中，客户端能否正常地进行请求操作？</p><p>如果客户端使用了读写分离，那么读请求可以在从库上正常执行，不会受到影响。但是由于此时主库已经挂了，而且哨兵还没有选出新的主库，所以在这期间写请求会失败，失败持续的时间 = 哨兵切换主从的时间 + 客户端感知到新主库 的时间。</p><p>如果不想让业务感知到异常，客户端只能把写失败的请求先缓存起来或写入消息队列中间件中，等哨兵切换完主从后，再把这些写请求发给新的主库，但这种场景只适合对写入请求返回值不敏感的业务，而且还需要业务层做适配，另外主从切换时间过长，也会导致客户端或消息队列中间件缓存写请求过多，切换完成之后重放这些请求的时间变长。</p><p>哨兵检测主库多久没有响应就提升从库为新的主库，这个时间是可以配置的（down-after-milliseconds参数）。配置的时间越短，哨兵越敏感，哨兵集群认为主库在短时间内连不上就会发起主从切换，这种配置很可能因为网络拥塞但主库正常而发生不必要的切换，当然，当主库真正故障时，因为切换得及时，对业务的影响最小。如果配置的时间比较长，哨兵越保守，这种情况可以减少哨兵误判的概率，但是主库故障发生时，业务写失败的时间也会比较久，缓存写请求数据量越多。</p><p>应用程序不感知服务的中断，还需要哨兵和客户端做些什么？当哨兵完成主从切换后，客户端需要及时感知到主库发生了变更，然后把缓存的写请求写入到新库中，保证后续写请求不会再受到影响，具体做法如下：</p><p>哨兵提升一个从库为新主库后，哨兵会把新主库的地址写入自己实例的pubsub（switch-master）中。客户端需要订阅这个pubsub，当这个pubsub有数据时，客户端就能感知到主库发生变更，同时可以拿到最新的主库地址，然后把写请求写到这个新主库即可，这种机制属于哨兵主动通知客户端。</p><p>如果客户端因为某些原因错过了哨兵的通知，或者哨兵通知后客户端处理失败了，安全起见，客户端也需要支持主动去获取最新主从的地址进行访问。</p><p>所以，客户端需要访问主从库时，不能直接写死主从库的地址了，而是需要从哨兵集群中获取最新的地址（sentinel get-master-addr-by-name命令），这样当实例异常时，哨兵切换后或者客户端断开重连，都可以从哨兵集群中拿到最新的实例地址。</p><p>一般Redis的SDK都提供了通过哨兵拿到实例地址，再访问实例的方式，我们直接使用即可，不需要自己实现这些逻辑。当然，对于只有主从实例的情况，客户端需要和哨兵配合使用，而在分片集群模式下，这些逻辑都可以做在proxy层，这样客户端也不需要关心这些逻辑了，Codis就是这么做的。</p><h4 id="基于-pub-sub-机制的哨兵集群组成"><a href="#基于-pub-sub-机制的哨兵集群组成" class="headerlink" title="基于 pub/sub 机制的哨兵集群组成"></a>基于 pub/sub 机制的哨兵集群组成</h4><p>哨兵实例之间可以相互发现，要归功于 Redis 提供的 pub/sub 机制，也就是发布 / 订阅机制。</p><p>哨兵只要和主库建立起了连接，就可以在主库上发布消息了，比如说发布它自己的连接信息（IP 和端口）。同时，它也可以从主库上订阅消息，获得其他哨兵发布的连接信息。当多个哨兵实例都在主库上做了发布和订阅操作后，它们之间就能知道彼此的 IP 地址和端口。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6b84b62cde68cdf6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="哨兵是如何知道从库的-IP-地址和端口的呢？"><a href="#哨兵是如何知道从库的-IP-地址和端口的呢？" class="headerlink" title="哨兵是如何知道从库的 IP 地址和端口的呢？"></a>哨兵是如何知道从库的 IP 地址和端口的呢？</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-94fc763588b6ce6c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="基于-pub-sub-机制的客户端事件通知"><a href="#基于-pub-sub-机制的客户端事件通知" class="headerlink" title="基于 pub/sub 机制的客户端事件通知"></a>基于 pub/sub 机制的客户端事件通知</h4><p>从本质上说，哨兵就是一个运行在特定模式下的 Redis 实例，只不过它并不服务请求操作，只是完成监控、选主和通知的任务。所以，每个哨兵实例也提供 pub/sub 机制，客户端可以从哨兵订阅消息。哨兵提供的消息订阅频道有很多，不同频道包含了主从库切换过程中的不同关键事件。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-07107bea955b9872.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>知道了这些频道之后，你就可以让客户端从哨兵这里订阅消息了。具体的操作步骤是，客户端读取哨兵的配置文件后，可以获得哨兵的地址和端口，和哨兵建立网络连接。然后，我们可以在客户端执行订阅命令，来获取不同的事件消息。</p><h4 id="由哪个哨兵执行主从切换？"><a href="#由哪个哨兵执行主从切换？" class="headerlink" title="由哪个哨兵执行主从切换？"></a>由哪个哨兵执行主从切换？</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-03685d2753b96c9c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为“客观下线”。这个所需的赞成票数是通过哨兵配置文件中的 quorum 配置项设定的。例如，现在有 5 个哨兵，quorum 配置的是 3，那么，一个哨兵需要 3 张赞成票，就可以标记主库为“客观下线”了。这 3 张赞成票包括哨兵自己的一张赞成票和另外两个哨兵的赞成票。</p><p>此时，这个哨兵就可以再给其他哨兵发送命令，表明希望由自己来执行主从切换，并让所有其他哨兵进行投票。这个投票过程称为“Leader 选举”。因为最终执行主从切换的哨兵称为 Leader，投票过程就是确定 Leader。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-52bac6f66d7248dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="Redis集群"><a href="#Redis集群" class="headerlink" title="Redis集群"></a>Redis集群</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-68315bffaa5f389c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>那么，在切片集群中，实例在为 5GB 数据生成 RDB 时，数据量就小了很多，fork 子进程一般不会给主线程带来较长时间的阻塞。采用多个实例保存数据切片后，我们既能保存 25GB 数据，又避免了 fork 子进程阻塞主线程而导致的响应突然变慢。</p><h4 id="数据切片和实例的对应分布关系"><a href="#数据切片和实例的对应分布关系" class="headerlink" title="数据切片和实例的对应分布关系"></a>数据切片和实例的对应分布关系</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0df4fbe7e3ae0c47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="客户端如何定位数据？"><a href="#客户端如何定位数据？" class="headerlink" title="客户端如何定位数据？"></a>客户端如何定位数据？</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a98e6ff555602e61.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>需要注意的是，在上图中，当客户端给实例 2 发送命令时，<strong>Slot 2 中的数据已经全部迁移到了实例 3</strong>。在实际应用时，如果 Slot 2 中的数据比较多，就可能会出现一种情况：客户端向实例 2 发送请求，但此时，<strong>Slot 2 中的数据只有一部分迁移到了实例 3</strong>，还有部分数据没有迁移。在这种迁移部分完成的情况下，客户端就会收到<strong>一条 ASK 报错信息</strong>，如下所示：</p><p>和 MOVED 命令不同，ASK 命令并不会更新客户端缓存的哈希槽分配信息。所以，在上图中，如果客户端再次请求 Slot 2 中的数据，它还是会给实例 2 发送请求。这也就是说，ASK 命令的作用只是让客户端能给新实例发送一次请求，而不像 MOVED 命令那样，会更改本地缓存，让后续所有命令都发往新实例。</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="整数数组和压缩列表在查找时间复杂度方面并没有很大的优势，那为什么-Redis-还会把它们作为底层数据结构呢？"><a href="#整数数组和压缩列表在查找时间复杂度方面并没有很大的优势，那为什么-Redis-还会把它们作为底层数据结构呢？" class="headerlink" title="整数数组和压缩列表在查找时间复杂度方面并没有很大的优势，那为什么 Redis 还会把它们作为底层数据结构呢？"></a>整数数组和压缩列表在查找时间复杂度方面并没有很大的优势，那为什么 Redis 还会把它们作为底层数据结构呢？</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6dba8a479077ed50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Redis 之所以采用不同的数据结构，其实是在性能和内存使用效率之间进行的平衡。</p><p>1、内存利用率，数组和压缩列表都是非常紧凑的数据结构，它比链表占用的内存要更少。Redis是内存数据库，大量数据存到内存中，此时需要做尽可能的优化，提高内存的利用率。<br>2、数组对CPU高速缓存支持更友好，所以Redis在设计时，集合数据元素较少情况下，默认采用内存紧凑排列的方式存储，同时利用CPU高速缓存不会降低访问速度。当数据元素超过设定阈值后，避免查询时间复杂度太高，转为哈希和跳表数据结构存储，保证查询效率。</p><h3 id="Redis-基本-IO-模型中还有哪些潜在的性能瓶颈？"><a href="#Redis-基本-IO-模型中还有哪些潜在的性能瓶颈？" class="headerlink" title="Redis 基本 IO 模型中还有哪些潜在的性能瓶颈？"></a>Redis 基本 IO 模型中还有哪些潜在的性能瓶颈？</h3><p>这个问题是希望你能进一步理解阻塞操作对 Redis 单线程性能的影响。在 Redis 基本 IO 模型中，主要是主线程在执行操作，任何耗时的操作，例如 bigkey、全量返回等操作，都是潜在的性能瓶颈。</p><ol><li>任意一个请求在server中一旦发生耗时，都会影响整个server的性能，也就是说后面的请求都要等前面这个耗时请求处理完成，自己才能被处理到。耗时的操作包括以下几种：<ul><li>操作bigkey：写入一个bigkey在分配内存时需要消耗更多的时间，同样，删除bigkey释放内存同样会产生耗时；</li><li>使用复杂度过高的命令：例如SORT/SUNION/ZUNIONSTORE，或者O(N)命令，但是N很大，例如lrange key 0 -1一次查询全量数据；</li><li>大量key集中过期：Redis的过期机制也是在主线程中执行的，大量key集中过期会导致处理一个请求时，耗时都在删除过期key，耗时变长；</li><li>淘汰策略：淘汰策略也是在主线程执行的，当内存超过Redis内存上限后，每次写入都需要淘汰一些key，也会造成耗时变长；</li><li>AOF刷盘开启always机制：每次写入都需要把这个操作刷到磁盘，写磁盘的速度远比写内存慢，会拖慢Redis的性能；</li><li>主从全量同步生成RDB：虽然采用fork子进程生成数据快照，但fork这一瞬间也是会阻塞整个线程的，实例越大，阻塞时间越久；</li></ul></li><li>并发量非常大时，单线程读写客户端IO数据存在性能瓶颈，虽然采用IO多路复用机制，但是读写客户端数据依旧是同步IO，只能单线程依次读取客户端的数据，无法利用到CPU多核。</li></ol><p>针对问题1，一方面需要业务人员去规避，一方面Redis在4.0推出了lazy-free机制，把bigkey释放内存的耗时操作放在了异步线程中执行，降低对主线程的影响。</p><p>针对问题2，Redis在6.0推出了多线程，可以在高并发场景下利用CPU多核多线程读写客户端数据，进一步提升server性能，当然，只是针对客户端的读写是并行的，每个命令的真正操作依旧是单线程的。</p><h3 id="AOF-重写过程中有没有其他潜在的阻塞风险？"><a href="#AOF-重写过程中有没有其他潜在的阻塞风险？" class="headerlink" title="AOF 重写过程中有没有其他潜在的阻塞风险？"></a>AOF 重写过程中有没有其他潜在的阻塞风险？</h3><p>风险一：Redis 主线程 fork 创建 bgrewriteaof 子进程时，内核需要创建用于管理子进程的相关数据结构，这些数据结构在操作系统中通常叫作进程控制块（Process Control Block，简称为 PCB）。内核要把主线程的 PCB 内容拷贝给子进程。这个创建和拷贝过程由内核执行，是会阻塞主线程的。而且，在拷贝过程中，子进程要拷贝父进程的页表，这个过程的耗时和 Redis 实例的内存大小有关。如果 Redis 实例内存大，页表就会大，fork 执行时间就会长，这就会给主线程带来阻塞风险。</p><p>风险二：bgrewriteaof 子进程会和主线程共享内存。当主线程收到新写或修改的操作时，主线程会申请新的内存空间，用来保存新写或修改的数据，如果操作的是 bigkey，也就是数据量大的集合类型数据，那么，主线程会因为申请大空间而面临阻塞风险。因为操作系统在分配内存空间时，有查找和锁的开销，这就会导致阻塞。</p><h3 id="AOF-重写为什么不共享使用-AOF-本身的日志？"><a href="#AOF-重写为什么不共享使用-AOF-本身的日志？" class="headerlink" title="AOF 重写为什么不共享使用 AOF 本身的日志？"></a>AOF 重写为什么不共享使用 AOF 本身的日志？</h3><p>如果都用 AOF 日志的话，主线程要写，bgrewriteaof 子进程也要写，这两者会竞争文件系统的锁，这就会对 Redis 主线程的性能造成影响。</p><h3 id="使用一个-2-核-CPU、4GB-内存、500GB-磁盘的云主机运行-Redis，Redis-数据库的数据量大小差不多是-2GB。当时-Redis-主要以修改操作为主，写读比例差不多在-8-2-左右，也就是说，如果有-100-个请求，80-个请求执行的是修改操作。在这个场景下，用-RDB-做持久化有什么风险吗？"><a href="#使用一个-2-核-CPU、4GB-内存、500GB-磁盘的云主机运行-Redis，Redis-数据库的数据量大小差不多是-2GB。当时-Redis-主要以修改操作为主，写读比例差不多在-8-2-左右，也就是说，如果有-100-个请求，80-个请求执行的是修改操作。在这个场景下，用-RDB-做持久化有什么风险吗？" class="headerlink" title="使用一个 2 核 CPU、4GB 内存、500GB 磁盘的云主机运行 Redis，Redis 数据库的数据量大小差不多是 2GB。当时 Redis 主要以修改操作为主，写读比例差不多在 8:2 左右，也就是说，如果有 100 个请求，80 个请求执行的是修改操作。在这个场景下，用 RDB 做持久化有什么风险吗？"></a>使用一个 2 核 CPU、4GB 内存、500GB 磁盘的云主机运行 Redis，Redis 数据库的数据量大小差不多是 2GB。当时 Redis 主要以修改操作为主，写读比例差不多在 8:2 左右，也就是说，如果有 100 个请求，80 个请求执行的是修改操作。在这个场景下，用 RDB 做持久化有什么风险吗？</h3><p>内存不足的风险：Redis fork 一个 bgsave 子进程进行 RDB 写入，如果主线程再接收到写操作，就会采用写时复制。写时复制需要给写操作的数据分配新的内存空间。本问题中写的比例为 80%，那么，在持久化过程中，为了保存 80% 写操作涉及的数据，写时复制机制会在实例内存中，为这些数据再分配新内存空间，分配的内存量相当于整个实例数据量的 80%，大约是 1.6GB，这样一来，整个系统内存的使用量就接近饱和了。此时，如果实例还有大量的新 key 写入或 key 修改，云主机内存很快就会被吃光。如果云主机开启了 Swap 机制，就会有一部分数据被换到磁盘上，当访问磁盘上的这部分数据时，性能会急剧下降。如果云主机没有开启 Swap，会直接触发 OOM，整个 Redis 实例会面临被系统 kill 掉的风险。</p><p>主线程和子进程竞争使用 CPU 的风险：生成 RDB 的子进程需要 CPU 核运行，主线程本身也需要 CPU 核运行，而且，如果 Redis 还启用了后台线程，此时，主线程、子进程和后台线程都会竞争 CPU 资源。由于云主机只有 2 核 CPU，这就会影响到主线程处理请求的速度。</p><h3 id="为什么主从库间的复制不使用-AOF？"><a href="#为什么主从库间的复制不使用-AOF？" class="headerlink" title="为什么主从库间的复制不使用 AOF？"></a>为什么主从库间的复制不使用 AOF？</h3><ol><li>RDB 文件是二进制文件，无论是要把 RDB 写入磁盘，还是要通过网络传输 RDB，IO 效率都比记录和传输 AOF 的高。</li><li>在从库端进行恢复时，用 RDB 的恢复效率要高于用 AOF。</li></ol><h3 id="在主从切换过程中，客户端能否正常地进行请求操作呢？"><a href="#在主从切换过程中，客户端能否正常地进行请求操作呢？" class="headerlink" title="在主从切换过程中，客户端能否正常地进行请求操作呢？"></a>在主从切换过程中，客户端能否正常地进行请求操作呢？</h3><p>主从集群一般是采用读写分离模式，当主库故障后，客户端仍然可以把读请求发送给从库，让从库服务。但是，对于写请求操作，客户端就无法执行了。</p><h3 id="如果想要应用程序不感知服务的中断，还需要哨兵或客户端再做些什么吗？"><a href="#如果想要应用程序不感知服务的中断，还需要哨兵或客户端再做些什么吗？" class="headerlink" title="如果想要应用程序不感知服务的中断，还需要哨兵或客户端再做些什么吗？"></a>如果想要应用程序不感知服务的中断，还需要哨兵或客户端再做些什么吗？</h3><p>一方面，客户端需要能缓存应用发送的写请求。只要不是同步写操作（Redis 应用场景一般也没有同步写），写请求通常不会在应用程序的关键路径上，所以，客户端缓存写请求后，给应用程序返回一个确认就行。</p><p>另一方面，主从切换完成后，客户端要能和新主库重新建立连接，哨兵需要提供订阅频道，让客户端能够订阅到新主库的信息。同时，客户端也需要能主动和哨兵通信，询问新主库的信息。</p><h3 id="哨兵实例是不是越多越好呢？如果同时调大-down-after-milliseconds-值，对减少误判是不是也有好处？"><a href="#哨兵实例是不是越多越好呢？如果同时调大-down-after-milliseconds-值，对减少误判是不是也有好处？" class="headerlink" title="哨兵实例是不是越多越好呢？如果同时调大 down-after-milliseconds 值，对减少误判是不是也有好处？"></a>哨兵实例是不是越多越好呢？如果同时调大 down-after-milliseconds 值，对减少误判是不是也有好处？</h3><p>哨兵实例越多，误判率会越低，但是在判定主库下线和选举 Leader 时，实例需要拿到的赞成票数也越多，等待所有哨兵投完票的时间可能也会相应增加，主从库切换的时间也会变长，客户端容易堆积较多的请求操作，可能会导致客户端请求溢出，从而造成请求丢失。如果业务层对 Redis 的操作有响应时间要求，就可能会因为新主库一直没有选定，新操作无法执行而发生超时报警。</p><p>调大 down-after-milliseconds 后，可能会导致这样的情况：主库实际已经发生故障了，但是哨兵过了很长时间才判断出来，这就会影响到 Redis 对业务的可用性。</p><h3 id="为什么-Redis-不直接用一个表，把键值对和实例的对应关系记录下来？"><a href="#为什么-Redis-不直接用一个表，把键值对和实例的对应关系记录下来？" class="headerlink" title="为什么 Redis 不直接用一个表，把键值对和实例的对应关系记录下来？"></a>为什么 Redis 不直接用一个表，把键值对和实例的对应关系记录下来？</h3><p>如果使用表记录键值对和实例的对应关系，一旦键值对和实例的对应关系发生了变化（例如实例有增减或者数据重新分布），就要修改表。如果是单线程操作表，那么所有操作都要串行执行，性能慢；如果是多线程操作表，就涉及到加锁开销。此外，如果数据量非常大，使用表记录键值对和实例的对应关系，需要的额外存储空间也会增加。</p><p>基于哈希槽计算时，虽然也要记录哈希槽和实例的对应关系，但是哈希槽的个数要比键值对的个数少很多，无论是修改哈希槽和实例的对应关系，还是使用额外空间存储哈希槽和实例的对应关系，都比直接记录键值对和实例的关系的开销小得多。</p><h3 id="Redis-什么时候做-rehash？"><a href="#Redis-什么时候做-rehash？" class="headerlink" title="Redis 什么时候做 rehash？"></a>Redis 什么时候做 rehash？</h3><p>Redis 会使用装载因子（load factor）来判断是否需要做 rehash。装载因子的计算方式是，哈希表中所有 entry 的个数除以哈希表的哈希桶个数。Redis 会根据装载因子的两种情况，来触发 rehash 操作：</p><ul><li>装载因子≥1，同时，哈希表被允许进行 rehash；</li><li>装载因子≥5。</li></ul><p>在第一种情况下，如果装载因子等于 1，同时我们假设，所有键值对是平均分布在哈希表的各个桶中的，那么，此时，哈希表可以不用链式哈希，因为一个哈希桶正好保存了一个键值对。</p><p>但是，如果此时再有新的数据写入，哈希表就要使用链式哈希了，这会对查询性能产生影响。在进行 RDB 生成和 AOF 重写时，哈希表的 rehash 是被禁止的，这是为了避免对 RDB 和 AOF 重写造成影响。如果此时，Redis 没有在生成 RDB 和重写 AOF，那么，就可以进行 rehash。否则的话，再有数据写入时，哈希表就要开始使用查询较慢的链式哈希了。</p><p>在第二种情况下，也就是装载因子大于等于 5 时，就表明当前保存的数据量已经远远大于哈希桶的个数，哈希桶里会有大量的链式哈希存在，性能会受到严重影响，此时，就立马开始做 rehash。</p><h3 id="采用渐进式-hash-时，如果实例暂时没有收到新请求，是不是就不做-rehash-了？"><a href="#采用渐进式-hash-时，如果实例暂时没有收到新请求，是不是就不做-rehash-了？" class="headerlink" title="采用渐进式 hash 时，如果实例暂时没有收到新请求，是不是就不做 rehash 了？"></a>采用渐进式 hash 时，如果实例暂时没有收到新请求，是不是就不做 rehash 了？</h3><p>其实不是的。Redis 会执行定时任务，定时任务中就包含了 rehash 操作。所谓的定时任务，就是按照一定频率（例如每 100ms/ 次）执行的任务。</p><p>在 rehash 被触发后，即使没有收到新请求，Redis 也会定时执行一次 rehash 操作，而且，每次执行时长不会超过 1ms，以免对其他任务造成影响。</p><h3 id="主线程、子进程和后台线程的联系与区别"><a href="#主线程、子进程和后台线程的联系与区别" class="headerlink" title="主线程、子进程和后台线程的联系与区别"></a>主线程、子进程和后台线程的联系与区别</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d4f082cb19238d2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在主线程中，我们还可以使用 fork 创建子进程，或是使用 pthread_create 创建线程。下面我先介绍下 Redis 中用 fork 创建的子进程有哪些。</p><ul><li>创建 RDB 的后台子进程，同时由它负责在主从同步时传输 RDB 给从库；</li><li>通过无盘复制方式传输 RDB 的子进程；</li><li>bgrewriteaof 子进程。</li></ul><h3 id="写时复制的底层实现机制"><a href="#写时复制的底层实现机制" class="headerlink" title="写时复制的底层实现机制"></a>写时复制的底层实现机制</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ed6ecd25a95cdb7e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="replication-buffer-和-repl-backlog-buffer-的区别"><a href="#replication-buffer-和-repl-backlog-buffer-的区别" class="headerlink" title="replication buffer 和 repl_backlog_buffer 的区别"></a>replication buffer 和 repl_backlog_buffer 的区别</h3><p>总的来说，replication buffer 是主从库在进行全量复制时，主库上用于和从库连接的客户端的 buffer，而 repl_backlog_buffer 是为了支持从库增量复制，主库上用于持续保存写操作的一块专用 buffer。</p><p>Redis 主从库在进行复制时，当主库要把全量复制期间的写操作命令发给从库时，主库会先创建一个客户端，用来连接从库，然后通过这个客户端，把写操作命令发给从库。在内存中，主库上的客户端就会对应一个 buffer，这个 buffer 就被称为 replication buffer。Redis 通过 client_buffer 配置项来控制这个 buffer 的大小。主库会给每个从库建立一个客户端，所以 replication buffer 不是共享的，而是每个从库都有一个对应的客户端。</p><p>repl_backlog_buffer 是一块专用 buffer，在 Redis 服务器启动后，开始一直接收写操作命令，这是所有从库共享的。主库和从库会各自记录自己的复制进度，所以，不同的从库在进行恢复时，会把自己的复制进度（slave_repl_offset）发给主库，主库就可以和它独立同步。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-bee7dca00a820694.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><h3 id="SDS为什么占空间"><a href="#SDS为什么占空间" class="headerlink" title="SDS为什么占空间"></a>SDS为什么占空间</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fee6ccdb8387fb15.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因为 10 位数的图片 ID 和图片存储对象 ID 是 Long 类型整数，所以可以直接用 int 编码的 RedisObject 保存。每个 int 编码的 RedisObject 元数据部分占 8 字节，指针部分被直接赋值为 8 字节的整数了。此时，每个 ID 会使用 16 字节，加起来一共是 32 字节。但是，另外的 32 字节去哪儿了呢？</p><p>我在第 2 讲中说过，Redis 会使用一个全局哈希表保存所有键值对，哈希表的每一项是一个 dictEntry 的结构体，用来指向一个键值对。dictEntry 结构中有三个 8 字节的指针，分别指向 key、value 以及下一个 dictEntry，三个指针共 24 字节，如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0ca80d257ebf049c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>但是，这三个指针只有 24 字节，为什么会占用了 32 字节呢？这就要提到 Redis 使用的内存分配库 jemalloc 了。</p><p>jemalloc 在分配内存时，会根据我们申请的字节数 N，找一个比 N 大，但是最接近 N 的 2 的幂次数作为分配的空间，这样可以减少频繁分配的次数。</p><p>举个例子。如果你申请 6 字节空间，jemalloc 实际会分配 8 字节空间；如果你申请 24 字节空间，jemalloc 则会分配 32 字节。所以，在我们刚刚说的场景里，dictEntry 结构就占用了 32 字节。</p><p>你看，明明有效信息只有 16 字节，使用 String 类型保存时，却需要 64 字节的内存空间，有 48 字节都没有用于保存实际的数据。我们来换算下，如果要保存的图片有 1 亿张，那么 1 亿条的图片 ID 记录就需要 6.4GB 内存空间，其中有 4.8GB 的内存空间都用来保存元数据了，额外的内存空间开销很大。那么，有没有更加节省内存的方法呢？</p><h3 id="异步的子线程机制"><a href="#异步的子线程机制" class="headerlink" title="异步的子线程机制"></a>异步的子线程机制</h3><p>Redis 主线程启动后，会使用操作系统提供的 pthread_create 函数创建 3 个子线程，分别由它们负责 AOF 日志写操作、键值对删除以及文件关闭的异步执行。</p><p>主线程通过一个链表形式的任务队列和子线程进行交互。当收到键值对删除和清空数据库的操作时，主线程会把这个操作封装成一个任务，放入到任务队列中，然后给客户端返回一个完成信息，表明删除已经完成。</p><p>但实际上，这个时候删除还没有执行，等到后台子线程从任务队列中读取任务后，才开始实际删除键值对，并释放相应的内存空间。因此，我们把这种异步删除也称为惰性删除（lazy free）。此时，删除或清空操作不会阻塞主线程，这就避免了对主线程的性能影响。</p><p>和惰性删除类似，当 AOF 日志配置成 everysec 选项后，主线程会把 AOF 写日志操作封装成一个任务，也放到任务队列中。后台子线程读取任务后，开始自行写入 AOF 日志，这样主线程就不用一直等待 AOF 日志写完了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d275c2b8293b9f09.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这里有个地方需要你注意一下，异步的键值对删除和数据库清空操作是 Redis 4.0 后提供的功能，Redis 也提供了新的命令来执行这两个操作。</p><ul><li>键值对删除：当你的集合类型中有大量元素（例如有百万级别或千万级别元素）需要删除时，我建议你使用 UNLINK 命令。</li><li>清空数据库：可以在 FLUSHDB 和 FLUSHALL 命令后加上 ASYNC 选项，这样就可以让后台子线程异步地清空数据库，如下所示：</li></ul><h3 id="绑核的风险和解决方案"><a href="#绑核的风险和解决方案" class="headerlink" title="绑核的风险和解决方案"></a>绑核的风险和解决方案</h3><p>当我们把 Redis 实例绑到一个 CPU 逻辑核上时，就会导致子进程、后台线程和 Redis 主线程竞争 CPU 资源，一旦子进程或后台线程占用 CPU 时，主线程就会被阻塞，导致 Redis 请求延迟增加。</p><h4 id="一个-Redis-实例对应绑一个物理核"><a href="#一个-Redis-实例对应绑一个物理核" class="headerlink" title="一个 Redis 实例对应绑一个物理核"></a>一个 Redis 实例对应绑一个物理核</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9bd83bc480efbf44.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><pre><code>lscpuArchitecture: x86_64...NUMA node0 CPU(s): 0-5,12-17NUMA node1 CPU(s): 6-11,18-23...</code></pre><p>我们还是以刚才的 NUMA 架构为例，NUMA node0 的 CPU 核编号是 0 到 5、12 到 17。其中，编号 0 和 12、1 和 13、2 和 14 等都是表示一个物理核的 2 个逻辑核。所以，在绑核时，我们使用属于同一个物理核的 2 个逻辑核进行绑核操作。例如，我们执行下面的命令，就把 Redis 实例绑定到了逻辑核 0 和 12 上，而这两个核正好都属于物理核 1。</p><pre><code>taskset -c 0,12 ./redis-server</code></pre><p>和只绑一个逻辑核相比，把 Redis 实例和物理核绑定，可以让主线程、子进程、后台线程共享使用 2 个逻辑核，可以在一定程度上缓解 CPU 资源竞争。但是，因为只用了 2 个逻辑核，它们相互之间的 CPU 竞争仍然还会存在。如果你还想进一步减少 CPU 竞争，我再给你介绍一种方案。</p><h4 id="优化-Redis-源码"><a href="#优化-Redis-源码" class="headerlink" title="优化 Redis 源码"></a>优化 Redis 源码</h4><p>通过编程实现绑核时，要用到操作系统提供的 1 个数据结构 cpu_set_t 和 3 个函数 CPU_ZERO、CPU_SET 和 sched_setaffinity，我先来解释下它们。</p><ul><li>cpu_set_t 数据结构：是一个位图，每一位用来表示服务器上的一个 CPU 逻辑核。</li><li>CPU_ZERO 函数：以 cpu_set_t 结构的位图为输入参数，把位图中所有的位设置为 0。</li><li>CPU_SET 函数：以 CPU 逻辑核编号和 cpu_set_t 位图为参数，把位图中和输入的逻辑核编号对应的位设置为 </li><li>sched_setaffinity 函数：以进程 / 线程 ID 号和 cpu_set_t 为参数，检查 cpu_set_t 中哪一位为 1，就把输入的 ID 号所代表的进程 / 线程绑在对应的逻辑核上。</li></ul><p>那么，怎么在编程时把这三个函数结合起来实现绑核呢？很简单，我们分四步走就行。</p><ul><li>第一步：创建一个 cpu_set_t 结构的位图变量；</li><li>第二步：使用 CPU_ZERO 函数，把 cpu_set_t 结构的位图所有的位都设置为 0；</li><li>第三步：根据要绑定的逻辑核编号，使用 CPU_SET 函数，把 cpu_set_t 结构的位图相应位设置为 1；</li><li>第四步：使用 sched_setaffinity 函数，把程序绑定在 cpu_set_t 结构位图中为 1 的逻辑核上。</li></ul><p>先说后台线程。为了让你更好地理解编程实现绑核，你可以看下这段示例代码，它实现了为线程绑核的操作：</p><pre><code>//线程函数void worker(int bind_cpu){    cpu_set_t cpuset;  //创建位图变量    CPU_ZERO(&amp;cpu_set); //位图变量所有位设置0    CPU_SET(bind_cpu, &amp;cpuset); //根据输入的bind_cpu编号，把位图对应为设置为1    sched_setaffinity(0, sizeof(cpuset), &amp;cpuset); //把程序绑定在cpu_set_t结构位图中为1的逻辑核    //实际线程函数工作}int main(){    pthread_t pthread1    //把创建的pthread1绑在编号为3的逻辑核上    pthread_create(&amp;pthread1, NULL, (void *)worker, 3);}</code></pre><p>对于 Redis 来说，它是在 bio.c 文件中的 bioProcessBackgroundJobs 函数中创建了后台线程。bioProcessBackgroundJobs 函数类似于刚刚的例子中的 worker 函数，在这个函数中实现绑核四步操作，就可以把后台线程绑到和主线程不同的核上了。</p><p>和给线程绑核类似，当我们使用 fork 创建子进程时，也可以把刚刚说的四步操作实现在 fork 后的子进程代码中，示例代码如下：</p><pre><code>int main(){   //用fork创建一个子进程   pid_t p = fork();   if(p &lt; 0){      printf(" fork error\n");   }   //子进程代码部分   else if(!p){      cpu_set_t cpuset;  //创建位图变量      CPU_ZERO(&amp;cpu_set); //位图变量所有位设置0      CPU_SET(3, &amp;cpuset); //把位图的第3位设置为1      sched_setaffinity(0, sizeof(cpuset), &amp;cpuset);  //把程序绑定在3号逻辑核      //实际子进程工作      exit(0);   }   ...}</code></pre><p>对于 Redis 来说，生成 RDB 和 AOF 日志重写的子进程分别是下面两个文件的函数中实现的。</p><ul><li>rdb.c 文件：rdbSaveBackground 函数；</li><li>aof.c 文件：rewriteAppendOnlyFileBackground 函数。</li></ul><h3 id="客户端输入和输出缓冲区"><a href="#客户端输入和输出缓冲区" class="headerlink" title="客户端输入和输出缓冲区"></a>客户端输入和输出缓冲区</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-97f0471c6cad0bcc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们前面已经分析过了，输入缓冲区就是用来暂存客户端发送的请求命令的，所以可能导致溢出的情况主要是下面两种：</p><ul><li>写入了 bigkey，比如一下子写入了多个百万级别的集合类型数据；</li><li>服务器端处理请求的速度过慢，例如，Redis 主线程出现了间歇性阻塞，无法及时处理正常发送的请求，导致客户端发送的请求在缓冲区越积越多。</li></ul><p>所以，我们必须得想办法避免输入缓冲区溢出。我们可以从两个角度去考虑如何避免，一是把缓冲区调大，二是从数据命令的发送和处理速度入手。</p><p>那什么情况下会发生输出缓冲区溢出呢？ 我为你总结了三种：</p><ul><li>服务器端返回 bigkey 的大量结果；</li><li>执行了 MONITOR 命令；</li><li>缓冲区大小设置得不合理。</li></ul><p>我们来总结下如何应对输出缓冲区溢出：</p><ul><li>避免 bigkey 操作返回大量数据结果；</li><li>避免在线上环境中持续使用 MONITOR 命令。</li><li>使用 client-output-buffer-limit 设置合理的缓冲区大小上限，或是缓冲区连续写入时间和写入量上限。</li></ul><p>现在，从缓冲区溢出对 Redis 的影响的角度，我再把这四个缓冲区分成两类做个总结。</p><ul><li>缓冲区溢出导致网络连接关闭：普通客户端、订阅客户端，以及从节点客户端，它们使用的缓冲区，本质上都是 Redis 客户端和服务器端之间，或是主从节点之间为了传输命令数据而维护的。这些缓冲区一旦发生溢出，处理机制都是直接把客户端和服务器端的连接，或是主从节点间的连接关闭。网络连接关闭造成的直接影响，就是业务程序无法读写 Redis，或者是主从节点全量同步失败，需要重新执行。</li><li>缓冲区溢出导致命令数据丢失：主节点上的复制积压缓冲区属于环形缓冲区，一旦发生溢出，新写入的命令数据就会覆盖旧的命令数据，导致旧命令数据的丢失，进而导致主从节点重新进行全量复制。</li></ul><h3 id="问题二"><a href="#问题二" class="headerlink" title="问题二"></a>问题二</h3><h4 id="CPU问题"><a href="#CPU问题" class="headerlink" title="CPU问题"></a>CPU问题</h4><p>在一台有 2 个 CPU Socket（每个 Socket 8 个物理核）的服务器上，我们部署了有 8 个实例的 Redis 切片集群（8 个实例都为主节点，没有主备关系），现在有两个方案：</p><ul><li>在同一个 CPU Socket 上运行 8 个实例，并和 8 个 CPU 核绑定；</li><li>在 2 个 CPU Socket 上各运行 4 个实例，并和相应 Socket 上的核绑定。</li></ul><p>如果不考虑网络数据读取的影响，你会选择哪个方案呢？</p><p>答案：建议使用第二个方案，主要有两方面的原因。</p><ol><li>同一个 CPU Socket 上的进程，会共享 L3 缓存。如果把 8 个实例都部署在同一个 Socket 上，它们会竞争 L3 缓存，这就会导致它们的 L3 缓存命中率降低，影响访问性能。</li><li>同一个 CPU Socket 上的进程，会使用同一个 Socket 上的内存空间。8 个实例共享同一个 Socket 上的内存空间，肯定会竞争内存资源。如果有实例保存的数据量大，其他实例能用到的内存空间可能就不够了，此时，其他实例就会跨 Socket 申请内存，进而造成跨 Socket 访问内存，造成实例的性能降低。</li></ol><h4 id="Redis-变慢的情况"><a href="#Redis-变慢的情况" class="headerlink" title="Redis 变慢的情况"></a>Redis 变慢的情况</h4><ol><li>使用复杂度过高的命令或一次查询全量数据；</li><li>操作 bigkey；</li><li>大量 key 集中过期；</li><li>内存达到 maxmemory；</li><li>客户端使用短连接和 Redis 相连；</li><li>当 Redis 实例的数据量大时，无论是生成 RDB，还是 AOF 重写，都会导致 fork 耗时严重；</li><li>AOF 的写回策略为 always，导致每个操作都要同步刷回磁盘；</li><li>Redis 实例运行机器的内存不足，导致 swap 发生，Redis 需要到 swap 分区读取数据；</li><li>进程绑定 CPU 不合理；</li><li>Redis 实例运行机器上开启了透明内存大页机制；</li><li>网卡压力过大。</li></ol><h4 id="缓存不一致问题"><a href="#缓存不一致问题" class="headerlink" title="缓存不一致问题"></a>缓存不一致问题</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d0544ead210840e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="缓存雪崩、击穿、穿透"><a href="#缓存雪崩、击穿、穿透" class="headerlink" title="缓存雪崩、击穿、穿透"></a>缓存雪崩、击穿、穿透</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6173168179abd2f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="Redis-可以在秒杀场景的哪些环节发挥作用？"><a href="#Redis-可以在秒杀场景的哪些环节发挥作用？" class="headerlink" title="Redis 可以在秒杀场景的哪些环节发挥作用？"></a>Redis 可以在秒杀场景的哪些环节发挥作用？</h4><p>我们一般可以把秒杀活动分成三个阶段。在每一个阶段，Redis 所发挥的作用也不一样。</p><p>第一阶段是秒杀活动前。</p><p>在这个阶段，用户会不断刷新商品详情页，这会导致详情页的瞬时请求量剧增。这个阶段的应对方案，一般是尽量把商品详情页的页面元素静态化，然后使用 CDN 或是浏览器把这些静态化的元素缓存起来。这样一来，秒杀前的大量请求可以直接由 CDN 或是浏览器缓存服务，不会到达服务器端了，这就减轻了服务器端的压力。</p><p>在这个阶段，有 CDN 和浏览器缓存服务请求就足够了，我们还不需要使用 Redis。</p><p>第二阶段是秒杀活动开始。</p><p>此时，大量用户点击商品详情页上的秒杀按钮，会产生大量的并发请求查询库存。一旦某个请求查询到有库存，紧接着系统就会进行库存扣减。然后，系统会生成实际订单，并进行后续处理，例如订单支付和物流服务。如果请求查不到库存，就会返回。用户通常会继续点击秒杀按钮，继续查询库存。</p><p>简单来说，这个阶段的操作就是三个：库存查验、库存扣减和订单处理。因为每个秒杀请求都会查询库存，而请求只有查到有库存余量后，后续的库存扣减和订单处理才会被执行。所以，这个阶段中最大的并发压力都在库存查验操作上。</p><p>为了支撑大量高并发的库存查验请求，我们需要在这个环节使用 Redis 保存库存量，这样一来，请求可以直接从 Redis 中读取库存并进行查验。</p><p>那么，库存扣减和订单处理是否都可以交给后端的数据库来执行呢?</p><p>其实，订单处理可以在数据库中执行，但库存扣减操作，不能交给后端数据库处理。</p><p>在数据库中处理订单的原因比较简单，我先说下。</p><p>订单处理会涉及支付、商品出库、物流等多个关联操作，这些操作本身涉及数据库中的多张数据表，要保证处理的事务性，需要在数据库中完成。而且，订单处理时的请求压力已经不大了，数据库可以支撑这些订单处理请求。</p><p>那为啥库存扣减操作不能在数据库执行呢？这是因为，一旦请求查到有库存，就意味着发送该请求的用户获得了商品的购买资格，用户就会下单了。同时，商品的库存余量也需要减少一个。如果我们把库存扣减的操作放到数据库执行，会带来两个问题。</p><ol><li>额外的开销。Redis 中保存了库存量，而库存量的最新值又是数据库在维护，所以数据库更新后，还需要和 Redis 进行同步，这个过程增加了额外的操作逻辑，也带来了额外的开销。</li><li>下单量超过实际库存量，出现超售。由于数据库的处理速度较慢，不能及时更新库存余量，这就会导致大量库存查验的请求读取到旧的库存值，并进行下单。此时，就会出现下单数量大于实际的库存量，导致出现超售，这就不符合业务层的要求了。</li></ol><p>所以，我们就需要直接在 Redis 中进行库存扣减。具体的操作是，当库存查验完成后，一旦库存有余量，我们就立即在 Redis 中扣减库存。而且，为了避免请求查询到旧的库存值，库存查验和库存扣减这两个操作需要保证原子性。</p><p>第三阶段就是秒杀活动结束后。</p><p>在这个阶段，可能还会有部分用户刷新商品详情页，尝试等待有其他用户退单。而已经成功下单的用户会刷新订单详情，跟踪订单的进展。不过，这个阶段中的用户请求量已经下降很多了，服务器端一般都能支撑，我们就不重点讨论了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f043aa8017b0aecf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="数据量倾斜的成因？"><a href="#数据量倾斜的成因？" class="headerlink" title="数据量倾斜的成因？"></a>数据量倾斜的成因？</h4><h5 id="bigkey-导致倾斜"><a href="#bigkey-导致倾斜" class="headerlink" title="bigkey 导致倾斜"></a>bigkey 导致倾斜</h5><p>第一个原因是，某个实例上正好保存了 bigkey。bigkey 的 value 值很大（String 类型），或者是 bigkey 保存了大量集合元素（集合类型），会导致这个实例的数据量增加，内存资源消耗也相应增加。</p><p>其实，bigkey 已经是我们课程中反复提到的一个关键点了。为了避免 bigkey 造成的数据倾斜，一个根本的应对方法是，我们在业务层生成数据时，要尽量避免把过多的数据保存在同一个键值对中。</p><h5 id="Slot-分配不均衡导致倾斜"><a href="#Slot-分配不均衡导致倾斜" class="headerlink" title="Slot 分配不均衡导致倾斜"></a>Slot 分配不均衡导致倾斜</h5><p>如果集群运维人员没有均衡地分配 Slot，就会有大量的数据被分配到同一个 Slot 中，而同一个 Slot 只会在一个实例上分布，这就会导致，大量数据被集中到一个实例上，造成数据倾斜。</p><h5 id="Hash-Tag-导致倾斜"><a href="#Hash-Tag-导致倾斜" class="headerlink" title="Hash Tag 导致倾斜"></a>Hash Tag 导致倾斜</h5><p>Hash Tag 是指加在键值对 key 中的一对花括号{}。这对括号会把 key 的一部分括起来，客户端在计算 key 的 CRC16 值时，只对 Hash Tag 花括号中的 key 内容进行计算。如果没用 Hash Tag 的话，客户端计算整个 key 的 CRC16 的值。</p><p>我的建议是，如果使用 Hash Tag 进行切片的数据会带来较大的访问压力，就优先考虑避免数据倾斜，最好不要使用 Hash Tag 进行数据切片。因为事务和范围查询都还可以放在客户端来执行，而数据倾斜会导致实例不稳定，造成服务不可用。</p><h4 id="数据访问倾斜的成因和应对方法？"><a href="#数据访问倾斜的成因和应对方法？" class="headerlink" title="数据访问倾斜的成因和应对方法？"></a>数据访问倾斜的成因和应对方法？</h4><p>通常来说，热点数据以服务读操作为主，在这种情况下，我们可以采用热点数据多副本的方法来应对。</p><p>这个方法的具体做法是，我们把热点数据复制多份，在每一个数据副本的 key 中增加一个随机前缀，让它和其它副本数据不会被映射到同一个 Slot 中。这样一来，热点数据既有多个副本可以同时服务请求，同时，这些副本数据的 key 又不一样，会被映射到不同的 Slot 中。在给这些 Slot 分配实例时，我们也要注意把它们分配到不同的实例上，那么，热点数据的访问压力就被分散到不同的实例上了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d0bd20cb0f8ef019.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Redis-Cluster-瓶颈"><a href="#Redis-Cluster-瓶颈" class="headerlink" title="Redis Cluster 瓶颈"></a>Redis Cluster 瓶颈</h2><p>Redis Cluster 能保存的数据量以及支撑的吞吐量，跟集群的实例规模密切相关。Redis 官方给出了 Redis Cluster 的规模上限，就是一个集群运行 1000 个实例。</p><p>那么，你可能会问，为什么要限定集群规模呢？其实，这里的一个关键因素就是，实例间的通信开销会随着实例规模增加而增大，在集群超过一定规模时（比如 800 节点），集群吞吐量反而会下降。所以，集群的实际规模会受到限制。</p><p>经过刚刚的分析，我们可以很直观地看到，实例间使用 Gossip 协议进行通信时，通信开销受到通信消息大小和通信频率这两方面的影响，</p><p>消息越大、频率越高，相应的通信开销也就越大。如果想要实现高效的通信，可以从这两方面入手去调优。接下来，我们就来具体分析下这两方面的实际情况。</p><h3 id="实例间通信频率"><a href="#实例间通信频率" class="headerlink" title="实例间通信频率"></a>实例间通信频率</h3><p>Redis Cluster 的实例启动后，默认会每秒从本地的实例列表中随机选出 5 个实例，再从这 5 个实例中找出一个最久没有通信的实例，把 PING 消息发送给该实例。这是实例周期性发送 PING 消息的基本做法。</p><p>但是，这里有一个问题：实例选出来的这个最久没有通信的实例，毕竟是从随机选出的 5 个实例中挑选的，这并不能保证这个实例就一定是整个集群中最久没有通信的实例。</p><p>所以，这有可能会出现，有些实例一直没有被发送 PING 消息，导致它们维护的集群状态已经过期了。</p><p>为了避免这种情况，Redis Cluster 的实例会按照每 100ms 一次的频率，扫描本地的实例列表，如果发现有实例最近一次接收 PONG 消息的时间，已经大于配置项 cluster-node-timeout 的一半了（cluster-node-timeout/2），就会立刻给该实例发送 PING 消息，更新这个实例上的集群状态信息。</p><h3 id="如何降低实例间的通信开销？"><a href="#如何降低实例间的通信开销？" class="headerlink" title="如何降低实例间的通信开销？"></a>如何降低实例间的通信开销？</h3><p>经过刚才的学习，我们现在知道，实例间发送消息的频率有两个。</p><ul><li>每个实例每 1 秒发送一条 PING 消息。这个频率不算高，如果再降低该频率的话，集群中各实例的状态可能就没办法及时传播了。</li><li>每个实例每 100 毫秒会做一次检测，给 PONG 消息接收超过 cluster-node-timeout/2 的节点发送 PING 消息。实例按照每 100 毫秒进行检测的频率，是 Redis 实例默认的周期性检查任务的统一频率，我们一般不需要修改它。</li></ul><p>那么，就只有 cluster-node-timeout 这个配置项可以修改了。</p><p>配置项 cluster-node-timeout 定义了集群实例被判断为故障的心跳超时时间，默认是 15 秒。如果 cluster-node-timeout 值比较小，那么，在大规模集群中，就会比较频繁地出现 PONG 消息接收超时的情况，从而导致实例每秒要执行 10 次“给 PONG 消息超时的实例发送 PING 消息”这个操作。</p><p>所以，为了避免过多的心跳消息挤占集群带宽，我们可以调大 cluster-node-timeout 值，比如说调大到 20 秒或 25 秒。这样一来， PONG 消息接收超时的情况就会有所缓解，单实例也不用频繁地每秒执行 10 次心跳发送操作了。</p><h2 id="Redis-6-0新特性"><a href="#Redis-6-0新特性" class="headerlink" title="Redis 6.0新特性"></a>Redis 6.0新特性</h2><p><img src="https://upload-images.jianshu.io/upload_images/12321605-eff6d48dba43409c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="从单线程处理网络请求到多线程处理"><a href="#从单线程处理网络请求到多线程处理" class="headerlink" title="从单线程处理网络请求到多线程处理"></a>从单线程处理网络请求到多线程处理</h3><p>在 Redis 6.0 中，非常受关注的第一个新特性就是多线程。这是因为，Redis 一直被大家熟知的就是它的单线程架构，虽然有些命令操作可以用后台线程或子进程执行（比如数据删除、快照生成、AOF 重写），但是，从网络 IO 处理到实际的读写命令处理，都是由单个线程完成的。</p><p>随着网络硬件的性能提升，Redis 的性能瓶颈有时会出现在网络 IO 的处理上，也就是说，单个主线程处理网络请求的速度跟不上底层网络硬件的速度。</p><p>为了应对这个问题，一般有两种方法。</p><p>第一种方法是，用用户态网络协议栈（例如 DPDK）取代内核网络协议栈，让网络请求的处理不用在内核里执行，直接在用户态完成处理就行。</p><p>对于高性能的 Redis 来说，避免频繁让内核进行网络请求处理，可以很好地提升请求处理效率。但是，这个方法要求在 Redis 的整体架构中，添加对用户态网络协议栈的支持，需要修改 Redis 源码中和网络相关的部分（例如修改所有的网络收发请求函数），这会带来很多开发工作量。而且新增代码还可能引入新 Bug，导致系统不稳定。所以，Redis 6.0 中并没有采用这个方法。</p><p>第二种方法就是采用多个 IO 线程来处理网络请求，提高网络请求处理的并行度。Redis 6.0 就是采用的这种方法。</p><p>但是，Redis 的多 IO 线程只是用来处理网络请求的，对于读写命令，Redis 仍然使用单线程来处理。这是因为，Redis 处理请求时，网络处理经常是瓶颈，通过多个 IO 线程并行处理网络操作，可以提升实例的整体处理性能。而继续使用单线程执行命令操作，就不用为了保证 Lua 脚本、事务的原子性，额外开发多线程互斥机制了。这样一来，Redis 线程模型实现就简单了。</p><p>我们来看下，在 Redis 6.0 中，主线程和 IO 线程具体是怎么协作完成请求处理的。掌握了具体原理，你才能真正地会用多线程。为了方便你理解，我们可以把主线程和多 IO 线程的协作分成四个阶段。</p><p>阶段一：服务端和客户端建立 Socket 连接，并分配处理线程</p><p>首先，主线程负责接收建立连接请求。当有客户端请求和实例建立 Socket 连接时，主线程会创建和客户端的连接，并把 Socket 放入全局等待队列中。紧接着，主线程通过轮询方法把 Socket 连接分配给 IO 线程。</p><p>阶段二：IO 线程读取并解析请求</p><p>主线程一旦把 Socket 分配给 IO 线程，就会进入阻塞状态，等待 IO 线程完成客户端请求读取和解析。因为有多个 IO 线程在并行处理，所以，这个过程很快就可以完成。</p><p>阶段三：主线程执行请求操作</p><p>等到 IO 线程解析完请求，主线程还是会以单线程的方式执行这些命令操作。下面这张图显示了刚才介绍的这三个阶段，你可以看下，加深理解。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-810edf49237a7d3c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>阶段四：IO 线程回写 Socket 和主线程清空全局队列</p><p>当主线程执行完请求操作后，会把需要返回的结果写入缓冲区，然后，主线程会阻塞等待 IO 线程把这些结果回写到 Socket 中，并返回给客户端。</p><p>和 IO 线程读取和解析请求一样，IO 线程回写 Socket 时，也是有多个线程在并发执行，所以回写 Socket 的速度也很快。等到 IO 线程回写 Socket 完毕，主线程会清空全局队列，等待客户端的后续请求。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f1e96f4d91df1f10.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Redis-和-Memcached、RocksDB-的对比"><a href="#Redis-和-Memcached、RocksDB-的对比" class="headerlink" title="Redis 和 Memcached、RocksDB 的对比"></a>Redis 和 Memcached、RocksDB 的对比</h2><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3d57108cc0c9febe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-91800a3647cd885c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c3d1b71666f89fd8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DDD-领域驱动设计</title>
      <link href="/2021/02/08/ddd/"/>
      <url>/2021/02/08/ddd/</url>
      
        <content type="html"><![CDATA[<h2 id="一、DDD的基础概念"><a href="#一、DDD的基础概念" class="headerlink" title="一、DDD的基础概念"></a>一、DDD的基础概念</h2><h3 id="1-1-什么是-DDD"><a href="#1-1-什么是-DDD" class="headerlink" title="1.1 什么是 DDD"></a>1.1 什么是 DDD</h3><blockquote><p>2004 年埃里克·埃文斯（Eric Evans）发表了《领域驱动设计》（Domain-Driven Design –Tackling Complexity in the Heart of Software）这本书，从此领域驱动设计（Domain Driven Design，简称 DDD）诞生。<strong>DDD 核心思想是通过领域驱动设计方法定义领域模型，从而确定业务和应用边界，保证业务模型与代码模型的一致性</strong>。<br>领域驱动设计，主要是用来指导<strong>如何解耦业务系统，划分业务模块</strong>，定义业务领域模型及其交互。领域驱动设计这个概念并不新颖，早在 2004 年就被提出了，到现在已经有十几年的历史了。不过，它被大众熟知，还是基于另一个概念的兴起，那就是微服务。<br>不过，我个人觉得，领域驱动设计有点儿类似敏捷开发、SOA、PAAS 等概念，听起来很高大上，但实际上只值“五分钱”。即便你没有听说过领域驱动设计，对这个概念一无所知，只要你是在开发业务系统，也或多或少都在使用它。做好领域驱动设计的关键是，看你对自己所做业务的熟悉程度，而并不是对领域驱动设计这个概念本身的掌握程度。即便你对领域驱动搞得再清楚，但是对业务不熟悉，也并不一定能做出合理的领域设计。所以，<strong>不要把领域驱动设计当银弹，不要花太多的时间去过度地研究它。</strong></p></blockquote><p>引自：<a href="https://time.geekbang.org/column/article/169600">https://time.geekbang.org/column/article/169600</a></p><h3 id="1-2-DDD-主要解决什么问题"><a href="#1-2-DDD-主要解决什么问题" class="headerlink" title="1.2 DDD 主要解决什么问题"></a>1.2 DDD 主要解决什么问题</h3><p>DDD主要解决微服务边界划分困难的问题。</p><h3 id="1-3-DDD-与微服务的关系"><a href="#1-3-DDD-与微服务的关系" class="headerlink" title="1.3 DDD 与微服务的关系"></a>1.3 DDD 与微服务的关系</h3><blockquote><p>DDD 是一种<strong>架构设计方法/思想</strong>，微服务是一种架构风格，两者从本质上都是为了追求高响应力，而从业务视角去分离应用系统建设复杂度的手段。两者都强调从业务出发，其核心要义是强调根据业务发展，合理划分领域边界，持续调整现有架构，优化现有代码，以保持架构和代码的生命力，也就是我们常说的演进式架构。<br><strong>DDD 主要关注</strong>：从业务领域视角划分领域边界，构建通用语言进行高效沟通，通过业务抽象，建立领域模型，维持业务和代码的逻辑一致性。<br><strong>微服务主要关注</strong>：运行时的进程间通信、容错和故障隔离，实现去中心化数据管理和去中心化服务治理，关注微服务的独立开发、测试、构建和部署。</p></blockquote><h3 id="1-4-领域、核心域、通用域、支撑域"><a href="#1-4-领域、核心域、通用域、支撑域" class="headerlink" title="1.4 领域、核心域、通用域、支撑域"></a>1.4 领域、核心域、通用域、支撑域</h3><blockquote><p>领域就是用来确定范围的，范围即边界，这也是 DDD 在设计中不断强调边界的原因。<br>在领域不断划分的过程中，领域会细分为不同的子域，子域可以根据自身重要性和功能属性划分为三类子域，它们分别是：核心域、通用域和支撑域。<br>核心域：产品的核心的模块，能够给产品提供核心竞争力。<br>通用域：有一定通用性，比如认证、权限、邮件发送服务，不是业务的核心，但是没有他们业务也无法运转。<br>支撑域：则具有企业特性，但不具有通用性，例如数据代码类的数据字典等系统</p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0023cee42b88a41a?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-40cc56f2d8c55f61?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="1-5-通用语言和限界上下文"><a href="#1-5-通用语言和限界上下文" class="headerlink" title="1.5 通用语言和限界上下文"></a>1.5 通用语言和限界上下文</h3><blockquote><p>在事件风暴过程中，<strong>通过团队交流达成共识的，能够简单、清晰、准确描述业务涵义和规则的语言就是通用语言</strong>。也就是说，通用语言是团队统一的语言，不管你在团队中承担什么角色，在同一个领域的软件生命周期里都<strong>使用统一的语言进行交流</strong>。<br>我们可以将限界上下文拆解为两个词：限界和上下文。限界就是领域的边界，而上下文则是语义环境。通过领域的限界上下文，我们就可以在统一的领域边界内用统一的语言进行交流。<br><strong>理论上限界上下文就是微服务的边界</strong>。我们将限界上下文内的领域模型映射到微服务，就完成了从问题域到软件的解决方案。<br><img src="https://upload-images.jianshu.io/upload_images/12321605-94dda6eba5345bb7?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p></blockquote><h3 id="1-6-防腐层（Anti-corruption-layer）"><a href="#1-6-防腐层（Anti-corruption-layer）" class="headerlink" title="1.6 防腐层（Anti-corruption layer）"></a>1.6 <strong>防腐层（Anti-corruption layer）</strong></h3><blockquote><p>大多数应用程序依赖于其他系统的某些数据或功能<strong>。</strong> 例如，旧版应用程序迁移到新式系统时，可能仍需要现有的旧的资源。 新功能必须能够调用旧系统。 逐步迁移尤其如此，随着时间推移，较大型应用程序的不同功能迁移到新式系统中。<br>这些旧系统通常会出现质量问题，如复杂的数据架构或过时的 API。 旧系统使用的功能和技术可能与新式系统中的功能和技术有很大差异。 若要与旧系统进行互操作，新应用程序可能需要支持过时的基础结构、协议、数据模型、API、或其他不会引入新式应用程序的功能。<br>保持新旧系统之间的访问可以<strong>强制新系统至少支持某些旧系统的 API 或其他语义</strong>。 这些旧的功能出现质量问题时，支持它们“损坏”可能会是完全设计的新式应用程序。<br>不仅仅是旧系统，不受开发团队控制的**任何外部系统(第三方系统)**都可能出现类似的问题。<br><strong>解决方案</strong><br><strong>在不同的子系统之间放置防损层以将其隔离</strong>。 此层转换两个系统之间的通信，在一个系统保持不变的情况下，使另一个系统可以避免破坏其设计和技术方法。</p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6ec64f901f97f1cb?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="1-7-贫血模型、充血模型"><a href="#1-7-贫血模型、充血模型" class="headerlink" title="1.7 贫血模型、充血模型"></a>1.7 贫血模型、充血模型</h3><blockquote><p>贫血模型（Anemic Domain Model）：像 VirtualWalletBo 这样，只包含数据，不包含业务逻辑的类，就叫作贫血模型。在贫血模型中，数据和业务逻辑被分割到不同的类中， 例如 MVC架构。</p></blockquote><pre><code>public class VirtualWalletBo {  private Long id;  private Long createTime;  private BigDecimal balance;}</code></pre><blockquote><p>充血模型（Rich Domain Model）：充血模型正好相反，数据和对应的业务逻辑被封装到同一个类中。因此，这种充血模型满足面向对象的封装特性，是典型的面向对象编程风格，例如DDD架构，领域模型。</p></blockquote><pre><code>public class VirtualWallet { // Domain领域模型(充血模型)  private Long id;  private Long createTime = System.currentTimeMillis();;  private BigDecimal balance = BigDecimal.ZERO;  public void debit(BigDecimal amount) {    if (this.balance.compareTo(amount) &lt; 0) {      throw new InsufficientBalanceException(...);    }    this.balance = this.balance.subtract(amount);  }  public void credit(BigDecimal amount) {    if (amount.compareTo(BigDecimal.ZERO) &lt; 0) {      throw new InvalidAmountException(...);    }    this.balance = this.balance.add(amount);  }}</code></pre><h3 id="1-8-实体和对象"><a href="#1-8-实体和对象" class="headerlink" title="1.8 实体和对象"></a>1.8 实体和对象</h3><p><strong>实体</strong></p><blockquote><p>在 DDD 不同的设计过程中，实体的形态是不同的。在战略设计时，实体是领域模型的一个重要对象。领域模型中的实体是多个属性、操作或行为的载体。在事件风暴中，我们可以根据命令、操作或者事件，找出产生这些行为的业务实体对象，进而按照一定的业务规则将依存度高和业务关联紧密的多个实体对象和值对象进行聚类，形成聚合。你可以这么理解，实体和值对象是组成领域模型的基础单元。<br>在代码模型中，实体的表现形式是实体类，这个类包含了实体的属性和方法，通过这些方法实现实体自身的业务逻辑。在 DDD 里，<strong>这些实体类通常采用充血模型</strong>，与这个实体相关的所有业务逻辑都在实体类的方法中实现，跨多个实体的领域逻辑则在领域服务中实现。</p></blockquote><p><strong>值对象</strong></p><blockquote><p>值对象是 DDD 领域模型中的一个基础对象，它跟实体一样都来源于事件风暴所构建的领域模型，都包含了若干个属性，它与实体一起构成聚合。<br>当一个概念缺乏明显身份时，就基本可以断定它是模型中的一个值对象。比如Content、Extra就是一个值对象，值对象一般都是贫血对象。</p></blockquote><pre><code>type Message struct {        ID                    int64                                      Type                  MessageConstants.MessageType         CreateTime            int64                                      FromID                *int64                                     Content               *Content                                   Extra                 *Extra                                     RootID                int64                                      .......               }</code></pre><h3 id="1-9-聚合和聚合根"><a href="#1-9-聚合和聚合根" class="headerlink" title="1.9 聚合和聚合根"></a>1.9 聚合和聚合根</h3><p><strong>聚合</strong></p><blockquote><p>领域模型内的实体和值对象就好比个体，<strong>而能让实体和值对象协同工作的组织就是聚合</strong>，它用来确保这些领域对象在实现共同的业务逻辑时，能保证数据的一致性。<br>聚合就是由业务和逻辑紧密关联的实体和值对象组合而成的，<strong>聚合是数据修改和持久化的基本单元，每一个聚合对应一个仓储，实现数据的持久化。</strong><br>聚合在 DDD 分层架构里属于领域层，领域层包含了多个聚合，共同实现核心业务逻辑。聚合内实体以充血模型实现个体业务能力，以及业务逻辑的高内聚。跨多个实体的业务逻辑通过领域服务来实现，跨多个聚合的业务逻辑通过应用服务来实现。比如有的业务场景需要同一个聚合的 A 和 B 两个实体来共同完成，我们就可以将这段业务逻辑用领域服务来实现；而有的业务逻辑需要聚合 C 和聚合 D 中的两个服务共同完成，这时你就可以用应用服务来组合这两个服务。<br><strong>聚合的特点：高内聚、低耦合，它是领域模型中最底层的边界，可以作为拆分微服务的最小单位。</strong></p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/12321605-531efebb998de361?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p><strong>聚合根</strong></p><blockquote><p>如果把聚合比作组织，那聚合根就是这个组织的负责人。聚合根也称为根实体，<strong>它不仅是实体，还是聚合的管理者</strong>。</p></blockquote><h3 id="1-10-领域事件"><a href="#1-10-领域事件" class="headerlink" title="1.10 领域事件"></a>1.10 领域事件</h3><p><strong>微服务内的领域事件</strong></p><blockquote><p>当领域事件发生在微服务内的聚合之间，领域事件发生后完成事件实体构建和事件数据持久化，发布方聚合将事件发布到事件总线，订阅方接收事件数据完成后续业务操作。</p></blockquote><p><strong>微服务之间的领域事件</strong></p><blockquote><p>跨微服务的领域事件会在不同的限界上下文或领域模型之间实现业务协作，其主要目的是实现微服务解耦，减轻微服务之间实时服务访问的压力。</p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d9ca14e82fa79c5c?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="1-11-PO、DO、DTO、VO"><a href="#1-11-PO、DO、DTO、VO" class="headerlink" title="1.11 PO、DO、DTO、VO"></a>1.11 PO、DO、DTO、VO</h3><blockquote><p>PO：数据持久化对象 (Persistent Object， PO)，与数据库结构一一映射，它是数据持久化过程中的数据载体。<br>DO：领域对象（ Domain Object， DO），微服务运行时核心业务对象的载体， DO 一般包括实体或值对象。<br>DTO：数据传输对象（ Data Transfer Object， DTO），用于前端应用与微服务应用层或者微服务之间的数据组装和传输，是应用之间数据传输的载体。<br>VO：视图对象（View Object， VO），用于封装展示层指定页面或组件的数据。<br>DTO 是用于数据传输的对象，我们可以把kite生成的Request、Response当做DTO对象，在Http服务中可以把用户传输的Json对象作为DTO。<br>VO 是视图对象，处于MVC架构的Logic层，多用于UI组件的数据的封装。</p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c2e2fdaeffd4b619?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="1-12-CQRS"><a href="#1-12-CQRS" class="headerlink" title="1.12 CQRS"></a>1.12 CQRS</h3><blockquote><p>CQRS — Command Query Responsibility Segregation。CQRS 将系统中的操作分为两类，即「命令」(Command) 与「查询」(Query)。命令则是对会引起数据发生变化操作的总称，即我们常说的新增，更新，删除这些操作，都是命令。而查询则和字面意思一样，即不会对数据产生变化的操作，只是按照某些条件查找数据。<br>CQRS 的核心思想是将这两类不同的操作进行分离，然后在两个独立的「服务」中实现。这里的「服务」一般是指两个独立部署的应用。在某些特殊情况下，也可以部署在同一个应用内的不同接口上。<br>CQRS 在 DDD 中是一种常常被提及的模式，它的用途在于将领域模型与查询功能进行分离，让一些复杂的查询摆脱领域模型的限制，以更为简单的 DTO 形式展现查询结果。同时分离了不同的数据存储结构，让开发者按照查询的功能与要求更加自由的选择数据存储引擎。</p></blockquote><h3 id="1-13-DDD的战略设计和战术设计"><a href="#1-13-DDD的战略设计和战术设计" class="headerlink" title="1.13 DDD的战略设计和战术设计"></a>1.13 DDD的战略设计和战术设计</h3><p><strong>战略设计</strong></p><p>主要从业务视角出发，建立业务领域模型，划分领域边界，建立通用语言的限界上下文，限界上下文可以作为微服务设计的参考边界。</p><p><strong>战术设计</strong></p><p>则从技术视角出发，侧重于领域模型的技术实现，完成软件开发和落地，包括：聚合根、实体、值对象、领域服务、应用服务和资源库等代码逻辑的设计和实现。</p><h3 id="1-14-从DDD设计到代码落地的大致流程"><a href="#1-14-从DDD设计到代码落地的大致流程" class="headerlink" title="1.14 从DDD设计到代码落地的大致流程"></a>1.14 从DDD设计到代码落地的大致流程</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-30195b6cad75f228?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p>事件风暴 -&gt; 领域故事分析 -&gt; 提取领域对象 -&gt; 领域对象与代码模型映射 -&gt; 代码落地</p><p>在事件风暴的过程中，领域专家会和设计、开发人员一起建立领域模型，在领域建模的过程中会形成通用的业务术语和用户故事。</p><p>事件风暴也是一个项目团队统一语言的过程。通过用户故事分析会形成一个个的领域对象，这些领域对象对应领域模型的业务对象，每一个业务对象和领域对象都有通用的名词术语，并且一一映射。</p><p>微服务代码模型来源于领域模型，每个代码模型的代码对象跟领域对象一一对应。</p><h3 id="1-15-如何划定领域模型和微服务的边界"><a href="#1-15-如何划定领域模型和微服务的边界" class="headerlink" title="1.15 如何划定领域模型和微服务的边界"></a>1.15 如何划定领域模型和微服务的边界</h3><ul><li>第一步：在事件风暴中梳理业务过程中的用户操作、事件以及外部依赖关系等，根据这些要素梳理出领域实体等领域对象。</li><li>第二步：根据领域实体之间的业务关联性，将业务紧密相关的实体进行组合形成聚合，同时确定聚合中的聚合根、值对象和实体。</li><li>第三步：根据业务及语义边界等因素，将一个或者多个聚合划定在一个限界上下文内，形成领域模型。在这个图里，限界上下文之间的边界是第二层边界，这一层边界可能就是未来微服务的边界，不同限界上下文内的领域逻辑被隔离在不同的微服务实例中运行，物理上相互隔离，所以是物理边界，边界之间用实线来表示。</li></ul><p>首先，领域可以拆分为多个子领域。一个领域相当于一个问题域，领域拆分为子域的过程就是大问题拆分为小问题的过程。</p><h2 id="二、DDD-战略设计流程"><a href="#二、DDD-战略设计流程" class="headerlink" title="二、DDD 战略设计流程"></a>二、DDD 战略设计流程</h2><p><strong>战略设计是根据用户旅程分析，找出领域对象和聚合根，对实体和值对象进行聚类组成聚合，划分限界上下文，建立领域模型的过程</strong>。</p><p>战略设计采用的方法是事件风暴，包括：产品愿景、场景分析、领域建模和微服务拆分等几个主要过程。</p><p>战略设计阶段建议参与人员：领域专家、业务需求方、产品经理、架构师、项目经理、开发经理和测试经理。</p><p>假设我们项目基本信息<strong>项目的目标</strong>是实现在线请假和考勤管理功能描述如下：</p><ul><li>  请假人填写请假单提交审批，根据请假人身份、请假类型和请假天数进行校验，根据审批规则逐级递交上级审批，逐级核批通过则完成审批，否则审批不通过退回申请人。</li><li>  根据考勤规则，核销请假数据后，对考勤数据进行校验，输出考勤统计。</li></ul><h3 id="2-1-产品愿景"><a href="#2-1-产品愿景" class="headerlink" title="2.1 产品愿景"></a>2.1 产品愿景</h3><p>产品愿景是对产品顶层价值设计，对产品目标用户、核心价值、差异化竞争点等信息达成一致，避免产品偏离方向。</p><p>事件风暴时，所有参与者针对每一个要点，在贴纸上写出自己的意见，贴到白板上。事件风暴主持者会对每个贴纸，讨论并对发散的意见进行收敛和统一，形成下面的产品愿景图。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c5172e8bed5f09f8?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="2-2-场景分析"><a href="#2-2-场景分析" class="headerlink" title="2.2 场景分析"></a>2.2 场景分析</h3><p>场景分析是从用户视角出发，探索业务领域中的典型场景，产出领域中需要支撑的场景分类、用例操作以及不同子域之间的依赖关系，用以支撑领域建模。</p><p>项目团队成员一起用事件风暴分析请假和考勤的用户旅程。根据不同角色的旅程和场景分析，尽可能全面地梳理从前端操作到后端业务逻辑发生的所有操作、命令、领域事件以及外部依赖关系等信息。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1be5705b3210533c?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="2-3-领域建模"><a href="#2-3-领域建模" class="headerlink" title="2.3 领域建模"></a>2.3 领域建模</h3><p>领域建模是通过对业务和问题域进行分析，建立领域模型。向上通过限界上下文指导微服务边界设计，向下通过聚合指导实体对象设计。</p><p>领域建模是一个收敛的过程，分三步：</p><h4 id="第一步：找出实体和值对象等领域对象"><a href="#第一步：找出实体和值对象等领域对象" class="headerlink" title="第一步：找出实体和值对象等领域对象"></a><strong>第一步：找出实体和值对象等领域对象</strong></h4><p>根据场景分析，分析并找出发起或产生这些命令或领域事件的实体和值对象，将与实体或值对象有关的命令和事件聚集到实体。</p><p>下面这个图是分析后的实体与命令的关系。通过分析，我们找到了：请假单、审批意见、审批规则、人员、组织关系、刷卡明细、考勤明细以及考勤统计等实体和值对象。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4f2bb757b11f09ca?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h4 id="第二步：定义聚合"><a href="#第二步：定义聚合" class="headerlink" title="第二步：定义聚合"></a>第二步：定义聚合</h4><p>定义聚合前，先找出聚合根。从上面的实体中，我们可以找出“请假单”和“人员”两个聚合根。然后找出与聚合根紧密依赖的实体和值对象。我们发现审批意见、审批规则和请假单紧密关联，组织关系和人员紧密关联。</p><p>刷卡明细、考勤明细和考勤统计这几个实体，它们之间相互独立，找不出聚合根，不是富领域模型，但它们一起完成考勤业务逻辑，具有很高的业务内聚性。我们将这几个业务关联紧密的实体，放在一个考勤聚合内。在微服务设计时，我们依然采用 DDD 的设计和分析方法。由于没有聚合根来管理聚合内的实体，我们可以用传统的方法来管理实体。</p><p>经过分析，我们建立了请假、人员组织关系和考勤三个聚合。其中请假聚合有请假单、审批意见实体和审批规则等值对象。人员组织关系聚合有人员和组织关系等实体。考勤聚合有刷卡明细、考勤明细和考勤统计等实体。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-03e1c6c1dcd90557?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h4 id="第三步：定义限界上下文"><a href="#第三步：定义限界上下文" class="headerlink" title="第三步：定义限界上下文"></a>第三步：定义限界上下文</h4><p>由于人员组织关系聚合与请假聚合，共同完成请假的业务功能，两者在请假的限界上下文内。考勤聚合则单独构成考勤统计限界上下文。因此我们为业务划分请假和考勤统计两个限界上下文，建立请假和考勤两个领域模型。</p><h3 id="2-4-微服务的拆分"><a href="#2-4-微服务的拆分" class="headerlink" title="2.4 微服务的拆分"></a>2.4 微服务的拆分</h3><p>理论上一个限界上下文就可以设计为一个微服务，但还需要综合考虑多种外部因素，比如：职责单一性、敏态与稳态业务分离、非功能性需求（如弹性伸缩、版本发布频率和安全等要求）、软件包大小、团队沟通效率和技术异构等非业务要素。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0c66711c35455f78?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h2 id="三、DDD的战术设计流程"><a href="#三、DDD的战术设计流程" class="headerlink" title="三、DDD的战术设计流程"></a>三、DDD的战术设计流程</h2><p>则从技术视角出发，<strong>侧重于领域模型的技术实现，完成软件开发和落地</strong>，包括：聚合根、实体、值对象、领域服务、应用服务和资源库等代码逻辑的设计和实现。</p><h3 id="3-1-分析微服务领域对象"><a href="#3-1-分析微服务领域对象" class="headerlink" title="3.1 分析微服务领域对象"></a>3.1 分析微服务领域对象</h3><p>领域模型有很多领域对象，但是这些对象带有比较重的业务属性。要完成从领域模型到微服务的落地，还需要进一步的分析和设计。在事件风暴基础上，我们进一步细化领域对象以及它们的关系，补充事件风暴可能遗漏的业务和技术细节。</p><p>我们分析微服务内应该有哪些服务？服务的分层？应用服务由哪些服务组合和编排完成？领域服务包括哪些实体和实体方法？哪个实体是聚合根？实体有哪些属性和方法？哪些对象应该设计为值对象等。</p><p><strong>服务的识别和设计</strong></p><p>事件风暴的命令是外部的一些操作和业务行为，也是微服务对外提供的能力。它往往与微服务的应用服务或者领域服务对应。我们可以将命令作为服务识别和设计的起点。具体步骤如下：</p><ul><li>  根据命令设计应用服务，确定应用服务的功能，服务集合，组合和编排方式。服务集合中的服务包括领域服务或其它微服务的应用服务。</li><li>  根据应用服务功能要求设计领域服务，定义领域服务。这里需要注意：应用服务可能是由多个聚合的领域服务组合而成的。</li><li>  根据领域服务的功能，确定领域服务内的实体以及功能。</li><li>  设计实体基本属性和方法。</li></ul><h3 id="3-2-DDD-微服务的四层架构"><a href="#3-2-DDD-微服务的四层架构" class="headerlink" title="3.2 DDD 微服务的四层架构"></a>3.2 DDD 微服务的四层架构</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9039d51b27a23a73?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h4 id="Interfaces（用户接口层）"><a href="#Interfaces（用户接口层）" class="headerlink" title="Interfaces（用户接口层）"></a><strong>Interfaces（用户接口层）</strong></h4><blockquote><p>它主要存放<strong>用户接口层与前端交互、展现数据相关的代码</strong>。前端应用通过这一层的接口，向应用服务获取展现所需的数据。这一层主要用来处理用户发送的 RESTful 请求，解析用户输入的配置文件，并将数据传递给 Application 层。数据的组装、数据传输格式以及 Facade 接口等代码都会放在这一层目录里。</p></blockquote><p>这一层主要对应kite生成的handle.go，handle会调用Application Service接口。</p><pre><code>func (s *MessageServiceImpl) PushMessage(ctx context.Context, request *message.PushMessageRequest) (*message.PushMessageResponse, error) {   return service.PushMessage(ctx, request)}</code></pre><h4 id="Application（应用层）"><a href="#Application（应用层）" class="headerlink" title="Application（应用层）"></a><strong>Application（应用层）</strong></h4><blockquote><p>它主要存放应用层<strong>服务组合和编排相关</strong>的代码。应用服务向下基于微服务内的领域服务或外部微服务的应用服务完成服务的编排和组合，向上为用户接口层提供各种应用数据展现支持服务。应用服务和事件等代码会放在这一层目录里。</p></blockquote><p>Application层应该是很薄的一层，实现服务组合和编排，适应业务流程快速变化的需求。</p><pre><code>func PushMessage(ctx context.Context, request *message.PushMessageRequest) (*message.PushMessageResponse, error) {   if request.Message == nil {      return nil, errors.New("message is nil")   } else if request.Message.ChannelID == nil {      return nil, errors.New("message channel id is nil")   }   pushMessageDO := assembler.PushMsgDTO.ToDO(request)   err := DomainService.NewPushDomainService().PushMessage(ctx, pushMessageDO)   if err != nil {         log.WithError(err).Errorf("Push.PushMessage fail")         return nil, err   }   return resp, nil}</code></pre><h4 id="Domain（领域层）"><a href="#Domain（领域层）" class="headerlink" title="Domain（领域层）"></a><strong>Domain（领域层）</strong></h4><blockquote><p>它主要存放领域层核心业务逻辑相关的代码。领域层可以包含多个聚合代码包，它们共同实现领域模型的核心业务逻辑。聚合以及聚合内的实体、方法、领域服务和事件等代码会放在这一层目录里。</p></blockquote><p>实现核心业务逻辑。这一层聚集了领域模型的聚合、聚合根、实体、值对象、领域服务和事件等领域对象，以及它们组合所形成的业务能力。</p><pre><code>type PushDomainService interface {//暴露给Application 层的 interface   PushMessage(ctx context.Context, pushMessage *entity.PushMessagesDO) error   SaveChannelLastPushMessagePosition(ctx context.Context, channelID int64, position int32) error}func NewPushDomainService() PushDomainService {   return &amp;pushDomainServiceImpl{}}type pushDomainServiceImpl struct {}func (p *pushDomainServiceImpl) PushMessage(ctx context.Context, pushMessage *entity.PushMessagesDO) error {   return nil}</code></pre><p><strong>Domain Object （充血模型）</strong></p><pre><code>type Message struct {   *MessageEntity.Message}// IsDelete 此处表明 此消息已撤回.func (m *Message) IsDeleted() bool {   if !m.IsValidated() {      return false   }   return ture}.....</code></pre><p><strong>Repository 定义，一般一个聚合对应一个Repository</strong></p><pre><code>type PushRepository interface { //定义repository接口，infra层会实现这个接口   // 保存最后一个 push message 的 position   SaveChannelLastPushMessagePosition(ctx context.Context, channelID int64, position int32) (bool, error)}func NewPushRepo() PushRepository {   return persistence.PackRepoImpl{} // 这里也可以用依赖注入思想}</code></pre><h4 id="Infrastructure（基础层）"><a href="#Infrastructure（基础层）" class="headerlink" title="Infrastructure（基础层）"></a><strong>Infrastructure（基础层）</strong></h4><blockquote><p>它主要存放基础资源服务相关的代码，为其它各层提供的通用技术能力、三方软件包、数据库服务、配置和基础资源服务的代码都会放在这一层目录里。</p></blockquote><p>基础层贯穿所有层，为各层提供基础资源服务。这一层聚集了各种底层资源相关的服务和能力。</p><pre><code>// PushRepositoryImpl 可以包含mysql、abase、cache、MQ相关的实现type PushRepositoryImpl struct{}func (p PushRepositoryImpl) SaveChannelLastPushMessagePosition(ctx context.Context, channelID int64, position int32) (bool, error) {   return abase.SaveChannelLastPushMessagePosition(ctx, channelID, position)}</code></pre><p><strong>调用关系如下图</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4741d33605fdb613?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fc6b67727aea94f7?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h2 id="四、DDD常见的一些误区"><a href="#四、DDD常见的一些误区" class="headerlink" title="四、DDD常见的一些误区"></a>四、DDD常见的一些误区</h2><h3 id="4-1-所有的领域都用-DDD"><a href="#4-1-所有的领域都用-DDD" class="headerlink" title="4.1 所有的领域都用 DDD"></a>4.1 所有的领域都用 DDD</h3><p>很多人在学会 DDD 后，可能会将其用在所有业务域，即全部使用 DDD 来设计。DDD 从战略设计到战术设计，是一个相对复杂的过程，首先企业内要培养 DDD 的文化，其次对团队成员的设计和技术能力要求相对比较高。在资源有限的情况下，应聚焦核心域，建议你先从富领域模型的核心域开始，而不必一下就在全业务域推开。</p><h3 id="4-2-全部采用-DDD-战术设计方法"><a href="#4-2-全部采用-DDD-战术设计方法" class="headerlink" title="4.2 全部采用 DDD 战术设计方法"></a>4.2 全部采用 DDD 战术设计方法</h3><p>不同的设计方法有它的适用环境，我们应选择它最擅长的场景。DDD 有很多的概念和战术设计方法，比如聚合根和值对象等。聚合根利用仓储管理聚合内实体数据之间的一致性，这种方法对于管理新建和修改数据非常有效，比如在修改订单数据时，它可以保证订单总金额与所有商品明细金额的一致，但它并不擅长较大数据量的查询处理，甚至有延迟加载进而影响效率的问题。</p><p>而传统的设计方法，可能一条简单的 SQL 语句就可以很快地解决问题。而很多贫领域模型的业务，比如数据统计和分析，DDD 很多方法可能都用不上，或用得并不顺手，而传统的方法很容易就解决了。</p><p>因此，在遵守领域边界和微服务分层等大原则下，在进行战术层面设计时，我们应该选择最适合的方法，不只是 DDD 设计方法，当然还应该包括传统的设计方法。这里要以快速、高效解决实际问题为最佳，不要为做 DDD 而做 DDD。</p><h3 id="4-3-重战术设计而轻战略设计"><a href="#4-3-重战术设计而轻战略设计" class="headerlink" title="4.3 重战术设计而轻战略设计"></a>4.3 重战术设计而轻战略设计</h3><p>很多 DDD 初学者，学习 DDD 的主要目的，可能是为了开发微服务，因此更看重 DDD 的战术设计实现。殊不知 DDD 是一种从领域建模到微服务落地的全方位的解决方案。</p><p>战略设计时构建的领域模型，是微服务设计和开发的输入，它确定了微服务的边界、聚合、代码对象以及服务等关键领域对象。领域模型边界划分得清不清晰，领域对象定义得明不明确，会决定微服务的设计和开发质量。没有领域模型的输入，基于 DDD 的微服务的设计和开发将无从谈起。因此我们不仅要重视战术设计，更要重视战略设计。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://docs.microsoft.com/zh-cn/azure/architecture/patterns/cqrs">https://docs.microsoft.com/zh-cn/azure/architecture/patterns/cqrs</a></p><p><a href="https://www.infoq.cn/article/s_LFUlU6ZQODd030RbH9">https://www.infoq.cn/article/s_LFUlU6ZQODd030RbH9</a></p><p><a href="https://time.geekbang.org/column/article/169600">https://time.geekbang.org/column/article/169600</a></p><p><a href="https://mp.weixin.qq.com/s/Wt2Fssm8kd8b8evVD9aujA">https://mp.weixin.qq.com/s/Wt2Fssm8kd8b8evVD9aujA</a></p><p><a href="https://insights.thoughtworks.cn/path-to-ddd/?hmsr=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io">https://insights.thoughtworks.cn/path-to-ddd/?hmsr=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io</a></p><p><a href="https://blog.csdn.net/fly910905/article/details/104145292">https://blog.csdn.net/fly910905/article/details/104145292</a></p>]]></content>
      
      
      <categories>
          
          <category> Architecture </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《数据密集型应用系统设计》</title>
      <link href="/2020/12/07/note/data_intensive/"/>
      <url>/2020/12/07/note/data_intensive/</url>
      
        <content type="html"><![CDATA[<p>数据密集型应用（<code>data-intensive applications</code>）正在通过使用这些技术进步来推动可能性的 边界。一个应用被称为数据密集型的，如果数据是其主要挑战（数据量，数据复杂度或数据变化速度）—— 与之相对的是计算密集型，即处理器速度是其瓶颈。</p><h2 id="数据系统的基石"><a href="#数据系统的基石" class="headerlink" title="数据系统的基石"></a>数据系统的基石</h2><h3 id="可靠性、可扩展性、可维护性"><a href="#可靠性、可扩展性、可维护性" class="headerlink" title="可靠性、可扩展性、可维护性"></a>可靠性、可扩展性、可维护性</h3><p>现今很多应用程序都是数据密集型（data-intensive）的，而非计算密集型（compute-intensive）的。因此CPU很少成为这类应用的瓶颈，更大的问题通常来自数据量、数据复杂性、以及数据的变更速度。</p><h4 id="可靠性（Reliability）"><a href="#可靠性（Reliability）" class="headerlink" title="可靠性（Reliability）"></a>可靠性（Reliability）</h4><p>系统在困境（adversity）（硬件故障、软件故障、人为错误）中仍可正常工作（正确完成功 能，并能达到期望的性能水准）。</p><p>人们对于一个东西是否可靠，都有一个直观的想法。人们对可靠软件的典型期望包括：</p><ul><li>应用程序表现出用户所期望的功能。 </li><li>允许用户犯错，允许用户以出乎意料的方式使用软件。</li><li>在预期的负载和数据量下，性能满足要求。 </li><li>系统能防止未经授权的访问和滥用。</li></ul><p>造成错误的原因叫做 <strong>故障（fault）</strong>  ，能预料并应对故障的系统特性可称为 <strong>容错（fault- tolerant）</strong> 或<strong>韧性（resilient）</strong>  。“容错”一词可能会产生误导，因为它暗示着系统可以容忍所 有可能的错误，但在实际中这是不可能的。比方说，如果整个地球（及其上的所有服务器） 都被黑洞吞噬了，想要容忍这种错误，需要把网络托管到太空中——这种预算能不能批准就 祝你好运了。所以在讨论容错时，只有谈论特定类型的错误才有意义。</p><p>注意 <strong>故障（fault）</strong> 不同于 <strong>失效（failure）</strong>【2】。故障通常定义为系统的一部分状态偏离其 标准，而失效则是系统作为一个整体停止向用户提供服务。故障的概率不可能降到零，因此 最好设计容错机制以防因故障而导致失效。本书中我们将介绍几种用不可靠的部件构建可靠系统的技术。</p><h5 id="硬件故障"><a href="#硬件故障" class="headerlink" title="硬件故障"></a>硬件故障</h5><p>当想到系统失效的原因时， <strong>硬件故障（hardware faults）</strong> 总会第一个进入脑海。硬盘崩溃、 内存出错、机房断电、有人拔错网线……任何与大型数据中心打过交道的人都会告诉你：一旦你拥有很多机器，这些事情总会发生！</p><p>据报道称， <strong>硬盘的平均无故障时间（MTTF, mean time to failure）</strong> 约为10到50年【5】 【6】。因此从数学期望上讲，在拥有10000个磁盘的存储集群上，平均每天会有1个磁盘出故障。</p><p>为了减少系统的故障率，第一反应通常都是增加单个硬件的<strong>冗余度</strong>，例如：磁盘可以组建 RAID，服务器可能有双路电源和热插拔CPU，数据中心可能有电池和柴油发电机作为后备电 源，某个组件挂掉时冗余组件可以立刻接管。这种方法虽然不能完全防止由硬件问题导致的 系统失效，但它简单易懂，通常也足以让机器不间断运行很多年。 </p><p>直到最近，硬件冗余对于大多数应用来说已经足够了，它使单台机器完全失效变得相当罕 见。只要你能快速地把备份恢复到新机器上，故障停机时间对大多数应用而言都算不上灾难 性的。只有少量高可用性至关重要的应用才会要求有多套硬件冗余。 </p><p>但是随着数据量和应用计算需求的增加，越来越多的应用开始大量使用机器，这会相应地增 加硬件故障率。此外在一些云平台（如亚马逊网络服务（AWS, Amazon Web Services）） 中，虚拟机实例不可用却没有任何警告也是很常见的【7】，因为云平台的设计就是优先考虑<strong>灵活性（flexibility）和弹性（elasticity）</strong>，而不是单机可靠性。 </p><p>如果在硬件冗余的基础上进一步引入软件容错机制，那么系统在容忍整个（单台）机器故障 的道路上就更进一步了。这样的系统也有运维上的便利，例如：如果需要重启机器（例如应 用操作系统安全补丁），单服务器系统就需要计划停机。而允许机器失效的系统则可以一次 修复一个节点，无需整个系统停机。</p><h5 id="软件故障"><a href="#软件故障" class="headerlink" title="软件故障"></a>软件故障</h5><p>我们通常认为硬件故障是随机的、相互独立的：一台机器的磁盘失效并不意味着另一台机器 的磁盘也会失效。大量硬件组件不可能同时发生故障，除非它们存在比较弱的相关性（同样 的原因导致关联性错误，例如服务器机架的温度）。</p><p>另一类错误是内部的系统性错误（systematic error）【7】。这类错误难以预料，而且因为 是跨节点相关的，所以比起不相关的硬件故障往往可能造成更多的系统失效【5】。</p><p>虽然软件中的系统性故障没有速效药，但我们还是有很多小办法，例如：仔细考虑系统中的 假设和交互；彻底的测试；进程隔离；允许进程崩溃并重启；测量、监控并分析生产环境中 的系统行为。如果系统能够提供一些保证（例如在一个消息队列中，进入与发出的消息数量 相等），那么系统就可以在运行时不断自检，并在出现差异（discrepancy）时报警、</p><h5 id="人为错误"><a href="#人为错误" class="headerlink" title="人为错误"></a>人为错误</h5><p>设计并构建了软件系统的工程师是人类，维持系统运行的运维也是人类。即使他们怀有最大 的善意，人类也是不可靠的。举个例子，一项关于大型互联网服务的研究发现，运维配置错 误是导致服务中断的首要原因，而硬件故障（服务器或网络）仅导致了10-25％的服务中断</p><h4 id="可扩展性（Scalability）"><a href="#可扩展性（Scalability）" class="headerlink" title="可扩展性（Scalability）"></a>可扩展性（Scalability）</h4><p>有合理的办法应对系统的增长（数据量、流量、复杂性）（参阅“可扩展性”）</p><p>系统今天能可靠运行，并不意味未来也能可靠运行。服务降级（degradation）的一个常见 原因是负载增加，例如：系统负载已经从一万个并发用户增长到十万个并发用户，或者从一 百万增长到一千万。也许现在处理的数据量级要比过去大得多。 </p><p>可扩展性（Scalability）是用来<strong>描述系统应对负载增长能力的术语</strong>。但是请注意，这不是贴 在系统上的一维标签：说“X可扩展”或“Y不可扩展”是没有任何意义的。相反，讨论可扩展性意 味着考虑诸如“如果系统以特定方式增长，有什么选项可以应对增长？”和“如何增加计算资源 来处理额外的负载？”等问题。</p><h5 id="描述负载"><a href="#描述负载" class="headerlink" title="描述负载"></a>描述负载</h5><p>首先要能简要描述系统的当前负载。负 载可以用一些称为负载参数（load parameters）的数字来描述。参数的最佳选择取决于系统 架构，它可能是每秒向Web服务器发出的请求、数据库中的读写比率、聊天室中同时活跃的 用户数量、缓存命中率或其他东西。除此之外，也许平均情况对你很重要，也许你的瓶颈是 少数极端场景。</p><p><strong>扇出（Fan Out）</strong>：从电子工程学中借用的术语，它描述了输入连接到另一个门输出的逻辑门数量。 输出需要提供足够的电流来驱动所有连接的输入。在事务处理系统中，我们使用它来描述为了服务一个传入请求而需要执行其他服务的请求数量。</p><h5 id="推特实现方案一（读扩散）"><a href="#推特实现方案一（读扩散）" class="headerlink" title="推特实现方案一（读扩散）"></a>推特实现方案一（读扩散）</h5><p>好处：写入简单，坏处：查询复杂</p><p>发布推文时，只需将新推文插入全局推文集合即可。当一个用户请求自己的主页时间线时，首先查找他关注的所有人，查询这些被关注用户发布的推文并按时间顺序合并。在如图1-2所示的关系型数据库中，可以编写这样的查询：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6b76319c0e2779a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h5 id="推特实现方案二（写扩散）"><a href="#推特实现方案二（写扩散）" class="headerlink" title="推特实现方案二（写扩散）"></a>推特实现方案二（写扩散）</h5><p>为每个用户的主页时间线维护一个缓存，就像每个用户的推文收件箱（图1-3）。 当一个用户发布推文时，查找所有关注该用户的人，并将新的推文插入到每个主页时间线缓存中。 因此读取主页时间线的请求开销很小，因为结果已经提前计算好了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-311bb4461e949b67.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h5 id="推特实现方案方案三（推拉结合）"><a href="#推特实现方案方案三（推拉结合）" class="headerlink" title="推特实现方案方案三（推拉结合）"></a>推特实现方案方案三（推拉结合）</h5><p>推特轶事的最终转折：现在已经稳健地实现了方法2，推特逐步转向了两种方法的混合。大多数用户发的推文会被<strong>扇出</strong>写入其粉丝主页时间线缓存中。但是少数拥有海量粉丝的用户（即名流）会被排除在外。当用户读取主页时间线时，分别地获取出该用户所关注的每位名流的推文，再与用户的主页时间线缓存合并，如方法1所示。这种混合方法能始终如一地提供良好性能。</p><h5 id="描述性能"><a href="#描述性能" class="headerlink" title="描述性能"></a>描述性能</h5><p>一旦系统的负载被描述好，就可以研究当负载增加会发生什么。我们可以从两种角度来看： </p><ul><li>增加负载参数并保持系统资源（CPU、内存、网络带宽等）不变时，系统性能将受到什 么影响？</li><li>增加负载参数并希望保持性能不变时，需要增加多少系统资源？</li></ul><p>对于Hadoop这样的批处理系统，通常关心的是<strong>吞吐量（throughput）</strong>，即每秒可以处理的 记录数量，或者在特定规模数据集上运行作业的总时间iii。对于在线系统，通常更重要的是<strong>服务的响应时间（response time）</strong>，即客户端发送请求到接收响应之间的时间。</p><p> 理想情况下，批量作业的运行时间是数据集的大小除以吞吐量。 在实践中由于<strong>数据倾斜（数据不是均匀分布在每个工作进程中）</strong>，需要等待最慢的任务完成，所以运行时间往往更长。</p><h5 id="延迟和响应时间"><a href="#延迟和响应时间" class="headerlink" title="延迟和响应时间"></a>延迟和响应时间</h5><p>延迟（latency）和响应时间（response time）经常用作同义词，但实际上它们并不一 样。响应时间是客户所看到的，除了实际处理请求的时间（服务时间（service time））之外，还包括网络延迟和排队延迟。延迟是某个请求等待处理的持续时长，在此期间它 处于休眠（latent）状态，并等待服务【17】。</p><h5 id="应对负载的方法"><a href="#应对负载的方法" class="headerlink" title="应对负载的方法"></a>应对负载的方法</h5><p>人们经常讨论纵向扩展（scaling up）（垂直扩展（vertical scaling），转向更强大的机器）和横向扩展（scaling out）（水平扩展（horizontal scaling），将负载分布到多台小机 器上）之间的对立。跨多台机器分配负载也称为“无共享（shared-nothing）”架构。可以在单 台机器上运行的系统通常更简单，但高端机器可能非常贵，所以非常密集的负载通常无法避 免地需要横向扩展。现实世界中的优秀架构需要将这两种方法务实地结合，因为使用几台足够强大的机器可能比使用大量的小型虚拟机更简单也更便宜。</p><p>有些系统<strong>是弹性（elastic）的</strong>，这意味着可以在检测到负载增加时自动增加计算资源，而其 他系统则是手动扩展（人工分析容量并决定向系统添加更多的机器）。如果负载极难预测（highly unpredictable），则弹性系统可能很有用，但手动扩展系统更简单，并且意外操作 可能会更少（参阅“重新平衡分区”）。</p><p>大规模的系统架构通常是应用特定的—— 没有一招鲜吃遍天的通用可扩展架构（不正式的叫法：<strong>万金油（magic scaling sauce）</strong> ）。应用的问题可能是读取量、写入量、要存储的数据量、数据的复杂度、响应时间要求、访问模式或者所有问题的大杂烩。</p><p>一个良好适配应用的可扩展架构，<strong>是围绕着假设（assumption）建立的</strong>：哪些操作是常见 的？哪些操作是罕见的？这就是所谓负载参数。如果假设最终是错误的，那么为扩展所做的工程投入就白费了，最糟糕的是适得其反。在早期创业公司或非正式产品中，通常支持产品 快速迭代的能力，要比可扩展至未来的假想负载要重要的多。</p><h4 id="可维护性（Maintainability）"><a href="#可维护性（Maintainability）" class="headerlink" title="可维护性（Maintainability）"></a>可维护性（Maintainability）</h4><p>许多不同的人（工程师、运维）在不同的生命周期，都能在高效地在系统上工作（使系统保 持现有行为，并适应新的应用场景）。（参阅”可维护性“）</p><p>众所周知，软件的大部分开销并不在最初的开发阶段，而是在持续的维护阶段，包括修复漏洞、保持系统正常运行、调查失效、适配新的平台、为新的场景进行修改、偿还技术债、添 加新的功能等等。</p><p>不幸的是，许多从事软件系统行业的人不喜欢维护所谓的<strong>遗留（legacy）系统</strong>，——也许因 为涉及修复其他人的错误、和过时的平台打交道，或者系统被迫使用于一些份外工作。每一 个遗留系统都以自己的方式让人不爽，所以很难给出一个通用的建议来和它们打交道。</p><p>但是我们可以，也应该以这样一种方式来设计软件：在设计之初就尽量考虑尽可能减少维护 期间的痛苦，从而避免自己的软件系统变成遗留系统。为此，我们将特别关注软件系统的三个设计原则：</p><ul><li><strong>可操作性（Operability）</strong>:便于运维团队保持系统平稳运行。</li><li><strong>简单性（Simplicity）</strong>:从系统中消除尽可能多的<strong>复杂度（complexity）</strong>，使新工程师也能轻松理解系统。（注意这 和用户接口的简单性不一样。）</li><li>可演化性（evolability）:使工程师在未来能轻松地对系统进行更改，当需求变化时为新应用场景做适配。也称为<strong>可扩 展性（extensibility）</strong>，<strong>可修改性（modifiability</strong>）或<strong>可塑性（plasticity）</strong></li></ul><p>复杂度（complexity）有各种可能的症状，例如：状态空间激增、模块间紧密耦合、纠结的 依赖关系、不一致的命名和术语、解决性能问题的Hack、需要绕开的特例等等，现在已经有 很多关于这个话题的讨论【31,32,33】。</p><p>简化系统并不一定意味着减少功能；它也可以意味着<strong>消除额外的（accidental）的复杂度</strong>。 Moseley和Marks【32】把额外复杂度定义为：由具体实现中涌现，而非（从用户视角看，系 统所解决的）问题本身固有的复杂度。</p><p>用于消除额外复杂度的最好工具之一是<strong>抽象（abstraction）</strong>。一个好的抽象可以将大量实现细节隐藏在一个干净，简单易懂的外观下面。一个好的抽象也可以广泛用于各类不同应用。 比起重复造很多轮子，重用抽象不仅更有效率，而且有助于开发高质量的软件。抽象组件的 质量改进将使所有使用它的应用受益。</p><p>例如，高级编程语言是一种抽象，隐藏了机器码、CPU寄存器和系统调用。 SQL也是一种抽象，隐藏了复杂的磁盘/内存数据结构、来自其他客户端的并发请求、崩溃后的不一致性。当然在用高级语言编程时，我们仍然用到了机器码；只不过没有直接（directly）使用罢了，正是因为编程语言的抽象，我们才不必去考虑这些实现细节。</p><p>抽象可以帮助我们将系统的复杂度控制在可管理的水平，不过，找到好的抽象是非常困难 的。在分布式系统领域虽然有许多好的算法，但我们并不清楚它们应该打包成什么样抽象。</p><p>在组织流程方面，敏捷（agile）工作模式为适应变化提供了一个框架。敏捷社区还开发了对 在频繁变化的环境中开发软件很有帮助的技术工具和模式，如测试驱动开发（TDD, test- driven development）和重构（refactoring）。</p><h3 id="数据模型与查询语言"><a href="#数据模型与查询语言" class="headerlink" title="数据模型与查询语言"></a>数据模型与查询语言</h3><h4 id="关系模型与文档模"><a href="#关系模型与文档模" class="headerlink" title="关系模型与文档模"></a>关系模型与文档模</h4><p>关系模型曾是一个理论性的提议，当时很多人都怀疑是否能够有效实现它。然而到了20世纪80年代中期，关系数据库管理系统（RDBMSes）和SQL已成为大多数人们存储和查询某些常规结构的数据的首选工具。关系数据库已经持续称霸了大约25~30年——这对计算机史来说是极其漫长的时间。</p><h4 id="NoSQL的诞生"><a href="#NoSQL的诞生" class="headerlink" title="NoSQL的诞生"></a>NoSQL的诞生</h4><p>现在 - 2010年代，NoSQL开始了最新一轮尝试，试图推翻关系模型的统治地位。“NoSQL”这 个名字让人遗憾，因为实际上它并没有涉及到任何特定的技术。最初它只是作为一个醒目的 Twitter标签，用在2009年一个关于分布式，非关系数据库上的开源聚会上。无论如何，这个 术语触动了某些神经，并迅速在网络创业社区内外传播开来。好些有趣的数据库系统现在都 与#NoSQL#标签相关联，并且NoSQL被追溯性地重新解释为不仅是SQL（Not Only SQL） 【4】。</p><p>采用NoSQL数据库的背后有几个驱动因素，其中包括：</p><ul><li>需要比关系数据库更好的可扩展性，包括非常大的数据集或非常高的写入吞吐量</li><li>相比商业数据库产品，免费和开源软件更受偏爱。 </li><li>关系模型不能很好地支持一些特殊的查询操作</li><li>受挫于关系模型的限制性，渴望一种更具多动态性与表现力的数据模型【5】</li></ul><p>不同的应用程序有不同的需求，一个用例的最佳技术选择可能不同于另一个用例的最佳技术选择。因此，在可预见的未来，关系数据库似乎可能会继续与各种非关系数据库一起使用-这种想法有时也被称为混合持久化（polyglot persistence）</p><h4 id="对象关系不匹配"><a href="#对象关系不匹配" class="headerlink" title="对象关系不匹配"></a>对象关系不匹配</h4><p>目前大多数应用程序开发都使用面向对象的编程语言来开发，这导致了对SQL数据模型的普 遍批评：如果数据存储在关系表中，那么需要一个笨拙的转换层，处于应用程序代码中的对 象和表，行，列的数据库模型之间。模型之间的不连贯有时被称为阻抗不匹配（impedance mismatch）i。</p><p>像ActiveRecord和Hibernate这样的对象关系映射（object-relational mapping, ORM）框架 可以减少这个转换层所需的样板代码的数量，但是它们不能完全隐藏这两个模型之间的差异。</p><h4 id="多对一和多对多的关系"><a href="#多对一和多对多的关系" class="headerlink" title="多对一和多对多的关系"></a>多对一和多对多的关系</h4><p>使用ID的好处是，ID对人类没有任何意义，因而永远不需要改变：ID可以保持不变，即使它 标识的信息发生变化。任何对人类有意义的东西都可能需要在将来某个时候改变——如果这 些信息被复制，所有的冗余副本都需要更新。这会导致写入开销，也存在不一致的风险（一些副本被更新了，还有些副本没有被更新）。去除此类重复是数据库规范化 （normalization）的关键思想。</p><h4 id="文档数据库是否在重蹈覆辙？"><a href="#文档数据库是否在重蹈覆辙？" class="headerlink" title="文档数据库是否在重蹈覆辙？"></a>文档数据库是否在重蹈覆辙？</h4><p>在多对多的关系和连接已常规用在关系数据库时，文档数据库和NoSQL重启了辩论：如何最好地在数据库中表示多对多关系。那场辩论可比NoSQL古老得多，事实上，最早可以追溯到 计算机化数据库系统。</p><p>20世纪70年代最受欢迎的业务数据处理数据库是IBM的信息管理系统（IMS），最初是为了阿 波罗太空计划的库存管理而开发的，并于1968年有了首次商业发布【13】。目前它仍在使用 和维护，运行在IBM大型机的OS/390上【14】。</p><p>IMS的设计中使用了一个相当简单的数据模型，称为层次模型（hierarchical model），它与文档数据库使用的JSON模型有一些惊人的相似之处【2】。它将所有数据表示为嵌套在记录中的记录树，这很像图2-2的JSON结构。</p><p>同文档数据库一样，IMS能良好处理一对多的关系，但是很难应对多对多的关系，并且不支持连接。开发人员必须决定是否复制（非规范化）数据或手动解决从一个记录到另一个记录的 引用。这些二十世纪六七十年代的问题与现在开发人员遇到的文档数据库问题非常相似 【15】。</p><p>那时人们提出了各种不同的解决方案来解决层次模型的局限性。其中最突出的两个是关系模型（relational model）（它变成了SQL，统治了世界）和网络模型（network model）（最初很受关注，但最终变得冷门）。这两个阵营之间的“大辩论”在70年代持续了很久时间 【2】。</p><h4 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h4><p>网络模型中记录之间的链接不是外键，而更像编程语言中的指针（同时仍然存储在磁盘 上）。访问记录的唯一方法是跟随从根记录起沿这些链路所形成的路径。这被称为访问路径 （access path）。</p><p>最简单的情况下，访问路径类似遍历链表：从列表头开始，每次查看一条记录，直到找到所 需的记录。但在多对多关系的情况中，数条不同的路径可以到达相同的记录，网络模型的程 序员必须跟踪这些不同的访问路径。</p><p>尽管手动选择访问路径够能最有效地利用20世纪70年代非常有限的硬件功能（如磁带驱动器，其搜索速度非常慢），但这使得查询和更新数据库的代码变得复杂不灵活。无论是分层还是网络模型，如果你没有所需数据的路径，就会陷入困境。你可以改变访问路径，但是必须浏览大量手写数据库查询代码，并重写来处理新的访问路径。更改应用程序的数据模型是很难的。</p><h4 id="关系模型"><a href="#关系模型" class="headerlink" title="关系模型"></a>关系模型</h4><p>相比之下，关系模型做的就是将所有的数据放在光天化日之下：一个关系（表）只是一个元 组（行）的集合，仅此而已。如果你想读取数据，它没有迷宫似的嵌套结构，也没有复杂的 访问路径。你可以选中符合任意条件的行，读取表中的任何或所有行。你可以通过指定某些 列作为匹配关键字来读取特定行。你可以在任何表中插入一个新的行，而不必担心与其他表的外键关系iv。</p><p>关系数据库的查询优化器是复杂的，已耗费了多年的研究和开发精力【18】。关系模型的一 个关键洞察是：只需构建一次查询优化器，随后使用该数据库的所有应用程序都可以从中受 益。如果你没有查询优化器的话，那么为特定查询手动编写访问路径比编写通用优化器更容 易——不过从长期看通用解决方案更好。</p><h4 id="文档模型中的架构灵活性"><a href="#文档模型中的架构灵活性" class="headerlink" title="文档模型中的架构灵活性"></a>文档模型中的架构灵活性</h4><p>大多数文档数据库以及关系数据库中的JSON支持都不会强制文档中的数据采用何种模式。关 系数据库的XML支持通常带有可选的模式验证。没有模式意味着可以将任意的键和值添加到 文档中，并且当读取时，客户端对无法保证文档可能包含的字段。</p><p>文档数据库有时称为无模式（schemaless），但这具有误导性，因为读取数据的代码通常假 定某种结构——即存在隐式模式，但不由数据库强制执行【20】。一个更精确的术语是读时 模式（schema-on-read）（数据的结构是隐含的，只有在数据被读取时才被解释），相应的 是写时模式（schema-on-write）（传统的关系数据库方法中，模式明确，且数据库确保所 有的数据都符合其模式）【21】。</p><h4 id="查询的数据局部性"><a href="#查询的数据局部性" class="headerlink" title="查询的数据局部性"></a>查询的数据局部性</h4><p>文档通常以单个连续字符串形式进行存储，编码为JSON，XML或其二进制变体（如 MongoDB的BSON）。如果应用程序经常需要访问整个文档（例如，将其渲染至网页），那 么存储局部性会带来性能优势。如果将数据分割到多个表中（如图2-1所示），则需要进行多次索引查找才能将其全部检索出来，这可能需要更多的磁盘查找并花费更多的时间。</p><h4 id="文档和关系数据库的融合"><a href="#文档和关系数据库的融合" class="headerlink" title="文档和关系数据库的融合"></a>文档和关系数据库的融合</h4><p>自2000年代中期以来，大多数关系数据库系统（MySQL除外）都已支持XML。这包括对XML 文档进行本地修改的功能，以及在XML文档中进行索引和查询的功能。这允许应用程序使用 那种与文档数据库应当使用的非常类似的数据模型。</p><p>从9.3版本开始的PostgreSQL 【8】，从5.7版本开始的MySQL以及从版本10.5开始的IBM DB2 [30]也对JSON文档提供了类似的支持级别。鉴于用在Web APIs的JSON流行趋势，其他关系数据库很可能会跟随他们的脚步并添加JSON支持。</p><h4 id="图数据模型"><a href="#图数据模型" class="headerlink" title="图数据模型"></a>图数据模型</h4><p>一个图由两种对象组成：顶点（vertices）（也称为节点（nodes） 或实体（entities））， 和边（edges）（ 也称为关系（relationships）或弧 （arcs） ）。多种数据可以被建模为 一个图形。典型的例子包括：社交图谱</p><h3 id="存储与检索"><a href="#存储与检索" class="headerlink" title="存储与检索"></a>存储与检索</h3><p>一个数据库在最基础的层次上需要完成两件事情：当你把数据交给数据库时，它应当把数据存储起来；而后当你向数据库要数据时，它应当把数据返回给你。</p><h4 id="驱动数据库的数据结构"><a href="#驱动数据库的数据结构" class="headerlink" title="驱动数据库的数据结构"></a>驱动数据库的数据结构</h4><p>为了高效查找数据库中特定键的值，我们需要一个数据结构：索引（index）。本章将介绍一系列的索引结构，并它们进行对比。索引背后的大致思想是，保存一些额外的元数据作为路标，帮助你找到想要的数据。如果您想在同一份数据中以几种不同的方式进行搜索，那么你也许需要不同的索引，建在数据的不同部分上。</p><p>索引是从主数据衍生的附加（additional）结构。许多数据库允许添加与删除索引，这不会影响数据的内容，它只影响查询的性能。维护额外的结构会产生开销，特别是在写入时。写入性能很难超过简单地追加写入文件，因为追加写入是最简单的写入操作。任何类型的索引通 常都会减慢写入速度，因为每次写入数据时都需要更新索引。</p><p>这是存储系统中一个重要的权衡：精心选择的索引加快了读查询的速度，但是每个索引都会拖慢写入速度。因为这个原因，数据库默认并不会索引所有的内容，而需要你（程序员或 DBA）通过对应用查询模式的了解来手动选择索引。你可以选择能为应用带来最大收益，同时又不会引入超出必要开销的索引。</p><h4 id="哈希索引"><a href="#哈希索引" class="headerlink" title="哈希索引"></a>哈希索引</h4><p>假设我们的数据存储只是一个追加写入的文件，就像前面的例子一样。那么最简单的索引策 略就是：保留一个内存中的哈希映射，其中每个键都映射到一个数据文件中的字节偏移量， 指明了可以找到对应值的位置，如图3-1所示。当你将新的键值对追加写入文件中时，还要更 新散列映射，以反映刚刚写入的数据的偏移量（这同时适用于插入新键与更新现有键）。当 你想查找一个值时，使用哈希映射来查找数据文件中的偏移量，寻找（seek）该位置并读取 该值。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2767341e52e4b1e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>听上去简单，但这是一个可行的方法。现实中，Bitcask实际上就是这么做的（Riak中默认的 存储引擎）【3】。 Bitcask提供高性能的读取和写入操作，但所有键必须能放入可用内存 中，因为哈希映射完全保留在内存中。这些值可以使用比可用内存更多的空间，因为可以从 磁盘上通过一次 seek 加载所需部分，如果数据文件的那部分已经在文件系统缓存中，则读取 根本不需要任何磁盘I/O</p><p>像Bitcask这样的存储引擎非常适合每个键的值经常更新的情况。例如，键可能是视频的URL，值可能是它播放的次数（每次有人点击播放按钮时递增）。在这种类型的工作负载中，有很多写操作，<strong>但是没有太多不同的键</strong>——每个键有很多的写操作，但是将所有键保存 在内存中是可行的。</p><p>直到现在，我们只是追加写入一个文件 —— 所以如何避免最终用完磁盘空间？一种好的解决方案是，将日志分为特定大小的段，当日志增长到特定尺寸时关闭当前段文件，并开始写入 一个新的段文件。然后，我们就可以对这些段进行压缩（compaction），如图3-2所示。压缩意味着在日志中丢弃重复的键，只保留每个键的最近更新。</p><p>每个段现在都有自己的内存散列表，将键映射到文件偏移量。为了找到一个键的值，我们首先检查最近段的哈希映射;如果键不存在，我们检查第二个最近的段，依此类推。合并过程保 持细分的数量，所以查找不需要检查许多哈希映射。 大量的细节进入实践这个简单的想法工作。简而言之，一些真正实施中重要的问题是：</p><ul><li>文件格式，CSV不是日志的最佳格式。使用二进制格式更快，更简单，首先以字节为单位对字符串的长度进行编码，然后使用原始字符串（不需要转义）。</li><li>删除记录，如果要删除一个键及其关联的值，则必须在数据文件（有时称为逻辑删除）中附加一个特殊的删除记录。当日志段被合并时，逻辑删除告诉合并过程放弃删除键的任何以前的值。</li><li>崩溃恢复，如果数据库重新启动，则内存散列映射将丢失。原则上，您可以通过从头到尾读取整个段文件并在每次按键时注意每个键的最近值的偏移量来恢复每个段的哈希映射。但是，如果段文件很大，这可能需要很长时间，这将使服务器重新启动痛苦。 Bitcask通过存储加速恢复磁盘上每个段的哈希映射的快照，可以更快地加载到内存中。</li><li>部分写入记录，数据库可能随时崩溃，包括将记录附加到日志中途。 Bitcask文件包含校验和，允许检测和忽略日志的这些损坏部分。</li><li>并发控制，由于写操作是以严格顺序的顺序附加到日志中的，所以常见的实现选择是只有一个写入器线程。数据文件段是附加的，否则是不可变的，所以它们可以被多个线程同时读取。</li></ul><p>乍一看，只有追加日志看起来很浪费：为什么不更新文件，用新值覆盖旧值？但是只能追加设计的原因有几个：</p><ul><li>追加和分段合并是顺序写入操作，通常比随机写入快得多，尤其是在磁盘旋转硬盘上。 在某种程度上，顺序写入在基于闪存的固态硬盘（SSD）上也是优选的【4】。我们将在 第83页的“比较B-树和LSM-树”中进一步讨论这个问题。</li><li>如果段文件是附加的或不可变的，并发和崩溃恢复就简单多了。例如，您不必担心在覆盖值时发生崩溃的情况，而将包含旧值和新值的一部分的文件保留在一起。</li><li>合并旧段可以避免数据文件随着时间的推移而分散的问题。</li></ul><p>但是，哈希表索引也有局限性： </p><ul><li>散列表必须能放进内存，如果你有非常多的键，那真是倒霉。原则上可以在磁盘上保留一个哈希映射，不幸的是 磁盘哈希映射很难表现优秀。它需要大量的随机访问I/O，当它变满时增长是很昂贵的， 并且散列冲突需要很多的逻辑【5】。</li><li>范围查询效率不高。例如，您无法轻松扫描kitty00000和kitty99999之间的所有键——您 必须在散列映射中单独查找每个键。</li></ul><h4 id="SSTables和LSM树"><a href="#SSTables和LSM树" class="headerlink" title="SSTables和LSM树"></a>SSTables和LSM树</h4><p>现在我们可以对段文件的格式做一个简单的改变：我们要求键值对的序列按键排序。乍一 看，这个要求似乎打破了我们使用顺序写入的能力，但是我们马上就会明白这一点。</p><p>我们把这个格式称为排序字符串表（Sorted String Table），简称SSTable。我们还要求每个 键只在每个合并的段文件中出现一次（压缩过程已经保证）。与使用散列索引的日志段相 比，SSTable有几个很大的优势：</p><ol><li>合并段是简单而高效的，即使文件大于可用内存。这种方法就像归并排序算法中使用的 方法一样，如图3-4所示：您开始并排读取输入文件，查看每个文件中的第一个键，复制 最低键（根据排序顺序）到输出文件，并重复。这产生一个新的合并段文件，也按键排序。<br> <img src="https://upload-images.jianshu.io/upload_images/12321605-8b58adfcd17ba9bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li><li>为了在文件中找到一个特定的键，你不再需要保存内存中所有键的索引。以图3-5为例： 假设你正在内存中寻找键 handiwork ，但是你不知道段文件中该关键字的确切偏移量。 然而，你知道 handbag 和 handsome 的偏移，而且由于排序特性，你知道 handiwork 必须出现在这两者之间。这意味着您可以跳到 handbag 的偏移位置并从那里扫描，直到 您找到 handiwork （或没找到，如果该文件中没有该键）。<br> <img src="https://upload-images.jianshu.io/upload_images/12321605-d3a9a8a656514beb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>您仍然需要一个内存中索引来告诉您一些键的偏移量，但它可能很稀疏：每几千字节的 段文件就有一个键就足够了，因为几千字节可以很快被扫描 。</li><li>由于读取请求无论如何都需要扫描所请求范围内的多个键值对，因此可以将这些记录分 组到块中，并在将其写入磁盘之前对其进行压缩（如图3-5中的阴影区域所示） 。稀疏内 存中索引的每个条目都指向压缩块的开始处。除了节省磁盘空间之外，压缩还可以减少 IO带宽的使用。</li></ol><h4 id="构建和维护SSTables"><a href="#构建和维护SSTables" class="headerlink" title="构建和维护SSTables"></a>构建和维护SSTables</h4><p>到目前为止，但是如何让你的数据首先被按键排序呢？我们的传入写入可以以任何顺序发生。</p><p>在磁盘上维护有序结构是可能的（参阅“B树”），但在内存保存则要容易得多。有许多可以使用的众所周知的树形数据结构，例如红黑树或AVL树【2】。使用这些数据结构，您可以按任 何顺序插入键，并按排序顺序读取它们。</p><p>现在我们可以使我们的存储引擎工作如下：</p><ul><li>写入时，将其添加到内存中的平衡树数据结构（例如，红黑树）。这个内存树有时被称为内存表（memtable）。</li><li>当内存表大于某个阈值（通常为几兆字节）时，将其作为SSTable文件写入磁盘。这可以高效地完成，因为树已经维护了按键排序的键值对。新的SSTable文件成为数据库的最新 部分。当SSTable被写入磁盘时，写入可以继续到一个新的内存表实例。</li><li>为了提供读取请求，首先尝试在内存表中找到关键字，然后在最近的磁盘段中，然后在下一个较旧的段中找到该关键字。 </li><li>有时会在后台运行合并和压缩过程以组合段文件并丢弃覆盖或删除的值。</li></ul><p>这个方案效果很好。它只会遇到一个问题：如果数据库崩溃，则最近的写入（在内存表中， 但尚未写入磁盘）将丢失。为了避免这个问题，我们可以在磁盘上保存一个单独的日志，每 个写入都会立即被附加到磁盘上，就像在前一节中一样。该日志不是按排序顺序，但这并不 重要，因为它的唯一目的是在崩溃后恢复内存表。每当内存表写出到SSTable时，相应的日志 都可以被丢弃。</p><h4 id="用SSTables制作LSM树"><a href="#用SSTables制作LSM树" class="headerlink" title="用SSTables制作LSM树"></a>用SSTables制作LSM树</h4><p>这里描述的算法本质上是LevelDB 【6】和RocksDB 【7】中使用的关键值存储引擎库，被设 计嵌入到其他应用程序中。除此之外，LevelDB可以在Riak中用作Bitcask的替代品。在 Cassandra和HBase中使用了类似的存储引擎【8】，这两种引擎都受到了Google的Bigtable 文档【9】（引入了SSTable和memtable）的启发。</p><p>最初这种索引结构是由Patrick O’Neil等人描述的。在日志结构合并树（或LSM树）【10】的 基础上，建立在以前的工作上日志结构的文件系统【11】。基于这种合并和压缩排序文件原 理的存储引擎通常被称为LSM存储引擎。</p><h4 id="性能优化"><a href="#性能优化" class="headerlink" title="性能优化"></a>性能优化</h4><p>与往常一样，大量的细节使得存储引擎在实践中表现良好。例如，当查找数据库中不存在的 键时，LSM树算法可能会很慢：您必须检查内存表，然后将这些段一直回到最老的（可能必 须从磁盘读取每一个），然后才能确定键不存在。为了优化这种访问，存储引擎通常使用额 外的Bloom过滤器【15】。 （布隆过滤器是用于近似集合内容的内存高效数据结构，它可以 告诉您数据库中是否出现键，从而为不存在的键节省许多不必要的磁盘读取操作。</p><p>还有不同的策略来确定SSTables如何被压缩和合并的顺序和时间。最常见的选择是大小分层 压实。 LevelDB和RocksDB使用平坦压缩（LevelDB因此得名），HBase使用大小分层， Cassandra同时支持【16】。在规模级别的调整中，更新和更小的SSTables先后被合并到更 老的和更大的SSTable中。在水平压实中，关键范围被拆分成更小的SSTables，而较旧的数 据被移动到单独的“水平”，这使得压缩能够更加递增地进行，并且使用更少的磁盘空间。</p><p>即使有许多微妙的东西，LSM树的基本思想 —— 保存一系列在后台合并的SSTables —— 简 单而有效。即使数据集比可用内存大得多，它仍能继续正常工作。由于数据按排序顺序存 储，因此可以高效地执行范围查询（扫描所有高于某些最小值和最高值的所有键），并且因 为磁盘写入是连续的，所以LSM树可以支持非常高的写入吞吐量。</p><h4 id="B树"><a href="#B树" class="headerlink" title="B树"></a>B树</h4><p>刚才讨论的日志结构索引正处在逐渐被接受的阶段，但它们并不是最常见的索引类型。使用 最广泛的索引结构在1970年被引入【17】，不到10年后变得“无处不在”【18】，B树经受了时 间的考验。在几乎所有的关系数据库中，它们仍然是标准的索引实现，许多非关系数据库也 使用它们。</p><p>像SSTables一样，B树保持按键排序的键值对，这允许高效的键值查找和范围查询。但这就是 相似之处的结尾：B树有着非常不同的设计理念。</p><p>我们前面看到的日志结构索引将数据库分解为可变大小的段，通常是几兆字节或更大的大 小，并且总是按顺序编写段。相比之下，B树将数据库分解成固定大小的块或页面，传统上大 小为4KB（有时会更大），并且一次只能读取或写入一个页面。这种设计更接近于底层硬件， 因为磁盘也被安排在固定大小的块中。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-972c0e5dc7a61773.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-419fb94f0b82ea78.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="让B树更可靠"><a href="#让B树更可靠" class="headerlink" title="让B树更可靠"></a>让B树更可靠</h4><p>为了使数据库对崩溃具有韧性，B树实现通常会带有一个额外的磁盘数据结构：预写式日志 （WAL, write-ahead-log）（也称为重做日志（redo log））。这是一个仅追加的文件，每 个B树修改都可以应用到树本身的页面上。当数据库在崩溃后恢复时，这个日志被用来使B树 恢复到一致的状态【5,20】。</p><p>更新页面的一个额外的复杂情况是，如果多个线程要同时访问B树，则需要仔细的并发控制 —— 否则线程可能会看到树处于不一致的状态。这通常通过使用锁存器（latches）（轻量级 锁）保护树的数据结构来完成。日志结构化的方法在这方面更简单，因为它们在后台进行所 有的合并，而不会干扰传入的查询，并且不时地将旧的分段原子交换为新的分段。</p><h4 id="B树优化"><a href="#B树优化" class="headerlink" title="B树优化"></a>B树优化</h4><p>由于B树已经存在了这么久，许多优化已经发展了多年，这并不奇怪。仅举几例：</p><ul><li>一些数据库（如LMDB）使用写时复制方案【21】，而不是覆盖页面并维护WAL进行崩 溃恢复。修改的页面被写入到不同的位置，并且树中的父页面的新版本被创建，指向新 的位置。这种方法对于并发控制也很有用，我们将在“快照隔离和可重复读”中看到。</li><li>我们可以通过不存储整个键来节省页面空间，但可以缩小它的大小。特别是在树内部的页面上，键只需要提供足够的信息来充当键范围之间的边界。在页面中包含更多的键允 许树具有更高的分支因子，因此更少的层次 </li><li>通常，页面可以放置在磁盘上的任何位置；没有什么要求附近的键范围页面附近的磁盘上。如果查询需要按照排序顺序扫描大部分关键字范围，那么每个页面的布局可能会非 常不方便，因为每个读取的页面都可能需要磁盘查找。因此，许多B树实现尝试布局树， 使得叶子页面按顺序出现在磁盘上。但是，随着树的增长，维持这个顺序是很困难的。 相比之下，由于LSM树在合并过程中一次又一次地重写存储的大部分，所以它们更容易 使顺序键在磁盘上彼此靠近。 </li><li>额外的指针已添加到树中。例如，每个叶子页面可以在左边和右边具有对其兄弟页面的 引用，这允许不跳回父页面就能顺序扫描。 </li><li>B树的变体如分形树【22】借用一些日志结构的思想来减少磁盘寻道（而且它们与分形无关）。</li></ul><h4 id="比较B树和LSM树"><a href="#比较B树和LSM树" class="headerlink" title="比较B树和LSM树"></a>比较B树和LSM树</h4><p>尽管B树实现通常比LSM树实现更成熟，但LSM树由于其性能特点也非常有趣。根据经验，通常LSM树的写入速度更快，而B树的读取速度更快【23】。LSM树上的读取通常比较慢，因为它们必须在压缩的不同阶段检查几个不同的数据结构和SSTables。</p><p>然而，基准通常对工作量的细节不确定和敏感。 您需要测试具有特定工作负载的系统，以便 进行有效的比较。 在本节中，我们将简要讨论一些在衡量存储引擎性能时值得考虑的事情。</p><h4 id="LSM树的优点"><a href="#LSM树的优点" class="headerlink" title="LSM树的优点"></a>LSM树的优点</h4><p>B树索引必须至少两次写入每一段数据：一次写入预先写入日志，一次写入树页面本身（也许 再次分页）。即使在该页面中只有几个字节发生了变化，也需要一次编写整个页面的开销。 有些存储引擎甚至会覆盖同一个页面两次，以免在电源故障的情况下导致页面部分更新。</p><p>由于反复压缩和合并SSTables，日志结构索引也会重写数据。这种影响 —— 在数据库的生命周期中写入数据库导致对磁盘的多次写入 —— 被称为写放大（write amplification）。需要特别关注的是固态硬盘，固态硬盘在磨损之前只能覆写一段时间。</p><p>在写入繁重的应用程序中，性能瓶颈可能是数据库可以写入磁盘的速度。在这种情况下，写放大会导致直接的性能代价：存储引擎写入磁盘的次数越多，可用磁盘带宽内的每秒写入次数越少。</p><p>而且，LSM树通常能够比B树支持更高的写入吞吐量，部分原因是它们有时具有较低的写放大 （尽管这取决于存储引擎配置和工作负载），部分是因为它们顺序地写入紧凑的SSTable文件 而不是必须覆盖树中的几个页面【26】。这种差异在磁性硬盘驱动器上尤其重要，顺序写入 比随机写入快得多。</p><p>LSM树可以被压缩得更好，因此经常比B树在磁盘上产生更小的文件。 B树存储引擎会由于分 割而留下一些未使用的磁盘空间：当页面被拆分或某行不能放入现有页面时，页面中的某些 空间仍未被使用。由于LSM树不是面向页面的，并且定期重写SSTables以去除碎片，所以它 们具有较低的存储开销，特别是当使用平坦压缩时【27】。</p><p>在许多固态硬盘上，固件内部使用日志结构化算法，将随机写入转变为顺序写入底层存储芯 片，因此存储引擎写入模式的影响不太明显【19】。但是，较低的写入放大率和减少的碎片 对SSD仍然有利：更紧凑地表示数据可在可用的I/O带宽内提供更多的读取和写入请求。</p><h4 id="LSM树的缺点"><a href="#LSM树的缺点" class="headerlink" title="LSM树的缺点"></a>LSM树的缺点</h4><p>日志结构存储的缺点是压缩过程有时会干扰正在进行的读写操作。尽管存储引擎尝试逐步执 行压缩而不影响并发访问，但是磁盘资源有限，所以很容易发生请求需要等待而磁盘完成昂 贵的压缩操作。对吞吐量和平均响应时间的影响通常很小，但是在更高百分比的情况下（参 阅“描述性能”），对日志结构化存储引擎的查询响应时间有时会相当长，而B树的行为则相对 更具可预测性【28】。</p><p>压缩的另一个问题出现在高写入吞吐量：磁盘的有限写入带宽需要在初始写入（记录和刷新 内存表到磁盘）和在后台运行的压缩线程之间共享。写入空数据库时，可以使用全磁盘带宽 进行初始写入，但数据库越大，压缩所需的磁盘带宽就越多。</p><p>如果写入吞吐量很高，并且压缩没有仔细配置，压缩跟不上写入速率。在这种情况下，磁盘 上未合并段的数量不断增加，直到磁盘空间用完，读取速度也会减慢，因为它们需要检查更 多段文件。通常情况下，即使压缩无法跟上，基于SSTable的存储引擎也不会限制传入写入的 速率，所以您需要进行明确的监控来检测这种情况【29,30】。</p><p>B树的一个优点是每个键只存在于索引中的一个位置，而日志结构化的存储引擎可能在不同的 段中有相同键的多个副本。这个方面使得B树在想要提供强大的事务语义的数据库中很有吸引 力：在许多关系数据库中，事务隔离是通过在键范围上使用锁来实现的，在B树索引中，这些 锁可以直接连接到树【5】。在第7章中，我们将更详细地讨论这一点。</p><p>B树在数据库体系结构中是非常根深蒂固的，为许多工作负载提供始终如一的良好性能，所以 它们不可能很快就会消失。在新的数据存储中，日志结构化索引变得越来越流行。没有快速 和容易的规则来确定哪种类型的存储引擎对你的场景更好，所以值得进行一些经验上的测试</p><h4 id="其他索引结构"><a href="#其他索引结构" class="headerlink" title="其他索引结构"></a>其他索引结构</h4><p>到目前为止，我们只讨论了关键值索引，它们就像关系模型中的主键（primary key）索引。 主键唯一标识关系表中的一行，或文档数据库中的一个文档或图形数据库中的一个顶点。数 据库中的其他记录可以通过其主键（或ID）引用该行/文档/顶点，并且索引用于解析这样的引用。</p><p>有二级索引也很常见。在关系数据库中，您可以使用 CREATE INDEX 命令在同一个表上创建多 个二级索引，而且这些索引通常对于有效地执行联接而言至关重要。例如，在第2章中的图2-1中，很可能在 user_id 列上有一个二级索引，以便您可以在每个表中找到属于同一用户的所有行。</p><p>一个二级索引可以很容易地从一个键值索引构建。主要的不同是键不是唯一的。即可能有许 多行（文档，顶点）具有相同的键。这可以通过两种方式来解决：或者通过使索引中的每个 值，成为匹配行标识符的列表（如全文索引中的发布列表），或者通过向每个索引添加行标 识符来使每个关键字唯一。无论哪种方式，B树和日志结构索引都可以用作辅助索引。</p><h4 id="全文搜索和模糊索引"><a href="#全文搜索和模糊索引" class="headerlink" title="全文搜索和模糊索引"></a>全文搜索和模糊索引</h4><p>到目前为止所讨论的所有索引都假定您有确切的数据，并允许您查询键的确切值或具有排序 顺序的键的值范围。他们不允许你做的是搜索类似的键，如拼写错误的单词。这种模糊的查 询需要不同的技术。</p><p>例如，全文搜索引擎通常允许搜索一个单词以扩展为包括该单词的同义词，忽略单词的语法 变体，并且搜索在相同文档中彼此靠近的单词的出现，并且支持各种其他功能取决于文本的 语言分析。为了处理文档或查询中的拼写错误，Lucene能够在一定的编辑距离内搜索文本 （编辑距离1意味着添加，删除或替换了一个字母）【37】。</p><p>正如“在SSTables中创建LSM树”中所提到的，Lucene为其词典使用了一个类似于SSTable的 结构。这个结构需要一个小的内存索引，告诉查询在排序文件中哪个偏移量需要查找关键 字。在LevelDB中，这个内存中的索引是一些键的稀疏集合，但在Lucene中，内存中的索引 是键中字符的有限状态自动机，类似于trie 【38】。这个自动机可以转换成Levenshtein自动 机，它支持在给定的编辑距离内有效地搜索单词【39】。</p><p>其他的模糊搜索技术正朝着文档分类和机器学习的方向发展。有关更多详细信息，请参阅信 息检索教科书，例如【40】。</p><h4 id="在内存中存储一切"><a href="#在内存中存储一切" class="headerlink" title="在内存中存储一切"></a>在内存中存储一切</h4><p>本章到目前为止讨论的数据结构都是对磁盘限制的回答。与主内存相比，磁盘处理起来很尴 尬。对于磁盘和SSD，如果要在读取和写入时获得良好性能，则需要仔细地布置磁盘上的数 据。但是，我们容忍这种尴尬，因为磁盘有两个显着的优点：它们是耐用的（它们的内容在 电源关闭时不会丢失），并且每GB的成本比RAM低。</p><p>随着RAM变得更便宜，每GB的成本价格被侵蚀了。许多数据集不是那么大，所以将它们全部 保存在内存中是非常可行的，可能分布在多个机器上。这导致了内存数据库的发展。</p><p>某些内存中的键值存储（如Memcached）仅用于缓存，在重新启动计算机时丢失的数据是可 以接受的。但其他内存数据库的目标是持久性，可以通过特殊的硬件（例如电池供电的 RAM），将更改日志写入磁盘，将定时快照写入磁盘或通过复制内存来实现，记忆状态到其 他机器。</p><p>内存数据库重新启动时，需要从磁盘或通过网络从副本重新加载其状态（除非使用特殊的硬 件）。尽管写入磁盘，它仍然是一个内存数据库，因为磁盘仅用作耐久性附加日志，读取完 全由内存提供。写入磁盘也具有操作优势：磁盘上的文件可以很容易地由外部实用程序进行 备份，检查和分析。</p><p>诸如VoltDB，MemSQL和Oracle TimesTen等产品是具有关系模型的内存数据库，供应商声 称，通过消除与管理磁盘上的数据结构相关的所有开销，他们可以提供巨大的性能改进 【41,42】。 RAM Cloud是一个开源的内存键值存储器，具有持久性（对存储器中的数据以及 磁盘上的数据使用日志结构化方法）【43】。 Redis和Couchbase通过异步写入磁盘提供了 较弱的持久性。</p><h4 id="事务处理还是分析？"><a href="#事务处理还是分析？" class="headerlink" title="事务处理还是分析？"></a>事务处理还是分析？</h4><p>在业务数据处理的早期，对数据库的写入通常对应于正在进行的商业交易：进行销售，向供 应商下订单，支付员工工资等等。随着数据库扩展到那些没有不涉及钱易手，术语交易仍然 卡住，指的是形成一个逻辑单元的一组读写。 事务不一定具有ACID（原子性，一致性，隔离 性和持久性）属性。事务处理只是意味着允许客户端进行低延迟读取和写入 —— 而不是批量 处理作业，而这些作业只能定期运行（例如每天一次）。我们在第7章中讨论ACID属性，在 第10章中讨论批处理。</p><p>即使数据库开始被用于许多不同类型的博客文章，游戏中的动作，地址簿中的联系人等等， 基本访问模式仍然类似于处理业务事务。应用程序通常使用索引通过某个键查找少量记录。 根据用户的输入插入或更新记录。由于这些应用程序是交互式的，因此访问模式被称为在线 事务处理（OLTP, OnLine Transaction Processing）。</p><p>但是，数据库也开始越来越多地用于数据分析，这些数据分析具有非常不同的访问模式。通 常，分析查询需要扫描大量记录，每个记录只读取几列，并计算汇总统计信息（如计数，总 和或平均值），而不是将原始数据返回给用户。</p><h4 id="数据仓库"><a href="#数据仓库" class="headerlink" title="数据仓库"></a>数据仓库</h4><p>一个企业可能有几十个不同的交易处理系统：系统为面向客户的网站提供动力，控制实体商 店的销售点（checkout）系统，跟踪仓库中的库存，规划车辆路线，管理供应商，管理员工 等。这些系统中的每一个都是复杂的，需要一个人员去维护，所以系统最终都是自动运行的。</p><p>这些OLTP系统通常具有高度的可用性，并以低延迟处理事务，因为这些系统往往对业务运作 至关重要。因此数据库管理员密切关注他们的OLTP数据库他们通常不愿意让业务分析人员在 OLTP数据库上运行临时分析查询，因为这些查询通常很昂贵，扫描大部分数据集，这会损害 同时执行的事务的性能。</p><h4 id="OLTP数据库和数据仓库之间的分歧"><a href="#OLTP数据库和数据仓库之间的分歧" class="headerlink" title="OLTP数据库和数据仓库之间的分歧"></a>OLTP数据库和数据仓库之间的分歧</h4><p>数据仓库的数据模型通常是关系型的，因为SQL通常很适合分析查询。有许多图形数据分析 工具可以生成SQL查询，可视化结果，并允许分析人员探索数据（通过下钻，切片和切块等 操作）。</p><p>表面上，一个数据仓库和一个关系OLTP数据库看起来很相似，因为它们都有一个SQL查询接口。然而，系统的内部看起来可能完全不同，因为它们针对非常不同的查询模式进行了优化。现在许多数据库供应商都将重点放在支持事务处理或分析工作负载上，而不是两者都支持。</p><h4 id="列存储"><a href="#列存储" class="headerlink" title="列存储"></a>列存储</h4><p>面向列的存储背后的想法很简单：不要将所有来自一行的值存储在一起，而是将来自每一列 的所有值存储在一起。如果每个列存储在一个单独的文件中，查询只需要读取和解析查询中 使用的那些列，这可以节省大量的工作。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a6b9a01db2cb8c9d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="列压缩"><a href="#列压缩" class="headerlink" title="列压缩"></a>列压缩</h4><p>除了仅从磁盘加载查询所需的列以外，我们还可以通过压缩数据来进一步降低对磁盘吞吐量 的需求。幸运的是，面向列的存储通常很适合压缩。</p><p>看看图3-10中每一列的值序列：它们通常看起来是相当重复的，这是压缩的好兆头。根据列 中的数据，可以使用不同的压缩技术。在数据仓库中特别有效的一种技术是位图编码，如图3- 11所示。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-cc23eddf759085ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="内存带宽和向量处理"><a href="#内存带宽和向量处理" class="headerlink" title="内存带宽和向量处理"></a>内存带宽和向量处理</h4><p>对于需要扫描数百万行的数据仓库查询来说，一个巨大的瓶颈是从磁盘获取数据到内存的带 宽。但是，这不是唯一的瓶颈。分析数据库的开发人员也担心有效利用主存储器带宽到CPU 缓存中的带宽，避免CPU指令处理流水线中的分支错误预测和泡沫，以及在现代中使用单指 令多数据（SIMD）指令CPU 【59,60】。</p><p>除了减少需要从磁盘加载的数据量以外，面向列的存储布局也可以有效利用CPU周期。例 如，查询引擎可以将大量压缩的列数据放在CPU的L1缓存中，然后在紧密的循环中循环（即 没有函数调用）。一个CPU可以执行这样一个循环比代码要快得多，这个代码需要处理每个 记录的大量函数调用和条件。列压缩允许列中的更多行适合相同数量的L1缓存。前面描述的 按位“与”和“或”运算符可以被设计为直接在这样的压缩列数据块上操作。这种技术被称为矢量 化处理【58,49】。</p><h4 id="列存储中的排序顺序"><a href="#列存储中的排序顺序" class="headerlink" title="列存储中的排序顺序"></a>列存储中的排序顺序</h4><p>在列存储中，存储行的顺序并不一定很重要。按插入顺序存储它们是最简单的，因为插入一 个新行就意味着附加到每个列文件。但是，我们可以选择强制执行一个命令，就像我们之前 对SSTables所做的那样，并将其用作索引机制。</p><p>注意，每列独自排序是没有意义的，因为那样我们就不会知道列中的哪些项属于同一行。我 们只能重建一行，因为我们知道一列中的第k项与另一列中的第k项属于同一行。</p><p>相反，即使按列存储数据，也需要一次对整行进行排序。数据库的管理员可以使用他们对常 见查询的知识来选择表格应该被排序的列。例如，如果查询通常以日期范围为目标，例如上 个月，则可以将 date_key 作为第一个排序键。然后，查询优化器只能扫描上个月的行，这 比扫描所有行要快得多。</p><p>第二列可以确定第一列中具有相同值的任何行的排序顺序。例如，如果 date_key 是图3-10 中的第一个排序关键字，那么 product_sk 可能是第二个排序关键字，因此同一天的同一产 品的所有销售都将在存储中组合在一起。这将有助于需要在特定日期范围内按产品对销售进 行分组或过滤的查询。</p><p>排序顺序的另一个好处是它可以帮助压缩列。如果主要排序列没有多个不同的值，那么在排 序之后，它将具有很长的序列，其中相同的值连续重复多次。一个简单的运行长度编码（就 像我们用于图3-11中的位图一样）可以将该列压缩到几千字节 —— 即使表中有数十亿行。</p><p>第一个排序键的压缩效果最强。第二和第三个排序键会更混乱，因此不会有这么长时间的重 复值。排序优先级下面的列以基本上随机的顺序出现，所以它们可能不会被压缩。但前几列 排序仍然是一个整体。</p><h4 id="写入列存储"><a href="#写入列存储" class="headerlink" title="写入列存储"></a>写入列存储</h4><p>这些优化在数据仓库中是有意义的，因为大多数负载由分析人员运行的大型只读查询组成。 面向列的存储，压缩和排序都有助于更快地读取这些查询。然而，他们有写更加困难的缺点。</p><p>使用B树的更新就地方法对于压缩的列是不可能的。如果你想在排序表的中间插入一行，你很可能不得不重写所有的列文件。由于行由列中的位置标识，因此插入必须始终更新所有列。</p><p>幸运的是，本章前面已经看到了一个很好的解决方案：LSM树。所有的写操作首先进入一个内存中的存储，在这里它们被添加到一个已排序的结构中，并准备写入磁盘。内存中的存储是面向行还是列的，这并不重要。当已经积累了足够的写入数据时，它们将与磁盘上的列文 件合并，并批量写入新文件。这基本上是Vertica所做的【62】</p><p>查询需要检查磁盘上的列数据和最近在内存中的写入，并将两者结合起来。但是，查询优化器隐藏了用户的这个区别。从分析师的角度来看，通过插入，更新或删除操作进行修改的数 据会立即反映在后续查询中。</p><h3 id="编码与演化"><a href="#编码与演化" class="headerlink" title="编码与演化"></a>编码与演化</h3><h4 id="编码数据的格式"><a href="#编码数据的格式" class="headerlink" title="编码数据的格式"></a>编码数据的格式</h4><p>程序通常（至少）使用两种形式的数据：</p><ol><li>在内存中，数据保存在对象，结构体，列表，数组，哈希表，树等中。 这些数据结构针 对CPU的高效访问和操作进行了优化（通常使用指针）。</li><li>如果要将数据写入文件，或通过网络发送，则必须将其编码（encode）为某种自包含的 字节序列（例如，JSON文档）。 由于每个进程都有自己独立的地址空间，一个进程中 的指针对任何其他进程都没有意义，所以这个字节序列表示会与通常在内存中使用的数 据结构完全不同i。</li></ol><p>所以，需要在两种表示之间进行某种类型的翻译。 从内存中表示到字节序列的转换称为编码 （Encoding）（也称为序列化（serialization）或编组（marshalling）），反过来称为解码 （Decoding） （解析（Parsing），反序列化（deserialization），反编组() unmarshalling）） 。</p><h4 id="语言特定的格式"><a href="#语言特定的格式" class="headerlink" title="语言特定的格式"></a>语言特定的格式</h4><p>许多编程语言都内建了将内存对象编码为字节序列的支持。例如，Java 有 java.io.Serializable 【1】，Ruby有 Marshal 【2】，Python有 pickle 【3】等等。许多 第三方库也存在，例如 Kryo for Java 【4】。</p><p>这些编码库非常方便，可以用很少的额外代码实现内存对象的保存与恢复。但是它们也有一 些深层次的问题：</p><ul><li>这类编码通常与特定的编程语言深度绑定，其他语言很难读取这种数据。如果以这类编 码存储或传输数据，那你就和这门语言绑死在一起了。并且很难将系统与其他组织的系 统（可能用的是不同的语言）进行集成。</li><li>为了恢复相同对象类型的数据，解码过程需要实例化任意类的能力，这通常是安全问题 的一个来源【5】：如果攻击者可以让应用程序解码任意的字节序列，他们就能实例化任 意的类，这会允许他们做可怕的事情，如远程执行任意代码【6,7】。</li><li>在这些库中，数据版本控制通常是事后才考虑的。因为它们旨在快速简便地对数据进行 编码，所以往往忽略了前向后向兼容性带来的麻烦问题。</li><li>效率（编码或解码所花费的CPU时间，以及编码结构的大小）往往也是事后才考虑的。 例如，Java的内置序列化由于其糟糕的性能和臃肿的编码而臭名昭着【8】。</li></ul><h4 id="JSON，XML和二进制变体"><a href="#JSON，XML和二进制变体" class="headerlink" title="JSON，XML和二进制变体"></a>JSON，XML和二进制变体</h4><p>JSON，XML和CSV是文本格式，因此具有人类可读性（尽管语法是一个热门辩题）。除了表 面的语法问题之外，它们也有一些微妙的问题：</p><ul><li>数字的编码多有歧义之处。XML和CSV不能区分数字和字符串（除非引用外部模式）。 JSON虽然区分字符串和数字，但不区分整数和浮点数，而且不能指定精度。</li><li>当处理大量数据时，这个问题更严重了。例如，大于$2^{53}$的整数不能在IEEE 754双 精度浮点数中精确表示，因此在使用浮点数（例如JavaScript）的语言进行分析时，这些 数字会变得不准确。 Twitter上有一个大于$2^{53}$的数字的例子，它使用一个64位的数 字来标识每条推文。 Twitter API返回的JSON包含了两种推特ID，一个JSON数字，另一 个是十进制字符串，以此避免JavaScript程序无法正确解析数字的问题【10】。</li><li>JSON和XML对Unicode字符串（即人类可读的文本）有很好的支持，但是它们不支持二 进制数据（不带字符编码(character encoding)的字节序列）。二进制串是很实用的功 能，所以人们通过使用Base64将二进制数据编码为文本来绕开这个限制。模式然后用于 表示该值应该被解释为Base64编码。这个工作，但它有点hacky，并增加了33％的数据 大小。 XML 【11】和JSON 【12】都有可选的模式支持。这些模式语言相当强大，所以 学习和实现起来相当复杂。 XML模式的使用相当普遍，但许多基于JSON的工具嫌麻烦 才不会使用模式。由于数据的正确解释（例如数字和二进制字符串）取决于模式中的信 息，因此不使用XML/JSON模式的应用程序可能需要对相应的编码/解码逻辑进行硬编码。</li><li>CSV没有任何模式，因此应用程序需要定义每行和每列的含义。如果应用程序更改添加 新的行或列，则必须手动处理该变更。 CSV也是一个相当模糊的格式（如果一个值包含 逗号或换行符，会发生什么？）。尽管其转义规则已经被正式指定【13】，但并不是所 有的解析器都正确的实现了标准。</li></ul><p>尽管存在这些缺陷，但JSON，XML和CSV已经足够用于很多目的。特别是作为数据交换格式 （即将数据从一个组织发送到另一个组织），它们很可能仍然很受欢迎。这种情况下，只要 人们对格式是什么意见一致，格式多么美观或者高效就没有关系。让不同的组织达成一致的 难度超过了其他大多数问题。</p><h4 id="二进制编码"><a href="#二进制编码" class="headerlink" title="二进制编码"></a>二进制编码</h4><p>对于仅在组织内部使用的数据，使用最小公分母编码格式的压力较小。例如，可以选择更紧 凑或更快的解析格式。虽然对小数据集来说，收益可以忽略不计，但一旦达到TB级别，数据 格式的选择就会产生巨大的影响。</p><h4 id="MessagePack"><a href="#MessagePack" class="headerlink" title="MessagePack"></a>MessagePack</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-100fe16aca3640cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="Thrift"><a href="#Thrift" class="headerlink" title="Thrift"></a>Thrift</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-68f6c2dc248dce50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-da7b186bab151cd7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="Protobuf"><a href="#Protobuf" class="headerlink" title="Protobuf"></a>Protobuf</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1073701b17a401b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="字段标签和模式演变"><a href="#字段标签和模式演变" class="headerlink" title="字段标签和模式演变"></a>字段标签和模式演变</h4><p>我们之前说过，模式不可避免地需要随着时间而改变。我们称之为模式演变。 Thrift和 Protocol Buffers如何处理模式更改，同时保持向后兼容性？</p><p>从示例中可以看出，编码的记录就是其编码字段的拼接。每个字段由其标签号码（样本模式 中的数字1,2,3）标识，并用数据类型（例如字符串或整数）注释。如果没有设置字段值，则 简单地从编码记录中省略。从中可以看到，字段标记对编码数据的含义至关重要。您可以更 改架构中字段的名称，因为编码的数据永远不会引用字段名称，但不能更改字段的标记，因 为这会使所有现有的编码数据无效。</p><p>您可以添加新的字段到架构，只要您给每个字段一个新的标签号码。如果旧的代码（不知道 你添加的新的标签号码）试图读取新代码写入的数据，包括一个新的字段，其标签号码不能识别，它可以简单地忽略该字段。数据类型注释允许解析器确定需要跳过的字节数。这保持了前向兼容性：旧代码可以读取由新代码编写的记录。</p><p>向后兼容性呢？只要每个字段都有一个唯一的标签号码，新的代码总是可以读取旧的数据， 因为标签号码仍然具有相同的含义。唯一的细节是，如果你添加一个新的领域，你不能要 求。如果您要添加一个字段并将其设置为必需，那么如果新代码读取旧代码写入的数据，则该检查将失败，因为旧代码不会写入您添加的新字段。因此，为了保持向后兼容性，在模式 的初始部署之后添加的每个字段必须是可选的或具有默认值。</p><p>删除一个字段就像添加一个字段，倒退和向前兼容性问题相反。这意味着您只能删除一个可 选的字段（必填字段永远不能删除），而且您不能再次使用相同的标签号码（因为您可能仍 然有数据写在包含旧标签号码的地方，而该字段必须被新代码忽略）。</p><h4 id="数据类型和模式演变"><a href="#数据类型和模式演变" class="headerlink" title="数据类型和模式演变"></a>数据类型和模式演变</h4><p>如何改变字段的数据类型？这可能是可能的——检查文件的细节——但是有一个风险，值将 失去精度或被扼杀。例如，假设你将一个32位的整数变成一个64位的整数。新代码可以轻松 读取旧代码写入的数据，因为解析器可以用零填充任何缺失的位。但是，如果旧代码读取由 新代码写入的数据，则旧代码仍使用32位变量来保存该值。如果解码的64位值不适合32位， 则它将被截断。</p><p>Protobuf的一个奇怪的细节是，它没有列表或数组数据类型，而是有一个字段的重复标记（这 是第三个选项旁边必要和可选）。如图4-4所示，重复字段的编码正如它所说的那样：同一个 字段标记只是简单地出现在记录中。这具有很好的效果，可以将可选（单值）字段更改为重 复（多值）字段。读取旧数据的新代码会看到一个包含零个或一个元素的列表（取决于该字 段是否存在）。读取新数据的旧代码只能看到列表的最后一个元素。</p><p>Thrift有一个专用的列表数据类型，它使用列表元素的数据类型进行参数化。这不允许 Protocol Buffers所做的从单值到多值的相同演变，但是它具有支持嵌套列表的优点。</p><h4 id="Avro"><a href="#Avro" class="headerlink" title="Avro"></a>Avro</h4><p>Apache Avro 【20】是另一种二进制编码格式，与Protocol Buffers和Thrift有趣的不同。 它是 作为Hadoop的一个子项目在2009年开始的，因为Thrift不适合Hadoop的用例【21】。</p><p>Avro也使用模式来指定正在编码的数据的结构。 它有两种模式语言：一种（Avro IDL）用于 人工编辑，一种（基于JSON），更易于机器读取。</p><pre><code>record Person {     string userName;     union { null, long }     favoriteNumber = null;     array&lt;string&gt; interests; }</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ad347478a3e07095.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="动态生成的模式"><a href="#动态生成的模式" class="headerlink" title="动态生成的模式"></a>动态生成的模式</h4><p>与Protocol Buffers和Thrift相比，Avro方法的一个优点是架构不包含任何标签号码。但为什么 这很重要？在模式中保留一些数字有什么问题？</p><p>不同之处在于Avro对动态生成的模式更友善。例如，假如你有一个关系数据库，你想要把它 的内容转储到一个文件中，并且你想使用二进制格式来避免前面提到的文本格式（JSON， CSV，SQL）的问题。如果你使用Avro，你可以很容易地从关系模式生成一个Avro模式（在 我们之前看到的JSON表示中），并使用该模式对数据库内容进行编码，并将其全部转储到 Avro对象容器文件【25】中。您为每个数据库表生成一个记录模式，每个列成为该记录中的 一个字段。数据库中的列名称映射到Avro中的字段名称。</p><p>现在，如果数据库模式发生变化（例如，一个表中添加了一列，删除了一列），则可以从更 新的数据库模式生成新的Avro模式，并在新的Avro模式中导出数据。数据导出过程不需要注 意模式的改变 - 每次运行时都可以简单地进行模式转换。任何读取新数据文件的人都会看到记 录的字段已经改变，但是由于字段是通过名字来标识的，所以更新的作者的模式仍然可以与 旧的读者模式匹配。</p><p>相比之下，如果您为此使用Thrift或Protocol Buffers，则字段标记可能必须手动分配：每次数 据库模式更改时，管理员都必须手动更新从数据库列名到字段标签。 （这可能会自动化，但 模式生成器必须非常小心，不要分配以前使用的字段标记。）这种动态生成的模式根本不是 Thrift或Protocol Buffers的设计目标，而是为Avro。</p><h4 id="代码生成和动态类型的语言"><a href="#代码生成和动态类型的语言" class="headerlink" title="代码生成和动态类型的语言"></a>代码生成和动态类型的语言</h4><p>Thrift和Protobuf依赖于代码生成：在定义了模式之后，可以使用您选择的编程语言生成实现 此模式的代码。这在Java，C ++或C＃等静态类型语言中很有用，因为它允许将高效的内存 中结构用于解码的数据，并且在编写访问数据结构的程序时允许在IDE中进行类型检查和自动 完成。</p><p>在动态类型编程语言（如JavaScript，Ruby或Python）中，生成代码没有太多意义，因为没 有编译时类型检查器来满足。代码生成在这些语言中经常被忽视，因为它们避免了明确的编 译步骤。而且，对于动态生成的模式（例如从数据库表生成的Avro模式），代码生成对获取 数据是一个不必要的障碍。</p><p>Avro为静态类型编程语言提供了可选的代码生成功能，但是它也可以在不生成任何代码的情 况下使用。如果你有一个对象容器文件（它嵌入了作者的模式），你可以简单地使用Avro库 打开它，并以与查看JSON文件相同的方式查看数据。该文件是自描述的，因为它包含所有必 要的元数据。</p><p>这个属性特别适用于动态类型的数据处理语言如Apache Pig 【26】。在Pig中，您可以打开 一些Avro文件，开始分析它们，并编写派生数据集以Avro格式输出文件，而无需考虑模式。</p><h4 id="模式的优点"><a href="#模式的优点" class="headerlink" title="模式的优点"></a>模式的优点</h4><p>正如我们所看到的，Protocol Buffers，Thrift和Avro都使用模式来描述二进制编码格式。他们 的模式语言比XML模式或者JSON模式简单得多，它支持更详细的验证规则（例如，“该字段 的字符串值必须与该正则表达式匹配”或“该字段的整数值必须在0和100之间“）。由于Protocol Buffers，Thrift和Avro实现起来更简单，使用起来也更简单，所以它们已经发展到支持相当广 泛的编程语言。</p><p>这些编码所基于的想法绝不是新的。例如，它们与ASN.1有很多相似之处，它是1984年首次 被标准化的模式定义语言【27】。它被用来定义各种网络协议，其二进制编码（DER）仍然 被用于编码SSL证书（X.509），例如【28】。 ASN.1支持使用标签号码的模式演进，类似于 Protocol Buf-fers和Thrift 【29】。然而，这也是非常复杂和严重的文件记录，所以ASN.1可 能不是新应用程序的好选择。</p><p>许多数据系统也为其数据实现某种专有的二进制编码。例如，大多数关系数据库都有一个网 络协议，您可以通过该协议向数据库发送查询并获取响应。这些协议通常特定于特定的数据 库，并且数据库供应商提供将来自数据库的网络协议的响应解码为内存数据结构的驱动程序 （例如使用ODBC或JDBC API）。</p><p>所以，我们可以看到，尽管JSON，XML和CSV等文本数据格式非常普遍，但基于模式的二进 制编码也是一个可行的选择。他们有一些很好的属性：</p><ul><li>它们可以比各种“二进制JSON”变体更紧凑，因为它们可以省略编码数据中的字段名称。</li><li>模式是一种有价值的文档形式，因为模式是解码所必需的，所以可以确定它是最新的 （而手动维护的文档可能很容易偏离现实）。</li><li>保留模式数据库允许您在部署任何内容之前检查模式更改的向前和向后兼容性。</li><li>对于静态类型编程语言的用户来说，从模式生成代码的能力是有用的，因为它可以在编 译时进行类型检查。</li></ul><h4 id="数据流的类型"><a href="#数据流的类型" class="headerlink" title="数据流的类型"></a>数据流的类型</h4><p>在本章的开始部分，我们曾经说过，无论何时您想要将某些数据发送到不共享内存的另一个 进程，例如，只要您想通过网络发送数据或将其写入文件，就需要将它编码为一个字节序 列。然后我们讨论了做这个的各种不同的编码。 我们讨论了向前和向后的兼容性，这对于可演化性来说非常重要（通过允许您独立升级系统的不同部分，而不必一次改变所有内容，可 以轻松地进行更改）。兼容性是编码数据的一个进程和解码它的另一个进程之间的一种关系。</p><p>这是一个相当抽象的概念 - 数据可以通过多种方式从一个流程流向另一个流程。谁编码数据， 谁解码？在本章的其余部分中，我们将探讨数据如何在流程之间流动的一些最常见的方式：</p><ul><li>通过数据库（参阅“通过数据库的数据流”）</li><li>通过服务调用（参阅“通过服务传输数据流：REST和RPC”）</li><li>通过异步消息传递（参阅“消息传递数据流”）MQ</li></ul><h2 id="分布式数据"><a href="#分布式数据" class="headerlink" title="分布式数据"></a>分布式数据</h2><h3 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h3><h4 id="扩展至更高的载荷"><a href="#扩展至更高的载荷" class="headerlink" title="扩展至更高的载荷"></a>扩展至更高的载荷</h4><p>如果你需要的只是扩展至更高的载荷（load），最简单的方法就是购买更强大的机器（有时 称为垂直扩展（vertical scaling）或向上扩展（scale up））。许多处理器，内存和磁盘可 以在同一个操作系统下相互连接，快速的相互连接允许任意处理器访问内存或磁盘的任意部 分。在这种共享内存架构（shared-memory architecture）中，所有的组件都可以看作一台单独的机器。</p><p>共享内存方法的问题在于，成本增长速度快于线性增长：一台有着双倍处理器数量，双倍内存大小，双倍磁盘容量的机器，通常成本会远远超过原来的两倍。而且可能因为存在瓶颈， 并不足以处理双倍的载荷。</p><p>共享内存架构可以提供有限的容错能力，高端机器可以使用热插拔的组件（不关机更换磁 盘，内存模块，甚至处理器）——但它必然囿于单个地理位置的桎梏。</p><p>另一种方法是共享磁盘架构（shared-disk architecture），它使用多台具有独立处理器和内 存的机器，但将数据存储在机器之间共享的磁盘阵列上，这些磁盘通过快速网络连接ii。这种 架构用于某些数据仓库，但竞争和锁定的开销限制了共享磁盘方法的可扩展性【2】。</p><h4 id="无共享架构"><a href="#无共享架构" class="headerlink" title="无共享架构"></a>无共享架构</h4><p>相比之下，无共享架构（shared-nothing architecture）（有时称为水平扩展（horizontal scale） 或向外扩展（scale out））已经相当普及。在这种架构中，运行数据库软件的每台 机器/虚拟机都称为节点（node）。每个节点只使用各自的处理器，内存和磁盘。节点之间的 任何协调，都是在软件层面使用传统网络实现的。</p><p>无共享系统不需要使用特殊的硬件，所以你可以用任意机器——比如性价比最好的机器。你 也许可以跨多个地理区域分布数据从而减少用户延迟，或者在损失一整个数据中心的情况下 幸免于难。随着云端虚拟机部署的出现，即使是小公司，现在无需Google级别的运维，也可 以实现异地分布式架构。</p><p>在这一部分里，我们将重点放在无共享架构上。它不见得是所有场景的最佳选择，但它是最 需要你谨慎从事的架构。如果你的数据分布在多个节点上，你需要意识到这样一个分布式系 统中约束和权衡 ——数据库并不能魔术般地把这些东西隐藏起来。</p><h4 id="复制-vs-分区"><a href="#复制-vs-分区" class="headerlink" title="复制 vs 分区"></a>复制 vs 分区</h4><p>数据分布在多个节点上有两种常见的方式：</p><p><strong>复制（Replication）</strong></p><p>在几个不同的节点上保存数据的相同副本，可能放在不同的位置。 复制提供了冗余：如果一 些节点不可用，剩余的节点仍然可以提供数据服务。 复制也有助于改善性能。 第五章将讨论 复制。</p><p><strong>分区 (Partitioning)</strong></p><p>将一个大型数据库拆分成较小的子集（称为分区（partitions）），从而不同的分区可以指派 给不同的节点（node）（亦称分片（shard））。 第六章将讨论分区。</p><p>理解了这些概念，就可以开始讨论在分布式系统中需要做出的困难抉择。第七章将讨论事务 (Transaction)，这对于了解数据系统中可能出现的各种问题，以及我们可以做些什么很有帮助。第八章和第九章将讨论分布式系统的根本局限性。</p><h4 id="领导者与追随者"><a href="#领导者与追随者" class="headerlink" title="领导者与追随者"></a>领导者与追随者</h4><p>存储数据库副本的每个节点称为副本（replica）。当存在多个副本时，会不可避免的出现一 个问题：如何确保所有数据都落在了所有的副本上？</p><p>每一次向数据库的写入操作都需要传播到所有副本上，否则副本就会包含不一样的数据。最 常见的解决方案被称为基于领导者的复制（leader-based replication）（也称主动/被动 （active/passive） 或 主/从（master/slave）复制），如图5-1所示。它的工作原理如下：</p><ol><li>副本之一被指定为领导者（leader），也称为 主库（master） ，首要（primary）。当 客户端要向数据库写入时，它必须将请求发送给领导者，领导者会将新数据写入其本地 存储。</li><li>其他副本被称为追随者（followers），亦称为只读副本（read replicas），从库 （slaves），次要（ sencondaries），热备（hot-standby）i。每当领导者将新数据写 入本地存储时，它也会将数据变更发送给所有的追随者，称之为复制日志（replication log）记录或变更流（change stream）。每个跟随者从领导者拉取日志，并相应更新其 本地数据库副本，方法是按照领导者处理的相同顺序应用所有写入。</li><li>当客户想要从数据库中读取数据时，它可以向领导者或追随者查询。 但只有领导者才能 接受写操作（从客户端的角度来看从库都是只读的）。</li></ol><p>不同的人对热（hot），温（warn），冷（cold） 备份服务器有不同的定义。 例如在 PostgreSQL中，热备（hot standby）指的是能接受客户端读请求的副本。而温备 （warm standby）只是追随领导者，但不处理客户端的任何查询。 就本书而言，这些 差异并不重要。</p><h4 id="同步复制与异步复制"><a href="#同步复制与异步复制" class="headerlink" title="同步复制与异步复制"></a>同步复制与异步复制</h4><p>复制系统的一个重要细节是：复制是同步（synchronously）发生还是异步 （asynchronously）发生。 （在关系型数据库中这通常是一个配置项，其他系统通常硬编 码为其中一个）。</p><p>同步复制的优点是，从库保证有与主库一致的最新数据副本。如果主库突然失效，我们可以 确信这些数据仍然能在从库上上找到。缺点是，如果同步从库没有响应（比如它已经崩溃， 或者出现网络故障，或其它任何原因），主库就无法处理写入操作。主库必须阻止所有写 入，并等待同步副本再次可用。</p><p>因此，将所有从库都设置为同步的是不切实际的：任何一个节点的中断都会导致整个系统停 滞不前。实际上，如果在数据库上启用同步复制，通常意味着其中一个跟随者是同步的，而 其他的则是异步的。如果同步从库变得不可用或缓慢，则使一个异步从库同步。这保证你至少在两个节点上拥有最新的数据副本：主库和同步从库。 这种配置有时也被称为半同步 （semi-synchronous）</p><p>通常情况下，基于领导者的复制都配置为完全异步。 在这种情况下，如果主库失效且不可恢复，则任何尚未复制给从库的写入都会丢失。 这意味着即使已经向客户端确认成功，写入也不能保证持久（Durable）。 然而，一个完全异步的配置也有优点：即使所有的从库都落后 了，主库也可以继续处理写入。</p><h4 id="设置新从库"><a href="#设置新从库" class="headerlink" title="设置新从库"></a>设置新从库</h4><p>有时候需要设置一个新的从库：也许是为了增加副本的数量，或替换失败的节点。如何确保 新的从库拥有主库数据的精确副本？</p><p>简单地将数据文件从一个节点复制到另一个节点通常是不够的：客户端不断向数据库写入数 据，数据总是在不断变化，标准的数据副本会在不同的时间点总是不一样。复制的结果可能 没有任何意义。</p><p>可以通过锁定数据库（使其不可用于写入）来使磁盘上的文件保持一致，但是这会违背高可 用的目标。幸运的是，拉起新的从库通常并不需要停机。从概念上讲，过程如下所示：</p><ol><li>在某个时刻获取主库的一致性快照（如果可能），而不必锁定整个数据库。大多数数据 库都具有这个功能，因为它是备份必需的。对于某些场景，可能需要第三方工具，例如 MySQL的innobackupex。</li><li>将快照复制到新的从库节点。</li><li>从库连接到主库，并拉取快照之后发生的所有数据变更。这要求快照与主库复制日志中 的位置精确关联。该位置有不同的名称：例如，PostgreSQL将其称为日志序列号（log sequence number, LSN），MySQL将其称为二进制日志坐标（binlog coordinates）。</li><li>当从库处理完快照之后积压的数据变更，我们说它赶上（caught up）了主库。现在它可 以继续处理主库产生的数据变化了。</li></ol><p>建立从库的实际步骤因数据库而异。在某些系统中，这个过程是完全自动化的，而在另外一 些系统中，它可能是一个需要由管理员手动执行的，有点神秘的多步骤工作流。</p><h4 id="处理节点宕机"><a href="#处理节点宕机" class="headerlink" title="处理节点宕机"></a>处理节点宕机</h4><p>系统中的任何节点都可能宕机，可能因为意外的故障，也可能由于计划内的维护（例如，重 启机器以安装内核安全补丁）。对运维而言，能在系统不中断服务的情况下重启单个节点好 处多多。我们的目标是，即使个别节点失效，也能保持整个系统运行，并尽可能控制节点停 机带来的影响。</p><h4 id="从库失效：追赶恢复"><a href="#从库失效：追赶恢复" class="headerlink" title="从库失效：追赶恢复"></a>从库失效：追赶恢复</h4><p>在其本地磁盘上，每个从库记录从主库收到的数据变更。如果从库崩溃并重新启动，或者， 如果主库和从库之间的网络暂时中断，则比较容易恢复：从库可以从日志中知道，在发生故 障之前处理的最后一个事务。因此，从库可以连接到主库，并请求在从库断开连接时发生的 所有数据变更。当应用完所有这些变化后，它就赶上了主库，并可以像以前一样继续接收数 据变更流。</p><h4 id="主库失效：故障转移"><a href="#主库失效：故障转移" class="headerlink" title="主库失效：故障转移"></a>主库失效：故障转移</h4><p>主库失效处理起来相当棘手：其中一个从库需要被提升为新的主库，需要重新配置客户端， 以将它们的写操作发送给新的主库，其他从库需要开始拉取来自新主库的数据变更。这个过 程被称为故障转移（failover）。</p><p>故障转移可以手动进行（通知管理员主库挂了，并采取必要的步骤来创建新的主库）或自动 进行。自动故障转移过程通常由以下步骤组成：</p><ol><li>确认主库失效。有很多事情可能会出错：崩溃，停电，网络问题等等。没有万无一失的 方法来检测出现了什么问题，所以大多数系统只是简单使用超时（Timeout）：节点频繁 地相互来回传递消息，并且如果一个节点在一段时间内（例如30秒）没有响应，就认为 它挂了（因为计划内维护而故意关闭主库不算）。</li><li>选择一个新的主库。这可以通过选举过程（主库由剩余副本以多数选举产生）来完成， 或者可以由之前选定的控制器节点（controller node）来指定新的主库。主库的最佳人 选通常是拥有旧主库最新数据副本的从库（最小化数据损失）。让所有的节点同意一个 新的领导者，是一个共识问题，将在第9章详细讨论。</li><li>重新配置系统以启用新的主库。客户端现在需要将它们的写请求发送给新主库（将在“请 求路由”中讨论这个问题）。如果老领导回来，可能仍然认为自己是主库，没有意识到其 他副本已经让它下台了。系统需要确保老领导认可新领导，成为一个从库。</li></ol><p>故障转移会出现很多大麻烦：</p><ul><li>如果使用异步复制，则新主库可能没有收到老主库宕机前最后的写入操作。在选出新主 库后，如果老主库重新加入集群，新主库在此期间可能会收到冲突的写入，那这些写入 该如何处理？最常见的解决方案是简单丢弃老主库未复制的写入，这很可能打破客户对 于数据持久性的期望。</li><li>如果数据库需要和其他外部存储相协调，那么丢弃写入内容是极其危险的操作。例如在 GitHub 【13】的一场事故中，一个过时的MySQL从库被提升为主库。数据库使用自增ID 作为主键，因为新主库的计数器落后于老主库的计数器，所以新主库重新分配了一些已 经被老主库分配掉的ID作为主键。这些主键也在Redis中使用，主键重用使得MySQL和 Redis中数据产生不一致，最后导致一些私有数据泄漏到错误的用户手中。</li><li>发生某些故障时（见第8章）可能会出现两个节点都以为自己是主库的情况。这种情况称 为脑裂(split brain)，非常危险：如果两个主库都可以接受写操作，却没有冲突解决机制 （参见“多领导者复制”），那么数据就可能丢失或损坏。一些系统采取了安全防范措施： 当检测到两个主库节点同时存在时会关闭其中一个节点ii，但设计粗糙的机制可能最后会 导致两个节点都被关闭【14】。</li><li>主库被宣告死亡之前的正确超时应该怎么配置？在主库失效的情况下，超时时间越长， 意味着恢复时间也越长。但是如果超时设置太短，又可能会出现不必要的故障转移。例 如，临时负载峰值可能导致节点的响应时间超时，或网络故障可能导致数据包延迟。如 果系统已经处于高负载或网络问题的困扰之中，那么不必要的故障切换可能会让情况变 得更糟糕。</li></ul><h4 id="复制日志的实现"><a href="#复制日志的实现" class="headerlink" title="复制日志的实现"></a>复制日志的实现</h4><h5 id="基于语句的复制"><a href="#基于语句的复制" class="headerlink" title="基于语句的复制"></a>基于语句的复制</h5><p>在最简单的情况下，主库记录下它执行的每个写入请求（语句（statement））并将该语句 日志发送给其从库。对于关系数据库来说，这意味着每个 INSERT ， UPDATE 或 DELETE 语句都 被转发给每个从库，每个从库解析并执行该SQL语句，就像从客户端收到一样。</p><ul><li>任何调用非确定性函数（nondeterministic）的语句，可能会在每个副本上生成不同的 值。例如，使用 NOW() 获取当前日期时间，或使用 RAND() 获取一个随机数。</li><li>如果语句使用了自增列（auto increment），或者依赖于数据库中的现有数据（例 如， UPDATE … WHERE &lt;某些条件&gt; ），则必须在每个副本上按照完全相同的顺序执行它 们，否则可能会产生不同的效果。当有多个并发执行的事务时，这可能成为一个限制。</li><li>有副作用的语句（例如，触发器，存储过程，用户定义的函数）可能会在每个副本上产 生不同的副作用，除非副作用是绝对确定的。</li></ul><p>的确有办法绕开这些问题 ——例如，当语句被记录时，主库可以用固定的返回值替换任何不 确定的函数调用，以便从库获得相同的值。但是由于边缘情况实在太多了，现在通常会选择 其他的复制方法。</p><p>基于语句的复制在5.1版本前的MySQL中使用。因为它相当紧凑，现在有时候也还在用。但 现在在默认情况下，如果语句中存在任何不确定性，MySQL会切换到基于行的复制（稍后讨 论）。 VoltDB使用了基于语句的复制，但要求事务必须是确定性的，以此来保证安全</p><h5 id="传输预写式日志（WAL）"><a href="#传输预写式日志（WAL）" class="headerlink" title="传输预写式日志（WAL）"></a>传输预写式日志（WAL）</h5><p>PostgreSQL和Oracle等使用这种复制方法【16】。主要缺点是日志记录的数据非常底层： WAL包含哪些磁盘块中的哪些字节发生了更改。这使复制与存储引擎紧密耦合。如果数据库 将其存储格式从一个版本更改为另一个版本，通常不可能在主库和从库上运行不同版本的数 据库软件。</p><p>看上去这可能只是一个微小的实现细节，但却可能对运维产生巨大的影响。如果复制协议允 许从库使用比主库更新的软件版本，则可以先升级从库，然后执行故障转移，使升级后的节 点之一成为新的主库，从而执行数据库软件的零停机升级。如果复制协议不允许版本不匹配 （传输WAL经常出现这种情况），则此类升级需要停机。</p><h5 id="逻辑日志复制（基于行）"><a href="#逻辑日志复制（基于行）" class="headerlink" title="逻辑日志复制（基于行）"></a>逻辑日志复制（基于行）</h5><p>另一种方法是，复制和存储引擎使用不同的日志格式，这样可以使复制日志从存储引擎内部 分离出来。这种复制日志被称为逻辑日志，以将其与存储引擎的（物理）数据表示区分开来。</p><p>关系数据库的逻辑日志通常是以行的粒度描述对数据库表的写入的记录序列：</p><ul><li>对于插入的行，日志包含所有列的新值。</li><li>对于删除的行，日志包含足够的信息来唯一标识已删除的行。通常是主键，但是如果表 上没有主键，则需要记录所有列的旧值。</li><li>对于更新的行，日志包含足够的信息来唯一标识更新的行，以及所有列的新值（或至少 所有已更改的列的新值）。</li></ul><p>修改多行的事务会生成多个这样的日志记录，后面跟着一条记录，指出事务已经提交。 MySQL的二进制日志（当配置为使用基于行的复制时）使用这种方法【17】</p><p>由于逻辑日志与存储引擎内部分离，因此可以更容易地保持向后兼容，从而使领导者和跟随 者能够运行不同版本的数据库软件甚至不同的存储引擎。</p><h5 id="基于触发器的复制"><a href="#基于触发器的复制" class="headerlink" title="基于触发器的复制"></a>基于触发器的复制</h5><p>到目前为止描述的复制方法是由数据库系统实现的，不涉及任何应用程序代码。在很多情况 下，这就是你想要的。但在某些情况下需要更多的灵活性。例如，如果您只想复制数据的一 个子集，或者想从一种数据库复制到另一种数据库，或者如果您需要冲突解决逻辑（参阅“处 理写入冲突”），则可能需要将复制移动到应用程序层。</p><p>一些工具，如Oracle Golden Gate 【19】，可以通过读取数据库日志，使得其他应用程序可 以使用数据。另一种方法是使用许多关系数据库自带的功能：触发器和存储过程。</p><p>触发器允许您注册在数据库系统中发生数据更改（写入事务）时自动执行的自定义应用程序 代码。触发器有机会将更改记录到一个单独的表中，使用外部程序读取这个表，再加上任何 业务逻辑处理，会后将数据变更复制到另一个系统去。例如，Databus for Oracle 【20】和 Bucardo for Postgres 【21】就是这样工作的。</p><h4 id="复制延迟问题"><a href="#复制延迟问题" class="headerlink" title="复制延迟问题"></a>复制延迟问题</h4><p>容忍节点故障只是需要复制的一个原因。正如在第二部分的介绍中提到的，另一个原因是可 扩展性（处理比单个机器更多的请求）和延迟（让副本在地理位置上更接近用户）。</p><p>基于主库的复制要求所有写入都由单个节点处理，但只读查询可以由任何副本处理。所以对 于读多写少的场景（Web上的常见模式），一个有吸引力的选择是创建很多从库，并将读请 求分散到所有的从库上去。这样能减小主库的负载，并允许向最近的副本发送读请求。</p><p>在这种扩展体系结构中，只需添加更多的追随者，就可以提高只读请求的服务容量。但是， 这种方法实际上只适用于异步复制——如果尝试同步复制到所有追随者，则单个节点故障或 网络中断将使整个系统无法写入。而且越多的节点越有可能会被关闭，所以完全同步的配置 是非常不可靠的。</p><p>不幸的是，当应用程序从异步从库读取时，如果从库落后，它可能会看到过时的信息。这会 导致数据库中出现明显的不一致：同时对主库和从库执行相同的查询，可能得到不同的结 果，因为并非所有的写入都反映在从库中。这种不一致只是一个暂时的状态——如果停止写 入数据库并等待一段时间，从库最终会赶上并与主库保持一致。出于这个原因，这种效应被 称为最终一致性（eventually consistency）iii【22,23】</p><p>“最终”一词故意含糊不清：总的来说，副本落后的程度是没有限制的。在正常的操作中，复 制延迟（replication lag），即写入主库到反映至从库之间的延迟，可能仅仅是几分之一秒， 在实践中并不显眼。但如果系统在接近极限的情况下运行，或网络中存在问题，延迟可以轻 而易举地超过几秒，甚至几分钟。</p><h4 id="复制延迟的解决方案"><a href="#复制延迟的解决方案" class="headerlink" title="复制延迟的解决方案"></a>复制延迟的解决方案</h4><p>在使用最终一致的系统时，如果复制延迟增加到几分钟甚至几小时，则应该考虑应用程序的 行为。如果答案是“没问题”，那很好。但如果结果对于用户来说是不好体验，那么设计系统来 提供更强的保证是很重要的，例如写后读。明明是异步复制却假设复制是同步的，这是很多 麻烦的根源。</p><p>如前所述，应用程序可以提供比底层数据库更强有力的保证，例如通过主库进行某种读取。 但在应用程序代码中处理这些问题是复杂的，容易出错。</p><p>如果应用程序开发人员不必担心微妙的复制问题，并可以信赖他们的数据库“做了正确的事 情”，那该多好呀。这就是事务（transaction）存在的原因：数据库通过事务提供强大的保 证，所以应用程序可以更假简单。</p><p>单节点事务已经存在了很长时间。然而在走向分布式（复制和分区）数据库时，许多系统放 弃了事务。声称事务在性能和可用性上的代价太高，并断言在可扩展系统中最终一致性是不 可避免的。这个叙述有一些道理，但过于简单了，本书其余部分将提出更为细致的观点。第 七章和第九章将回到事务的话题，并讨论一些替代机制。</p><h5 id="单主复制"><a href="#单主复制" class="headerlink" title="单主复制"></a>单主复制</h5><p>客户端将所有写入操作发送到单个节点（领导者），该节点将数据更改事件流发送到其他副 本（追随者）。读取可以在任何副本上执行，但从追随者读取可能是陈旧的。</p><h5 id="多主复制"><a href="#多主复制" class="headerlink" title="多主复制"></a>多主复制</h5><p>客户端发送每个写入到几个领导节点之一，其中任何一个都可以接受写入。领导者将数据更 改事件流发送给彼此以及任何跟随者节点。</p><p>本章到目前为止，我们只考虑使用单个领导者的复制架构。 虽然这是一种常见的方法，但也 有一些有趣的选择。</p><p>基于领导者的复制有一个主要的缺点：只有一个主库，而所有的写入都必须通过它。如果出 于任何原因（例如和主库之间的网络连接中断）无法连接到主库， 就无法向数据库写入。</p><p>基于领导者的复制模型的自然延伸是允许多个节点接受写入。 复制仍然以同样的方式发生： 处理写入的每个节点都必须将该数据更改转发给所有其他节点。 称之为多领导者配置（也称 多主、多活复制）。 在这种情况下，每个领导者同时扮演其他领导者的追随者。</p><h5 id="无主复制"><a href="#无主复制" class="headerlink" title="无主复制"></a>无主复制</h5><p>我们在本章到目前为止所讨论的复制方法 ——单主复制、多主复制——都是这样的想法：客 户端向一个主库发送写请求，而数据库系统负责将写入复制到其他副本。主库决定写入的顺 序，而从库按相同顺序应用主库的写入。</p><p>一些数据存储系统采用不同的方法，放弃主库的概念，并允许任何副本直接接受来自客户端 的写入。最早的一些的复制数据系统是无领导的（leaderless）【1,44】，但是在关系数据库 主导的时代，这个想法几乎已被忘却。在亚马逊将其用于其内部的Dynamo系统vi之后，它再 一次成为数据库的一种时尚架构【37】。（Dynamo不适用于Amazon以外的用户。 令人困惑 的是，AWS提供了一个名为DynamoDB的托管数据库产品，它使用了完全不同的体系结构： 它基于单主程序复制。） Riak，Cassandra和Voldemort是由Dynamo启发的无领导复制模型 的开源数据存储，所以这类数据库也被称为Dynamo风格。</p><h5 id="读写的法定人数"><a href="#读写的法定人数" class="headerlink" title="读写的法定人数"></a>读写的法定人数</h5><p>更一般地说，如果有n个副本，每个写入必须由w节点确认才能被认为是成功的，并且我们必 须至少为每个读取查询r个节点。 （在我们的例子中，$n = 3，w = 2，r = 2$）。只要$w + r&gt; n$，我们期望在读取时获得最新的值，因为r个读取中至少有一个节点是最新的。遵循这些r值，w值的读写称为法定人数（quorum）vii的读和写。【44】 ，你可以认为，r和w是有效读 写所需的最低票数。</p><p>在Dynamo风格的数据库中，参数n，w和r通常是可配置的。一个常见的选择是使n为奇数 （通常为3或5）并设置 $w = r =（n + 1）/ 2$（向上取整）。但是可以根据需要更改数字。例 如，设置$w = n$和$r = 1$的写入很少且读取次数较多的工作负载可能会受益。这使得读取速 度更快，但具有只有一个失败节点导致所有数据库写入失败的缺点。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-96958e54f1ade4dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此，尽管法定人数似乎保证读取返回最新的写入值，但在实践中并不那么简单。 Dynamo 风格的数据库通常针对可以忍受最终一致性的用例进行优化。允许通过参数w和r来调整读取 陈旧值的概率，但把它们当成绝对的保证是不明智的。</p><p>尤其是，通常没有得到“与延迟有关的问题”（读取您的写入，单调读取或一致的前缀读取） 中讨论的保证，因此前面提到的异常可能会发生在应用程序中。更强有力的保证通常需要事 务或共识。我们将在第七章和第九章回到这些话题。</p><h5 id="最后写入为准（丢弃并发写入）"><a href="#最后写入为准（丢弃并发写入）" class="headerlink" title="最后写入为准（丢弃并发写入）"></a>最后写入为准（丢弃并发写入）</h5><p>实现最终融合的一种方法是声明每个副本只需要存储最“最近”的值，并允许“更旧”的值被覆 盖和抛弃。然后，只要我们有一种明确的方式来确定哪个写是“最近的”，并且每个写入最终都 被复制到每个副本，那么复制最终会收敛到相同的值。</p><p>正如“最近”的引号所表明的，这个想法其实颇具误导性。在图5-12的例子中，当客户端向数 据库节点发送写入请求时，客户端都不知道另一个客户端，因此不清楚哪一个先发生了。事 实上，说“发生”是没有意义的：我们说写入是并发（concurrent）的，所以它们的顺序是不确 定的。</p><p>即使写入没有自然的排序，我们也可以强制任意排序。例如，可以为每个写入附加一个时间 戳，挑选最“最近”的最大时间戳，并丢弃具有较早时间戳的任何写入。这种冲突解决算法被 称为最后写入为准（LWW, last write wins），是Cassandra 【53】唯一支持的冲突解决方 法，也是Riak 【35】中的一个可选特征。</p><p>LWW实现了最终收敛的目标，但以持久性为代价：如果同一个Key有多个并发写入，即使它 们都被报告为客户端成功（因为它们被写入 w 个副本），其中一个写道会生存下来，其他的 将被无声丢弃。此外，LWW甚至可能会删除不是并发的写入，我们将在的“有序事件的时间 戳”中讨论。</p><h5 id="版本向量"><a href="#版本向量" class="headerlink" title="版本向量"></a>版本向量</h5><p>图5-13使用单个版本号来捕获操作之间的依赖关系，但是当多个副本并发接受写入时，这是 不够的。相反，除了对每个键使用版本号之外，还需要在每个副本中版本号。每个副本在处 理写入时增加自己的版本号，并且跟踪从其他副本中看到的版本号。这个信息指出了要覆盖哪些值，以及保留哪些值作为兄弟。</p><p>所有副本的版本号集合称为版本向量（version vector）【56】。这个想法的一些变体正在 使用，但最有趣的可能是在Riak 2.0 【58,59】中使用的分散版本矢量（dotted version vector）【57】。我们不会深入细节，但是它的工作方式与我们在购物车示例中看到的非常 相似。</p><p>与图5-13中的版本号一样，当读取值时，版本向量会从数据库副本发送到客户端，并且随后 写入值时需要将其发送回数据库。 （Riak将版本向量编码为一个字符串，它称为因果上下文 （causal context））。版本向量允许数据库区分覆盖写入和并发写入。</p><p>另外，就像在单个副本的例子中，应用程序可能需要合并兄弟。版本向量结构确保从一个副 本读取并随后写回到另一个副本是安全的。这样做可能会创建兄弟，但只要兄弟姐妹合并正 确，就不会丢失数据。</p><h3 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h3><h4 id="分区与复制"><a href="#分区与复制" class="headerlink" title="分区与复制"></a>分区与复制</h4><p>分区通常与复制结合使用，使得每个分区的副本存储在多个节点上。 这意味着，即使每条记 录属于一个分区，它仍然可以存储在多个不同的节点上以获得容错能力。 </p><p>一个节点可能存储多个分区。 如果使用主从复制模型，则分区和复制的组合如图6-1所示。 每个分区领导者(主)被分配给一个节点，追随者(从)被分配给其他节点。 每个节点可能是某些 分区的领导者，同时是其他分区的追随者。 我们在第5章讨论的关于数据库复制的所有内容同 样适用于分区的复制。 大多数情况下，分区方案的选择与复制方案的选择是独立的，为简单 起见，本章中将忽略复制。</p><h4 id="根据键的范围分区"><a href="#根据键的范围分区" class="headerlink" title="根据键的范围分区"></a>根据键的范围分区</h4><p>一种分区的方法是为每个分区指定一块连续的键范围（从最小值到最大值），如纸百科全书 的卷（图6-2）。如果知道范围之间的边界，则可以轻松确定哪个分区包含某个值。如果您还 知道分区所在的节点，那么可以直接向相应的节点发出请求（对于百科全书而言，就像从书 架上选取正确的书籍）。</p><p>键的范围不一定均匀分布，因为数据也很可能不均匀分布。例如在图6-2中，第1卷包含以A 和B开头的单词，但第12卷则包含以T，U，V，X，Y和Z开头的单词。只是简单的规定每个卷 包含两个字母会导致一些卷比其他卷大。为了均匀分配数据，分区边界需要依据数据调整。</p><h4 id="根据键的散列分区"><a href="#根据键的散列分区" class="headerlink" title="根据键的散列分区"></a>根据键的散列分区</h4><p>由于偏斜和热点的风险，许多分布式数据存储使用散列函数来确定给定键的分区。</p><p>一个好的散列函数可以将将偏斜的数据均匀分布。假设你有一个32位散列函数,无论何时给定 一个新的字符串输入，它将返回一个0到$2^{32}$ -1之间的”随机”数。即使输入的字符串非常 相似，它们的散列也会均匀分布在这个数字范围内。</p><p>出于分区的目的，散列函数不需要多么强壮的加密算法：例如，Cassandra和MongoDB使用 MD5，Voldemort使用Fowler-Noll-Vo函数。许多编程语言都有内置的简单哈希函数（它们用 于哈希表），但是它们可能不适合分区：例如，在Java的 Object.hashCode() 和Ruby 的 Object#hash ，同一个键可能在不同的进程中有不同的哈希值【6】。</p><p>一旦你有一个合适的键散列函数，你可以为每个分区分配一个散列范围（而不是键的范 围），每个通过哈希散列落在分区范围内的键将被存储在该分区中。如图6-3所示。</p><h4 id="负载倾斜与消除热点"><a href="#负载倾斜与消除热点" class="headerlink" title="负载倾斜与消除热点"></a>负载倾斜与消除热点</h4><p>如今，大多数数据系统无法自动补偿这种高度偏斜的负载，因此应用程序有责任减少偏斜。 例如，如果一个主键被认为是非常火爆的，一个简单的方法是在主键的开始或结尾添加一个 随机数。只要一个两位数的十进制随机数就可以将主键分散为100钟不同的主键,从而存储在 不同的分区中。</p><p>然而，将主键进行分割之后，任何读取都必须要做额外的工作，因为他们必须从所有100个 主键分布中读取数据并将其合并。此技术还需要额外的记录：只需要对少量热点附加随机数; 对于写入吞吐量低的绝大多数主键来是不必要的开销。因此，您还需要一些方法来跟踪哪些 键需要被分割。</p><p>也许在将来，数据系统将能够自动检测和补偿偏斜的工作负载；但现在，您需要自己来权 衡。</p><h4 id="分片与次级索引"><a href="#分片与次级索引" class="headerlink" title="分片与次级索引"></a>分片与次级索引</h4><p>次级索引的问题是它们不能整齐地映射到分区。有两种用二级索引对数据库进行分区的方 法：基于文档的分区（document-based）和基于关键词（term-based）的分区。</p><h4 id="分区再平衡"><a href="#分区再平衡" class="headerlink" title="分区再平衡"></a>分区再平衡</h4><p>随着时间的推移，数据库会有各种变化。</p><ul><li>查询吞吐量增加，所以您想要添加更多的CPU来处理负载。</li><li>数据集大小增加，所以您想添加更多的磁盘和RAM来存储它。</li><li>机器出现故障，其他机器需要接管故障机器的责任。</li></ul><p>所有这些更改都需要数据和请求从一个节点移动到另一个节点。 将负载从集群中的一个节点 向另一个节点移动的过程称为再平衡（reblancing）。</p><ul><li>再平衡之后，负载（数据存储，读取和写入请求）应该在集群中的节点之间公平地共 享。</li><li>再平衡发生时，数据库应该继续接受读取和写入。</li><li>节点之间只移动必须的数据，以便快速再平衡，并减少网络和磁盘I/O负载。</li></ul><h5 id="反面教材：hash-mod-N"><a href="#反面教材：hash-mod-N" class="headerlink" title="反面教材：hash mod N"></a>反面教材：hash mod N</h5><p>也许你想知道为什么我们不使用mod（许多编程语言中的％运算符）。例如， hash(key) mod 10 会返回一个介于0和9之间的数字（如果我们将散列写为十进制数，散列模10将是最后一个 数字）。如果我们有10个节点，编号为0到9，这似乎是将每个键分配给一个节点的简单方 法。</p><p>模$N$方法的问题是，如果节点数量N发生变化，大多数密钥将需要从一个节点移动到另一个 节点。例如，假设$hash(key)=123456$。如果最初有10个节点，那么这个键一开始放在节点 6上（因为$123456\ mod\ 10 = 6$）。当您增长到11个节点时，密钥需要移动到节点 3（$123456\ mod\ 11 = 3$），当您增长到12个节点时，需要移动到节点0（$123456\ mod\ 12 = 0$）。这种频繁的举动使得重新平衡过于昂贵。</p><h5 id="固定数量的分区"><a href="#固定数量的分区" class="headerlink" title="固定数量的分区"></a>固定数量的分区</h5><p>幸运的是，有一个相当简单的解决方案：创建比节点更多的分区，并为每个节点分配多个分 区。例如，运行在10个节点的集群上的数据库可能会从一开始就被拆分为1,000个分区，因此 大约有100个分区被分配给每个节点。</p><p>现在，如果一个节点被添加到集群中，新节点可以从当前每个节点中窃取一些分区，直到分 区再次公平分配。这个过程如图6-6所示。如果从集群中删除一个节点，则会发生相反的情况。</p><p>只有分区在节点之间的移动。分区的数量不会改变，键所指定的分区也不会改变。唯一改变 的是分区所在的节点。这种变更并不是即时的 — 在网络上传输大量的数据需要一些时间 — 所以在传输过程中，原有分区仍然会接受读写操作。</p><h5 id="动态分区"><a href="#动态分区" class="headerlink" title="动态分区"></a>动态分区</h5><p>动态分区的一个优点是分区数量适应总数据量。如果只有少量的数据，少量的分区就足够 了，所以开销很小;如果有大量的数据，每个分区的大小被限制在一个可配置的最大值</p><p>需要注意的是，一个空的数据库从一个分区开始，因为没有关于在哪里绘制分区边界的先验 信息。数据集开始时很小，直到达到第一个分区的分割点，所有写入操作都必须由单个节点 处理，而其他节点则处于空闲状态。为了解决这个问题，HBase和MongoDB允许在一个空的 数据库上配置一组初始分区（这被称为预分割（pre-splitting））。在键范围分区的情况中， 预分割需要提前知道键是如何进行分配的【4,26】。</p><p>动态分区不仅适用于数据的范围分区，而且也适用于散列分区。从版本2.4开始，MongoDB 同时支持范围和哈希分区，并且都是进行动态分割分区。</p><h5 id="按节点比例分区"><a href="#按节点比例分区" class="headerlink" title="按节点比例分区"></a>按节点比例分区</h5><p>Cassandra和Ketama使用的第三种方法是使分区数与节点数成正比——换句话说，每个节点 具有固定数量的分区【23,27,28】。在这种情况下，每个分区的大小与数据集大小成比例地增 长，而节点数量保持不变，但是当增加节点数时，分区将再次变小。由于较大的数据量通常 需要较大数量的节点进行存储，因此这种方法也使每个分区的大小较为稳定。</p><p>当一个新节点加入集群时，它随机选择固定数量的现有分区进行拆分，然后占有这些拆分分 区中每个分区的一半，同时将每个分区的另一半留在原地。随机化可能会产生不公平的分 割，但是平均在更大数量的分区上时（在Cassandra中，默认情况下，每个节点有256个分 区），新节点最终从现有节点获得公平的负载份额。 Cassandra 3.0引入了另一种再分配的算 法来避免不公平的分割【29】。</p><p>随机选择分区边界要求使用基于散列的分区（可以从散列函数产生的数字范围中挑选边 界）。实际上，这种方法最符合一致性哈希的原始定义【7】（参阅“一致性哈希”）。最新的 哈希函数可以在较低元数据开销的情况下达到类似的效果【8】。</p><h5 id="运维：手动还是自动平衡"><a href="#运维：手动还是自动平衡" class="headerlink" title="运维：手动还是自动平衡"></a>运维：手动还是自动平衡</h5><p>在全自动重新平衡（系统自动决定何时将分区从一个节点移动到另一个节点，无须人工干 预）和完全手动（分区指派给节点由管理员明确配置，仅在管理员明确重新配置时才会更 改）之间有一个权衡。例如，Couchbase，Riak和Voldemort会自动生成建议的分区分配，但 需要管理员提交才能生效。</p><p>全自动重新平衡可以很方便，因为正常维护的操作工作较少。但是，这可能是不可预测的。 再平衡是一个昂贵的操作，因为它需要重新路由请求并将大量数据从一个节点移动到另一个 节点。如果没有做好，这个过程可能会使网络或节点负载过重，降低其他请求的性能。</p><p>这种自动化与自动故障检测相结合可能十分危险。例如，假设一个节点过载，并且对请求的 响应暂时很慢。其他节点得出结论：过载的节点已经死亡，并自动重新平衡集群，使负载离 开它。这会对已经超负荷的节点，其他节点和网络造成额外的负载，从而使情况变得更糟， 并可能导致级联失败。</p><p>出于这个原因，再平衡的过程中有人参与是一件好事。这比完全自动的过程慢，但可以帮助 防止运维意外。</p><h4 id="请求路由"><a href="#请求路由" class="headerlink" title="请求路由"></a>请求路由</h4><p>现在我们已经将数据集分割到多个机器上运行的多个节点上。但是仍然存在一个悬而未决的 问题：当客户想要发出请求时，如何知道要连接哪个节点？随着分区重新平衡，分区对节点 的分配也发生变化。为了回答这个问题，需要有人知晓这些变化：如果我想读或写键“foo”， 需要连接哪个IP地址和端口号？</p><p>这个问题可以概括为 服务发现(service discovery) ，它不仅限于数据库。任何可通过网络访 问的软件都有这个问题，特别是如果它的目标是高可用性（在多台机器上运行冗余配置）。 许多公司已经编写了自己的内部服务发现工具，其中许多已经作为开源发布【30】。</p><p>概括来说，这个问题有几种不同的方案（如图6-7所示）:</p><ul><li>允许客户联系任何节点（例如，通过循环策略的负载均衡（Round-Robin Load Balancer））。如果该节点恰巧拥有请求的分区，则它可以直接处理该请求;否则，它将 请求转发到适当的节点，接收回复并传递给客户端。</li><li> 首先将所有来自客户端的请求发送到路由层，它决定了应该处理请求的节点，并相应地 转发。此路由层本身不处理任何请求；它仅负责分区的负载均衡。</li><li> 要求客户端知道分区和节点的分配。在这种情况下，客户端可以直接连接到适当的节 点，而不需要任何中介。</li></ul><p>许多分布式数据系统都依赖于一个独立的协调服务，比如ZooKeeper来跟踪集群元数据，如 图6-8所示。 每个节点在ZooKeeper中注册自己，ZooKeeper维护分区到节点的可靠映射。 其 他参与者（如路由层或分区感知客户端）可以在ZooKeeper中订阅此信息。 只要分区分配发 生的改变，或者集群中添加或删除了一个节点，ZooKeeper就会通知路由层使路由信息保持 最新状态。</p><h3 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h3><p>数十年来，事务（transaction） 一直是简化这些问题的首选机制。事务是应用程序将多个 读写操作组合成一个逻辑单元的一种方式。从概念上讲，事务中的所有读写操作被视作单个 操作来执行：整个事务要么成功（提交（commit））要么失败（中止（abort），回滚 （rollback））。如果失败，应用程序可以安全地重试。对于事务来说，应用程序的错误处理 变得简单多了，因为它不用再担心部分失败的情况了，即某些操作成功，某些失败（无论出 于何种原因）。</p><p>和事务打交道时间长了，你可能会觉得它显而易见。但我们不应将其视为理所当然。事务不 自然法；它们是为了简化应用编程模型而创建的。通过使用事务，应用程序可以自由地忽略 某些潜在的错误情况和并发问题，因为数据库会替应用处理好这些。（我们称之为安全保证 （safety guarantees））。</p><h4 id="ACID的含义"><a href="#ACID的含义" class="headerlink" title="ACID的含义"></a>ACID的含义</h4><p>事务所提供的安全保证，通常由众所周知的首字母缩略词ACID来描述，ACID代表原子性 （Atomicity），一致性（Consistency），隔离性（Isolation）和持久性（Durability）。 它由TheoHärder和Andreas Reuter于1983年创建，旨在为数据库中的容错机制建立精确的术 语。</p><p>但实际上，不同数据库的ACID实现并不相同。例如，我们将会看到，围绕着隔离性 （Isolation） 的含义有许多含糊不清【8】。高层次上的想法是合理的，但魔鬼隐藏在细节 里。今天，当一个系统声称自己“符合ACID”时，实际上能期待的是什么保证并不清楚。不幸 的是，ACID现在几乎已经变成了一个营销术语。 </p><p>（不符合ACID标准的系统有时被称为BASE，它代表基本可用性（Basically Available）， 软状态（Soft State）和最终一致性（Eventual consistency）【9】，这比ACID的定义更加 模糊，似乎BASE的唯一合理的定义是“不是ACID”，即它几乎可以代表任何你想要的东西。） </p><p>让我们深入了解原子性，一致性，隔离性和持久性的定义，这可以让我们提炼出事务的思想。</p><h4 id="两阶段锁定（2PL）"><a href="#两阶段锁定（2PL）" class="headerlink" title="两阶段锁定（2PL）"></a>两阶段锁定（2PL）</h4><p>大约30年来，在数据库中只有一种广泛使用的序列化算法：两阶段锁定（2PL，two-phase locking）</p><p>请注意，虽然两阶段锁定（2PL）听起来非常类似于两阶段提交（2PC），但它们是完全 不同的东西。我们将在第9章讨论2PC。</p><p>之前我们看到锁通常用于防止脏写（参阅“没有脏写”一节）：如果两个事务同时尝试写入同一 个对象，则锁可确保第二个写入必须等到第一个写入完成事务（中止或提交），然后才能继续。</p><p>两阶段锁定定类似，但使锁的要求更强。只要没有写入，就允许多个事务同时读取同一个对 象。但对象只要有写入（修改或删除），就需要独占访问（exclusive access） 权限：</p><ul><li>如果事务A读取了一个对象，并且事务B想要写入该对象，那么B必须等到A提交或中止才 能继续。 （这确保B不能在A底下意外地改变对象。）</li><li>如果事务A写入了一个对象，并且事务B想要读取该对象，则B必须等到A提交或中止才能 继续。 （像图7-1那样读取旧版本的对象在2PL下是不可接受的。）</li></ul><p>在2PL中，写入不仅会阻塞其他写入，也会阻塞读，反之亦然。快照隔离使得读不阻塞写，写也不阻塞读（参阅“实现快照隔离”），这是2PL和快照隔离之间的关键区别。另一方面，因为 2PL提供了可序列化的性质，它可以防止早先讨论的所有竞争条件，包括丢失更新和写入偏差。</p><h4 id="实现两阶段锁"><a href="#实现两阶段锁" class="headerlink" title="实现两阶段锁"></a>实现两阶段锁</h4><p>2PL用于MySQL（InnoDB）和SQL Server中的可序列化隔离级别，以及DB2中的可重复读隔 离级别【23,36】。</p><p>读与写的阻塞是通过为数据库中每个对象添加锁来实现的。锁可以处于共享模式（shared mode）或独占模式（exclusive mode）。锁使用如下：</p><ul><li>若事务要读取对象，则须先以共享模式获取锁。允许多个事务同时持有共享锁。但如果 另一个事务已经在对象上持有排它锁，则这些事务必须等待。</li><li>若事务要写入一个对象，它必须首先以独占模式获取该锁。没有其他事务可以同时持有 锁（无论是共享模式还是独占模式），所以如果对象上存在任何锁，该事务必须等待。</li><li>如果事务先读取再写入对象，则它可能会将其共享锁升级为独占锁。升级锁的工作与直 接获得排他锁相同。</li><li>事务获得锁之后，必须继续持有锁直到事务结束（提交或中止）。这就是“两阶段”这个名 字的来源：第一阶段（当事务正在执行时）获取锁，第二阶段（在事务结束时）释放所有的锁。</li></ul><p>由于使用了这么多的锁，因此很可能会发生：事务A等待事务B释放它的锁，反之亦然。这种 情况叫做死锁（Deadlock）。数据库会自动检测事务之间的死锁，并中止其中一个，以便另一个继续执行。被中止的事务需要由应用程序重试。</p><h4 id="两阶段锁定的性"><a href="#两阶段锁定的性" class="headerlink" title="两阶段锁定的性"></a>两阶段锁定的性</h4><p>两阶段锁定的巨大缺点，以及70年代以来没有被所有人使用的原因，是其性能问题。两阶段 锁定下的事务吞吐量与查询响应时间要比弱隔离级别下要差得多。</p><p>这一部分是由于获取和释放所有这些锁的开销，但更重要的是由于并发性的降低。按照设计，如果两个并发事务试图做任何可能导致竞争条件的事情，那么必须等待另一个完成。</p><p>传统的关系数据库不限制事务的持续时间，因为它们是为等待人类输入的交互式应用而设计 的。因此，当一个事务需要等待另一个事务时，等待的时长并没有限制。即使你保证所有的 事务都很短，如果有多个事务想要访问同一个对象，那么可能会形成一个队列，所以事务可 能需要等待几个其他事务才能完成。</p><p>因此，运行2PL的数据库可能具有相当不稳定的延迟，如果在工作负载中存在争用，那么可能 高百分位点处的响应会非常的慢（参阅“描述性能”）。可能只需要一个缓慢的事务，或者一个 访问大量数据并获取许多锁的事务，就能把系统的其他部分拖慢，甚至迫使系统停机。当需 要稳健的操作时，这种不稳定性是有问题的。</p><p>基于锁实现的读已提交隔离级别可能发生死锁，但在基于2PL实现的可序列化隔离级别中，它 们会出现的频繁的多（取决于事务的访问模式）。这可能是一个额外的性能问题：当事务由 于死锁而被中止并被重试时，它需要从头重做它的工作。如果死锁很频繁，这可能意味着巨 大的浪费。</p><h3 id="分布式系统的麻烦"><a href="#分布式系统的麻烦" class="headerlink" title="分布式系统的麻烦"></a>分布式系统的麻烦</h3><p>本章对分布式系统中可能出现的问题进行彻底的悲观和沮丧的总结。 我们将研究网络的问题 （“无法访问的网络”）; 时钟和时序问题（“不可靠时钟”）; 我们将讨论他们可以避免的程度。 所有这些问题的后果都是困惑的，所以我们将探索如何思考一个分布式系统的状态，以及如 何推理发生的事情（“知识，真相和谎言”）。</p><h4 id="从不可靠的组件构建可靠的系统"><a href="#从不可靠的组件构建可靠的系统" class="headerlink" title="从不可靠的组件构建可靠的系统"></a>从不可靠的组件构建可靠的系统</h4><p>您可能想知道这是否有意义——直观地看来，系统只能像其最不可靠的组件（最薄弱的 环节）一样可靠。事实并非如此：事实上，从不太可靠的潜在基础构建更可靠的系统是 计算机领域的一个古老思想【11】。例如：</p><ul><li>纠错码允许数字数据在通信信道上准确传输，偶尔会出现一些错误，例如由于无线 网络上的无线电干扰【12】。</li><li>互联网协议（Internet Protocol, IP）不可靠：可能丢弃，延迟，复制或重排数据包。 传输控制协议（Transmission Control Protocol, TCP）在互联网协议（IP）之 上提供了更可靠的传输层：它确保丢失的数据包被重新传输，消除重复，并且数据 包被重新组装成它们被发送的顺序。</li></ul><p>虽然这个系统可以比它的底层部分更可靠，但它的可靠性总是有限的。例如，纠错码可 以处理少量的单比特错误，但是如果你的信号被干扰所淹没，那么通过信道可以得到多 少数据，是有根本性的限制的【13】。 TCP可以隐藏数据包的丢失，重复和重新排序， 但是它不能神奇地消除网络中的延迟。</p><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>在本章中，我们讨论了分布式系统中可能发生的各种问题，包括：</p><ul><li>当您尝试通过网络发送数据包时，数据包可能会丢失或任意延迟。同样，答复可能会丢 失或延迟，所以如果你没有得到答复，你不知道消息是否通过。</li><li>节点的时钟可能会与其他节点显着不同步（尽管您尽最大努力设置NTP），它可能会突 然跳转或跳回，依靠它是很危险的，因为您很可能没有好的测量你的时钟的错误间隔。</li><li>一个进程可能会在其执行的任何时候暂停一段相当长的时间（可能是因为世界上的垃圾 收集器），被其他节点宣告死亡，然后再次复活，却没有意识到它被暂停了。</li></ul><p>这类部分失效可能发生的事实是分布式系统的决定性特征。每当软件试图做任何涉及其他节 点的事情时，偶尔就有可能会失败，或者随机变慢，或者根本没有响应（最终超时）。在分 布式系统中，我们试图在软件中建立部分失效的容错机制，这样整个系统即使在某些组成部 分被破坏的情况下，也可以继续运行。</p><p>为了容忍错误，第一步是检测它们，但即使这样也很难。大多数系统没有检测节点是否发生 故障的准确机制，所以大多数分布式算法依靠超时来确定远程节点是否仍然可用。但是，超 时无法区分网络失效和节点失效，并且可变的网络延迟有时会导致节点被错误地怀疑发生故障。此外，有时一个节点可能处于降级状态：例如，由于驱动程序错误【94】，千兆网卡可 能突然下降到1 Kb/s的吞吐量。这样一个“跛行”而不是死掉的节点可能比一个干净的失效节点 更难处理。</p><p>一旦检测到故障，使系统容忍它也并不容易：没有全局变量，没有共享内存，没有共同的知 识，或机器之间任何其他种类的共享状态。节点甚至不能就现在是什么时间达成一致，就不 用说更深奥的了。信息从一个节点流向另一个节点的唯一方法是通过不可靠的网络发送信 息。重大决策不能由一个节点安全地完成，因此我们需要一个能从其他节点获得帮助的协 议，并争取达到法定人数以达成一致。</p><p>如果你习惯于在理想化的数学完美（同一个操作总能确定地返回相同的结果）的单机环境中 编写软件，那么转向分布式系统的凌乱的物理现实可能会有些令人震惊。相反，如果能够在 单台计算机上解决一个问题，那么分布式系统工程师通常会认为这个问题是平凡的【5】，现 在单个计算机确实可以做很多事情【95】。如果你可以避免打开潘多拉的盒子，把东西放在 一台机器上，那么通常是值得的。</p><p>但是，正如在第二部分的介绍中所讨论的那样，可扩展性并不是使用分布式系统的唯一原 因。容错和低延迟（通过将数据放置在距离用户较近的地方）是同等重要的目标，而这些不 能用单个节点实现。</p><p>在本章中，我们也转换了几次话题，探讨了网络，时钟和进程的不可靠性是否是不可避免的 自然规律。我们看到这并不是：有可能给网络提供硬实时的响应保证和有限的延迟，但是这 样做非常昂贵，且导致硬件资源的利用率降低。大多数非安全关键系统会选择便宜而不可 靠，而不是昂贵和可靠。</p><p>我们还谈到了超级计算机，它们采用可靠的组件，因此当组件发生故障时必须完全停止并重 新启动。相比之下，分布式系统可以永久运行而不会在服务层面中断，因为所有的错误和维 护都可以在节点级别进行处理——至少在理论上是如此。 （实际上，如果一个错误的配置变 更被应用到所有的节点，仍然会使分布式系统瘫痪）。</p><h3 id="一致性与共识"><a href="#一致性与共识" class="headerlink" title="一致性与共识"></a>一致性与共识</h3><p>现在我们将继续沿着同样的路线前进，寻求可以让应用忽略分布式系统部分问题的抽象概 念。例如，分布式系统最重要的抽象之一就是共识（consensus）：就是让所有的节点对某 件事达成一致。正如我们在本章中将会看到的那样，尽管存在网络故障和流程故障，可靠地 达成共识是一个令人惊讶的棘手问题。</p><p>一旦达成共识，应用可以将其用于各种目的。例如，假设你有一个单主复制的数据库。如果 主库挂点，并且需要故障转移到另一个节点，剩余的数据库节点可以使用共识来选举新的领 导者。正如在“处理节点宕机”中所讨论的那样，重要的是只有一个领导者，且所有的节点都认 同其领导。如果两个节点都认为自己是领导者，这种情况被称为脑裂（split brain），且经常 导致数据丢失。正确实现共识有助于避免这种问题。</p><h4 id="一致性保证"><a href="#一致性保证" class="headerlink" title="一致性保证"></a>一致性保证</h4><p>大多数复制的数据库至少提供了最终一致性，这意味着如果你停止向数据库写入数据并等待 一段不确定的时间，那么最终所有的读取请求都会返回相同的值【1】。换句话说，不一致性 是暂时的，最终会自行解决（假设网络中的任何故障最终都会被修复）。最终一致性的一个 更好的名字可能是收敛（convergence），因为我们预计所有的复本最终会收敛到相同的值 【2】。</p><p>然而，这是一个非常弱的保证 —— 它并没有说什么什么时候副本会收敛。在收敛之前，读操 作可能会返回任何东西或什么都没有【1】。例如，如果你写入了一个值，然后立即再次读 取，这并不能保证你能看到刚跟写入的值，因为读请求可能会被路由到另外的副本上。（参 阅“读己之写” ）。</p><h4 id="线性一致性"><a href="#线性一致性" class="headerlink" title="线性一致性"></a>线性一致性</h4><p>这就是线性一致性（linearizability）背后的想法【6】（也称为原子一致性（atomic consistency）【7】，强一致性（strong consistency），立即一致性（immediate consistency）或外部一致性（external consistency ）【8】）。线性一致性的精确定义相 当微妙，我们将在本节的剩余部分探讨它。但是基本的想法是让一个系统看起来好像只有一 个数据副本，而且所有的操作都是原子性的。有了这个保证，即使实际中可能有多个副本， 应用也不需要担心它们。</p><h4 id="锁定和领导选举"><a href="#锁定和领导选举" class="headerlink" title="锁定和领导选举"></a>锁定和领导选举</h4><p>一个使用单主复制的系统，需要确保领导真的只有一个，而不是几个（脑裂）。一种选择领 导者的方法是使用锁：每个节点在启动时尝试获取锁，成功者成为领导者【14】。不管这个 锁是如何实现的，它必须是线性一致的：所有节点必须就哪个节点拥有锁达成一致，否则就 没用了。</p><h4 id="实现线性一致的系统"><a href="#实现线性一致的系统" class="headerlink" title="实现线性一致的系统"></a>实现线性一致的系统</h4><h5 id="单主复制（可能线性一致）"><a href="#单主复制（可能线性一致）" class="headerlink" title="单主复制（可能线性一致）"></a>单主复制（可能线性一致）</h5><p>在具有单主复制功能的系统中（参见“领导者与追随者”），主库具有用于写入的数据的主副 本，而追随者在其他节点上保留数据的备份副本。如果从主库或同步更新的从库读取数据， 它们可能（protential）是线性一致性的iv。然而，并不是每个单主数据库都是实际线性一致 性的，无论是通过设计（例如，因为使用快照隔离）还是并发错误【10】。</p><p>从主库读取依赖一个假设，你确定领导是谁。正如在“真理在多数人手中”中所讨论的那样， 一个节点很可能会认为它是领导者，而事实上并非如此——如果具有错觉的领导者继续为请 求提供服务，可能违反线性一致性【20】。使用异步复制，故障转移时甚至可能会丢失已提 交的写入（参阅“处理节点宕机”），这同时违反了持久性和线性一致性。</p><h4 id="共识算法（线性一致）"><a href="#共识算法（线性一致）" class="headerlink" title="共识算法（线性一致）"></a>共识算法（线性一致）</h4><p>一些在本章后面讨论的共识算法，与单领导者复制类似。然而，共识协议包含防止脑裂和陈 旧副本的措施。由于这些细节，共识算法可以安全地实现线性一致性存储。例如，Zookeeper 【21】和etcd 【22】就是这样工作的。</p><h5 id="多主复制（非线性一致）"><a href="#多主复制（非线性一致）" class="headerlink" title="多主复制（非线性一致）"></a>多主复制（非线性一致）</h5><p>具有多主程序复制的系统通常不是线性一致的，因为它们同时在多个节点上处理写入，并将 其异步复制到其他节点。因此，它们可能会产生冲突的写入，需要解析（参阅“处理写入冲 突”）。这种冲突是因为缺少单一数据副本人为产生的。</p><h5 id="无主复制（也许不是线性一致的）"><a href="#无主复制（也许不是线性一致的）" class="headerlink" title="无主复制（也许不是线性一致的）"></a>无主复制（也许不是线性一致的）</h5><p>对于无领导者复制的系统（Dynamo风格；参阅“无主复制”），有时候人们会声称通过要求法 定人数读写（ $w + r&gt; n$ ）可以获得“强一致性”。这取决于法定人数的具体配置，以及强一 致性如何定义（通常不完全正确）。</p><h4 id="基于时钟"><a href="#基于时钟" class="headerlink" title="基于时钟"></a>基于时钟</h4><p>基于时钟（例如，在Cassandra中；参见“依赖同步时钟”）的“最后写入胜利”冲突解决方法几 乎可以确定是非线性的，由于时钟偏差，不能保证时钟的时间戳与实际事件顺序一致。松散 的法定人数也破坏了线性一致的可能性。即使使用严格的法定人数，非线性一致的行为也是 可能的，如下节所示。</p><h4 id="线性一致性和法定人数"><a href="#线性一致性和法定人数" class="headerlink" title="线性一致性和法定人数"></a>线性一致性和法定人数</h4><p>直觉上在Dynamo风格的模型中，严格的法定人数读写应该是线性一致性的。但是当我们有 可变的网络延迟时，就可能存在竞争条件，如图9-6所示。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-add67a83f68b2f62.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>总而言之，最安全的做法是：假设采用Dynamo风格无主复制的系统不能提供线性一致性。</p><h4 id="CAP定理"><a href="#CAP定理" class="headerlink" title="CAP定理"></a>CAP定理</h4><p>CAP有时以这种面目出现：一致性，可用性和分区容忍：三者只能择其二。不幸的是这 种说法很有误导性【32】，因为网络分区是一种错误，所以它并不是一个选项：不管你 喜不喜欢它都会发生【38】</p><p>在网络正常工作的时候，系统可以提供一致性（线性一致性）和整体可用性。发生网络 故障时，你必须在线性一致性和整体可用性之间做出选择。因此，一个更好的表达CAP 的方法可以是一致的，或者在分区时可用【39】。一个更可靠的网络需要减少这个选 择，但是在某些时候选择是不可避免的。</p><p>在CAP的讨论中，术语可用性有几个相互矛盾的定义，形式化作为一个定理【30】并不 符合其通常的含义【40】。许多所谓的“高可用”（容错）系统实际上不符合CAP对可用 性的特殊定义。总而言之，围绕着CAP有很多误解和困惑，并不能帮助我们更好地理解 系统，所以最好避免使用CAP。</p><h4 id="线性一致性和网络延迟"><a href="#线性一致性和网络延迟" class="headerlink" title="线性一致性和网络延迟"></a>线性一致性和网络延迟</h4><p>虽然线性一致是一个很有用的保证，但实际上，线性一致的系统惊人的少。例如，现代多核 CPU上的内存甚至都不是线性一致的【43】：如果一个CPU核上运行的线程写入某个内存地 址，而另一个CPU核上运行的线程不久之后读取相同的地址，并没有保证一定能一定读到第 一个线程写入的值（除非使用了内存屏障（memory barrier）或围栏（fence）</p><h4 id="因果顺序不是全序的"><a href="#因果顺序不是全序的" class="headerlink" title="因果顺序不是全序的"></a>因果顺序不是全序的</h4><p>全序（total order）允许任意两个元素进行比较，所以如果有两个元素，你总是可以说出哪 个更大，哪个更小。例如，自然数集是全序的：给定两个自然数，比如说5和13，那么你可以 告诉我，13大于5。</p><p>然而数学集合并不完全是全序的： {a, b} 比 {b, c} 更大吗？好吧，你没法真正比较它 们，因为二者都不是对方的子集。我们说它们是无法比较（incomparable）的，因此数学集 合是偏序（partially order）的：在某些情况下，可以说一个集合大于另一个（如果一个集合 包含另一个集合的所有元素），但在其他情况下它们是无法比较的 。</p><p>全序和偏序之间的差异反映在不同的数据库一致性模型中</p><p>线性一致性：</p><p>在线性一致的系统中，操作是全序的：如果系统表现的就好像只有一个数据副本，并且所有 操作都是原子性的，这意味着对任何两个操作，我们总是能判定哪个操作先发生。这个全序 图9-4中以时间线表示。</p><p>因果性：</p><p>我们说过，如果两个操作都没有在彼此之前发生，那么这两个操作是并发的（参阅“此前发 生”的关系和并发）。换句话说，如果两个事件是因果相关的（一个发生在另一个事件之 前），则它们之间是有序的，但如果它们是并发的，则它们之间的顺序是无法比较的。这意 味着因果关系定义了一个偏序，而不是一个全序：一些操作相互之间是有顺序的，但有些则 是无法比较的。 </p><p>因此，根据这个定义，在线性一致的数据存储中是不存在并发操作的：必须有且仅有一条时 间线，所有的操作都在这条时间线上，构成一个全序关系。可能有几个请求在等待处理，但 是数据存储确保了每个请求都是在唯一时间线上的某个时间点自动处理的，不存在任何并 发。</p><p>并发意味着时间线会分岔然后合并 —— 在这种情况下，不同分支上的操作是无法比较的（即 并发操作）。在第五章中我们看到了这种现象：例如，图5-14 并不是一条直线的全序关系， 而是一堆不同的操作并发进行。图中的箭头指明了因果依赖 —— 操作的偏序</p><h4 id="线性一致性强于因果一致性"><a href="#线性一致性强于因果一致性" class="headerlink" title="线性一致性强于因果一致性"></a>线性一致性强于因果一致性</h4><p>那么因果顺序和线性一致性之间的关系是什么？答案是线性一致性隐含着（implies）因果关 系：任何线性一致的系统都能正确保持因果性【7】。特别是，如果系统中有多个通信通道 （如图9-5 中的消息队列和文件存储服务），线性一致性可以自动保证因果性，系统无需任何 特殊操作（如在不同组件间传递时间戳）</p><p>线性一致性确保因果性的事实使线性一致系统变得简单易懂，更有吸引力。然而，正如“线性 一致性的代价”中所讨论的，使系统线性一致可能会损害其性能和可用性，尤其是在系统具有 严重的网络延迟的情况下（例如，如果系统在地理上散布）。出于这个原因，一些分布式数 据系统已经放弃了线性一致性，从而获得更好的性能，但它们用起来也更为困难。</p><p>好消息是存在折衷的可能性。线性一致性并不是保持因果性的唯一途径 —— 还有其他方法。 一个系统可以是因果一致的，而无需承担线性一致带来的性能折损（尤其对于CAP定理不适 用的情况）。实际上在所有的不会被网络延迟拖慢的一致性模型中，因果一致性是可行的最 强的一致性模型。而且在网络故障时仍能保持可用【2,42】。</p><p>在许多情况下，看上去需要线性一致性的系统，实际上需要的只是因果一致性，因果一致性 可以更高效地实现。基于这种观察结果，研究人员正在探索新型的数据库，既能保证因果一 致性，且性能与可用性与最终一致的系统类似【49,50,51】。</p><h4 id="捕获因果关系"><a href="#捕获因果关系" class="headerlink" title="捕获因果关系"></a>捕获因果关系</h4><p>为了维持因果性，你需要知道哪个操作发生在哪个其他操作之前（happened before）。这 是一个偏序：并发操作可以以任意顺序进行，但如果一个操作发生在另一个操作之前，那它 们必须在所有副本上以那个顺序被处理。因此，当一个副本处理一个操作时，它必须确保所 有因果前驱的操作（之前发生的所有操作）已经被处理；如果前面的某个操作丢失了，后面 的操作必须等待，直到前面的操作被处理完毕。</p><p>为了确定因果依赖，我们需要一些方法来描述系统中节点的“知识”。如果节点在发出写入Y 的 请求时已经看到了 X的值，则 X 和 Y 可能存在因果关系。这个分析使用了那些在欺诈指控刑 事调查中常见的问题：CEO在做出决定 Y 时是否知道 X ？</p><p>用于确定哪些操作发生在其他操作之前 的技术，与我们在“检测并发写入”中所讨论的内容类 似。那一节讨论了无领导者数据存储中的因果性：为了防止丢失更新，我们需要检测到对同 一个键的并发写入。因果一致性则更进一步：它需要跟踪整个数据库中的因果依赖，而不仅 仅是一个键。可以推广版本向量以解决此类问题【54】。</p><p>为了确定因果顺序，数据库需要知道应用读取了哪个版本的数据。这就是为什么在 图5-13 中，来自先前操作的版本号在写入时被传回到数据库的原因。在SSI 的冲突检测中会出现类似 的想法，如“可序列化的快照隔离（SSI）”中所述：当事务要提交时，数据库将检查它所读取 的数据版本是否仍然是最新的。为此，数据库跟踪哪些数据被哪些事务所读取。</p><h4 id="序列号顺序"><a href="#序列号顺序" class="headerlink" title="序列号顺序"></a>序列号顺序</h4><p>虽然因果是一个重要的理论概念，但实际上跟踪所有的因果关系是不切实际的。在许多应用 中，客户端在写入内容之前会先读取大量数据，我们无法弄清写入因果依赖于先前全部的读 取内容，还是仅包括其中一部分。显式跟踪所有已读数据意味着巨大的额外开销。</p><p>但还有一个更好的方法：我们可以使用序列号（sequence nunber）或时间戳 （timestamp）来排序事件。时间戳不一定来自时钟（或物理时钟，存在许多问题，如 “不可 靠时钟” 中所述）。它可以来自一个逻辑时钟（logical clock），这是一个用来生成标识操作 的数字序列的算法，典型实现是使用一个每次操作自增的计数器。</p><p>这样的序列号或时间戳是紧凑的（只有几个字节大小），它提供了一个全序关系：也就是说 每操作都有一个唯一的序列号，而且总是可以比较两个序列号，确定哪一个更大（即哪些操 作后发生）。</p><p>特别是，我们可以使用与因果一致（consistent with causality）的全序来生成序列号vii： 我们保证，如果操作 A 因果后继于操作 B，那么在这个全序中 A 在 B 前（ A 具有比 B 更小的 序列号）。并行操作之间可以任意排序。这样一个全序关系捕获了所有关于因果的信息，但 也施加了一个比因果性要求更为严格的顺序。</p><h4 id="全序广播"><a href="#全序广播" class="headerlink" title="全序广播"></a>全序广播</h4><p>如果你的程序只运行在单个CPU核上，那么定义一个操作全序是很容易的：可以简单地就是 CPU执行这些操作的顺序。但是在分布式系统中，让所有节点对同一个全局操作顺序达成一 致可能相当棘手。在上一节中，我们讨论了按时间戳或序列号进行排序，但发现它还不如单 主复制给力（如果你使用时间戳排序来实现唯一性约束，而且不能容忍任何错误）。</p><p>如前所述，单主复制通过选择一个节点作为主库来确定操作的全序，并在主库的单个CPU核 上对所有操作进行排序。接下来的挑战是，如果吞吐量超出单个主库的处理能力，这种情况 下如何扩展系统；以及，如果主库失效（“处理节点宕机”），如何处理故障转移。在分布式系统文献中，这个问题被称为全序广播（total order broadcast）或原子广播（atomic broadcast）ix【25,57,58】。</p><p>“原子广播”是一个传统的术语，非常混乱，而且与“原子”一词的其他用法不一致：它与 ACID事务中的原子性没有任何关系，只是与原子操作（在多线程编程的意义上 ）或原子 寄存器（线性一致存储）有间接的联系。全序广播是另一个同义词。</p><p>全序广播通常被描述为在节点间交换消息的协议。 非正式地讲，它要满足两个安全属性</p><p>可靠交付（reliable delivery）</p><p>没有消息丢失：如果消息被传递到一个节点，它将被传递到所有节点。</p><p>全序交付（totally ordered delivery）</p><p>消息以相同的顺序传递给每个节点。</p><p>正确的全序广播算法必须始终保证可靠性和有序性，即使节点或网络出现故障。当然在网络 中断的时候，消息是传不出去的，但是算法可以不断重试，以便在网络最终修复时，消息能 及时通过并送达（当然它们必须仍然按照正确的顺序传递）。</p><h4 id="使用全序广播"><a href="#使用全序广播" class="headerlink" title="使用全序广播"></a>使用全序广播</h4><p>像ZooKeeper和etcd这样的共识服务实际上实现了全序广播。这一事实暗示了全序广播与共 识之间有着紧密联系，我们将在本章稍后进行探讨。</p><p>全序广播正是数据库复制所需的：如果每个消息都代表一次数据库的写入，且每个副本都按 相同的顺序处理相同的写入，那么副本间将相互保持一致（除了临时的复制延迟）。这个原 理被称为状态机复制（state machine replication）【60】，我们将在第11章中重新回到这个概念。</p><p>与之类似，可以使用全序广播来实现可序列化的事务：如“真的串行执行”中所述，如果每个 消息都表示一个确定性事务，以存储过程的形式来执行，且每个节点都以相同的顺序处理这 些消息，那么数据库的分区和副本就可以相互保持一致【61】。</p><p>全序广播的一个重要表现是，顺序在消息送达时被固化：如果后续的消息已经送达，节点就 不允许追溯地将（先前）消息插入顺序中的较早位置。这个事实使得全序广播比时间戳命令 更强。</p><p>考量全序广播的另一种方式是，这是一种创建日志的方式（如在复制日志，事务日志或预写 式日志中）：传递消息就像附加写入日志。由于所有节点必须以相同的顺序传递相同的消 息，因此所有节点都可以读取日志，并看到相同的消息序列。</p><p>全序广播对于实现提供防护令牌的锁服务也很有用（参见“防护令牌”）。每个获取锁的请求 都作为一条消息追加到日志末尾，并且所有的消息都按它们在日志中出现的顺序依次编号。 序列号可以当成防护令牌用，因为它是单调递增的。在ZooKeeper中，这个序列号被称 为 zxid 【15】。</p><h4 id="使用全序广播实现线性一致的存储"><a href="#使用全序广播实现线性一致的存储" class="headerlink" title="使用全序广播实现线性一致的存储"></a>使用全序广播实现线性一致的存储</h4><p>全序广播是异步的：消息被保证以固定的顺序可靠地传送，但是不能保证消息何时被送达 （所以一个接收者可能落后于其他接收者）。相比之下，线性一致性是新鲜性的保证：读取 一定能看见最新的写入值。</p><p>但如果有了全序广播，你就可以在此基础上构建线性一致的存储。例如，你可以确保用户名 能唯一标识用户帐户。</p><p>设想对于每一个可能的用户名，你都可以有一个带有CAS原子操作的线性一致寄存器。每个 寄存器最初的值为空值（表示不使用用户名）。当用户想要创建一个用户名时，对该用户名 的寄存器执行CAS操作，在先前寄存器值为空的条件，将其值设置为用户的账号ID。如果多 个用户试图同时获取相同的用户名，则只有一个CAS操作会成功，因为其他用户会看到非空 的值（由于线性一致性）。</p><p>你可以通过将全序广播当成仅追加日志【62,63】的方式来实现这种线性一致的CAS操作：</p><ol><li>在日志中追加一条消息，试探性地指明你要声明的用户名。</li><li>读日志，并等待你所附加的信息被回送。</li><li>检查是否有任何消息声称目标用户名的所有权。如果这些消息中的第一条就你自己的消 息，那么你就成功了：你可以提交声称的用户名（也许是通过向日志追加另一条消息） 并向客户端确认。如果所需用户名的第一条消息来自其他用户，则中止操作。</li></ol><p>由于日志项是以相同顺序送达至所有节点，因此如果有多个并发写入，则所有节点会对最先 到达者达成一致。选择冲突写入中的第一个作为胜利者，并中止后来者，以此确定所有节点 对某个写入是提交还是中止达成一致。类似的方法可以在一个日志的基础上实现可序列化的 多对象事务【62】。</p><p>尽管这一过程保证写入是线性一致的，但它并不保证读取也是线性一致的 —— 如果你从与日 志异步更新的存储中读取数据，结果可能是陈旧的。 （精确地说，这里描述的过程提供了顺 序一致性（sequential consistency）【47,64】，有时也称为时间线一致性（timeline consistency）【65,66】，比线性一致性稍微弱一些的保证）。为了使读取也线性一致，有 几个选项：</p><ul><li>你可以通过追加一条消息，当消息回送时读取日志，执行实际的读取。消息在日志中的 位置因此定义了读取发生的时间点。 （etcd的法定人数读取有些类似这种情况 【16】。）</li><li>如果日志允许以线性一致的方式获取最新日志消息的位置，则可以查询该位置，等待直 到该位置前的所有消息都传达到你，然后执行读取。 （这是Zookeeper sync() 操作背 后的思想【15】）。</li><li>你可以从同步更新的副本中进行读取，因此可以确保结果是最新的。 （这种技术用于链 式复制【63】；参阅“复制研究”。）</li></ul><h4 id="使用线性一致性存储实现全序广播"><a href="#使用线性一致性存储实现全序广播" class="headerlink" title="使用线性一致性存储实现全序广播"></a>使用线性一致性存储实现全序广播</h4><p>上一节介绍了如何从全序广播构建一个线性一致的CAS操作。我们也可以把它反过来，假设 我们有线性一致的存储，接下来会展示如何在此基础上构建全序广播。</p><p>最简单的方法是假设你有一个线性一致的寄存器来存储一个整数，并且有一个原子自增并返 回操作【28】。或者原子CAS操作也可以完成这项工作。</p><p>该算法很简单：每个要通过全序广播发送的消息首先对线性一致寄存器执行自增并返回操 作。然后将从寄存器获得的值作为序列号附加到消息中。然后你可以将消息发送到所有节点 （重新发送任何丢失的消息），而收件人将按序列号连续发送消息。</p><p>请注意，与兰伯特时间戳不同，通过自增线性一致性寄存器获得的数字形式上是一个没有间 隙的序列。因此，如果一个节点已经发送了消息 4 并且接收到序列号为 6 的传入消息，则它 知道它在传递消息 6 之前必须等待消息 5 。兰伯特时间戳则与之不同 —— 事实上，这是全序 广播和时间戳排序间的关键区别。</p><p>实现一个带有原子性自增并返回操作的线性一致寄存器有多困难？像往常一样，如果事情从 来不出差错，那很容易：你可以简单地把它保存在单个节点内的变量中。问题在于处理当该 节点的网络连接中断时的情况，并在该节点失效时能恢复这个值【59】。一般来说，如果你 对线性一致性的序列号生成器进行深入过足够深入的思考，你不可避免地会得出一个共识算 法。</p><p>这并非巧合：可以证明，线性一致的CAS（或自增并返回）寄存器与全序广播都都等价于共 识问题【28,67】。也就是说，如果你能解决其中的一个问题，你可以把它转化成为其他问题 的解决方案。这是相当深刻和令人惊讶的洞察！</p><h4 id="分布式事务与共识"><a href="#分布式事务与共识" class="headerlink" title="分布式事务与共识"></a>分布式事务与共识</h4><p>共识是分布式计算中最重要也是最基本的问题之一。从表面上看似乎很简单：非正式地讲， 目标只是让几个节点达成一致（get serveral nodes to agree on something）。你也许会认 为这不会太难。不幸的是，许多出故障的系统都是因为错误地轻信这个问题很容易解决。</p><p>尽管共识非常重要，但关于它的内容出现在本书的后半部分，因为这个主题非常微妙，欣赏 细微之处需要一些必要的知识。即使在学术界，对共识的理解也是在几十年的过程中逐渐沉 淀而来，一路上也有着许多误解。现在我们已经讨论了复制（第5章），事务（第7章），系 统模型（第8章），线性一致以及全序（本章），我们终于准备好解决共识问题了。</p><p>节点能达成一致，在很多场景下都非常重要，例如：</p><h5 id="领导选举"><a href="#领导选举" class="headerlink" title="领导选举"></a>领导选举</h5><p>在单主复制的数据库中，所有节点需要就哪个节点是领导者达成一致。如果一些节点由于网 络故障而无法与其他节点通信，则可能会对领导权的归属引起争议。在这种情况下，共识对 于避免错误的故障切换非常重要。错误的故障切换会导致两个节点都认为自己是领导者（脑 裂，参阅“处理节点宕机”）。如果有两个领导者，它们都会接受写入，它们的数据会发生分 歧，从而导致不一致和数据丢失。</p><h5 id="原子提交"><a href="#原子提交" class="headerlink" title="原子提交"></a>原子提交</h5><p>在支持跨多节点或跨多分区事务的数据库中，一个事务可能在某些节点上失败，但在其他节点上成功。如果我们想要维护事务的原子性（就ACID而言，请参“原子性”），我们必须让所有节点对事务的结果达成一致：要么全部中止/回滚（如果出现任何错误），要么它们全部提交（如果没有出错）。这个共识的例子被称为原子提交（atomic commit）问题。</p><h4 id="两阶段提交简介"><a href="#两阶段提交简介" class="headerlink" title="两阶段提交简介"></a>两阶段提交简介</h4><p>两阶段提交（two-phase commit）是一种用于实现跨多个节点的原子事务提交的算法，即 确保所有节点提交或所有节点中止。 它是分布式数据库中的经典算法【13,35,75】。 2PC在 某些数据库内部使用，也以XA事务的形式对应用可用【76,77】（例如Java Transaction API 支持）或以SOAP Web服务的 WS-AtomicTransaction 形式提供给应用【78,79】。</p><p>为了理解它的工作原理，我们必须更详细地分解这个过程：</p><ol><li>当应用想要启动一个分布式事务时，它向协调者请求一个事务ID。此事务ID是全局唯一的。</li><li>应用在每个参与者上启动单节点事务，并在单节点事务上捎带上这个全局事务ID。所有 的读写都是在这些单节点事务中各自完成的。如果在这个阶段出现任何问题（例如，节 点崩溃或请求超时），则协调者或任何参与者都可以中止。</li><li> 当应用准备提交时，协调者向所有参与者发送一个准备请求，并打上全局事务ID的标 记。如果任意一个请求失败或超时，则协调者向所有参与者发送针对该事务ID的中止请 求。</li><li>参与者收到准备请求时，需要确保在任意情况下都的确可以提交事务。这包括将所有事 务数据写入磁盘（出现故障，电源故障，或硬盘空间不足都不能是稍后拒绝提交的理 由）以及检查是否存在任何冲突或违反约束。通过向协调者回答“是”，节点承诺，只要请 求，这个事务一定可以不出差错地提交。换句话说，参与者放弃了中止事务的权利，但 没有实际提交。</li><li>当协调者收到所有准备请求的答复时，会就提交或中止事务作出明确的决定（只有在所 有参与者投赞成票的情况下才会提交）。协调者必须把这个决定写到磁盘上的事务日志 中，如果它随后就崩溃，恢复后也能知道自己所做的决定。这被称为提交点（commit point）。</li><li>一旦协调者的决定落盘，提交或放弃请求会发送给所有参与者。如果这个请求失败或超 时，协调者必须永远保持重试，直到成功为止。没有回头路：如果已经做出决定，不管 需要多少次重试它都必须被执行。如果参与者在此期间崩溃，事务将在其恢复后提交 ——由于参与者投了赞成，因此恢复后它不能拒绝提交。</li></ol><p>因此，该协议包含两个关键的“不归路”点：当参与者投票“是”时，它承诺它稍后肯定能够提交 （尽管协调者可能仍然选择放弃）。一旦协调者做出决定，这一决定是不可撤销的。这些承 诺保证了2PC的原子性。 （单节点原子提交将这两个事件混为一谈：将提交记录写入事务日 志。）</p><h5 id="协调者失效"><a href="#协调者失效" class="headerlink" title="协调者失效"></a>协调者失效</h5><p>我们已经讨论了在2PC期间，如果参与者之一或网络发生故障时会发生什么情况：如果任何 一个准备请求失败或者超时，协调者就会中止事务。如果任何提交或中止请求失败，协调者 将无条件重试。但是如果协调者崩溃，会发生什么情况就不太清楚了。</p><p>如果协调者在发送准备请求之前失败，参与者可以安全地中止事务。但是，一旦参与者收到 了准备请求并投了“是”，就不能再单方面放弃 —— 必须等待协调者回答事务是否已经提交或 中止。如果此时协调者崩溃或网络出现故障，参与者什么也做不了只能等待。参与者的这种 事务状态称为存疑（in doubt）的或不确定（uncertain）的。</p><p>情况如图9-10 所示。在这个特定的例子中，协调者实际上决定提交，数据库2 收到提交请 求。但是，协调者在将提交请求发送到数据库1 之前发生崩溃，因此数据库1 不知道是否提交 或中止。即使超时在这里也没有帮助：如果数据库1 在超时后单方面中止，它将最终与执行提 交的数据库2 不一致。同样，单方面提交也是不安全的，因为另一个参与者可能已经中止了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b97fed494758ce15.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>没有协调者的消息，参与者无法知道是提交还是放弃。原则上参与者可以相互沟通，找出每 个参与者是如何投票的，并达成一致，但这不是2PC协议的一部分。</p><p>可以完成2PC的唯一方法是等待协调者恢复。这就是为什么协调者必须在向参与者发送提交 或中止请求之前，将其提交或中止决定写入磁盘上的事务日志：协调者恢复后，通过读取其 事务日志来确定所有存疑事务的状态。任何在协调者日志中没有提交记录的事务都会中止。 因此，2PC的提交点归结为协调者上的常规单节点原子提交。</p><h4 id="三阶段提交"><a href="#三阶段提交" class="headerlink" title="三阶段提交"></a>三阶段提交</h4><p>两阶段提交被称为阻塞（blocking）原子提交协议，因为存在2PC可能卡住并等待协调者恢 复的情况。理论上，可以使一个原子提交协议变为非阻塞（nonblocking）的，以便在节点 失败时不会卡住。但是让这个协议能在实践中工作并没有那么简单。</p><p>作为2PC的替代方案，已经提出了一种称为三阶段提交（3PC）的算法【13,80】。然而， 3PC假定网络延迟有界，节点响应时间有限；在大多数具有无限网络延迟和进程暂停的实际 系统中（见第8章），它并不能保证原子性。</p><p>通常，非阻塞原子提交需要一个完美的故障检测器（perfect failure detector）【67,71】 —— 即一个可靠的机制来判断一个节点是否已经崩溃。在具有无限延迟的网络中，超时并不是一种可靠的故障检测机制，因为即使没有节点崩溃，请求也可能由于网络问题而超时。出于这个原因，2PC仍然被使用，尽管大家都清楚可能存在协调者故障的问题。</p><h4 id="XA事务"><a href="#XA事务" class="headerlink" title="XA事务"></a>XA事务</h4><p>X/Open XA（扩展架构（eXtended Architecture）的缩写）是跨异构技术实现两阶段提交 的标准【76,77】。它于1991年推出并得到了广泛的实现：许多传统关系数据库（包括 PostgreSQL，MySQL，DB2，SQL Server和Oracle）和消息代理（包括ActiveMQ， HornetQ，MSMQ和IBM MQ） 都支持XA。</p><p>XA不是一个网络协议——它只是一个用来与事务协调者连接的C API。其他语言也有这种API 的绑定；例如在Java EE应用的世界中，XA事务是使用Java事务API（JTA, Java Transaction API）实现的，而许多使用Java数据库连接（JDBC, Java Database Connectivity）的数据库驱动，以及许多使用Java消息服务（JMS）API的消息代理都支持 Java事务API（JTA）。</p><p>XA假定你的应用使用网络驱动或客户端库来与参与者进行通信（数据库或消息服务）。如果 驱动支持XA，则意味着它会调用XA API 以查明操作是否为分布式事务的一部分 —— 如果 是，则将必要的信息发往数据库服务器。驱动还会向协调者暴露回调接口，协调者可以通过 回调来要求参与者准备，提交或中止。</p><p>事务协调者需要实现XA API。标准没有指明应该如何实现，但实际上协调者通常只是一个 库，被加载到发起事务的应用的同一个进程中（而不是单独的服务）。它在事务中个跟踪所 有的参与者，并在要求它们准备之后收集参与者的响应（通过驱动回调），并使用本地磁盘 上的日志记录每次事务的决定（提交/中止）。</p><p>如果应用进程崩溃，或者运行应用的机器报销了，协调者也随之往生极乐。然后任何带有准 备了但未提交事务的参与者都会在疑虑中卡死。由于协调程序的日志位于应用服务器的本地 磁盘上，因此必须重启该服务器，且协调程序库必须读取日志以恢复每个事务的提交/中止结 果。只有这样，协调者才能使用数据库驱动的XA回调来要求参与者提交或中止。数据库服务 器不能直接联系协调者，因为所有通信都必须通过客户端库。</p><h4 id="共识算法和全序广播"><a href="#共识算法和全序广播" class="headerlink" title="共识算法和全序广播"></a>共识算法和全序广播</h4><p>最著名的容错共识算法是视图戳复制（VSR, viewstamped replication）【94,95】，Paxos 【96,97,98,99】，Raft 【22,100,101】以及 Zab 【15,21,102】 。这些算法之间有不少相似 之处，但它们并不相同【103】。在本书中我们不会介绍各种算法的详细细节：了解一些它们 共通的高级思想通常已经足够了，除非你准备自己实现一个共识系统。（可能并不明智，相 当难【98,104】）</p><p>大多数这些算法实际上并不直接使用这里描述的形式化模型（提议与决定单个值，一致同 意，完整性，有效性和终止属性）。取而代之的是，它们决定了值的顺序（sequence），这 使它们成为全序广播算法，正如本章前面所讨论的那样（参阅“全序广播”</p><p>请记住，全序广播要求将消息按照相同的顺序，恰好传递一次，准确传送到所有节点。如果 仔细思考，这相当于进行了几轮共识：在每一轮中，节点提议下一条要发送的消息，然后决 定在全序中下一条要发送的消息【67】。</p><p>所以，全序广播相当于重复进行多轮共识（每次共识决定与一次消息传递相对应）：</p><ul><li>由于一致同意属性，所有节点决定以相同的顺序传递相同的消息。 </li><li>由于完整性属性，消息不会重复。 </li><li>由于有效性属性，消息不会被损坏，也不能凭空编造。 </li><li>由于终止属性，消息不会丢失。</li></ul><p>视图戳复制，Raft和Zab直接实现了全序广播，因为这样做比重复一次一值（one value a time）的共识更高效。在Paxos的情况下，这种优化被称为Multi-Paxos。</p><h4 id="共识的局限性"><a href="#共识的局限性" class="headerlink" title="共识的局限性"></a>共识的局限性</h4><p>共识算法对于分布式系统来说是一个巨大的突破：它为其他充满不确定性的系统带来了基础 的安全属性（一致同意，完整性和有效性），然而它们还能保持容错（只要多数节点正常工 作且可达，就能取得进展）。它们提供了全序广播，因此也可以它们也可以以一种容错的方式实现线性一致的原子操作（参见“使用全序广播实现线性一致性存储”）。</p><p>尽管如此，它们并不是在所有地方都用上了，因为好处总是有代价的。</p><p>节点在做出决定之前对提议进行投票的过程是一种同步复制。如“同步与异步复制”中所述， 通常数据库会配置为异步复制模式。在这种配置中发生故障切换时，一些已经提交的数据可 能会丢失 —— 但是为了获得更好的性能，许多人选择接受这种风险</p><p>共识系统总是需要严格多数来运转。这意味着你至少需要三个节点才能容忍单节点故障（其 余两个构成多数），或者至少有五个节点来容忍两个节点发生故障（其余三个构成多数）。 如果网络故障切断了某些节点同其他节点的连接，则只有多数节点所在的网络可以继续工 作，其余部分将被阻塞（参阅“线性一致性的代价”）。</p><p>大多数共识算法假定参与投票的节点是固定的集合，这意味着你不能简单的在集群中添加或 删除节点。共识算法的动态成员扩展（dynamic membership extension）允许集群中的节 点集随时间推移而变化，但是它们比静态成员算法要难理解得多</p><p>共识系统通常依靠超时来检测失效的节点。在网络延迟高度变化的环境中，特别是在地理上 散布的系统中，经常发生一个节点由于暂时的网络问题，错误地认为领导者已经失效。虽然 这种错误不会损害安全属性，但频繁的领导者选举会导致糟糕的性能表现，因系统最后可能 花在权力倾扎上的时间要比花在建设性工作的多得多。</p><p>有时共识算法对网络问题特别敏感。例如Raft已被证明存在让人不悦的极端情况【106】：如 果整个网络工作正常，但只有一条特定的网络连接一直不可靠，Raft可能会进入领导频繁二人 转的局面，或者当前领导者不断被迫辞职以致系统实质上毫无进展。其他一致性算法也存在 类似的问题，而设计能健壮应对不可靠网络的算法仍然是一个开放的研究问题。</p><h4 id="成员与协调服务"><a href="#成员与协调服务" class="headerlink" title="成员与协调服务"></a>成员与协调服务</h4><p>像ZooKeeper或etcd这样的项目通常被描述为“分布式键值存储”或“协调与配置服务”。这种服 务的API看起来非常像数据库：你可以读写给定键的值，并遍历键。所以如果它们基本上算是 数据库的话，为什么它们要把工夫全花在实现一个共识算法上呢？是什么使它们区别于其他 任意类型的数据库？</p><p>为了理解这一点，简单了解如何使用ZooKeeper这类服务是很有帮助的。作为应用开发人 员，你很少需要直接使用ZooKeeper，因为它实际上不适合当成通用数据库来用。更有可能 的是，你会通过其他项目间接依赖它，例如HBase，Hadoop YARN，OpenStack Nova和 Kafka都依赖ZooKeeper在后台运行。这些项目从它那里得到了什么？</p><p>ZooKeeper和etcd被设计为容纳少量完全可以放在内存中的数据（虽然它们仍然会写入磁盘 以保证持久性），所以你不会想着把所有应用数据放到这里。这些少量数据会通过容错的全 序广播算法复制到所有节点上。正如前面所讨论的那样，数据库复制需要的就是全序广播： 如果每条消息代表对数据库的写入，则以相同的顺序应用相同的写入操作可以使副本之间保 持一致。</p><p>ZooKeeper模仿了Google的Chubby锁服务【14,98】，不仅实现了全序广播（因此也实现了 共识），而且还构建了一组有趣的其他特性，这些特性在构建分布式系统时变得特别有用：</p><ul><li> 线性一致性的原子操作, 使用原子CAS操作可以实现锁：如果多个节点同时尝试执行相同的操作，只有一个节点会成 功。共识协议保证了操作的原子性和线性一致性，即使节点发生故障或网络在任意时刻中 断。分布式锁通常以租约（lease）的形式实现，租约有一个到期时间，以便在客户端失效的 情况下最终能被释放（参阅“进程暂停”）。</li><li> 操作的全序排序, 如“领导者与锁定”中所述，当某个资源受到锁或租约的保护时，你需要一个防护令牌来防止客 户端在进程暂停的情况下彼此冲突。防护令牌是每次锁被获取时单调增加的数字。 ZooKeeper通过全局排序操作来提供这个功能，它为每个操作提供一个单调递增的事务 ID（ zxid ）和版本号（ cversion ）【15】。</li><li> 失效检测,客户端在ZooKeeper服务器上维护一个长期会话，客户端和服务器周期性地交换心跳包来检 查节点是否还活着。即使连接暂时中断，或者ZooKeeper节点失效，会话仍保持在活跃状 态。但如果心跳停止的持续时间超出会话超时，ZooKeeper会宣告该会话已死亡。当会话超 时（ZooKeeper调用这些临时节点）时，会话持有的任何锁都可以配置为自动释放 （ZooKeeper称之为临时节点（ephemeral nodes））。</li><li> 变更通知，客户端不仅可以读取其他客户端创建的锁和值，还可以监听它们的变更。因此，客户端可以 知道另一个客户端何时加入集群（基于新客户端写入ZooKeeper的值），或发生故障（因其 会话超时，而其临时节点消失）。通过订阅通知，客户端不用再通过频繁轮询的方式来找出 变更。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Architecture </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《从0开始学架构》</title>
      <link href="/2020/09/20/note/zero-to-one-study-architecture/"/>
      <url>/2020/09/20/note/zero-to-one-study-architecture/</url>
      
        <content type="html"><![CDATA[<h3 id="架构设计理念"><a href="#架构设计理念" class="headerlink" title="架构设计理念"></a>架构设计理念</h3><p>架构设计理念，可以提炼为下面几个关键点：</p><ol><li>架构是系统的顶层结构。</li><li>架构设计的主要目的是为了解决软件系统复杂度带来的问题。</li><li>架构设计需要遵循三个主要原则：合适原则、简单原则、演化原则。</li><li>架构设计首先要掌握业界已经成熟的各种架构模式，然后再进行优化、调整、创新。</li></ol><h3 id="框架设计需要考的因素-影响架构复杂性的几个因素"><a href="#框架设计需要考的因素-影响架构复杂性的几个因素" class="headerlink" title="框架设计需要考的因素/影响架构复杂性的几个因素"></a>框架设计需要考的因素/影响架构复杂性的几个因素</h3><ol><li>高性能， 衡量软件性能包括了响应时间、TPS、服务器资源利用率等客观指标，也可以是用户的主观感受。</li><li>高可用，高可用性就是技术实力的象征，高可用性就是竞争力。99.99%（俗称4个9）网站不可用时间=52.56分钟</li><li>可扩展性，设计具备良好可扩展性的系统，有两个基本条件：“正确预测变化、完美封装变化”。</li><li>低成本，语言选择、方案选择。</li><li>安全，功能安全XSS、CSRF等，架构安全、访问策略。</li><li>规模，规模带来复杂度的主要原因就是“量变引起质变”</li></ol><h3 id="架构设计的三个原则"><a href="#架构设计的三个原则" class="headerlink" title="架构设计的三个原则"></a>架构设计的三个原则</h3><ol><li>合适原则 ，“合适优于业界领先”。不追求高大上方案，只追求最合适的。</li><li>简单原则，KISS原则（Keep It Simple, Stupid!），“简单优于复杂”。</li><li>演化原则，“演化优于一步到位”，“不要过度设计” “不要提前优化，不要为了优化而优化”。对于建筑来说，永恒是主题；而对于软件来说，变化才是主题。软件架构需要根据业务的发展而不断变化。</li></ol><h3 id="架构设计的流程"><a href="#架构设计的流程" class="headerlink" title="架构设计的流程"></a>架构设计的流程</h3><ol><li>识别复杂度 （1）构建复杂度的来源清单——高性能、可用性、扩展性、安全、低成本、规模等。（2）结合需求、技术、团队、资源等对上述复杂度逐一分析是否需要？是否关键？</li><li>设计备选方案，备选方案的数量以 3 ~ 5 个为最佳，备选方案的差异要比较明显，备选方案的技术不要只局限于已经熟悉的技术</li><li>评估和选择备选方案，列出我们需要关注的质量属性点，然后分别从这些质量属性的维度去评估每个方案，再综合挑选适合当时情况的最优方案。常见的方案质量属性点有：性能、可用性、硬件成本、项目投入、复杂度、安全性、可扩展性等。在评估这些质量属性时，需要遵循架构设计原则 1“合适原则”和原则 2“简单原则”，避免贪大求全，基本上某个质量属性能够满足一定时期内业务发展就可以了。</li><li>详细方案设计，详细方案设计就是将方案涉及的关键技术细节给确定下来。</li></ol><h3 id="高性能架构模式"><a href="#高性能架构模式" class="headerlink" title="高性能架构模式"></a>高性能架构模式</h3><ol><li>读写分离，读写分离的实现逻辑并不复杂，但有两个细节点将引入设计复杂度：主从复制延迟和分配机制。主从复制延迟和分配机制。<ul><li>解决主从复制延迟有几种常见的方法：1. 写操作后的读操作指定发给数据库主服务器 2. 读从机失败后再读一次主机. 3. 关键业务读写操作全部指向主机，非关键业务采用读写分离</li><li>程序代码封装和中间件封装 “读写分离逻辑”。</li></ul></li><li>分库分表，业务分库、分表（垂直分表和水平分表），垂直分表适合将表中某些不常用且占了大量空间的列拆分出去，水平分表适合表行数特别大的表。 路由方式：范围路由、Hash 路由、配置路由</li><li>NoSql数据库，K-V 存储：解决关系数据库无法存储数据结构的问题，以 Redis 为代表。文档数据库：解决关系数据库强 schema 约束的问题，以 MongoDB 为代表。列式数据库：解决关系数据库大数据场景下的 I/O 问题，以 HBase 为代表。全文搜索引擎：解决关系数据库的全文搜索性能问题，以 Elasticsearch 为代表。</li><li>负载均衡分类，DNS 负载均衡、硬件负载均衡、软件负载均衡（LVS、Nginx）</li><li>负责均衡算法，轮询、加权、负载最低优先、性能最优类、Hash 类</li></ol><h3 id="软件负载均衡的优缺点："><a href="#软件负载均衡的优缺点：" class="headerlink" title="软件负载均衡的优缺点："></a>软件负载均衡的优缺点：</h3><p><strong>优点</strong></p><ul><li>简单：无论是部署还是维护都比较简单。</li><li>便宜：只要买个 Linux 服务器，装上软件即可。</li><li>灵活：4 层和 7 层负载均衡可以根据业务进行选择；也可以根据业务进行比较方便的扩展，例如，可以通过 Nginx 的插件来实现业务的定制化功能。</li></ul><p><strong>缺点其实下面的缺点都是和硬件负载均衡相比的，并不是说软件负载均衡没法用</strong></p><ul><li>性能一般：一个 Nginx 大约能支撑 5 万并发。</li><li>功能没有硬件负载均衡那么强大。</li><li>一般不具备防火墙和防 DDoS 攻击等安全功能。</li></ul><h3 id="异地多活设计4大技巧"><a href="#异地多活设计4大技巧" class="headerlink" title="异地多活设计4大技巧"></a>异地多活设计4大技巧</h3><ul><li>技巧 1：保证核心业务的异地多活 ，注册问题、登录，登录功能用的频率最高，优先考虑登录功能高可用</li><li>技巧 2：保证核心数据最终一致性，1. 尽量减少异地多活机房的距离，搭建高速网络 2. 尽量减少数据同步，只同步核<br>心业务相关的数据 3. 保证最终一致性，不保证实时一致性。</li><li>技巧 3：采用多种手段同步数据，1. 消息队列方式 2. 二次读取方式 3. 存储系统同步方式 4. 回源读取方式 5.重新生成数据方式</li><li>技巧 4：只保证绝大部分用户的异地多活</li></ul><p>异地多活设计的理念可以总结为一句话：采用多种手段，保证绝大部分用户的核心业务异地多活！</p><h3 id="跨城异地多活架构设计的-4-个步骤"><a href="#跨城异地多活架构设计的-4-个步骤" class="headerlink" title="跨城异地多活架构设计的 4 个步骤"></a>跨城异地多活架构设计的 4 个步骤</h3><ul><li>第 1 步：业务分级，访问量大的业务、核心业务、产生大量收入的业务</li><li>第 2 步：数据分类， 数据量、唯一性、实时性、可丢失性、可恢复性</li><li>第 3 步：数据同步，存储系统同步、消息队列同步、重复生成</li><li>第 4 步：异常处理，异常处理主要有以下几个目的：<ul><li><ol><li>问题发生时，避免少量数据异常导致整体业务不可用。</li></ol></li><li><ol start="2"><li>问题恢复后，将异常的数据进行修正。</li></ol></li><li><ol start="3"><li>对用户进行安抚，弥补用户损失。常见的异常处理措施有这几类：1. 多通道同步 2. 同步和访问结合 3. 日志记录 4. 用户补偿</li></ol></li></ul></li></ul><h3 id="如何应对接口级的故障"><a href="#如何应对接口级的故障" class="headerlink" title="如何应对接口级的故障"></a>如何应对接口级的故障</h3><p>解决接口级故障的核心思想和异地多活基本类似：优先保证核心业务和优先保证绝大部分用户。</p><ul><li>降级，降级指系统将某些业务或者接口的功能降低，可以是只提供部分功能，也可以是完全停掉所有功能。例如，论坛可以降级为只能看帖子，不能发帖子。降级的核心思想就是丢车保帅，优先保证核心业务</li><li>熔断，熔断机制实现的关键是需要有一个统一的 API 调用层，由 API 调用层来进行采样或者统计，如果接口调用散落在代码各处就没法进行统一处理了。</li><li>限流，降级是从系统功能优先级的角度考虑如何应对故障，而限流则是从用户访问压力的角度来考虑如何应对故障。限流指只允许系统能够承受的访问量进来，超出系统访问能力的请求将被丢弃。</li><li>排队，排队实际上是限流的一个变种，限流是直接拒绝用户，排队是让用户等待一段时间，全世界最有名的排队当属 12306 网站排队了。排队虽然没有直接拒绝用户，但用户等了很长时间后进入系统，体验并不一定比限流好。</li></ul><h3 id="可扩展的基本思想"><a href="#可扩展的基本思想" class="headerlink" title="可扩展的基本思想"></a>可扩展的基本思想</h3><ul><li>面向流程拆分：将整个业务流程拆分为几个阶段，每个阶段作为一部分。</li><li>面向服务拆分：将系统提供的服务拆分，每个服务作为一部分。</li><li>面向功能拆分：将系统提供的功能拆分，每个功能作为一部分。</li><li>面向流程拆分：分层架构。1. C/S 架构、B/S 架构 2. MVC 架构、MVP 架构 3. 逻辑分层架构</li><li>面向服务拆分：SOA、微服务。</li></ul><h3 id="SOA-：“面向服务的架构”-SOA-提出了-3-个关键概念"><a href="#SOA-：“面向服务的架构”-SOA-提出了-3-个关键概念" class="headerlink" title="SOA ：“面向服务的架构”, SOA 提出了 3 个关键概念"></a>SOA ：“面向服务的架构”, SOA 提出了 3 个关键概念</h3><ol><li>服务 </li><li>ESB 的全称是 Enterprise Service Bus “企业服务总线” </li><li>松耦合 松耦合的目的是减少各个服务间的依赖和互相影响。</li></ol><h3 id="SOA-和微服务的关系和区别："><a href="#SOA-和微服务的关系和区别：" class="headerlink" title="SOA 和微服务的关系和区别："></a>SOA 和微服务的关系和区别：</h3><ol><li>微服务是 SOA 的实现方式 </li><li>微服务是去掉 ESB 后的 SOA </li><li>微服务是一种和 SOA 相似但本质上不同的架构理念</li></ol><p>SOA 和微服务本质上是两种不同的架构设计理念，只是在“服务”这个点上有交集而已，因此两者的关系应该是上面第三种观点。</p><h3 id="微服务的陷阱-微服务具体有哪些坑："><a href="#微服务的陷阱-微服务具体有哪些坑：" class="headerlink" title="微服务的陷阱/微服务具体有哪些坑："></a>微服务的陷阱/微服务具体有哪些坑：</h3><ol><li>服务划分过细，服务间关系复杂 </li><li>服务数量太多，团队效率急剧下降 </li><li>调用链太长，性能下降 </li><li>调用链太长，问题定位困难 </li><li>没有自动化支撑，无法快速交付.  </li><li>没有服务治理，微服务数量多了后管理混乱</li></ol><h3 id="微服务拆分方法："><a href="#微服务拆分方法：" class="headerlink" title="微服务拆分方法："></a>微服务拆分方法：</h3><ol><li>基于业务逻辑拆分 </li><li>基于可扩展拆分  </li><li>基于可靠性拆分 （避免非核心服务故障影响核心服务，核心服务高可用方案可以更简单，能够降低高可用成本）</li><li>基于性能拆分</li></ol><h3 id="面向功能拆分：微内核架构。"><a href="#面向功能拆分：微内核架构。" class="headerlink" title="面向功能拆分：微内核架构。"></a>面向功能拆分：微内核架构。</h3><ul><li>微内核架构包含两类组件：核心系统（core system）和插件模块（plug-in modules）。微内核的核心系统设计的关键技术有：插件管理、插件连接和插件通信。</li><li>OSGi 的全称是 Open Services Gateway initiative，本身其实是指 OSGi Alliance。这个联盟是 Sun Microsystems、IBM、爱立信等公司于 1999 年 3 月成立的开放的标准化组织，最初名为 Connected Alliance。它是一个非盈利的国际组织，旨在建立一个开放的服务规范，为通过网络向设备提供服务建立开放的标准，这个标准就是 OSGi specification。现在我们谈到 OSGi，如果没有特别说明，一般都是指 OSGi 的规范。</li></ul><h3 id="微服务基础设施"><a href="#微服务基础设施" class="headerlink" title="微服务基础设施"></a>微服务基础设施</h3><ol><li>自动化测试，自动化测试涵盖的范围包括代码级的单元测试、单个系统级的集成测试、系统间的接口测试，理想情况是每类测试都自动化.</li><li>自动化部署，自动化部署系统包括版本管理、资源管理（例如，机器管理、虚拟机管理）、部署操作、回退操作等功能。</li><li>配置中心，配置中心包括配置版本管理（例如，同样的微服务，有 10 个节点是给移动用户服务的，有 20 个节点给联通用户服务的，配置项都一样，配置值不一样）、增删改查配置、节点管理、配置同步、配置推送等功能。</li><li>接口框架，微服务提倡轻量级的通信方式，一般采用 HTTP/REST 或者 RPC 方式统一接口协议。但在实践过程中，光统一接口协议还不够，还需要统一接口传递的数据格式 </li><li>API 网关，API 网关是外部系统访问的接口，所有的外部系统接⼊系统都需要通过 API 网关，主要包括接入鉴权（是否允许接入）、权限控制（可以访问哪些功能）、传输加密、请求路由、流量控制等功能。</li><li>服务发现，服务发现主要有两种实现方式：自理式和代理式。 自理式：服务自己去注册表查询节点ip。代理式，就是指微服务之间有一个负载均衡系统（图中的 LOAD BALANCER 节点），由负载均衡系统来完成微服务之间的服务发现。</li><li>服务路由，常见的路由算法有：随机路由、轮询路由、最小压力路由、最小连接数路由等</li><li>服务发现，xxx</li><li>服务容错，常见的服务容错包括请求重试、流控和服务隔离。通常情况下，服务容错会集成在服务发现和服务路由系统中。</li><li>服务监控，作用：1. 实时搜集信息并进行分析，避免故障后再来分析，减少了处理时间。 2. 服务监控可以在实时分析的基础上进行预警，在问题萌芽的阶段发觉并预警，降低了问题影响的范围和时间。通常情况下，服务监控需要搜集并分析大量的数据，因此建议做成独立的系统，而不要集成到服务发现、API 网关等系统中。</li><li>服务跟踪，跟踪某一个请求在微服务中的完整路径。目前无论是分布式跟踪还是微服务的服务跟踪，绝大部分请求跟踪的实现技术都基于 Google 的 Dapper 论文《Dapper, a Large-Scale Distributed Systems Tracing Infrastructure》</li><li>服务安全，服务安全主要分为三部分：接入安全、数据安全、传输安全。通常情况下，服务安全可以集成到配置中心系统中进行实现，即配置中心配置微服务的接入安全策略和数据安全策略，微服务节点从配置中心获取这些配置信息，然后在处理具体的微服务调用请求时根据安全策略进行处理。由于这些策略是通用的，一般会把策略封装成通用的库提供给各个微服务调用。</li></ol><h3 id="我建议按照下面优先级来搭建基础设施："><a href="#我建议按照下面优先级来搭建基础设施：" class="headerlink" title="我建议按照下面优先级来搭建基础设施："></a>我建议按照下面优先级来搭建基础设施：</h3><ol><li>服务发现、服务路由、服务容错：这是最基本的微服务基础设施。</li><li>接口框架、API 网关：主要是为了提升开发效率，接口框架是提升内部服务的开发效率，API 网关是为了提升与外部服务对接的效率。</li><li>自动化部署、自动化测试、配置中心：主要是为了提升测试和运维效率。</li><li>服务监控、服务跟踪、服务安全：主要是为了进一步提升运维效率</li></ol><h4 id="运维平台"><a href="#运维平台" class="headerlink" title="运维平台"></a>运维平台</h4><p>运维平台核心的职责分为四大块：配置、部署、监控、应急，每个职责对应系统生命周期的一个阶段，如下图所示。</p><ul><li>配置：主要负责资源的管理。例如，机器管理、IP 地址管理、虚拟机管理等。</li><li>部署：主要负责将系统发布到线上。例如，包管理、灰度发布管理、回滚等。</li><li>监控：主要负责收集系统上线运行后的相关数据并进行监控，以便及时发现问题。</li><li>应急：主要负责系统出故障后的处理。例如，停止程序、下线故障机器、切换 IP 等。</li><li>运维平台的核心设计要素是“四化”：标准化、平台化、自动化、可视化。</li></ul><h4 id="测试平台"><a href="#测试平台" class="headerlink" title="测试平台"></a>测试平台</h4><p>测试平台核心的职责当然就是测试了，包括单元测试、集成测试、接口测试、性能测试等，都可以在测试平台来完成。</p><p>测试平台的核心目的是提升测试效率，从而提升产品质量，其设计关键就是自动化。传统的测试方式是测试人员手工执行测试用例，测试效率低，重复的工作多。通过测试平台提供的自动化能力，测试用例能够重复执行，无须人工参与，大大提升了测试效率。</p><ol><li>用例管理 </li><li>资源管理 </li><li>任务管理 </li><li>数据管理</li></ol><h4 id="数据平台"><a href="#数据平台" class="headerlink" title="数据平台"></a>数据平台</h4><p>数据平台的核心职责主要包括三部分：数据管理、数据分析和数据应用。</p><h4 id="管理平台"><a href="#管理平台" class="headerlink" title="管理平台"></a>管理平台</h4><p>  管理平台的核心职责就是权限管理，无论是业务系统（例如，淘宝网）、中间件系统（例如，消息队列 Kafka），还是平台系统（例如，运维平台），都需要进行管理。如果每个系统都自己来实现权限管理，效率太低，重复工作很多，因此需要统一的管理平台来管理所有的系统的权限。<br>权限管理主要分为两部分：身份认证、权限控制，其基本架构如下图所示。</p><h3 id="重构"><a href="#重构" class="headerlink" title="重构"></a>重构</h3><p>总结一下重构的做法，其实就是“分段实施”，将要解决的问题根据优先级、重要性、实施难度等划分为不同的阶段，每个阶段聚焦于一个整体的目标，集中精力和资源解决一类问题。</p><p>这样做有几个好处：</p><ul><li><p>每个阶段都有明确目标，做完之后效果明显，团队信心足，后续推进更加容易。</p></li><li><p>每个阶段的工作量不会太大，可以和业务并行。</p></li><li><p>每个阶段的改动不会太大，降低了总体风险。</p></li></ul><ol><li>优先级排序 </li><li>问题分类  </li><li>先易后难 </li><li>循序渐进</li></ol><h3 id="Reactor"><a href="#Reactor" class="headerlink" title="Reactor"></a>Reactor</h3><p>解决这个问题的最简单的方式是将 read 操作改为非阻塞，然后进程不断地轮询多个连接。这种方式能够解决阻塞的问题，但解决的方式并不优雅。首先，轮询是要消耗 CPU 的；其次，如果一个进程处理几千上万的连接，则轮询的效率是很低的。</p><p>I/O 多路复用技术归纳起来有两个关键实现点：</p><ol><li>当多条连接共用一个阻塞对象后，进程只需要在一个阻塞对象上等待，而无须再轮询所有连接，常见的实现方式有 select、epoll、kqueue 等</li><li>当某条连接有新的数据可以处理时，操作系统会通知进程，进程从阻塞状态返回，开始进行业务处理。</li></ol><p>最终 Reactor 模式有这三种典型的实现方案：</p><ol><li>单 Reactor 单进程 / 线程。</li><li>单 Reactor 多线程。</li><li>多 Reactor 多进程 / 线程。</li></ol><p>我多说一句，Nginx 采用的是多 Reactor 多进程的模式，但方案与标准的多 Reactor 多进程有差异。具体差异表现为主进程中仅仅创建了监听端口，并没有创建 mainReactor 来“accept”连接，而是由子进程的 Reactor 来“accept”连接，通过锁来控制一次只有一个子进程进行“accept”，子进程“accept”新连接后就放到自己的 Reactor 进行处理，不会再分配给其他子进程，更多细节请查阅相关资料或阅读 Nginx 源码。</p><h3 id="Proactor"><a href="#Proactor" class="headerlink" title="Proactor"></a>Proactor</h3><p>Reactor 是非阻塞同步网络模型，因为真正的 read 和 send 操作都需要用户进程同步操作。这里的“同步”指用户进程在执行 read 和 send 这类 I/O 操作的时候是同步的，如果把 I/O 操作改为异步就能够进一步提升性能，这就是异步网络模型 Proactor。</p><p>理论上 Proactor 比 Reactor 效率要高一些，异步 I/O 能够充分利用 DMA 特性，让 I/O 操作与计算重叠，但要实现真正的异步 I/O，操作系统需要做大量的工作。目前 Windows 下通过 IOCP 实现了真正的异步 I/O，而在 Linux 系统下的 AIO 并不完善，因此在 Linux 下实现高并发网络编程时都是以 Reactor 模式为主。所以即使 Boost.Asio 号称实现了 Proactor 模型，其实它在 Windows 下采用 IOCP，而在 Linux 下是用 Reactor 模式（采用 epoll）模拟出来的异步模型。</p><p>￼</p><h3 id="架构师条件"><a href="#架构师条件" class="headerlink" title="架构师条件"></a>架构师条件</h3><p>我认为，架构师的内功主要包含三部分：判断力、执行力、创新力，简单解释如下：</p><ul><li>判断力：能够准确判断系统的复杂度在哪里，就像武侠高手一样，能准确地看出对手的破绽和弱点。</li><li>执行力：能够使用合适的方案解决复杂度问题，就像武侠高手一样，能选择合适的招式或者方法打败对手。</li><li>创新力：能够创造新的解决方案解决复杂度问题，就像武侠世界里，小一些的创新是创新招式，而武学宗师能够创立新的武学或者心法，例如张三丰创立太极拳一样。</li></ul><p>因此，要成为一个优秀的架构师，就需要不断地提升自己这几方面的内功，而这三方面的能力主要来源于经验、视野、思考。</p><ul><li>经验：设计过的系统越多、系统越复杂，架构师的内功也就越强，不管是成功的架构，还是失败的架构，不管是踩坑的经验，还是填坑的经验，都将成为架构师内功的一部分。</li><li>视野：掌握的知识和技能越多、越深，架构师的内功也就越强，他山之石可以攻玉，站在巨人的肩膀上会看的更高更远。</li><li>思考：经验和视野都是外部输入，类似于我们吃的食物，但光吃还不行，还要消化，将其变为我们自己的营养，这就是思考的作用。思考能够将经验和视野中的模式、判断、选择、技巧等提炼出来为我所用，思考也能促使我们产生新的创意和灵感。</li></ul><p><img src="/2020/09/20/note/zero-to-one-study-architecture/arc1.png"></p>]]></content>
      
      
      <categories>
          
          <category> Architecture </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
            <tag> Architecture </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Linux内核技术实战》</title>
      <link href="/2020/09/13/note/linux-in-action/"/>
      <url>/2020/09/13/note/linux-in-action/</url>
      
        <content type="html"><![CDATA[<h2 id="Page-Cache"><a href="#Page-Cache" class="headerlink" title="Page Cache"></a>Page Cache</h2><h3 id="什么是-Page-Cache？"><a href="#什么是-Page-Cache？" class="headerlink" title="什么是 Page Cache？"></a>什么是 Page Cache？</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c13331b6d58f8471.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>通过这张图片你可以清楚地看到，红色的地方就是 Page Cache，很明显，Page Cache 是内核管理的内存，也就是说，它属于内核不属于用户。</p><pre><code>$ cat /proc/meminfo...Buffers:            1224 kBCached:           111472 kBSwapCached:        36364 kBActive:          6224232 kBInactive:         979432 kBActive(anon):    6173036 kBInactive(anon):   927932 kBActive(file):      51196 kBInactive(file):    51500 kB...Shmem:             10000 kB...SReclaimable:      43532 kB...</code></pre><blockquote><pre><code>Buffers + Cached + SwapCached = Active(file) + Inactive(file) + Shmem + SwapCached</code></pre></blockquote><p>那么等式两边的内容就是我们平时说的 Page Cache。请注意你没有看错，两边都有 SwapCached，之所以要把它放在等式里，就是说它也是 Page Cache 的一部分。</p><p>在 Page Cache 中，Active(file)+Inactive(file) 是 File-backed page（与文件对应的内存页），是你最需要关注的部分。因为你平时用的 mmap() 内存映射方式和 buffered I/O 来消耗的内存就属于这部分，最重要的是，这部分在真实的生产环境上也最容易产生问题，我们在接下来的课程案例篇会重点分析它。</p><p>而 SwapCached 是在打开了 Swap 分区后，把 Inactive(anon)+Active(anon) 这两项里的匿名页给交换到磁盘（swap out），然后再读入到内存（swap in）后分配的内存。由于读入到内存后原来的 Swap File 还在，所以 SwapCached 也可以认为是 File-backed page，即属于 Page Cache。这样做的目的也是为了减少 I/O。你是不是觉得这个过程有些复杂？我们用一张图直观地看一下：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e4b65848024e1f73.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>除了 SwapCached，Page Cache 中的 Shmem 是指匿名共享映射这种方式分配的内存（free 命令中 shared 这一项），比如 tmpfs（临时文件系统），这部分在真实的生产环境中产生的问题比较少，不是我们今天的重点内容，我们这节课不对它做过多关注，你知道有这回事就可以了。</p><pre><code>$ free -k              total        used        free      shared  buff/cache   availableMem:        7926580     7277960      492392       10000      156228      430680Swap:       8224764      380748     7844016</code></pre><p>通过 procfs 源码里面的<a href="https://gitlab.com/procps-ng/procps/-/blob/master/proc/sysinfo.c">proc/sysinfo.c</a>这个文件，你可以发现 buff/cache 包括下面这几项：</p><blockquote><p>buff/cache = Buffers + Cached + SReclaimable</p></blockquote><p>从这个公式中，你能看到 free 命令中的 buff/cache 是由 Buffers、Cached 和 SReclaimable 这三项组成的，它强调的是内存的可回收性，也就是说，可以被回收的内存会统计在这一项。</p><p>其中 SReclaimable 是指可以被回收的内核内存，包括 dentry 和 inode 等。而这部分内容是内核非常细节性的东西，对于应用开发者和运维人员理解起来相对有些难度，所以我们在这里不多说。</p><p>我相信有这样想法的人不在少数，如果不用内核管理的 Page Cache，那有两种思路来进行处理：</p><ul><li>第一种，应用程序维护自己的 Cache 做更加细粒度的控制，比如 MySQL 就是这样做的，你可以参考MySQL Buffer Pool ，它的实现复杂度还是很高的。对于大多数应用而言，实现自己的 Cache 成本还是挺高的，不如内核的 Page Cache 来得简单高效。</li><li>第二种，直接使用 Direct I/O 来绕过 Page Cache，不使用 Cache 了，省的去管它了。这种方法可行么？那我们继续用数据说话，看看这种做法的问题在哪儿？</li></ul><h3 id="为什么需要-Page-Cache？"><a href="#为什么需要-Page-Cache？" class="headerlink" title="为什么需要 Page Cache？"></a>为什么需要 Page Cache？</h3><pre><code>//1.  先生成一个 1G 的文件：dd if=/dev/zero of=/home/yafang/test/dd.out bs=4096 count=((1024*256))dd if=/dev/zero of=big_file count=10 bs=1G//2. 其次，清空 Page Cache，需要先执行一下 sync 来将脏页同步到磁盘再去 drop cache。$ sync &amp;&amp; echo 3 &gt; /proc/sys/vm/drop_caches// 第一次读取文件的耗时如下：$ time cat /home/yafang/test/dd.out &amp;&gt; /dev/nullreal  0m5.733suser  0m0.003ssys  0m0.213s// 再次读取文件的耗时如下：$ time cat /home/yafang/test/dd.out &amp;&gt; /dev/null real  0m0.132suser  0m0.001ssys  0m0.130s</code></pre><p>通过这样详细的过程你可以看到，第二次读取文件的耗时远小于第一次的耗时，这是因为第一次是从磁盘来读取的内容，磁盘 I/O 是比较耗时的，而第二次读取的时候由于文件内容已经在第一次读取时被读到内存了，所以是直接从内存读取的数据，内存相比磁盘速度是快很多的。 <strong>这就是 Page Cache 存在的意义：减少 I/O，提升应用的 I/O 速度</strong> 。</p><h3 id="Page-Cache-是如何“诞生”的？"><a href="#Page-Cache-是如何“诞生”的？" class="headerlink" title="Page Cache 是如何“诞生”的？"></a>Page Cache 是如何“诞生”的？</h3><p>Page Cache 的产生有两种不同的方式：</p><ul><li>Buffered I/O（标准 I/O）；</li><li>Memory-Mapped I/O（存储映射 I/O）。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-159e97bc1103e93c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>标准 I/O 是写的 (write(2)) 用户缓冲区 (Userpace Page 对应的内存)，然后再将用户缓冲区里的数据拷贝到内核缓冲区 (Pagecache Page 对应的内存)；如果是读的 (read(2)) 话则是先从内核缓冲区拷贝到用户缓冲区，再从用户缓冲区读数据，也就是 buffer 和文件内容不存在任何映射关系。</p><p>对于存储映射 I/O 而言，则是直接将 Pagecache Page 给映射到用户地址空间，用户直接读写 Pagecache Page 中内容。</p><p>显然，存储映射 I/O 要比标准 I/O 效率高一些，毕竟少了“用户空间到内核空间互相拷贝”的过程。这也是很多应用开发者发现，为什么使用内存映射 I/O 比标准 I/O 方式性能要好一些的主要原因。</p><p>我们来用具体的例子演示一下 Page Cache 是如何“诞生”的，就以其中的标准 I/O 为例，因为这是我们最常使用的一种方式，如下是一个简单的示例脚本：</p><pre><code>#!/bin/sh#这是我们用来解析的文件MEM_FILE="/proc/meminfo"#这是在该脚本中将要生成的一个新文件NEW_FILE="/home/yafang/dd.write.out"#我们用来解析的Page Cache的具体项active=0inactive=0pagecache=0IFS=' '#从/proc/meminfo中读取File Page Cache的大小function get_filecache_size(){        items=0        while read line        do                if [[ "$line" =~ "Active:" ]]; then                        read -ra ADDR &lt;&lt;&lt;"$line"                        active=${ADDR[1]}                        let "items=$items+1"                elif [[  "$line" =~ "Inactive:" ]]; then                        read -ra ADDR &lt;&lt;&lt;"$line"                        inactive=${ADDR[1]}                        let "items=$items+1"                fi                  if [ $items -eq 2 ]; then                        break;                fi          done &lt; $MEM_FILE}#读取File Page Cache的初始大小get_filecache_sizelet filecache="$active + $inactive"#写一个新文件，该文件的大小为1048576 KBdd if=/dev/zero of=$NEW_FILE bs=1024 count=1048576 &amp;&gt; /dev/null#文件写完后，再次读取File Page Cache的大小get_filecache_size#两次的差异可以近似为该新文件内容对应的File Page Cache#之所以用近似是因为在运行的过程中也可能会有其他Page Cache产生let size_increased="$active + $inactive - $filecache"#输出结果echo "File size 1048576KB, File Cache increased" $size_inc// File size 1048576KB, File Cache increased 1048648KB</code></pre><p>通过这个脚本你可以看到，在创建一个文件的过程中，代码中 /proc/meminfo 里的 Active(file) 和 Inactive(file) 这两项会随着文件内容的增加而增加，它们增加的大小跟文件大小是一致的（这里之所以略有不同，是因为系统中还有其他程序在运行）。另外，如果你观察得很仔细的话，你会发现增加的 Page Cache 是 Inactive(File) 这一项。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-079bc645feb0dd8f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个过程大致可以描述为：首先往用户缓冲区 buffer(这是 Userspace Page) 写入数据，然后 buffer 中的数据拷贝到内核缓冲区（这是 Pagecache Page），如果内核缓冲区中还没有这个 Page，就会发生 Page Fault 会去分配一个 Page，拷贝结束后该 Pagecache Page 是一个 Dirty Page（脏页），然后该 Dirty Page 中的内容会同步到磁盘，同步到磁盘后，该 Pagecache Page 变为 Clean Page 并且继续存在系统中。</p><pre><code>// 查看账页回写$ cat /proc/vmstat | egrep "dirty|writeback"nr_dirty 40nr_writeback 2</code></pre><h3 id="Page-Cache-是如何“死亡”-回收的？"><a href="#Page-Cache-是如何“死亡”-回收的？" class="headerlink" title="Page Cache 是如何“死亡”/回收的？"></a>Page Cache 是如何“死亡”/回收的？</h3><p>你可以把 Page Cache 的回收行为 (Page Reclaim) 理解为 Page Cache 的“自然死亡”。</p><pre><code>$ free -g       total  used  free  shared  buff/cache availableMem:     125    41     6       0          79        82Swap:      0     0     0</code></pre><p>free 命令中的 buff/cache 中的这些就是“活着”的 Page Cache，那它们什么时候会“死亡”（被回收）呢？我们来看一张图：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4e6970042f1ec0dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>你可以看到，应用在申请内存的时候，即使没有 free 内存，只要还有足够可回收的 Page Cache，就可以通过回收 Page Cache 的方式来申请到内存，回收的方式主要是两种：直接回收和后台回收。</p><p>那它是具体怎么回收的呢？你要怎么观察呢？其实在我看来，观察 Page Cache 直接回收和后台回收最简单方便的方式是使用 sar：</p><pre><code>$ sar -B 102:14:01 PM  pgpgin/s pgpgout/s   fault/s  majflt/s  pgfree/s pgscank/s pgscand/s pgsteal/s    %vmeff02:14:01 PM      0.14    841.53 106745.40      0.00  41936.13      0.00      0.00      0.00      0.0002:15:01 PM      5.84    840.97  86713.56      0.00  43612.15    717.81      0.00    717.66     99.9802:16:01 PM     95.02    816.53 100707.84      0.13  46525.81   3557.90      0.00   3556.14     99.9502:17:01 PM     10.56    901.38 122726.31      0.27  54936.13   8791.40      0.00   8790.17     99.9902:18:01 PM    108.14    306.69  96519.75      1.15  67410.50  14315.98     31.48  14319.38     99.8002:19:01 PM      5.97    489.67  88026.03      0.18  48526.07   1061.53      0.00   1061.42     99.99</code></pre><ul><li>pgscank/s : kswapd(后台回收线程) 每秒扫描的 page 个数。</li><li>pgscand/s: Application 在内存申请过程中每秒直接扫描的 page 个数。</li><li>pgsteal/s: 扫描的 page 中每秒被回收的个数。</li><li>%vmeff: pgsteal/(pgscank+pgscand), 回收效率，越接近 100 说明系统越安全，越接近 0 说明系统内存压力越大。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-294e8ed27891c2b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="如何处理Page-Cache难以回收产生的load飙高问题？"><a href="#如何处理Page-Cache难以回收产生的load飙高问题？" class="headerlink" title="如何处理Page Cache难以回收产生的load飙高问题？"></a>如何处理Page Cache难以回收产生的load飙高问题？</h3><p>大多是有三种情况：</p><ul><li>直接内存回收引起的 load 飙高；</li><li>系统中脏页积压过多引起的 load 飙高；</li><li>系统 NUMA 策略配置不当引起的 load 飙高。</li></ul><p><strong>直接内存回收引起 load 飙高或者业务时延抖动</strong></p><p>直接内存回收是指在进程上下文同步进行内存回收，那么它具体是怎么引起 load 飙高的呢？</p><p>因为直接内存回收是在进程申请内存的过程中同步进行的回收，而这个回收过程可能会消耗很多时间，进而导致进程的后续行为都被迫等待，这样就会造成很长时间的延迟，以及系统的 CPU 利用率会升高，最终引起 load 飙高。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8575b1da6940a9da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>从图里你可以看到，在开始内存回收后，首先进行后台异步回收（上图中蓝色标记的地方），这不会引起进程的延迟；如果后台异步回收跟不上进程内存申请的速度，就会开始同步阻塞回收，导致延迟（上图中红色和粉色标记的地方，这就是引起 load 高的地址）。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-43588c1eadb08e22.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>那么，针对直接内存回收引起 load 飙高或者业务 RT 抖动的问题，<strong>一个解决方案就是及早地触发后台回收来避免应用程序进行直接内存回收</strong>，那具体要怎么做呢？</p><p>它的意思是：当内存水位低于 watermark low 时，就会唤醒 kswapd 进行后台回收，然后 kswapd 会一直回收到 watermark high。</p><p>那么，我们可以增大 <code>min_free_kbytes</code> 这个配置选项来及早地触发后台回收。</p><blockquote><p>vm.min_free_kbytes = 4194304</p></blockquote><p>当然了，这样做也有一些缺陷：提高了内存水位后，应用程序可以直接使用的内存量就会减少，这在一定程度上浪费了内存。所以在调整这一项之前，你需要先思考一下，应用程序更加关注什么，如果关注延迟那就适当地增大该值，如果关注内存的使用量那就适当地调小该值。</p><p>除此之外，对 CentOS-6(对应于 2.6.32 内核版本) 而言，还有另外一种解决方案：</p><blockquote><p>vm.extra_free_kbytes = 4194304</p></blockquote><p>那就是将 <code>extra_free_kbytes</code> 配置为 4G。<code>extra_free_kbytes</code> 在 3.10 以及以后的内核上都被废弃掉了，不过由于在生产环境中还存在大量的机器运行着较老版本内核，你使用到的也可能会是较老版本的内核，所以在这里还是有必要提一下。它的大致原理如下所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0b02d7bedbf200e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><code>extra_free_kbytes</code> 的目的是为了解决 min_free_kbyte 造成的内存浪费，但是这种做法并没有被内核主线接收，因为这种行为很难维护会带来一些麻烦，感兴趣的可以看一下这个讨论：add extra free kbytes tunable</p><p>总的来说，通过调整内存水位，在一定程度上保障了应用的内存申请，但是同时也带来了一定的内存浪费，因为系统始终要保障有这么多的 free 内存，这就压缩了 Page Cache 的空间。调整的效果你可以通过 /proc/zoneinfo 来观察：</p><pre><code>$ egrep "min|low|high" /proc/zoneinfo ...        min      7019        low      8773        high     10527...</code></pre><p><strong>补充 kswapd0 说明</strong></p><p>除了直接内存回收，还有一个专门的内核线程用来定期回收内存，也就是 kswapd0。为了衡量内存的使用情况，kswapd0 定义了三个内存阈值（watermark，也称为水位），分别是</p><p>页最小阈值（pages_min）、页低阈值（pages_low）和页高阈值（pages_high）。剩余内存，则使用 pages_free 表示。</p><ul><li>剩余内存小于页最小阈值，说明进程可用内存都耗尽了，只有内核才可以分配内存。</li><li>剩余内存落在页最小阈值和页低阈值中间，说明内存压力比较大，剩余内存不多了。这时 kswapd0 会执行内存回收，直到剩余内存大于高阈值为止。</li><li>剩余内存落在页低阈值和页高阈值中间，说明内存有一定压力，但还可以满足新内存请求。</li><li>剩余内存大于页高阈值，说明剩余内存比较多，没有内存压力。</li></ul><p>我们可以看到，一旦剩余内存小于页低阈值，就会触发内存的回收。这个页低阈值，其实可以通过内核选项 /proc/sys/vm/min_free_kbytes 来间接设置。min_free_kbytes 设置了页最小阈值，而其他两个阈值，都是根据页最小阈值计算生成的，计算方法如下 ：</p><pre><code>pages_low = pages_min*5/4pages_high = pages_min*3/2</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3ba6fe3cc72fb753.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>系统中脏页过多引起 load 飙高</strong></p><p>接下来，我们分析下由于系统脏页过多引起 load 飙高的情况。在前一个案例中我们也提到，直接回收过程中，如果存在较多脏页就可能涉及在回收过程中进行回写，这可能会造成非常大的延迟，而且因为这个过程本身是阻塞式的，所以又可能进一步导致系统中处于 D 状态的进程数增多，最终的表现就是系统的 load 值很高。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7387f88156bdbd51.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>那如何解决这类问题呢？<strong>一个比较省事的解决方案是控制好系统中积压的脏页数据</strong>。很多人知道需要控制脏页，但是往往并不清楚如何来控制好这个度，脏页控制的少了可能会影响系统整体的效率，脏页控制的多了还是会触发问题，所以我们接下来看下如何来衡量好这个“度”。</p><p>首先你可以通过 sar -r 来观察系统中的脏页个数：</p><pre><code>$ sar -r 107:30:01 PM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty09:20:01 PM   5681588   2137312     27.34         0   1807432    193016      2.47    534416   1310876         409:30:01 PM   5677564   2141336     27.39         0   1807500    204084      2.61    539192   1310884        2009:40:01 PM   5679516   2139384     27.36         0   1807508    196696      2.52    536528   1310888        2009:50:01 PM   5679548   2139352     27.36         0   1807516    196624      2.51    536152   1310892        24</code></pre><p>至于这些值调整大多少比较合适，也是因系统和业务的不同而异，我的建议也是一边调整一边观察，将这些值调整到业务可以容忍的程度就可以了，即在调整后需要观察业务的服务质量 (SLA)，要确保 SLA 在可接受范围内。调整的效果你可以通过 /proc/vmstat 来查看：</p><pre><code>$ grep "nr_dirty_" /proc/vmstatnr_dirty_threshold 366998nr_dirty_background_threshold 183275</code></pre><p><strong>系统 NUMA 策略配置不当引起的 load 飙高</strong></p><p>比如说，我们在生产环境上就曾经遇到这样的问题：系统中还有一半左右的 free 内存，但还是频频触发 direct reclaim，导致业务抖动得比较厉害。后来经过排查发现是由于设置了 <code>zone_reclaim_mode</code>，这是 NUMA 策略的一种。</p><p>设置 <code>zone_reclaim_mode</code> 的目的是为了增加业务的 NUMA 亲和性，但是在实际生产环境中很少会有对 NUMA 特别敏感的业务，这也是为什么内核将该配置从默认配置 1 修改为了默认配置 0: mm: disable <code>zone_reclaim_mode</code> by default ，配置为 0 之后，就避免了在其他 node 有空闲内存时，不去使用这些空闲内存而是去回收当前 node 的 Page Cache，也就是说，通过减少内存回收发生的可能性从而避免它引发的业务延迟。</p><p>那么如何来有效地衡量业务延迟问题是否由 zone reclaim 引起的呢？它引起的延迟究竟有多大呢？这个衡量和观察方法也是我贡献给 Linux Kernel 的：mm/vmscan: add tracepoints for node reclaim ，大致的思路就是利用 linux 的 tracepoint 来做这种量化分析，这是性能开销相对较小的一个方案。</p><p>我们可以通过 numactl 来查看服务器的 NUMA 信息，如下是两个 node 的服务器：</p><pre><code>$ numactl --hardwareavailable: 2 nodes (0-1)node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 24 25 26 27 28 29 30 31 32 33 34 35node 0 size: 130950 MBnode 0 free: 108256 MBnode 1 cpus: 12 13 14 15 16 17 18 19 20 21 22 23 36 37 38 39 40 41 42 43 44 45 46 47node 1 size: 131072 MBnode 1 free: 122995 MBnode distances:node   0   1   0:  10  21   1:  21  10 </code></pre><h3 id="如何释放PageCache"><a href="#如何释放PageCache" class="headerlink" title="如何释放PageCache"></a>如何释放PageCache</h3><pre><code>echo 1 &gt; /proc/sys/vm/drop_caches  //释放掉Page Cache中的clean pages (干净页)echo 2 &gt; /proc/sys/vm/drop_caches  //释放掉Slab，包括dentry、inode等echo 3 &gt; /proc/sys/vm/drop_caches  //既释放Page Cache，又释放Slab</code></pre><h3 id="内核机制引起-Page-Cache-被回收而产生的业务性能下降"><a href="#内核机制引起-Page-Cache-被回收而产生的业务性能下降" class="headerlink" title="内核机制引起 Page Cache 被回收而产生的业务性能下降"></a>内核机制引起 Page Cache 被回收而产生的业务性能下降</h3><p>我们在前面已经提到过，在内存紧张的时候会触发内存回收，内存回收会尝试去回收 reclaimable（可以被回收的）内存，这部分内存既包含 Page Cache 又包含 reclaimable kernel memory(比如 slab)。我们可以用下图来简单描述这个过程：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0a23f14e089cbdee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我简单来解释一下这个图。Reclaimer 是指回收者，它可以是内核线程（包括 kswapd）也可以是用户线程。回收的时候，它会依次来扫描 pagecache page 和 slab page 中有哪些可以被回收的，如果有的话就会尝试去回收，如果没有的话就跳过。在扫描可回收 page 的过程中回收者一开始扫描的较少，然后逐渐增加扫描比例直至全部都被扫描完。这就是内存回收的大致过程。</p><h3 id="如何避免-Page-Cache-被回收而引起的性能问题？"><a href="#如何避免-Page-Cache-被回收而引起的性能问题？" class="headerlink" title="如何避免 Page Cache 被回收而引起的性能问题？"></a>如何避免 Page Cache 被回收而引起的性能问题？</h3><ul><li>从应用代码层面来优化；</li><li>从系统层面来调整。</li></ul><p>从应用程序代码层面来解决是相对比较彻底的方案，因为应用更清楚哪些 Page Cache 是重要的，哪些是不重要的，所以就可以明确地来对读写文件过程中产生的 Page Cache 区别对待。比如说，对于重要的数据，可以通过 mlock(2) 来保护它，防止被回收以及被 drop；对于不重要的数据（比如日志），那可以通过 madvise(2) 告诉内核来立即释放这些 Page Cache。</p><pre><code>#include &lt;sys/mman.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;unistd.h&gt;#include &lt;string.h&gt;#include &lt;fcntl.h&gt;#define FILE_NAME "/home/yafang/test/mmap/data"#define SIZE (1024*1000*1000)int main(){        int fd;         char *p;         int ret;        fd = open(FILE_NAME, O_CREAT|O_RDWR, S_IRUSR|S_IWUSR);        if (fd &lt; 0)                return -1;         /* Set size of this file */        ret = ftruncate(fd, SIZE);        if (ret &lt; 0)                return -1;         /* The current offset is 0, so we don't need to reset the offset. */        /* lseek(fd, 0, SEEK_CUR); */        /* Mmap virtual memory */        p = mmap(0, SIZE, PROT_READ|PROT_WRITE, MAP_FILE|MAP_SHARED, fd, 0);         if (!p)                return -1;         /* Alloc physical memory */        memset(p, 1, SIZE);        /* Lock these memory to prevent from being reclaimed */        mlock(p, SIZE);        /* Wait until we kill it specifically */        while (1) {                sleep(10);        }        /*        * Unmap the memory.        * Actually the kernel will unmap it automatically after the        * process exits, whatever we call munamp() specifically or not.        */        munmap(p, SIZE);        return 0;}</code></pre><p>在这个例子中，我们通过 mlock(2) 来锁住了读 FILE_NAME 这个文件内容对应的 Page Cache。在运行上述程序之后，我们来看下该如何来观察这种行为：确认这些 Page Cache 是否被保护住了，被保护了多大。这同样可以通过 /proc/meminfo 来观察:</p><pre><code>$ egrep "Unevictable|Mlocked" /proc/meminfo Unevictable:     1000000 kBMlocked:         1000000 kB</code></pre><p> 然后你可以发现，drop_caches 或者内存回收是回收不了这些内容的，我们的目的也就达到了。</p><p> 在有些情况下，对应用程序而言，修改源码是件比较麻烦的事，如果可以不修改源码来达到目的那就最好不过了。Linux 内核同样实现了这种不改应用程序的源码而从系统层面调整来保护重要数据的机制，这个机制就是 memory cgroup protection。</p><p> 它大致的思路是，将需要保护的应用程序使用 memory cgroup 来保护起来，这样该应用程序读写文件过程中所产生的 Page Cache 就会被保护起来不被回收或者最后被回收。memory cgroup protection 大致的原理如下图所示：</p><p> <img src="https://upload-images.jianshu.io/upload_images/12321605-fbad8f9e482f9da4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如上图所示，memory cgroup 提供了几个内存水位控制线 memory.{min, low, high, max} 。</p><ul><li>memory.max这是指 memory cgroup 内的进程最多能够分配的内存，如果不设置的话，就默认不做内存大小的限制。</li><li>memory.high如果设置了这一项，当 memory cgroup 内进程的内存使用量超过了该值后就会立即被回收掉，所以这一项的目的是为了尽快的回收掉不活跃的 Page Cache。</li><li>memory.low这一项是用来保护重要数据的，当 memory cgroup 内进程的内存使用量低于了该值后，在内存紧张触发回收后就会先去回收不属于该 memory cgroup 的 Page Cache，等到其他的 Page Cache 都被回收掉后再来回收这些 Page Cache。</li><li>memory.min这一项同样是用来保护重要数据的，只不过与 memoy.low 有所不同的是，当 memory cgroup 内进程的内存使用量低于该值后，即使其他不在该 memory cgroup 内的 Page Cache 都被回收完了也不会去回收这些 Page Cache，可以理解为这是用来保护最高优先级的数据的。</li></ul><p>那么，如果你想要保护你的 Page Cache 不被回收，你就可以考虑将你的业务进程放在一个 memory cgroup 中，然后设置 memory.{min,low} 来进行保护；与之相反，如果你想要尽快释放你的 Page Cache，那你可以考虑设置 memory.high 来及时的释放掉不活跃的 Page Cache。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-745ec6aae1887e36.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="内存篇"><a href="#内存篇" class="headerlink" title="内存篇"></a>内存篇</h2><h3 id="进程的地址空间"><a href="#进程的地址空间" class="headerlink" title="进程的地址空间"></a>进程的地址空间</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f0d8fa479c9cca7d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们用一张表格来简单汇总下这些不同的申请方式所对应的不同内存类型，这张表格也包含了我们在课程上一个模块讲的 Page Cache，所以你可以把它理解为是进程申请内存的类型大汇总：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5f5bf921f6ec1225.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="CPU寻址过程"><a href="#CPU寻址过程" class="headerlink" title="CPU寻址过程"></a>CPU寻址过程</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-529d947153c3c37b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如上图所示，Paging 的大致过程是，CPU 将要请求的虚拟地址传给 MMU（Memory Management Unit，内存管理单元），然后 MMU 先在高速缓存 TLB（Translation Lookaside Buffer，页表缓存）中查找转换关系，如果找到了相应的物理地址则直接访问；如果找不到则在地址转换表（Page Table）里查找计算。最终进程访问的虚拟地址就对应到了实际的物理地址。</p><h3 id="用数据观察进程的内存"><a href="#用数据观察进程的内存" class="headerlink" title="用数据观察进程的内存"></a>用数据观察进程的内存</h3><p>那么都有哪些观察进程的工具呢？我们常用来观察进程内存的工具，比如说 pmap、ps、top 等，都可以很好地来观察进程的内存。</p><p>首先我们可以使用 top 来观察系统所有进程的内存使用概况，打开 top 后，然后按 g 再输入 3，从而进入内存模式就可以了。在内存模式中，我们可以看到各个进程内存的 %MEM、VIRT、RES、CODE、DATA、SHR、nMaj、nDRT，这些信息通过 strace 来跟踪 top 进程，你会发现这些信息都是从 /proc/[pid]/statm 和 /proc/[pid]/stat 这个文件里面读取的：</p><pre><code>$ strace -p `pidof top`open("/proc/16348/statm", O_RDONLY)     = 9read(9, "40509 1143 956 24 0 324 0\n", 1024) = 26close(9)                                = 0...open("/proc/16366/stat", O_RDONLY)      = 9read(9, "16366 (kworker/u16:1-events_unbo"..., 1024) = 182close(9)...</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-88eacb262111063c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>另外如果你观察仔细的话，可能会发现，有些时候所有进程的 RES 相加起来要比系统总的物理内存大，这是因为 RES 中有一些内存是被一些进程给共享的。</p><p>在明白了系统中各个进程的内存使用概况后，如果想要继续看某个进程的内存使用细节，你可以使用 pmap。如下是 pmap 来展示 sshd 进程地址空间里的部分内容：</p><pre><code>$  pmap -x `pidof sshd`Address           Kbytes     RSS   Dirty Mode  Mapping 000055e798e1d000     768     652       0 r-x-- sshd000055e7990dc000      16      16      16 r---- sshd000055e7990e0000       4       4       4 rw--- sshd000055e7990e1000      40      40      40 rw---   [ anon ]...00007f189613a000    1800    1624       0 r-x-- libc-2.17.so00007f18962fc000    2048       0       0 ----- libc-2.17.so00007f18964fc000      16      16      16 r---- libc-2.17.so00007f1896500000       8       8       8 rw--- libc-2.17.so...00007ffd9d30f000     132      40      40 rw---   [ stack ]...</code></pre><p>每一行表示一种类型的内存（Virtual Memory Area），每一列的含义如下。</p><ul><li>Mapping，用来表示文件映射中占用内存的文件，比如 sshd 这个可执行文件，或者堆[heap]，或者栈[stack]，或者其他，等等。</li><li>Mode，它是该内存的权限，比如，“r-x”是可读可执行，它往往是代码段 (Text Segment)；“rw-”是可读可写，这部分往往是数据段 (Data Segment)；“r–”是只读，这往往是数据段中的只读部分。</li><li>Address、Kbytes、RSS、Dirty，Address 和 Kbytes 分别表示起始地址和虚拟内存的大小，RSS（Resident Set Size）则表示虚拟内存中已经分配的物理内存的大小，Dirty 则表示内存中数据未同步到磁盘的字节数。</li></ul><p>可以看到，通过 pmap 我们能够清楚地观察一个进程的整个的地址空间，包括它们分配的物理内存大小，这非常有助于我们对进程的内存使用概况做一个大致的判断。比如说，如果地址空间中[heap]太大，那有可能是堆内存产生了泄漏；再比如说，如果进程地址空间包含太多的 vma（可以把 maps 中的每一行理解为一个 vma），那很可能是应用程序调用了很多 mmap 而没有 munmap；再比如持续观察地址空间的变化，如果发现某些项在持续增长，那很可能是那里存在问题。</p><p>pmap 同样也是解析的 /proc 里的文件，具体文件是 /proc/[pid]/maps 和 /proc/[pid]/smaps，其中 smaps 文件相比 maps 的内容更详细，可以理解为是对 maps 的一个扩展。你可以对比 /proc/[pid]/maps 和 pmaps 的输出，你会发现二者的内容是一致的。</p><h3 id="Shmem"><a href="#Shmem" class="headerlink" title="Shmem"></a>Shmem</h3><pre><code>$ cat /proc/meminfo...Shmem  16777216 kB...</code></pre><p>我们在前面的基础篇里提到，Shmem 是指匿名共享内存，即进程以 mmap（MAP_ANON|MAP_SHARED）这种方式来申请的内存。你可能会有疑问，进程以这种方式来申请的内存不应该是属于进程的 RES（resident）吗？比如下面这个简单的示例：</p><pre><code>#include &lt;sys/mman.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#define SIZE (1024*1024*1024)int main(){        char *p;         p = mmap(NULL, SIZE, PROT_READ|PROT_WRITE, MAP_ANON|MAP_SHARED, -1, 0);        if (!p)                return -1;         memset(p, 1, SIZE);        while (1) {                sleep(1);        }           return 0;}</code></pre><p>先说答案：这跟一种特殊的 Shmem 有关。我们知道，磁盘的速度是远远低于内存的，有些应用程序为了提升性能，会避免将一些无需持续化存储的数据写入到磁盘，而是把这部分临时数据写入到内存中，然后定期或者在不需要这部分数据时，清理掉这部分内容来释放出内存。在这种需求下，就产生了一种特殊的 Shmem：tmpfs。tmpfs 如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-aaa95dd95e1b4534.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>它是一种内存文件系统，只存在于内存中，它无需应用程序去申请和释放内存，而是操作系统自动来规划好一部分空间，应用程序只需要往这里面写入数据就可以了，这样会很方便。我们可以使用 moun 命令或者 df 命令来看系统中 tmpfs 的挂载点：</p><pre><code>$ df -hFilesystem      Size  Used Avail Use% Mounted on...tmpfs            16G  15G   1G   94% /run...</code></pre><p>针对这个问题，解决方案就是限制 systemd 所使用的 tmpfs 的大小，在日志量达到 tmpfs 大小限制时，自动地清理掉临时日志，或者定期清理掉这部分日志，这都可以通过 systemd 的配置文件来做到。tmpfs 的大小可以通过如下命令（比如调整为 2G）调整：</p><pre><code>$ mount -o remount,size=2G /run</code></pre><p>tmpfs 作为一种特殊的 Shmem，它消耗的内存是不会体现在进程内存中的，这往往会给问题排查带来一些难度。要想高效地分析这种类型的问题，你必须要去熟悉系统中的内存类型。除了 tmpfs 之外，其他一些类型的内存也不会体现在进程内存中，比如内核消耗的内存：/proc/meminfo 中的 Slab（高速缓存）、KernelStack（内核栈）和 VmallocUsed（内核通过 vmalloc 申请的内存），这些也是你在不清楚内存被谁占用时需要去排查的。</p><h3 id="OOM-Kill方式"><a href="#OOM-Kill方式" class="headerlink" title="OOM Kill方式"></a>OOM Kill方式</h3><p>OOM killer 在杀进程的时候，会把系统中可以被杀掉的进程扫描一遍，根据进程占用的内存以及配置的 oom_score_adj 来计算出进程最终的得分，然后把得分（oom_score）最大的进程给杀掉，如果得分最大的进程有多个，那就把先扫描到的那个给杀掉。</p><p>进程的 oom_score 可以通过 /proc/[pid]/oom_score 来查看，你可以扫描一下你系统中所有进程的 oom_score，其中分值最大的那个就是在发生 OOM 时最先被杀掉的进程。不过你需要注意，由于 oom_score 和进程的内存开销有关，而进程的内存开销又是会动态变化的，所以该值也会动态变化。</p><p>如果你不想这个进程被首先杀掉，那你可以调整该进程的 oom_score_adj 改变这个 oom_score；如果你的进程无论如何都不能被杀掉，那你可以将 oom_score_adj 配置为 -1000。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ef7223104caff726.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="内核内存"><a href="#内核内存" class="headerlink" title="内核内存"></a>内核内存</h3><p>应用程序可以通过 malloc() 和 free() 在用户态申请和释放内存，与之对应，可以通过 kmalloc()/kfree() 以及 vmalloc()/vfree() 在内核态申请和释放内存。当然，还有其他申请和释放内存的方法，但大致可以分为这两类。</p><p>从最右侧的物理内存中你可以看出这两类内存申请方式的主要区别，kmalloc() 内存的物理地址是连续的，而 vmalloc() 内存的物理地址则是不连续的。这两种不同类型的内存也是可以通过 /proc/meminfo 来观察的：</p><pre><code>$ cat /proc/meminfo...Slab:            2400284 kBSReclaimable:      47248 kBSUnreclaim:      2353036 kB...VmallocTotal:   34359738367 kBVmallocUsed:     1065948 kB...</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-89fc7bc87f1c17a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>其中 vmalloc 申请的内存会体现在 VmallocUsed 这一项中，即已使用的 Vmalloc 区大小；而 kmalloc 申请的内存则是体现在 Slab 这一项中，它又分为两部分，其中 SReclaimable 是指在内存紧张的时候可以被回收的内存，而 SUnreclaim 则是不可以被回收只能主动释放的内存。</p><p>内核之所以将 kmalloc 和 vmalloc 的信息通过 /proc/meminfo 给导出来，也是为了在它们引起问题的时候，让我们可以有方法来进行排查。在讲述具体的案例以及排查方法之前，我们先以一个简单的程序来看下内核空间是如何进行内存申请和释放的。</p><pre><code>/* kmem_test */#include &lt;linux/init.h&gt;#include &lt;linux/vmalloc.h&gt;#define SIZE (1024 * 1024 * 1024)char *kaddr;char *kmem_alloc(unsigned long size){        char *p;        p = vmalloc(size);        if (!p)                pr_info("[kmem_test]: vmalloc failed\n");        return p;}void kmem_free(const void *addr){        if (addr)                vfree(addr);}int __init kmem_init(void){        pr_info("[kmem_test]: kernel memory init\n");        kaddr = kmem_alloc(SIZE);        return 0;}void __exit kmem_exit(void){        kmem_free(kaddr);        pr_info("[kmem_test]: kernel memory exit\n");}module_init(kmem_init)module_exit(kmem_exit)MODULE_LICENSE("GPLv2");/* kmem_test */#include &lt;linux/init.h&gt;#include &lt;linux/vmalloc.h&gt;#define SIZE (1024 * 1024 * 1024)char *kaddr;char *kmem_alloc(unsigned long size){        char *p;        p = vmalloc(size);        if (!p)                pr_info("[kmem_test]: vmalloc failed\n");        return p;}void kmem_free(const void *addr){        if (addr)                vfree(addr);}int __init kmem_init(void){        pr_info("[kmem_test]: kernel memory init\n");        kaddr = kmem_alloc(SIZE);        return 0;}void __exit kmem_exit(void){        kmem_free(kaddr);        pr_info("[kmem_test]: kernel memory exit\n");}module_init(kmem_init)module_exit(kmem_exit)MODULE_LICENSE("GPLv2");</code></pre><p>这是一个典型的内核模块，在这个内核模块中，我们使用 vmalloc 来分配了 1G 的内存空间，然后在模块退出的时候使用 vfree 释放掉它。这在形式上跟应用申请 / 释放内存其实是一致的，只是申请和释放内存的接口函数不一样而已。</p><p>我们需要使用 Makefile 来编译这个内核模块：</p><pre><code>obj-m = kmem_test.oall:        make -C /lib/modules/`uname -r`/build M=`pwd`clean:        rm -f *.o *.ko *.mod.c *.mod *.a modules.order Module.symvers        </code></pre><p>执行 make 命令后就会生成一个 kmem_test 的内核模块，接着执行下面的命令就可以安装该模块了：</p><pre><code>$ insmod kmem_test</code></pre><p>用 rmmod 命令则可以把它卸载掉：</p><pre><code>$ rmmod kmem_test</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-eba928eb16a22401.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="TCP"><a href="#TCP" class="headerlink" title="TCP"></a>TCP</h2><h3 id="TCP-连接的建立过程会受哪些配置项的影响？"><a href="#TCP-连接的建立过程会受哪些配置项的影响？" class="headerlink" title="TCP 连接的建立过程会受哪些配置项的影响？"></a>TCP 连接的建立过程会受哪些配置项的影响？</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-946f31dea498889f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>首先 Client 会给 Server 发送一个 SYN 包，但是该 SYN 包可能会在传输过程中丢失，或者因为其他原因导致 Server 无法处理，此时 Client 这一侧就会触发超时重传机制。但是也不能一直重传下去，重传的次数也是有限制的，这就是 <code>tcp_syn_retries</code> 这个配置项来决定的。</p><p>假设 <code>tcp_syn_retries</code> 为 3，那么 SYN 包重传的策略大致如下：</p><p>在 Client 发出 SYN 后，如果过了 1 秒 ，还没有收到 Server 的响应，那么就会进行第一次重传；如果经过 2s 的时间还没有收到 Server 的响应，就会进行第二次重传；一直重传 tcp_syn_retries 次。</p><p>对于 <code>tcp_syn_retries</code> 为 3 而言，总共会重传 3 次，也就是说从第一次发出 SYN 包后，会一直等待（1 + 2 + 4 + 8）秒，如果还没有收到 Server 的响应，connect() 就会产生 ETIMEOUT 的错误。</p><p>tcp_syn_retries 的默认值是 6，也就是说如果 SYN 一直发送失败，会在（1 + 2 + 4 + 8 + 16+ 32 + 64）秒，即 127 秒后产生 ETIMEOUT 的错误。</p><p>所以通常情况下，我们都会将数据中心内部服务器的 <code>tcp_syn_retries</code> 给调小，这里推荐设置为 2，来减少阻塞的时间。因为对于数据中心而言，它的网络质量是很好的，如果得不到 Server 的响应，很可能是 Server 本身出了问题。在这种情况下，Client 及早地去尝试连接其他的 Server 会是一个比较好的选择，所以对于客户端而言，一般都会做如下调整：</p><blockquote><p>net.ipv4.tcp_syn_retries = 2</p></blockquote><p>如果 Server 没有响应 Client 的 SYN，除了我们刚才提到的 Server 已经不存在了这种情况外，还有可能是因为 Server 太忙没有来得及响应，或者是 Server 已经积压了太多的半连接（incomplete）而无法及时去处理。</p><p>半连接，即收到了 SYN 后还没有回复 SYNACK 的连接，Server 每收到一个新的 SYN 包，都会创建一个半连接，然后把该半连接加入到半连接队列（syn queue）中。syn queue 的长度就是 <code>tcp_max_syn_backlog</code> 这个配置项来决定的，当系统中积压的半连接个数超过了该值后，新的 SYN 包就会被丢弃。对于服务器而言，可能瞬间会有非常多的新建连接，所以我们可以适当地调大该值，以免 SYN 包被丢弃而导致 Client 收不到 SYNACK：</p><blockquote><p>net.ipv4.tcp_max_syn_backlog = 16384</p></blockquote><p>Server 中积压的半连接较多，也有可能是因为有些恶意的 Client 在进行 SYN Flood 攻击。典型的 SYN Flood 攻击如下：Client 高频地向 Server 发 SYN 包，并且这个 SYN 包的源 IP 地址不停地变换，那么 Server 每次接收到一个新的 SYN 后，都会给它分配一个半连接，Server 的 SYNACK 根据之前的 SYN 包找到的是错误的 Client IP， 所以也就无法收到 Client 的 ACK 包，导致无法正确建立 TCP 连接，这就会让 Server 的半连接队列耗尽，无法响应正常的 SYN 包。</p><p>在 Server 收到 SYN 包时，不去分配资源来保存 Client 的信息，而是根据这个 SYN 包计算出一个 Cookie 值，然后将 Cookie 记录到 SYNACK 包中发送出去。对于正常的连接，该 Cookies 值会随着 Client 的 ACK 报文被带回来。然后 Server 再根据这个 Cookie 检查这个 ACK 包的合法性，如果合法，才去创建新的 TCP 连接。通过这种处理，SYN Cookies 可以防止部分 SYN Flood 攻击。所以对于 Linux 服务器而言，推荐开启 SYN Cookies：</p><blockquote><p>net.ipv4.tcp_syncookies = 1</p></blockquote><p>Server 向 Client 发送的 SYNACK 包也可能会被丢弃，或者因为某些原因而收不到 Client 的响应，这个时候 Server 也会重传 SYNACK 包。同样地，重传的次数也是由配置选项来控制的，该配置选项是 tcp_synack_retries。</p><p>tcp_synack_retries 的重传策略跟我们在前面讲的 tcp_syn_retries 是一致的，所以我们就不再画图来讲解它了。它在系统中默认是 5，对于数据中心的服务器而言，通常都不需要这么大的值，推荐设置为 2 :</p><blockquote><p>net.ipv4.tcp_synack_retries = 2</p></blockquote><p>Client 在收到 Serve 的 SYNACK 包后，就会发出 ACK，Server 收到该 ACK 后，三次握手就完成了，即产生了一个 TCP 全连接（complete），它会被添加到全连接队列（accept queue）中。然后 Server 就会调用 accept() 来完成 TCP 连接的建立。</p><p>但是，就像半连接队列（syn queue）的长度有限制一样，全连接队列（accept queue）的长度也有限制，目的就是为了防止 Server 不能及时调用 accept() 而浪费太多的系统资源。</p><p>全连接队列（accept queue）的长度是由 listen(sockfd, backlog) 这个函数里的 backlog 控制的，而该 backlog 的最大值则是 somaxconn。somaxconn 在 5.4 之前的内核中，默认都是 128（5.4 开始调整为了默认 4096），建议将该值适当调大一些：</p><blockquote><p>net.core.somaxconn = 16384</p></blockquote><p>当服务器中积压的全连接个数超过该值后，新的全连接就会被丢弃掉。Server 在将新连接丢弃时，有的时候需要发送 reset 来通知 Client，这样 Client 就不会再次重试了。不过，默认行为是直接丢弃不去通知 Client。至于是否需要给 Client 发送 reset，是由 tcp_abort_on_overflow 这个配置项来控制的，该值默认为 0，即不发送 reset 给 Client。推荐也是将该值配置为 0:</p><blockquote><p>net.ipv4.tcp_abort_on_overflow = 0</p></blockquote><p>这是因为，Server 如果来不及 accept() 而导致全连接队列满，这往往是由瞬间有大量新建连接请求导致的，正常情况下 Server 很快就能恢复，然后 Client 再次重试后就可以建连成功了。也就是说，将 tcp_abort_on_overflow 配置为 0，给了 Client 一个重试的机会。当然，你可以根据你的实际情况来决定是否要使能该选项。</p><p>accept() 成功返回后，一个新的 TCP 连接就建立完成了，TCP 连接进入到了 ESTABLISHED 状态：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6b8043b0ba122772.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>上图就是从 Client 调用 connect()，到 Server 侧 accept() 成功返回这一过程中的 TCP 状态转换。这些状态都可以通过 netstat 或者 ss 命令来看。至此，Client 和 Server 两边就可以正常通信了。</p><h3 id="TCP-连接的断开过程会受哪些配置项的影响？"><a href="#TCP-连接的断开过程会受哪些配置项的影响？" class="headerlink" title="TCP 连接的断开过程会受哪些配置项的影响？"></a>TCP 连接的断开过程会受哪些配置项的影响？</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-473b3f20c05dc25b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如上所示，当应用程序调用 close() 时，会向对端发送 FIN 包，然后会接收 ACK；对端也会调用 close() 来发送 FIN，然后本端也会向对端回 ACK，这就是 TCP 的四次挥手过程。</p><p>首先调用 close() 的一侧是 active close（主动关闭）；而接收到对端的 FIN 包后再调用 close() 来关闭的一侧，称之为 passive close（被动关闭）。在四次挥手的过程中，有三个 TCP 状态需要额外关注，就是上图中深红色的那三个状态：主动关闭方的 <code>FIN_WAIT_2</code> 和 <code>TIME_WAIT</code>，以及被动关闭方的 <code>CLOSE_WAIT</code> 状态。除了 CLOSE_WAIT 状态外，其余两个状态都有对应的系统配置项来控制。</p><p>我们首先来看 <code>FIN_WAIT_2</code> 状态，TCP 进入到这个状态后，如果本端迟迟收不到对端的 FIN 包，那就会一直处于这个状态，于是就会一直消耗系统资源。Linux 为了防止这种资源的开销，设置了这个状态的超时时间 <code>tcp_fin_timeout</code>，默认为 60s，超过这个时间后就会自动销毁该连接。</p><p>至于本端为何迟迟收不到对端的 FIN 包，通常情况下都是因为对端机器出了问题，或者是因为太繁忙而不能及时 close()。所以，通常我们都建议将 <code>tcp_fin_timeout</code> 调小一些，以尽量避免这种状态下的资源开销。对于数据中心内部的机器而言，将它调整为 2s 足以：</p><blockquote><p>net.ipv4.tcp_fin_timeout = 2</p></blockquote><p>我们再来看 <code>TIME_WAIT</code> 状态，<code>TIME_WAIT</code> 状态存在的意义是：最后发送的这个 ACK 包可能会被丢弃掉或者有延迟，这样对端就会再次发送 FIN 包。如果不维持 <code>TIME_WAIT</code> 这个状态，那么再次收到对端的 FIN 包后，本端就会回一个 Reset 包，这可能会产生一些异常。</p><p>所以维持 <code>TIME_WAIT</code> 状态一段时间，可以保障 TCP 连接正常断开。<code>TIME_WAIT</code> 的默认存活时间在 Linux 上是 60s（<code>TCP_TIMEWAIT_LEN</code>），这个时间对于数据中心而言可能还是有些长了，所以有的时候也会修改内核做些优化来减小该值，或者将该值设置为可通过 sysctl 来调节。</p><p>TIME_WAIT 状态存在这么长时间，也是对系统资源的一个浪费，所以系统也有配置项来限制该状态的最大个数，该配置选项就是 <code>tcp_max_tw_buckets</code>。对于数据中心而言，网络是相对很稳定的，基本不会存在 FIN 包的异常，所以建议将该值调小一些：</p><blockquote><p>net.ipv4.tcp_max_tw_buckets = 10000</p></blockquote><p>Client 关闭跟 Server 的连接后，也有可能很快再次跟 Server 之间建立一个新的连接，而由于 TCP 端口最多只有 65536 个，如果不去复用处于 <code>TIME_WAIT</code> 状态的连接，就可能在快速重启应用程序时，出现端口被占用而无法创建新连接的情况。所以建议你打开复用 <code>TIME_WAIT</code> 的选项：</p><blockquote><p>net.ipv4.tcp_tw_reuse = 1</p></blockquote><p>还有另外一个选项 <code>tcp_tw_recycle</code> 来控制 <code>TIME_WAIT</code> 状态，但是该选项是很危险的，因为它可能会引起意料不到的问题，比如可能会引起 NAT 环境下的丢包问题。所以建议将该选项关闭：</p><blockquote><p>net.ipv4.tcp_tw_recycle = 0</p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ba16b5ce1ff45ae1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="TCP-数据包的发送过程会受什么影响？"><a href="#TCP-数据包的发送过程会受什么影响？" class="headerlink" title="TCP 数据包的发送过程会受什么影响？"></a>TCP 数据包的发送过程会受什么影响？</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a8c13789a39c8ebc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>上图就是一个简略的 TCP 数据包的发送过程。应用程序调用 write(2) 或者 send(2) 系列系统调用开始往外发包时，这些系统调用会把数据包从用户缓冲区拷贝到 TCP 发送缓冲区（TCP Send Buffer），这个 TCP 发送缓冲区的大小是受限制的，这里也是容易引起问题的地方。</p><p>TCP 发送缓冲区的大小默认是受 net.ipv4.tcp_wmem 来控制：</p><pre><code>net.ipv4.tcp_wmem = 8192 65536 16777216</code></pre><p>tcp_wmem 中这三个数字的含义分别为 min、default、max。TCP 发送缓冲区的大小会在 min 和 max 之间动态调整，初始的大小是 default，这个动态调整的过程是由内核自动来做的，应用程序无法干预。自动调整的目的，是为了在尽可能少的浪费内存的情况下来满足发包的需要。</p><p>tcp_wmem 中的 max 不能超过 net.core.wmem_max 这个配置项的值，如果超过了，TCP 发送缓冲区最大就是 net.core.wmem_max。通常情况下，我们需要设置 net.core.wmem_max 的值大于等于 net.ipv4.tcp_wmem 的 max：</p><pre><code>net.core.wmem_max = 16777216</code></pre><p>对于 TCP 发送缓冲区的大小，我们需要根据服务器的负载能力来灵活调整。通常情况下我们需要调大它们的默认值，我上面列出的 tcp_wmem 的 min、default、max 这几组数值就是调大后的值，也是我们在生产环境中配置的值。</p><p>我之所以将这几个值给调大，是因为我们在生产环境中遇到过 TCP 发送缓冲区太小，导致业务延迟很大的问题，这类问题也是可以使用 systemtap 之类的工具在内核里面打点来进行观察的（观察 sk_stream_wait_memory 这个事件）:</p><p>如果你可以观察到 sk_stream_wait_memory 这个事件，就意味着 TCP 发送缓冲区太小了，你需要继续去调大 wmem_max 和 tcp_wmem:max 的值了。</p><p>应用程序有的时候会很明确地知道自己发送多大的数据，需要多大的 TCP 发送缓冲区，这个时候就可以通过 setsockopt(2) 里的 SO_SNDBUF 来设置固定的缓冲区大小。一旦进行了这种设置后，tcp_wmem 就会失效，而且这个缓冲区大小设置的是固定值，内核也不会对它进行动态调整。</p><p>但是，SO_SNDBUF 设置的最大值不能超过 net.core.wmem_max，如果超过了该值，内核会把它强制设置为 net.core.wmem_max。所以，如果你想要设置 SO_SNDBUF，一定要确认好 net.core.wmem_max 是否满足需求，否则你的设置可能发挥不了作用。通常情况下，我们都不会通过 SO_SNDBUF 来设置 TCP 发送缓冲区的大小，而是使用内核设置的 tcp_wmem，因为如果 SO_SNDBUF 设置得太大就会浪费内存，设置得太小又会引起缓冲区不足的问题。</p><p>另外，如果你关注过 Linux 的最新技术动态，你一定听说过 eBPF。你也可以通过 eBPF 来设置 SO_SNDBUF 和 SO_RCVBUF，进而分别设置 TCP 发送缓冲区和 TCP 接收缓冲区的大小。同样地，使用 eBPF 来设置这两个缓冲区时，也不能超过 wmem_max 和 rmem_max。不过 eBPF 在一开始增加设置缓冲区大小的特性时并未考虑过最大值的限制，我在使用的过程中发现这里存在问题，就给社区提交了一个 PATCH 把它给修复了。你感兴趣的话可以看下这个链接：bpf: sock recvbuff must be limited by rmem_max in bpf_setsockopt()。</p><p>tcp_wmem 以及 wmem_max 的大小设置都是针对单个 TCP 连接的，这两个值的单位都是 Byte（字节）。系统中可能会存在非常多的 TCP 连接，如果 TCP 连接太多，就可能导致内存耗尽。因此，所有 TCP 连接消耗的总内存也有限制：</p><pre><code>net.ipv4.tcp_mem = 8388608 12582912 16777216</code></pre><p>我们通常也会把这个配置项给调大。与前两个选项不同的是，该选项中这些值的单位是 Page（页数），也就是 4K。它也有 3 个值：min、pressure、max。当所有 TCP 连接消耗的内存总和达到 max 后，也会因达到限制而无法再往外发包。</p><p>TCP 层处理完数据包后，就继续往下来到了 IP 层。IP 层这里容易触发问题的地方是 net.ipv4.ip_local_port_range 这个配置选项，它是指和其他服务器建立 IP 连接时本地端口（local port）的范围。我们在生产环境中就遇到过默认的端口范围太小，以致于无法创建新连接的问题。所以通常情况下，我们都会扩大默认的端口范围：</p><pre><code>net.ipv4.ip_local_port_range = 1024 65535</code></pre><p>为了能够对 TCP/IP 数据流进行流控，Linux 内核在 IP 层实现了 qdisc（排队规则）。我们平时用到的 TC 就是基于 qdisc 的流控工具。qdisc 的队列长度是我们用 ifconfig 来看到的 txqueuelen，我们在生产环境中也遇到过因为 txqueuelen 太小导致数据包被丢弃的情况，这类问题可以通过下面这个命令来观察：</p><pre><code>$ ip -s -s link ls dev eth0…TX: bytes packets errors dropped carrier collsns3263284 25060 0 0 0 0</code></pre><p>如果观察到 dropped 这一项不为 0，那就有可能是 txqueuelen 太小导致的。当遇到这种情况时，你就需要增大该值了，比如增加 eth0 这个网络接口的 txqueuelen：</p><p>Linux 系统默认的 qdisc 为 pfifo_fast（先进先出），通常情况下我们无需调整它。如果你想使用TCP BBR来改善 TCP 拥塞控制的话，那就需要将它调整为 fq（fair queue, 公平队列）：</p><pre><code>net.core.default_qdisc = fq</code></pre><p>经过 IP 层后，数据包再往下就会进入到网卡了，然后通过网卡发送出去。至此，你需要发送出去的数据就走完了 TCP/IP 协议栈，然后正常地发送给对端了。</p><h3 id="TCP-数据包的接收过程会受什么影响？"><a href="#TCP-数据包的接收过程会受什么影响？" class="headerlink" title="TCP 数据包的接收过程会受什么影响？"></a>TCP 数据包的接收过程会受什么影响？</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7df50d4d2a0995aa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>从上图可以看出，TCP 数据包的接收流程在整体上与发送流程类似，只是方向是相反的。数据包到达网卡后，就会触发中断（IRQ）来告诉 CPU 读取这个数据包。但是在高性能网络场景下，数据包的数量会非常大，如果每来一个数据包都要产生一个中断，那 CPU 的处理效率就会大打折扣，所以就产生了 NAPI（New API）这种机制让 CPU 一次性地去轮询（poll）多个数据包，以批量处理的方式来提升效率，降低网卡中断带来的性能开销。</p><p>那在 poll 的过程中，一次可以 poll 多少个呢？这个 poll 的个数可以通过 sysctl 选项来控制：</p><pre><code>net.core.netdev_budget = 600</code></pre><p>该控制选项的默认值是 300，在网络吞吐量较大的场景中，我们可以适当地增大该值，比如增大到 600。增大该值可以一次性地处理更多的数据包。但是这种调整也是有缺陷的，因为这会导致 CPU 在这里 poll 的时间增加，如果系统中运行的任务很多的话，其他任务的调度延迟就会增加。</p><p>接下来继续看 TCP 数据包的接收过程。我们刚才提到，数据包到达网卡后会触发 CPU 去 poll 数据包，这些 poll 的数据包紧接着就会到达 IP 层去处理，然后再达到 TCP 层，这时就会面对另外一个很容易引发问题的地方了：TCP Receive Buffer（TCP 接收缓冲区）。</p><p>与 TCP 发送缓冲区类似，TCP 接收缓冲区的大小也是受控制的。通常情况下，默认都是使用 tcp_rmem 来控制缓冲区的大小。同样地，我们也会适当地增大这几个值的默认值，来获取更好的网络性能，调整为如下数值：</p><pre><code>net.ipv4.tcp_rmem = 8192 87380 16777216</code></pre><p>它也有 3 个字段：min、default、max。TCP 接收缓冲区大小也是在 min 和 max 之间动态调整 ，不过跟发送缓冲区不同的是，这个动态调整是可以通过控制选项来关闭的，这个选项是 tcp_moderate_rcvbuf 。通常我们都是打开它，这也是它的默认值：</p><pre><code>net.ipv4.tcp_moderate_rcvbuf = 1</code></pre><p>之所以接收缓冲区有选项可以控制自动调节，而发送缓冲区没有，那是因为 TCP 接收缓冲区会直接影响 TCP 拥塞控制，进而影响到对端的发包，所以使用该控制选项可以更加灵活地控制对端的发包行为。</p><p>除了 <code>tcp_moderate_rcvbuf</code> 可以控制 TCP 接收缓冲区的动态调节外，也可以通过 setsockopt() 中的配置选项 <code>SO_RCVBUF</code> 来控制，这与 TCP 发送缓冲区是类似的。如果应用程序设置了 <code>SO_RCVBUF</code> 这个标记，那么 TCP 接收缓冲区的动态调整就是关闭，即使 <code>tcp_moderate_rcvbuf</code> 为 1，接收缓冲区的大小始终就为设置的 SO_RCVBUF 这个值。</p><p>也就是说，只有在 <code>tcp_moderate_rcvbuf</code> 为 1，并且应用程序没有通过 SO_RCVBUF 来配置缓冲区大小的情况下，TCP 接收缓冲区才会动态调节。</p><p>同样地，与 TCP 发送缓冲区类似，SO_RCVBUF 设置的值最大也不能超过 <code>net.core.rmem_max</code>。通常情况下，我们也需要设置 <code>net.core.rmem_max</code> 的值大于等于 <code>net.ipv4.tcp_rmem</code> 的 max：</p><pre><code>net.core.rmem_max = 16777216</code></pre><p>我们在生产环境中也遇到过，因达到了 TCP 接收缓冲区的限制而引发的丢包问题。但是这类问题不是那么好追踪的，没有一种很直观地追踪这种行为的方式，所以我便在我们的内核里添加了针对这种行为的统计。</p><h2 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h2><h3 id="CPU-是如何选择线程执行的-？"><a href="#CPU-是如何选择线程执行的-？" class="headerlink" title="CPU 是如何选择线程执行的 ？"></a>CPU 是如何选择线程执行的 ？</h3><p>你知道，一个系统中可能会运行着非常多的线程，这些线程数可能远超系统中的 CPU 核数，这时候这些任务就需要排队，每个 CPU 都会维护着自己运行队列（runqueue）里的线程。这个运行队列的结构大致如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e85459e43642ee35.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>每个 CPU 都有自己的运行队列（runqueue），需要运行的线程会被加入到这个队列中。因为有些线程的优先级高，Linux 内核为了保障这些高优先级任务的执行，设置了不同的调度类（Scheduling Class），如下所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b44577ea158f2dbb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这几个调度类的优先级如下：Deadline &gt; Realtime &gt; Fair。Linux 内核在选择下一个任务执行时，会按照该顺序来进行选择，也就是先从 dl_rq 里选择任务，然后从 rt_rq 里选择任务，最后从 cfs_rq 里选择任务。所以实时任务总是会比普通任务先得到执行。</p><p>如果你的某些任务对延迟容忍度很低，比如说在嵌入式系统中就有很多这类任务，那就可以考虑将你的任务设置为实时任务，比如将它设置为 SCHED_FIFO 的任务：</p><pre><code>$ chrt -f -p 1 1327</code></pre><p>如果你不做任何设置的话，用户线程在默认情况下都是普通线程，也就是属于 Fair 调度类，由 CFS 调度器来进行管理。CFS 调度器的目的是为了实现线程运行的公平性，举个例子，假设一个 CPU 上有两个线程需要执行，那么每个线程都将分配 50% 的 CPU 时间，以保障公平性。其实，各个线程之间执行时间的比例，也是可以人为干预的，比如在 Linux 上可以调整进程的 nice 值来干预，从而让优先级高一些的线程执行更多时间。这就是 CFS 调度器的大致思想。</p><h3 id="TOP-指标"><a href="#TOP-指标" class="headerlink" title="TOP 指标"></a>TOP 指标</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fa2896f09bbc4b2e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在上面这几项中，idle 和 wait 是 CPU 不工作的时间，其余的项都是 CPU 工作的时间。idle 和 wait 的主要区别是，idle 是 CPU 无事可做，而 wait 则是 CPU 想做事却做不了。你也可以将 wait 理解为是一类特殊的 idle，即该 CPU 上有至少一个线程阻塞在 I/O 时的 idle。</p><p>而我们通过对 CPU 利用率的细化监控发现，案例中的 CPU 利用率飙高是由 sys 利用率变高导致的，也就是说 sys 利用率会忽然飙高一下，比如在 usr 低于 30% 的情况下，sys 会高于 15%，持续几秒后又恢复正常。</p><p>从该调用栈我们可以看出，此时这个 java 线程在申请 THP（do_huge_pmd_anonymous_page）。THP 就是透明大页，它是一个 2M 的连续物理内存。但是，因为这个时候物理内存中已经没有连续 2M 的内存空间了，所以触发了 direct compaction（直接内存规整），内存规整的过程可以用下图来表示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6a9735b7f83c1a08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个过程并不复杂，在进行 compcation 时，线程会从前往后扫描已使用的 movable page，然后从后往前扫描 free page，扫描结束后会把这些 movable page 给迁移到 free page 里，最终规整出一个 2M 的连续物理内存，这样 THP 就可以成功申请内存了。</p><p>direct compaction 这个过程是很耗时的，而且在 2.6.32 版本的内核上，该过程需要持有粗粒度的锁，所以在运行过程中线程还可能会主动检查（_cond_resched）是否有其他更高优先级的任务需要执行。如果有的话就会让其他线程先执行，这便进一步加剧了它的执行耗时。这也就是 sys 利用率飙高的原因。关于这些，你也都可以从内核源码的注释来看到：</p><p>关闭了生产环境上的 THP 后，我们又在线下测试环境中评估了 THP 对该业务的性能影响，我们发现 THP 并不能给该业务带来明显的性能提升，即使是在内存不紧张、不会触发内存规整的情况下。这也引起了我的思考，<strong>THP 究竟适合什么样的业务呢</strong>？</p><p>这就要从 THP 的目的来说起了。我们长话短说，THP 的目的是用一个页表项来映射更大的内存（大页），这样可以减少 Page Fault，因为需要的页数少了。当然，这也会提升 TLB 命中率，因为需要的页表项也少了。如果进程要访问的数据都在这个大页中，那么这个大页就会很热，会被缓存在 Cache 中。而大页对应的页表项也会出现在 TLB 中，从上一讲的存储层次我们可以知道，这有助于性能提升。但是反过来，假设应用程序的数据局部性比较差，它在短时间内要访问的数据很随机地位于不同的大页上，那么大页的优势就会消失。</p><p>因此，我们基于大页给业务做性能优化的时候，首先要评估业务的数据局部性，尽量把业务的热点数据聚合在一起，以便于充分享受大页的优势。以我在华为任职期间所做的大页性能优化为例，我们将业务的热点数据聚合在一起，然后将这些热点数据分配到大页上，再与不使用大页的情况相比，最终发现这可以带来 20% 以上的性能提升。对于 TLB 较小的架构（比如 MIPS 这种架构），它可以带来 50% 以上的性能提升。当然了，我们在这个过程中也对内核的大页代码做了很多优化，这里就不展开说了。</p><p>针对 THP 的使用，我在这里给你几点建议：</p><ul><li>不要将 /sys/kernel/mm/transparent_hugepage/enabled 配置为 always，你可以将它配置为 madvise。如果你不清楚该如何来配置，那就将它配置为 never；</li><li>如果你想要用 THP 优化业务，最好可以让业务以 madvise 的方式来使用大页，即通过修改业务代码来指定特定数据使用 THP，因为业务更熟悉自己的数据流；</li><li>很多时候修改业务代码会很麻烦，如果你不想修改业务代码的话，那就去优化 THP 的内核代码吧。</li></ul><h3 id="strace-原理"><a href="#strace-原理" class="headerlink" title="strace 原理"></a>strace 原理</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-691f7f14e99c1dd6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们从图中可以看到，对于正在运行的进程而言，strace 可以 attach 到目标进程上，这是通过 ptrace 这个系统调用实现的（gdb 工具也是如此）。ptrace 的 PTRACE_SYSCALL 会去追踪目标进程的系统调用；目标进程被追踪后，每次进入 syscall，都会产生 SIGTRAP 信号并暂停执行；追踪者通过目标进程触发的 SIGTRAP 信号，就可以知道目标进程进入了系统调用，然后追踪者会去处理该系统调用，我们用 strace 命令观察到的信息输出就是该处理的结果；追踪者处理完该系统调用后，就会恢复目标进程的执行。被恢复的目标进程会一直执行下去，直到下一个系统调用。</p><p>你可以发现，目标进程每执行一次系统调用都会被打断，等 strace 处理完后，目标进程才能继续执行，这就会给目标进程带来比较明显的延迟。因此，在生产环境中我不建议使用该命令，如果你要使用该命令来追踪生产环境的问题，那就一定要做好预案。</p><p>假设我们使用 strace 跟踪到，线程延迟抖动是由某一个系统调用耗时长导致的，那么接下来我们该怎么继续追踪呢？这就到了应用开发者和运维人员需要拓展分析边界的时刻了，对内核开发者来说，这才算是分析问题的开始。</p>]]></content>
      
      
      <categories>
          
          <category> System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Linux性能优化实战》</title>
      <link href="/2020/09/13/note/linux-optimize/"/>
      <url>/2020/09/13/note/linux-optimize/</url>
      
        <content type="html"><![CDATA[<h2 id="CPU性能篇"><a href="#CPU性能篇" class="headerlink" title="CPU性能篇"></a>CPU性能篇</h2><h3 id="uptime-什么是平均负载？"><a href="#uptime-什么是平均负载？" class="headerlink" title="uptime 什么是平均负载？"></a>uptime 什么是平均负载？</h3><p>简单来说，平均负载是指单位时间内，<strong>系统处于可运行状态和不可中断状态的平均进程数</strong>，也就是<strong>平均活跃进程数</strong>，它和 CPU 使用率并没有直接关系。这里我先解释下，可运行状态和不可中断状态这俩词儿。</p><p>所谓可运行状态的进程，是指正在使用 CPU 或者正在等待 CPU 的进程，也就是我们常用 ps 命令看到的，处于 R 状态（Running 或 Runnable）的进程。</p><p>不可中断状态的进程则是正处于内核态关键流程中的进程，并且这些流程是不可打断的，比如最常见的是等待硬件设备的 I/O 响应，也就是我们在 ps 命令中看到的 D 状态（Uninterruptible Sleep，也称为 Disk Sleep）的进程。</p><p>比如，当一个进程向磁盘读写数据时，为了保证数据的一致性，在得到磁盘回复前，它是不能被其他进程或者中断打断的，这个时候的进程就处于不可中断状态。如果此时的进程被打断了，就容易出现磁盘数据与进程数据不一致的问题。</p><p>所以，<strong>不可中断状态实际上是系统对进程和硬件设备的一种保护机制</strong>。</p><p>因此，你可以简单理解为，平均负载其实就是平均活跃进程数。平均活跃进程数，直观上的理解就是单位时间内的活跃进程数，但它实际上是活跃进程数的指数衰减平均值。这个“指数衰减平均”的详细含义你不用计较，这只是系统的一种更快速的计算方式，你把它直接当成活跃进程数的平均值也没问题。</p><p>既然平均的是活跃进程数，那么最理想的，就是每个 CPU 上都刚好运行着一个进程，这样每个 CPU 都得到了充分利用。比如当平均负载为 2 时，意味着什么呢？</p><ul><li>在只有 2 个 CPU 的系统上，意味着所有的 CPU 都刚好被完全占用。</li><li>在 4 个 CPU 的系统上，意味着 CPU 有 50% 的空闲。</li><li>而在只有 1 个 CPU 的系统中，则意味着有一半的进程竞争不到 CPU。</li></ul><h3 id="平均负载与-CPU-使用率"><a href="#平均负载与-CPU-使用率" class="headerlink" title="平均负载与 CPU 使用率"></a>平均负载与 CPU 使用率</h3><p>现实工作中，我们经常容易把平均负载和 CPU 使用率混淆，所以在这里，我也做一个区分。</p><p>可能你会疑惑，既然平均负载代表的是活跃进程数，那平均负载高了，不就意味着 CPU 使用率高吗？</p><p>我们还是要回到平均负载的含义上来，平均负载是指单位时间内，处于可运行状态和不可中断状态的进程数。所以，它不仅包括了<strong>正在使用 CPU 的进程，还包括等待 CPU 和等待 I/O 的进程</strong>。</p><p>而 CPU 使用率，是单位时间内 CPU 繁忙情况的统计，跟平均负载并不一定完全对应。比如：</p><ul><li>CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时这两者是一致的；</li><li>I/O 密集型进程，等待 I/O 也会导致平均负载升高，但 CPU 使用率不一定很高；</li><li>大量等待 CPU 的进程调度也会导致平均负载升高，此时的 CPU 使用率也会比较高。</li></ul><p><strong>场景一：CPU 密集型进程</strong></p><p>首先，我们在第一个终端运行 stress 命令，模拟一个 CPU 使用率 100% 的场景：</p><pre><code>$ stress --cpu 1 --timeout 600</code></pre><p>接着，在第二个终端运行 uptime 查看平均负载的变化情况：</p><pre><code># -d 参数表示高亮显示变化的区域$ watch -d uptime...,  load average: 1.00, 0.75, 0.39</code></pre><p>最后，在第三个终端运行 mpstat 查看 CPU 使用率的变化情况：</p><pre><code># -P ALL 表示监控所有CPU，后面数字5表示间隔5秒后输出一组数据$ mpstat -P ALL 5Linux 4.15.0 (ubuntu) 09/22/18 _x86_64_ (2 CPU)13:30:06     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle13:30:11     all   50.05    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   49.9513:30:11       0    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.0013:30:11       1  100.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00</code></pre><p>从终端二中可以看到，1 分钟的平均负载会慢慢增加到 1.00，而从终端三中还可以看到，正好有一个 CPU 的使用率为 100%，但它的 iowait 只有 0。这说明，平均负载的升高正是由于 CPU 使用率为 100% 。</p><p>那么，到底是哪个进程导致了 CPU 使用率为 100% 呢？你可以使用 pidstat 来查询：</p><pre><code># 间隔5秒后输出一组数据$ pidstat -u 5 113:37:07      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command13:37:12        0      2962  100.00    0.00    0.00    0.00  100.00     1  stress</code></pre><p>从这里可以明显看到，stress 进程的 CPU 使用率为 100%。</p><p><strong>场景二：I/O 密集型进程</strong></p><p>首先还是运行 stress 命令，但这次模拟 I/O 压力，即不停地执行 sync：</p><pre><code>$ stress -i 1 --timeout 600</code></pre><p>还是在第二个终端运行 uptime 查看平均负载的变化情况：</p><pre><code>$ watch -d uptime...,  load average: 1.06, 0.58, 0.37</code></pre><p>然后，第三个终端运行 mpstat 查看 CPU 使用率的变化情况：</p><pre><code># 显示所有CPU的指标，并在间隔5秒输出一组数据$ mpstat -P ALL 5 1Linux 4.15.0 (ubuntu)     09/22/18     _x86_64_    (2 CPU)13:41:28     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle13:41:33     all    0.21    0.00   12.07   32.67    0.00    0.21    0.00    0.00    0.00   54.8413:41:33       0    0.43    0.00   23.87   67.53    0.00    0.43    0.00    0.00    0.00    7.7413:41:33       1    0.00    0.00    0.81    0.20    0.00    0.00    0.00    0.00    0.00   98.99</code></pre><p>从这里可以看到，1 分钟的平均负载会慢慢增加到 1.06，其中一个 CPU 的系统 CPU 使用率升高到了 23.87，而 iowait 高达 67.53%。这说明，平均负载的升高是由于 iowait 的升高。</p><p>那么到底是哪个进程，导致 iowait 这么高呢？我们还是用 pidstat 来查询：</p><pre><code># 间隔5秒后输出一组数据，-u表示CPU指标$ pidstat -u 5 1Linux 4.15.0 (ubuntu)     09/22/18     _x86_64_    (2 CPU)13:42:08      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command13:42:13        0       104    0.00    3.39    0.00    0.00    3.39     1  kworker/1:1H13:42:13        0       109    0.00    0.40    0.00    0.00    0.40     0  kworker/0:1H13:42:13        0      2997    2.00   35.53    0.00    3.99   37.52     1  stress13:42:13        0      3057    0.00    0.40    0.00    0.00    0.40     0  pidstat</code></pre><p><strong>场景三：大量进程的场景</strong></p><p>当系统中运行进程超出 CPU 运行能力时，就会出现等待 CPU 的进程。</p><p>比如，我们还是使用 stress，但这次模拟的是 8 个进程：</p><pre><code>$ stress -c 8 --timeout 600</code></pre><p>由于系统只有 2 个 CPU，明显比 8 个进程要少得多，因而，系统的 CPU 处于严重过载状态，平均负载高达 7.97：</p><pre><code>$ uptime...,  load average: 7.97, 5.93, 3.02</code></pre><p>接着再运行 pidstat 来看一下进程的情况：</p><pre><code># 间隔5秒后输出一组数据$ pidstat -u 5 114:23:25      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command14:23:30        0      3190   25.00    0.00    0.00   74.80   25.00     0  stress14:23:30        0      3191   25.00    0.00    0.00   75.20   25.00     0  stress14:23:30        0      3192   25.00    0.00    0.00   74.80   25.00     1  stress14:23:30        0      3193   25.00    0.00    0.00   75.00   25.00     1  stress14:23:30        0      3194   24.80    0.00    0.00   74.60   24.80     0  stress14:23:30        0      3195   24.80    0.00    0.00   75.00   24.80     0  stress14:23:30        0      3196   24.80    0.00    0.00   74.60   24.80     1  stress14:23:30        0      3197   24.80    0.00    0.00   74.80   24.80     1  stress14:23:30        0      3200    0.00    0.20    0.00    0.20    0.20     0  pidstat</code></pre><p>可以看出，8 个进程在争抢 2 个 CPU，每个进程等待 CPU 的时间（也就是代码块中的 %wait 列）高达 75%。这些超出 CPU 计算能力的进程，最终导致 CPU 过载。</p><h3 id="CPU-上下文切换"><a href="#CPU-上下文切换" class="headerlink" title="CPU 上下文切换"></a>CPU 上下文切换</h3><p>所以，根据任务的不同，CPU 的上下文切换就可以分为几个不同的场景，也就是<strong>进程上下文切换</strong>、<strong>线程上下文切换</strong>以及<strong>中断上下文切换</strong>。</p><p>Linux 按照特权等级，把进程的运行空间分为内核空间和用户空间，分别对应着下图中， CPU 特权等级的 Ring 0 和 Ring 3。</p><ul><li>内核空间（Ring 0）具有最高权限，可以直接访问所有资源；</li><li>用户空间（Ring 3）只能访问受限资源，不能直接访问内存等硬件设备，必须通过系统调用陷入到内核中，才能访问这些特权资源。</li></ul><p>换个角度看，也就是说，进程既可以在用户空间运行，又可以在内核空间中运行。进程在用户空间运行时，被称为进程的用户态，而陷入内核空间的时候，被称为进程的内核态。</p><p>从用户态到内核态的转变，需要通过系统调用来完成。比如，当我们查看文件内容时，就需要多次系统调用来完成：首先调用 open() 打开文件，然后调用 read() 读取文件内容，并调用 write() 将内容写到标准输出，最后再调用 close() 关闭文件。</p><p>CPU 寄存器里原来用户态的指令位置，需要先保存起来。接着，为了执行内核态代码，CPU 寄存器需要更新为内核态指令的新位置。最后才是跳转到内核态运行内核任务。</p><p>而系统调用结束后，CPU 寄存器需要恢复原来保存的用户态，然后再切换到用户空间，继续运行进程。所以，<strong>一次系统调用的过程，其实是发生了两次 CPU 上下文切换</strong>。</p><p>不过，需要注意的是，<strong>系统调用过程中，并不会涉及到虚拟内存等进程用户态的资源，也不会切换进程</strong>。这跟我们通常所说的进程上下文切换是不一样的：</p><ul><li>进程上下文切换，是指从一个进程切换到另一个进程运行。</li><li>而系统调用过程中一直是同一个进程在运行。</li></ul><p>所以，<strong>系统调用过程通常称为特权模式切换，而不是上下文切换</strong>。但实际上，系统调用过程中，CPU 的上下文切换还是无法避免的。</p><p>首先，你需要知道，<strong>进程是由内核来管理和调度的，进程的切换只能发生在内核态</strong>。所以，进程的上下文不仅包括了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的状态。</p><p>根据 Tsuna 的测试报告，<strong>每次上下文切换都需要几十纳秒到数微秒的 CPU 时间</strong>。这个时间还是相当可观的，特别是在进程上下文切换次数较多的情况下，很容易导致 CPU 将大量时间耗费<strong>在寄存器、内核栈以及虚拟内存等资源的保存和恢复上</strong>，进而大大缩短了真正运行进程的时间。这也正是上一节中我们所讲的，导致平均负载升高的一个重要因素。</p><p>另外，我们知道， Linux 通过 TLB（Translation Lookaside Buffer）来管理虚拟内存到物理内存的映射关系。当虚拟内存更新后，TLB 也需要刷新，内存的访问也会随之变慢。特别是在多处理器系统上，缓存是被多个处理器共享的，刷新缓存不仅会影响当前处理器的进程，还会影响共享缓存的其他处理器的进程。</p><p>显然，进程切换时才需要切换上下文，换句话说，只有在进程调度的时候，才需要切换上下文。<strong>Linux 为每个 CPU 都维护了一个就绪队列</strong>，将活跃进程（即正在运行和正在等待 CPU 的进程）按照优先级和等待 CPU 的时间排序，然后选择最需要 CPU 的进程，<strong>也就是优先级最高和等待 CPU 时间最长的进程来运行</strong>。</p><p>那么，进程在什么时候才会被调度到 CPU 上运行呢？</p><p>最容易想到的一个时机，就是进程执行完终止了，它之前使用的 CPU 会释放出来，这个时候再从就绪队列里，拿一个新的进程过来运行。<strong>其实还有很多其他场景，也会触发进程调度，在这里我给你逐个梳理下</strong>。</p><p><strong>进程上下文切换场景：</strong></p><p>其一，为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，就会被系统挂起，切换到其它正在等待 CPU 的进程运行。</p><p>其二，进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行。</p><p>其三，当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度。</p><p>其四，当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行。</p><p>最后一个，发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。</p><p><strong>线程上下文切换</strong></p><p>这么一来，线程的上下文切换其实就可以分为两种情况：</p><p>第一种， 前后两个线程属于不同进程。此时，因为资源不共享，所以切换过程就跟进程上下文切换是一样。</p><p>第二种，前后两个线程属于同一个进程。此时，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据。</p><p>到这里你应该也发现了，虽然同为上下文切换，但同进程内的线程切换，要比多进程间的切换消耗更少的资源，而这，也正是多线程代替多进程的一个优势。（协程切换也需要冲用户态到内核态，其实损耗还是很大）</p><p><strong>中断上下文切换</strong></p><p>为了快速响应硬件的事件，<strong>中断处理会打断进程的正常调度和执行</strong>，转而调用中断处理程序，响应设备事件。而在打断其他进程时，就需要将进程当前的状态保存下来，这样在中断结束后，进程仍然可以从原来的状态恢复运行。</p><p>跟进程上下文不同，中断上下文切换并不涉及到进程的用户态。所以，即便中断过程打断了一个正处在用户态的进程，也不需要保存和恢复这个进程的虚拟内存、全局变量等用户态资源。<strong>中断上下文，其实只包括内核态中断服务程序执行所必需的状态，包括 CPU 寄存器、内核堆栈、硬件中断参数等</strong>。</p><p>对同一个 CPU 来说，<strong>中断处理比进程拥有更高的优先级，所以中断上下文切换并不会与进程上下文切换同时发生</strong>。同样道理，由于中断会打断正常进程的调度和执行，所以大部分中断处理程序都短小精悍，以便尽可能快的执行结束。</p><h3 id="上下文切换-vmstat、pidstat"><a href="#上下文切换-vmstat、pidstat" class="headerlink" title="上下文切换 - vmstat、pidstat"></a>上下文切换 - vmstat、pidstat</h3><p>vmstat 是一个常用的系统性能分析工具，主要用来分析系统的内存使用情况，也常用来分析 CPU 上下文切换和中断的次数。</p><pre><code># 每隔5秒输出1组数据$ vmstat 5 -Smprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st 0  0      0 7005360  91564 818900    0    0     0     0   25   33  0  0 100  0  0</code></pre><p>我们一起来看这个结果，你可以先试着自己解读每列的含义。在这里，我重点强调下，需要特别关注的四列内容：</p><ul><li>cs（context switch）是每秒上下文切换的次数。</li><li>in（interrupt）则是每秒中断的次数。</li><li>r（Running or Runnable）是就绪队列的长度，也就是正在运行和等待 CPU 的进程数。</li><li>b（Blocked）则是处于不可中断睡眠状态的进程数。</li></ul><p>vmstat 只给出了系统总体的上下文切换情况，要想查看每个进程的详细情况，就需要使用我们前面提到过的 pidstat 了。给它加上 -w 选项，你就可以查看每个进程上下文切换的情况了。</p><pre><code># 每隔5秒输出1组数据// pidstat -w 1 -p 959613$ pidstat -w 5Linux 4.15.0 (ubuntu)  09/23/18  _x86_64_  (2 CPU)08:18:26      UID       PID   cswch/s nvcswch/s  Command08:18:31        0         1      0.20      0.00  systemd08:18:31        0         8      5.40      0.00  rcu_sched...</code></pre><p>这个结果中有两列内容是我们的重点关注对象。一个是 cswch ，表示每秒自愿上下文切换（voluntary context switches）的次数，另一个则是 nvcswch ，表示每秒非自愿上下文切换（non voluntary context switches）的次数。</p><ul><li>所谓自愿上下文切换，是指进程无法获取所需资源，导致的上下文切换。比如说， I/O、内存等系统资源不足时，就会发生自愿上下文切换。</li><li>而非自愿上下文切换，则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换。</li></ul><p><strong>案例分析</strong></p><p>首先，在第一个终端里运行 sysbench ，模拟系统多线程调度的瓶颈：</p><pre><code># 以10个线程运行5分钟的基准测试，模拟多线程切换的问题$ sysbench --threads=10 --max-time=300 threads run</code></pre><p>接着，在第二个终端运行 vmstat ，观察上下文切换情况：</p><pre><code># 每隔1秒输出1组数据（需要Ctrl+C才结束）$ vmstat 1procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st 6  0      0 6487428 118240 1292772    0    0     0     0 9019 1398830 16 84  0  0  0 8  0      0 6487428 118240 1292772    0    0     0     0 10191 1392312 16 84  0  0  0</code></pre><p>你应该可以发现，cs 列的上下文切换次数从之前的 35 骤然上升到了 139 万。同时，注意观察其他几个指标：</p><ul><li>r 列：就绪队列的长度已经到了 8，远远超过了系统 CPU 的个数 2，所以肯定会有大量的 CPU 竞争。r 列：就绪队列的长度已经到了 8，远远超过了系统 CPU 的个数 2，所以肯定会有大量的 CPU 竞争。</li><li>us（user）和 sy（system）列：这两列的 CPU 使用率加起来上升到了 100%，其中系统 CPU 使用率，也就是 sy 列高达 84%，说明 CPU 主要是被内核占用了。</li><li>in 列：中断次数也上升到了 1 万左右，说明中断处理也是个潜在的问题。</li></ul><p>综合这几个指标，我们可以知道，系统的就绪队列过长，也就是正在运行和等待 CPU 的进程数过多，导致了大量的上下文切换，而上下文切换又导致了系统 CPU 的占用率升高。</p><p>那么到底是什么进程导致了这些问题呢？</p><p>我们继续分析，在第三个终端再用 pidstat 来看一下， CPU 和进程上下文切换的情况：</p><pre><code># 每隔1秒输出1组数据（需要 Ctrl+C 才结束）# -w参数表示输出进程切换指标，而-u参数则表示输出CPU使用指标$ pidstat -w -u 108:06:33      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command08:06:34        0     10488   30.00  100.00    0.00    0.00  100.00     0  sysbench08:06:34        0     26326    0.00    1.00    0.00    0.00    1.00     0  kworker/u4:208:06:33      UID       PID   cswch/s nvcswch/s  Command08:06:34        0         8     11.00      0.00  rcu_sched08:06:34        0        16      1.00      0.00  ksoftirqd/108:06:34        0       471      1.00      0.00  hv_balloon08:06:34        0      1230      1.00      0.00  iscsid08:06:34        0      4089      1.00      0.00  kworker/1:508:06:34        0      4333      1.00      0.00  kworker/0:308:06:34        0     10499      1.00    224.00  pidstat08:06:34        0     26326    236.00      0.00  kworker/u4:208:06:34     1000     26784    223.00      0.00  sshd</code></pre><p>从 pidstat 的输出你可以发现，CPU 使用率的升高果然是 sysbench 导致的，它的 CPU 使用率已经达到了 100%。但上下文切换则是来自其他进程，包括非自愿上下文切换频率最高的 pidstat ，以及自愿上下文切换频率最高的内核线程 kworker 和 sshd。</p><p>所以，我们可以在第三个终端里， Ctrl+C 停止刚才的 pidstat 命令，再加上 -t 参数，重试一下看看：</p><pre><code># 每隔1秒输出一组数据（需要 Ctrl+C 才结束）# -wt 参数表示输出线程的上下文切换指标$ pidstat -wt 108:14:05      UID      TGID       TID   cswch/s nvcswch/s  Command...08:14:05        0     10551         -      6.00      0.00  sysbench08:14:05        0         -     10551      6.00      0.00  |__sysbench08:14:05        0         -     10552  18911.00 103740.00  |__sysbench08:14:05        0         -     10553  18915.00 100955.00  |__sysbench08:14:05        0         -     10554  18827.00 103954.00  |__sysbench...</code></pre><p>现在你就能看到了，虽然 sysbench 进程（也就是主线程）的上下文切换次数看起来并不多，但它的子线程的上下文切换次数却有很多。看来，上下文切换罪魁祸首，还是过多的 sysbench 线程。</p><p>我们已经找到了上下文切换次数增多的根源，那是不是到这儿就可以结束了呢？</p><p>当然不是。不知道你还记不记得，前面在观察系统指标时，除了上下文切换频率骤然升高，还有一个指标也有很大的变化。是的，正是中断次数。中断次数也上升到了 1 万，但到底是什么类型的中断上升了，现在还不清楚。我们接下来继续抽丝剥茧找源头。</p><p>既然是中断，我们都知道，它只发生在内核态，而 pidstat 只是一个进程的性能分析工具，并不提供任何关于中断的详细信息，怎样才能知道中断发生的类型呢？</p><p>没错，那就是从 /proc/interrupts 这个只读文件中读取。/proc 实际上是 Linux 的一个虚拟文件系统，用于内核空间与用户空间之间的通信。/proc/interrupts 就是这种通信机制的一部分，提供了一个只读的中断使用情况。</p><p>我们还是在第三个终端里， Ctrl+C 停止刚才的 pidstat 命令，然后运行下面的命令，观察中断的变化情况：</p><pre><code># -d 参数表示高亮显示变化的区域$ watch -d cat /proc/interrupts           CPU0       CPU1...RES:    2450431    5279697   Rescheduling interrupts...</code></pre><p>观察一段时间，你可以发现，<strong>变化速度最快的是重调度中断（RES）</strong>，这个中断类型表示，唤醒空闲状态的 CPU 来调度新的任务运行。这是多处理器系统（SMP）中，调度器用来分散任务到不同 CPU 的机制，通常也被称为<strong>处理器间中断（Inter-Processor Interrupts，IPI）</strong>。</p><p>所以，这里的中断升高还是因为过多任务的调度问题，跟前面上下文切换次数的分析结果是一致的。</p><p>通过这个案例，你应该也发现了多工具、多方面指标对比观测的好处。如果最开始时，我们只用了 pidstat 观测，这些很严重的上下文切换线程，压根儿就发现不了了。</p><p>现在再回到最初的问题，每秒上下文切换多少次才算正常呢？</p><p><strong>这个数值其实取决于系统本身的 CPU 性能</strong>。在我看来，如果系统的上下文切换次数比较稳定，那么从数百到一万以内，都应该算是正常的。但当上下文切换次数超过一万次，或者切换次数出现数量级的增长时，就很可能已经出现了性能问题。</p><p>这时，你还需要根据上下文切换的类型，再做具体分析。比方说：</p><ul><li>自愿上下文切换变多了，说明进程都在等待资源，有可能发生了 I/O 等其他问题；</li><li>非自愿上下文切换变多了，说明进程都在被强制调度，也就是都在争抢 CPU，说明 CPU 的确成了瓶颈；</li><li>中断次数变多了，说明 CPU 被中断处理程序占用，还需要通过查看 /proc/interrupts 文件来分析具体的中断类型。</li></ul><h3 id="CPU-使用率"><a href="#CPU-使用率" class="headerlink" title="CPU 使用率"></a>CPU 使用率</h3><p>为了维护 CPU 时间，Linux 通过事先定义的节拍率（内核中表示为 HZ），触发时间中断，并使用全局变量 Jiffies 记录了开机以来的节拍数。每发生一次时间中断，Jiffies 的值就加 1。</p><p>节拍率 HZ 是内核的可配选项，可以设置为 100、250、1000 等。不同的系统可能设置不同数值，你可以通过查询 /boot/config 内核选项来查看它的配置值。比如在我的系统中，节拍率设置成了 250，也就是每秒钟触发 250 次时间中断。</p><pre><code>$ grep 'CONFIG_HZ=' /boot/config-$(uname -r)CONFIG_HZ=250</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d2b04b60d73778ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>同时，正因为节拍率 HZ 是内核选项，所以用户空间程序并不能直接访问。为了方便用户空间程序，内核还提供了一个用户空间节拍率 <code>USER_HZ</code>，它总是固定为 100，也就是 1/100 秒。这样，用户空间程序并不需要关心内核中 HZ 被设置成了多少，因为它看到的总是固定值 <code>USER_HZ</code>。</p><p>Linux 通过 /proc 虚拟文件系统，向用户空间提供了系统内部状态的信息，而 /proc/stat 提供的就是系统的 CPU 和任务统计信息。比方说，如果你只关注 CPU 的话，可以执行下面的命令：</p><pre><code># 只保留各个CPU的数据$ cat /proc/stat | grep ^cpucpu  280580 7407 286084 172900810 83602 0 583 0 0 0cpu0 144745 4181 176701 86423902 52076 0 301 0 0 0cpu1 135834 3226 109383 86476907 31525 0 282 0 0 0</code></pre><p>这里的输出结果是一个表格。其中，第一列表示的是 CPU 编号，如 cpu0、cpu1 ，而第一行没有编号的 cpu ，表示的是所有 CPU 的累加。其他列则表示不同场景下 CPU 的累加节拍数，它的单位是 <code>USER_HZ</code>，也就是 10 ms（1/100 秒），所以这其实就是不同场景下的 CPU 时间。</p><p>当然，这里每一列的顺序并不需要你背下来。你只要记住，有需要的时候，查询 man proc 就可以。不过，你要清楚 man proc 文档里每一列的涵义，它们都是 CPU 使用率相关的重要指标，你还会在很多其他的性能工具中看到它们。下面，我来依次解读一下。</p><ul><li>user（通常缩写为 us），代表用户态 CPU 时间。注意，它不包括下面的 nice 时间，但包括了 guest 时间。</li><li>nice（通常缩写为 ni），代表低优先级用户态 CPU 时间，也就是进程的 nice 值被调整为 1-19 之间时的 CPU 时间。这里注意，nice 可取值范围是 -20 到 19，数值越大，优先级反而越低。</li><li>system（通常缩写为 sys），代表内核态 CPU 时间。</li><li>idle（通常缩写为 id），代表空闲时间。注意，它不包括等待 I/O 的时间（iowait）。</li><li>iowait（通常缩写为 wa），代表等待 I/O 的 CPU 时间。</li><li>irq（通常缩写为 hi），代表处理硬中断的 CPU 时间。</li><li>softirq（通常缩写为 si），代表处理软中断的 CPU 时间。</li><li>steal（通常缩写为 st），代表当系统运行在虚拟机中的时候，被其他虚拟机占用的 CPU 时间。</li><li>guest（通常缩写为 guest），代表通过虚拟化运行其他操作系统的时间，也就是运行虚拟机的 CPU 时间。</li><li>guest_nice（通常缩写为 gnice），代表以低优先级运行虚拟机的时间。</li></ul><p>而我们通常所说的 CPU 使用率，就是除了空闲时间外的其他时间占总 CPU 时间的百分比，用公式来表示就是：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-629ccd0890e2217a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>根据这个公式，我们就可以从 /proc/stat 中的数据，很容易地计算出 CPU 使用率。当然，也可以用每一个场景的 CPU 时间，除以总的 CPU 时间，计算出每个场景的 CPU 使用率。</p><p>事实上，为了计算 CPU 使用率，性能工具一般都会取间隔一段时间（比如 3 秒）的两次值，作差后，再计算出这段时间内的平均 CPU 使用率，即</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-84a71ef2e43425c7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个公式，就是我们用各种性能工具所看到的 CPU 使用率的实际计算方法。</p><p>现在，我们知道了系统 CPU 使用率的计算方法，那进程的呢？跟系统的指标类似，Linux 也给每个进程提供了运行情况的统计信息，也就是 /proc/[pid]/stat。不过，这个文件包含的数据就比较丰富了，总共有 52 列的数据。</p><p>当然不是，各种各样的性能分析工具已经帮我们计算好了。不过要注意的是，性能分析工具给出的都是间隔一段时间的平均 CPU 使用率，所以要注意间隔时间的设置，特别是用多个工具对比分析时，你一定要保证它们用的是相同的间隔时间。</p><p>比如，下面的 pidstat 命令，就间隔 1 秒展示了进程的 5 组 CPU 使用率，包括：</p><ul><li>用户态 CPU 使用率 （%usr）；</li><li>内核态 CPU 使用率（%system）；</li><li>运行虚拟机 CPU 使用率（%guest）；</li><li>等待 CPU 使用率（%wait）；</li><li>以及总的 CPU 使用率（%CPU）。</li></ul><p>使用 <strong>perf</strong> 分析 CPU 性能问题，我来说两种最常见、也是我最喜欢的用法。</p><p>第一种常见用法是 perf top，类似于 top，它能够实时显示占用 CPU 时钟最多的函数或者指令，因此可以用来查找热点函数，使用界面如下所示：</p><pre><code>$ perf topSamples: 833  of event 'cpu-clock', Event count (approx.): 97742399Overhead  Shared Object       Symbol   7.28%  perf                [.] 0x00000000001f78a4   4.72%  [kernel]            [k] vsnprintf   4.32%  [kernel]            [k] module_get_kallsym   3.65%  [kernel]            [k] _raw_spin_unlock_irqrestore...</code></pre><p>输出结果中，第一行包含三个数据，分别是采样数（Samples）、事件类型（event）和事件总数量（Event count）。比如这个例子中，perf 总共采集了 833 个 CPU 时钟事件，而总事件数则为 97742399。</p><p>另外，采样数需要我们特别注意。如果采样数过少（比如只有十几个），那下面的排序和百分比就没什么实际参考价值了。</p><p>再往下看是一个表格式样的数据，每一行包含四列，分别是：</p><ul><li>第一列 Overhead ，是该符号的性能事件在所有采样中的比例，用百分比来表示。</li><li>第二列 Shared ，是该函数或指令所在的动态共享对象（Dynamic Shared Object），如内核、进程名、动态链接库名、内核模块名等。</li><li>第三列 Object ，是动态共享对象的类型。比如 [.] 表示用户空间的可执行程序、或者动态链接库，而 [k] 则表示内核空间。</li><li>最后一列 Symbol 是符号名，也就是函数名。当函数名未知时，用十六进制的地址来表示。</li></ul><p>还是以上面的输出为例，我们可以看到，占用 CPU 时钟最多的是 perf 工具自身，不过它的比例也只有 7.28%，说明系统并没有 CPU 性能问题。 perf top 的使用你应该很清楚了吧。</p><p>接着再来看第二种常见用法，也就是 perf record 和 perf report。 perf top 虽然实时展示了系统的性能信息，但它的缺点是并不保存数据，也就无法用于离线或者后续的分析。而 perf record 则提供了保存数据的功能，保存后的数据，需要你用 perf report 解析展示。</p><pre><code>$ perf record # 按Ctrl+C终止采样[ perf record: Woken up 1 times to write data ][ perf record: Captured and wrote 0.452 MB perf.data (6093 samples) ]$ perf report # 展示类似于perf top的报告</code></pre><p>在实际使用中，我们还经常为 perf top 和 perf record 加上 -g 参数，开启调用关系的采样，方便我们根据调用链来分析性能问题。</p><h3 id="execsnoop"><a href="#execsnoop" class="headerlink" title="execsnoop"></a>execsnoop</h3><p>execsnoop 就是一个专为短时进程设计的工具。它通过 ftrace 实时监控进程的 exec() 行为，并输出短时进程的基本信息，包括进程 PID、父进程 PID、命令行参数以及执行的结果。</p><p>比如，用 execsnoop 监控上述案例，就可以直接得到 stress 进程的父进程 PID 以及它的命令行参数，并可以发现大量的 stress 进程在不停启动：</p><pre><code># 按 Ctrl+C 结束$ execsnoopPCOMM            PID    PPID   RET ARGSsh               30394  30393    0stress           30396  30394    0 /usr/local/bin/stress -t 1 -d 1sh               30398  30393    0stress           30399  30398    0 /usr/local/bin/stress -t 1 -d 1sh               30402  30400    0stress           30403  30402    0 /usr/local/bin/stress -t 1 -d 1sh               30405  30393    0stress           30407  30405    0 /usr/local/bin/stress -t 1 -d 1...</code></pre><p>execsnoop 所用的 ftrace 是一种常用的动态追踪技术，一般用于分析 Linux 内核的运行时行为，后面课程我也会详细介绍并带你使用。</p><h3 id="进程状态"><a href="#进程状态" class="headerlink" title="进程状态"></a>进程状态</h3><p>top 和 ps 是最常用的查看进程状态的工具，我们就从 top 的输出开始。下面是一个 top 命令输出的示例，S 列（也就是 Status 列）表示进程的状态。从这个示例里，你可以看到 R、D、Z、S、I 等几个状态，它们分别是什么意思呢？</p><pre><code>$ top  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND28961 root      20   0   43816   3148   4040 R   3.2  0.0   0:00.01 top  620 root      20   0   37280  33676    908 D   0.3  0.4   0:00.01 app    1 root      20   0  160072   9416   6752 S   0.0  0.1   0:37.64 systemd 1896 root      20   0       0      0      0 Z   0.0  0.0   0:00.00 devapp    2 root      20   0       0      0      0 S   0.0  0.0   0:00.10 kthreadd    4 root       0 -20       0      0      0 I   0.0  0.0   0:00.00 kworker/0:0H    6 root       0 -20       0      0      0 I   0.0  0.0   0:00.00 mm_percpu_wq    7 root      20   0       0      0      0 S   0.0  0.0   0:06.37 ksoftirqd/0</code></pre><ul><li>R 是 Running 或 Runnable 的缩写，表示进程在 CPU 的就绪队列中，正在运行或者正在等待运行。</li><li>D 是 Disk Sleep 的缩写，也就是不可中断状态睡眠（Uninterruptible Sleep），一般表示进程正在跟硬件交互，并且交互过程不允许被其他进程或中断打断。</li><li>Z 是 Zombie 的缩写，如果你玩过“植物大战僵尸”这款游戏，应该知道它的意思。它表示僵尸进程，也就是进程实际上已经结束了，但是父进程还没有回收它的资源（比如进程的描述符、PID 等）。</li><li>S 是 Interruptible Sleep 的缩写，也就是可中断状态睡眠，表示进程因为等待某个事件而被系统挂起。当进程等待的事件发生时，它会被唤醒并进入 R 状态。</li><li>I 是 Idle 的缩写，也就是空闲状态，用在不可中断睡眠的内核线程上。前面说了，硬件交互导致的不可中断进程用 D 表示，但对某些内核线程来说，它们有可能实际上并没有任何负载，用 Idle 正是为了区分这种情况。要注意，D 状态的进程会导致平均负载升高， I 状态的进程却不会。</li><li>T 或者 t，也就是 Stopped 或 Traced 的缩写，表示进程处于暂停或者跟踪状态。向一个进程发送 SIGSTOP 信号，它就会因响应这个信号变成暂停状态（Stopped）；再向它发送 SIGCONT 信号，进程又会恢复运行（如果进程是终端里直接启动的，则需要你用 fg 命令，恢复到前台运行）。而当你用调试器（如 gdb）调试一个进程时，在使用断点中断进程后，进程就会变成跟踪状态，这其实也是一种特殊的暂停状态，只不过你可以用调试器来跟踪并按需要控制进程的运行。</li><li>X，也就是 Dead 的缩写，表示进程已经消亡，所以你不会在 top 或者 ps 命令中看到它。</li></ul><p>重点：</p><ul><li>不可中断状态，表示进程正在跟硬件交互，为了保护进程数据和硬件的一致性，系统不允许其他进程或中断打断这个进程。进程长时间处于不可中断状态，通常表示系统有 I/O 性能问题。</li><li>僵尸进程表示进程已经退出，但它的父进程还没有回收子进程占用的资源。短暂的僵尸状态我们通常不必理会，但进程长时间处于僵尸状态，就应该注意了，可能有应用程序没有正常处理子进程的退出。</li></ul><h3 id="iowait-分析"><a href="#iowait-分析" class="headerlink" title="iowait 分析"></a>iowait 分析</h3><p>我相信，一提到 iowait 升高，你首先会想要查询系统的 I/O 情况。我一般也是这种思路，那么什么工具可以查询系统的 I/O 情况呢？</p><p>这里，我推荐的正是上节课要求安装的 dstat ，它的好处是，可以同时查看 CPU 和 I/O 这两种资源的使用情况，便于对比分析。</p><pre><code># 间隔1秒输出10组数据$ dstat 1 10You did not select any stats, using -cdngy by default.--total-cpu-usage-- -dsk/total- -net/total- ---paging-- ---system--usr sys idl wai stl| read  writ| recv  send|  in   out | int   csw  0   0  96   4   0|1219k  408k|   0     0 |   0     0 |  42   885  0   0   2  98   0|  34M    0 | 198B  790B|   0     0 |  42   138  0   0   0 100   0|  34M    0 |  66B  342B|   0     0 |  42   135  0   0  84  16   0|5633k    0 |  66B  342B|   0     0 |  52   177  0   3  39  58   0|  22M    0 |  66B  342B|   0     0 |  43   144  0   0   0 100   0|  34M    0 | 200B  450B|   0     0 |  46   147  0   0   2  98   0|  34M    0 |  66B  342B|   0     0 |  45   134  0   0   0 100   0|  34M    0 |  66B  342B|   0     0 |  39   131  0   0  83  17   0|5633k    0 |  66B  342B|   0     0 |  46   168  0   3  39  59   0|  22M    0 |  66B  342B|   0     0 |  37   134</code></pre><p>从 dstat 的输出，我们可以看到，每当 iowait 升高（wai）时，磁盘的读请求（read）都会很大。这说明 iowait 的升高跟磁盘的读请求有关，很可能就是磁盘读导致的。</p><p>我们从 top 的输出找到 D 状态进程的 PID，你可以发现，这个界面里有两个 D 状态的进程，PID 分别是 4344 和 4345。</p><p>接着，我们查看这些进程的磁盘读写情况。对了，别忘了工具是什么。一般要查看某一个进程的资源使用情况，都可以用我们的老朋友 pidstat，不过这次记得加上 -d 参数，以便输出 I/O 使用情况。</p><p>比如，以 4344 为例，我们在终端里运行下面的 pidstat 命令，并用 -p 4344 参数指定进程号：</p><pre><code># -d 展示 I/O 统计数据，-p 指定进程号，间隔 1 秒输出 3 组数据$ pidstat -d -p 4344 1 306:38:50      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command06:38:51        0      4344      0.00      0.00      0.00       0  app06:38:52        0      4344      0.00      0.00      0.00       0  app06:38:53        0      4344      0.00      0.00      0.00       0  app</code></pre><p>在这个输出中， kB_rd 表示每秒读的 KB 数， kB_wr 表示每秒写的 KB 数，iodelay 表示 I/O 的延迟（单位是时钟周期）。它们都是 0，那就表示此时没有任何的读写，说明问题不是 4344 进程导致的。</p><p>strace 正是最常用的跟踪进程系统调用的工具。所以，我们从 pidstat 的输出中拿到进程的 PID 号，比如 6082，然后在终端中运行 strace 命令，并用 -p 参数指定 PID 号：</p><pre><code>$ strace -p 6082strace: attach: ptrace(PTRACE_SEIZE, 6082): Operation not permitted</code></pre><p>这儿出现了一个奇怪的错误，strace 命令居然失败了，并且命令报出的错误是没有权限。按理来说，我们所有操作都已经是以 root 用户运行了，为什么还会没有权限呢？你也可以先想一下，碰到这种情况，你会怎么处理呢？</p><p>一般遇到这种问题时，我会先检查一下进程的状态是否正常。比如，继续在终端中运行 ps 命令，并使用 grep 找出刚才的 6082 号进程：</p><pre><code>$ ps aux | grep 6082root      6082  0.0  0.0      0     0 pts/0    Z+   13:43   0:00 [app] &lt;defunct&gt;</code></pre><p>果然，进程 6082 已经变成了 Z 状态，也就是僵尸进程。僵尸进程都是已经退出的进程，所以就没法儿继续分析它的系统调用。关于僵尸进程的处理方法，我们一会儿再说，现在还是继续分析 iowait 的问题。</p><p>到这一步，你应该注意到了，系统 iowait 的问题还在继续，但是 top、pidstat 这类工具已经不能给出更多的信息了。这时，我们就应该求助那些基于事件记录的动态追踪工具了。</p><p>你可以用 perf top 看看有没有新发现。再或者，可以像我一样，在终端中运行 perf record，持续一会儿（例如 15 秒），然后按 Ctrl+C 退出，再运行 perf report 查看报告：</p><pre><code>$ perf record -g$ perf report</code></pre><p>我们来看其他信息，你可以发现， app 的确在通过系统调用 sys_read() 读取数据。并且从 new_sync_read 和 blkdev_direct_IO 能看出，进程正在对磁盘进行直接读，也就是绕过了系统缓存，每个读请求都会从磁盘直接读，这就可以解释我们观察到的 iowait 升高了。</p><p>看来，罪魁祸首是 app 内部进行了磁盘的直接 I/O 啊！</p><p>下面的问题就容易解决了。我们接下来应该从代码层面分析，究竟是哪里出现了直接读请求。查看源码文件 app.c，你会发现它果然使用了 O_DIRECT 选项打开磁盘，于是绕过了系统缓存，直接对磁盘进行读写。</p><pre><code>open(disk, O_RDONLY|O_DIRECT|O_LARGEFILE, 0755)</code></pre><p>直接读写磁盘，对 I/O 敏感型应用（比如数据库系统）是很友好的，因为你可以在应用中，直接控制磁盘的读写。但在大部分情况下，我们最好还是通过系统缓存来优化磁盘 I/O，换句话说，删除 O_DIRECT 这个选项就是了。</p><h3 id="软中断（softirq）"><a href="#软中断（softirq）" class="headerlink" title="软中断（softirq）"></a>软中断（softirq）</h3><p><strong>中断</strong></p><p><strong>中断其实是一种异步的事件处理机制，可以提高系统的并发处理能力</strong>。</p><p>由于中断处理程序会打断其他进程的运行，所以，<strong>为了减少对正常进程运行调度的影响，中断处理程序就需要尽可能快地运行</strong>。如果中断本身要做的事情不多，那么处理起来也不会有太大问题；但如果中断要处理的事情很多，中断服务程序就有可能要运行很长时间。</p><p>特别是，中断处理程序在响应中断时，还会临时关闭中断。这就会导致上一次中断处理完成之前，其他中断都不能响应，也就是说中断有可能会丢失。</p><p><strong>软中断</strong></p><p>事实上，为了解决中断处理程序执行过长和中断丢失的问题，Linux 将中断处理过程分成了两个阶段，也就是上半部和下半部：</p><ul><li>上半部用来快速处理中断，它在中断禁止模式下运行，主要处理跟硬件紧密相关的或时间敏感的工作。</li><li>下半部用来延迟处理上半部未完成的工作，通常以内核线程的方式运行。</li></ul><p>网卡接收到数据包后，会通过<strong>硬件中断</strong>的方式，通知内核有新的数据到了。这时，内核就应该调用中断处理程序来响应它。你可以自己先想一下，这种情况下的上半部和下半部分别负责什么工作呢？</p><p>对上半部来说，既然是快速处理，其实就是要把网卡的数据读到内存中，然后更新一下硬件寄存器的状态（表示数据已经读好了），最后再发送一个软中断信号，通知下半部做进一步的处理。</p><p>而下半部被软中断信号唤醒后，需要从内存中找到网络数据，再按照网络协议栈，对数据进行逐层解析和处理，直到把它送给应用程序。</p><p>所以，这两个阶段你也可以这样理解：</p><ul><li>上半部直接处理硬件请求，也就是我们常说的硬中断，特点是快速执行；</li><li>而下半部则是由内核触发，也就是我们常说的软中断，特点是延迟执行。</li></ul><p>实际上，<strong>上半部会打断 CPU 正在执行的任务，然后立即执行中断处理程序</strong>。而下半部以内核线程的方式执行，并且每个 CPU 都对应一个软中断内核线程，名字为 “ksoftirqd/CPU 编号”，比如说， 0 号 CPU 对应的软中断内核线程的名字就是 ksoftirqd/0。</p><p>不过要注意的是，软中断不只包括了刚刚所讲的硬件设备中断处理程序的下半部，一些内核自定义的事件也属于软中断，比如内核调度和 RCU 锁（Read-Copy Update 的缩写，RCU 是 Linux 内核中最常用的锁之一）等。</p><p><strong>软中断内容查看</strong></p><pre><code>$ cat /proc/softirqs                    CPU0       CPU1          HI:          0          0       TIMER:     811613    1972736      NET_TX:         49          7      NET_RX:    1136736    1506885       BLOCK:          0          0    IRQ_POLL:          0          0     TASKLET:     304787       3691       SCHED:     689718    1897539     HRTIMER:          0          0         RCU:    1330771    1354737</code></pre><p>在查看 /proc/softirqs 文件内容时，你要特别注意以下这两点。</p><p>第一，要注意软中断的类型，也就是这个界面中第一列的内容。从第一列你可以看到，软中断包括了 10 个类别，分别对应不同的工作类型。比如 NET_RX 表示网络接收中断，而 NET_TX 表示网络发送中断。</p><p>第二，要注意同一种软中断在不同 CPU 上的分布情况，也就是同一行的内容。正常情况下，同一种中断在不同 CPU 上的累积次数应该差不多。比如这个界面中，NET_RX 在 CPU0 和 CPU1 上的中断次数基本是同一个数量级，相差不大。</p><p>不过你可能发现，TASKLET 在不同 CPU 上的分布并不均匀。TASKLET 是最常用的软中断实现机制，每个 TASKLET 只运行一次就会结束 ，并且只在调用它的函数所在的 CPU 上运行。</p><p>另外，刚刚提到过，软中断实际上是以内核线程的方式运行的，每个 CPU 都对应一个软中断内核线程，这个软中断内核线程就叫做 ksoftirqd/CPU 编号。那要怎么查看这些线程的运行状况呢？</p><p>其实用 ps 命令就可以做到，比如执行下面的指令：</p><pre><code>$ ps aux | grep softirqroot         7  0.0  0.0      0     0 ?        S    Oct10   0:01 [ksoftirqd/0]root        16  0.0  0.0      0     0 ?        S    Oct10   0:01 [ksoftirqd/1]</code></pre><p>注意，这些线程的名字外面都有中括号，这说明 ps 无法获取它们的命令行参数（cmline）。一般来说，ps 的输出中，名字括在中括号里的，一般都是内核线程。</p><p><strong>无法模拟出 RES 中断的问题</strong></p><p>这个问题是说，即使运行了大量的线程，也无法模拟出重调度中断 RES 升高的问题。</p><p>其实我在 CPU 上下文切换的案例中已经提到，重调度中断是调度器用来分散任务到不同 CPU 的机制，也就是可以唤醒空闲状态的 CPU ，来调度新任务运行，而这通常借助处理器间中断（Inter-Processor Interrupts，IPI）来实现。</p><p>所以，这个中断在单核（只有一个逻辑 CPU）的机器上当然就没有意义了，因为压根儿就不会发生重调度的情况。</p><p>其实这个结论也可以从另一个角度获得。比如，你可以在 pidstat 的选项中，加入 -u 和 -t 参数，输出线程的 CPU 使用情况，你会看到下面的界面：</p><pre><code>$ pidstat -u -t 114:24:03      UID      TGID       TID    %usr %system  %guest   %wait    %CPU   CPU  Command14:24:04        0         -      2472    0.99    8.91    0.00   77.23    9.90     0  |__sysbench14:24:04        0         -      2473    0.99    8.91    0.00   68.32    9.90     0  |__sysbench14:24:04        0         -      2474    0.99    7.92    0.00   75.25    8.91     0  |__sysbench14:24:04        0         -      2475    2.97    6.93    0.00   70.30    9.90     0  |__sysbench14:24:04        0         -      2476    2.97    6.93    0.00   68.32    9.90     0  |__sysbench...</code></pre><p>从这个 pidstat 的输出界面，你可以发现，每个 stress 线程的 %wait 高达 70%，而 CPU 使用率只有不到 10%。换句话说， stress 线程大部分时间都消耗在了等待 CPU 上，这也表明，确实是过多的线程在争抢 CPU。</p><p>在这里顺便提一下，留言中很常见的一个错误。有些同学会拿 pidstat 中的 %wait 跟 top 中的 iowait% （缩写为 wa）对比，其实这是没有意义的，因为它们是完全不相关的两个指标。</p><ul><li>pidstat 中， %wait 表示进程等待 CPU 的时间百分比。</li><li>top 中 ，iowait% 则表示等待 I/O 的 CPU 时间百分比。</li></ul><p>回忆一下我们学过的进程状态，你应该记得，等待 CPU 的进程已经在 CPU 的就绪队列中，处于运行状态；而等待 I/O 的进程则处于不可中断状态。</p><h3 id="SYN-FLOOD-攻击-gt-软中断升高"><a href="#SYN-FLOOD-攻击-gt-软中断升高" class="headerlink" title="SYN FLOOD 攻击 -> 软中断升高"></a>SYN FLOOD 攻击 -&gt; 软中断升高</h3><pre><code># -S参数表示设置TCP协议的SYN（同步序列号），-p表示目的端口为80# -i u100表示每隔100微秒发送一个网络帧# 注：如果你在实践过程中现象不明显，可以尝试把100调小，比如调成10甚至1$ hping3 -S -p 80 -i u100 192.168.0.30</code></pre><p>我们在第一个终端中运行 sar 命令，并添加 -n DEV 参数显示网络收发的报告：</p><pre><code># -n DEV 表示显示网络收发的报告，间隔1秒输出一组数据$ sar -n DEV 115:03:46        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil15:03:47         eth0  12607.00   6304.00    664.86    358.11      0.00      0.00      0.00      0.0115:03:47      docker0   6302.00  12604.00    270.79    664.66      0.00      0.00      0.00      0.0015:03:47           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.0015:03:47    veth9f6bbcd   6302.00  12604.00    356.95    664.66      0.00      0.00      0.00      0.05</code></pre><p>对于 sar 的输出界面，我先来简单介绍一下，从左往右依次是：</p><ul><li>第一列：表示报告的时间。</li><li>第二列：IFACE 表示网卡。</li><li>第三、四列：rxpck/s 和 txpck/s 分别表示每秒接收、发送的网络帧数，也就是 PPS。</li><li>第五、六列：rxkB/s 和 txkB/s 分别表示每秒接收、发送的千字节数，也就是 BPS。</li><li>后面的其他参数基本接近 0，显然跟今天的问题没有直接关系，你可以先忽略掉。</li></ul><p>既然怀疑是网络接收中断的问题，我们还是重点来看 eth0 ：接收的 PPS 比较大，达到 12607，而接收的 BPS 却很小，只有 664 KB。直观来看网络帧应该都是比较小的，我们稍微计算一下，664*1024/12607 = 54 字节，说明平均每个网络帧只有 54 字节，这显然是很小的网络帧，也就是我们通常所说的小包问题。</p><p>接下来，我们在第一个终端中运行 tcpdump 命令，通过 -i eth0 选项指定网卡 eth0，并通过 tcp port 80 选项指定 TCP 协议的 80 端口：</p><pre><code># -i eth0 只抓取eth0网卡，-n不解析协议名和主机名# tcp port 80表示只抓取tcp协议并且端口号为80的网络帧$ tcpdump -i eth0 -n tcp port 8015:11:32.678966 IP 192.168.0.2.18238 &gt; 192.168.0.30.80: Flags [S], seq 458303614, win 512, length 0</code></pre><p>…</p><h3 id="CPU-性能指标"><a href="#CPU-性能指标" class="headerlink" title="CPU 性能指标"></a>CPU 性能指标</h3><p><strong>首先，最容易想到的应该是 CPU 使用率</strong>，这也是实际环境中最常见的一个性能指标。</p><p>CPU 使用率描述了非空闲时间占总 CPU 时间的百分比，根据 CPU 上运行任务的不同，又被分为用户 CPU、系统 CPU、等待 I/O CPU、软中断和硬中断等。</p><ul><li>用户 CPU 使用率，包括用户态 CPU 使用率（user）和低优先级用户态 CPU 使用率（nice），表示 CPU 在用户态运行的时间百分比。用户 CPU 使用率高，通常说明有应用程序比较繁忙。</li><li>系统 CPU 使用率，表示 CPU 在内核态运行的时间百分比（不包括中断）。系统 CPU 使用率高，说明内核比较繁忙。</li><li>等待 I/O 的 CPU 使用率，通常也称为 iowait，表示等待 I/O 的时间百分比。iowait 高，通常说明系统与硬件设备的 I/O 交互时间比较长。</li><li>软中断和硬中断的 CPU 使用率，分别表示内核调用软中断处理程序、硬中断处理程序的时间百分比。它们的使用率高，通常说明系统发生了大量的中断。</li><li>除了上面这些，还有在虚拟化环境中会用到的窃取 CPU 使用率（steal）和客户 CPU 使用率（guest），分别表示被其他虚拟机占用的 CPU 时间百分比，和运行客户虚拟机的 CPU 时间百分比。</li></ul><p><strong>第二个比较容易想到的，应该是平均负载（Load Average）</strong>，也就是系统的平均活跃进程数。它反应了系统的整体负载情况，主要包括三个数值，分别指过去 1 分钟、过去 5 分钟和过去 15 分钟的平均负载。</p><p>理想情况下，平均负载等于逻辑 CPU 个数，这表示每个 CPU 都恰好被充分利用。如果平均负载大于逻辑 CPU 个数，就表示负载比较重了。</p><p>第三个，也是在专栏学习前你估计不太会注意到的，进程上下文切换，包括：</p><ul><li>无法获取资源而导致的自愿上下文切换；</li><li>被系统强制调度导致的非自愿上下文切换。</li></ul><p>上下文切换，本身是保证 Linux 正常运行的一项核心功能。但过多的上下文切换，会将原本运行进程的 CPU 时间，消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，缩短进程真正运行的时间，成为性能瓶颈。</p><p>除了上面几种，<strong>还有一个指标，CPU 缓存的命中率</strong>。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-91a73081f1c00b2f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="性能工具"><a href="#性能工具" class="headerlink" title="性能工具"></a>性能工具</h3><p>首先，平均负载的案例。我们先用 uptime， 查看了系统的平均负载；而在平均负载升高后，又用 mpstat 和 pidstat ，分别观察了每个 CPU 和每个进程 CPU 的使用情况，进而找出了导致平均负载升高的进程，也就是我们的压测工具 stress。</p><p>第二个，上下文切换的案例。我们先用 vmstat ，查看了系统的上下文切换次数和中断次数；然后通过 pidstat ，观察了进程的自愿上下文切换和非自愿上下文切换情况；最后通过 pidstat ，观察了线程的上下文切换情况，找出了上下文切换次数增多的根源，也就是我们的基准测试工具 sysbench。</p><p>第三个，进程 CPU 使用率升高的案例。我们先用 top ，查看了系统和进程的 CPU 使用情况，发现 CPU 使用率升高的进程是 php-fpm；再用 perf top ，观察 php-fpm 的调用链，最终找出 CPU 升高的根源，也就是库函数 sqrt() 。</p><p>第四个，系统的 CPU 使用率升高的案例。我们先用 top 观察到了系统 CPU 升高，但通过 top 和 pidstat ，却找不出高 CPU 使用率的进程。于是，我们重新审视 top 的输出，又从 CPU 使用率不高但处于 Running 状态的进程入手，找出了可疑之处，最终通过 perf record 和 perf report ，发现原来是短时进程在捣鬼。</p><p>另外，对于短时进程，我还介绍了一个专门的工具 execsnoop，它可以实时监控进程调用的外部命令。</p><p>第五个，不可中断进程和僵尸进程的案例。我们先用 top 观察到了 iowait 升高的问题，并发现了大量的不可中断进程和僵尸进程；接着我们用 dstat 发现是这是由磁盘读导致的，于是又通过 pidstat 找出了相关的进程。但我们用 strace 查看进程系统调用却失败了，最终还是用 perf 分析进程调用链，才发现根源在于磁盘直接 I/O 。</p><p>最后一个，软中断的案例。我们通过 top 观察到，系统的软中断 CPU 使用率升高；接着查看 /proc/softirqs， 找到了几种变化速率较快的软中断；然后通过 sar 命令，发现是网络小包的问题，最后再用 tcpdump ，找出网络帧的类型和来源，确定是一个 SYN FLOOD 攻击导致的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d953e192440d1ce7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-441bdba6a4e4010d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d78ac2061c711b68.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>一个很典型的例子是我将在网络部分讲到的 DPDK（Data Plane Development Kit）。DPDK 是一种优化网络处理速度的方法，它通过绕开内核网络协议栈的方法，提升网络的处理能力。</p><h3 id="CPU-性能优化"><a href="#CPU-性能优化" class="headerlink" title="CPU 性能优化"></a>CPU 性能优化</h3><p><strong>应用程序优化</strong></p><ul><li>编译器优化：很多编译器都会提供优化选项，适当开启它们，在编译阶段你就可以获得编译器的帮助，来提升性能。比如， gcc 就提供了优化选项 -O2，开启后会自动对应用程序的代码进行优化。</li><li>算法优化：使用复杂度更低的算法，可以显著加快处理速度。比如，在数据比较大的情况下，可以用 O(nlogn) 的排序算法（如快排、归并排序等），代替 O(n^2) 的排序算法（如冒泡、插入排序等）。</li><li>异步处理：使用异步处理，可以避免程序因为等待某个资源而一直阻塞，从而提升程序的并发处理能力。比如，把轮询替换为事件通知，就可以避免轮询耗费 CPU 的问题。</li><li>多线程代替多进程：前面讲过，相对于进程的上下文切换，线程的上下文切换并不切换进程地址空间，因此可以降低上下文切换的成本。</li><li>善用缓存：经常访问的数据或者计算过程中的步骤，可以放到内存中缓存起来，这样在下次用时就能直接从内存中获取，加快程序的处理速度。</li></ul><p><strong>系统优化</strong></p><ul><li>CPU 绑定：把进程绑定到一个或者多个 CPU 上，可以提高 CPU 缓存的命中率，减少跨 CPU 调度带来的上下文切换问题。</li><li>CPU 独占：跟 CPU 绑定类似，进一步将 CPU 分组，并通过 CPU 亲和性机制为其分配进程。这样，这些 CPU 就由指定的进程独占，换句话说，不允许其他进程再来使用这些 CPU。</li><li>优先级调整：使用 nice 调整进程的优先级，正值调低优先级，负值调高优先级。优先级的数值含义前面我们提到过，忘了的话及时复习一下。在这里，适当降低非核心应用的优先级，增高核心应用的优先级，可以确保核心应用得到优先处理。</li><li>为进程设置资源限制：使用 Linux cgroups 来设置进程的 CPU 使用上限，可以防止由于某个应用自身的问题，而耗尽系统资源。</li><li>NUMA（Non-Uniform Memory Access）优化：支持 NUMA 的处理器会被划分为多个 node，每个 node 都有自己的本地内存空间。NUMA 优化，其实就是让 CPU 尽可能只访问本地内存。</li><li>中断负载均衡：无论是软中断还是硬中断，它们的中断处理程序都可能会耗费大量的 CPU。开启 irqbalance 服务或者配置 smp_affinity，就可以把中断处理过程自动负载均衡到多个 CPU 上。</li></ul><h2 id="内存性能篇"><a href="#内存性能篇" class="headerlink" title="内存性能篇"></a>内存性能篇</h2><h3 id="内存映射"><a href="#内存映射" class="headerlink" title="内存映射"></a>内存映射</h3><p>Linux 内核给每个进程都提供了一个独立的虚拟地址空间，并且这个地址空间是连续的。这样，进程就可以很方便地访问内存，更确切地说是访问虚拟内存。</p><p>虚拟地址空间的内部又被分为内核空间和用户空间两部分，不同字长（也就是单个 CPU 指令可以处理数据的最大长度）的处理器，地址空间的范围也不同。比如最常见的 32 位和 64 位系统，我画了两张图来分别表示它们的虚拟地址空间，如下所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-838c4ac17c43d9d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>既然每个进程都有一个这么大的地址空间，那么所有进程的虚拟内存加起来，自然要比实际的物理内存大得多。所以，并不是所有的虚拟内存都会分配物理内存，只有那些实际使用的虚拟内存才分配物理内存，并且分配后的物理内存，是通过内存映射来管理的。</p><p>内存映射，其实就是将虚拟内存地址映射到物理内存地址。为了完成内存映射，内核为每个进程都维护了一张页表，记录虚拟地址与物理地址的映射关系，如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-38403f4cb0b8753c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>页表实际上存储在 CPU 的内存管理单元 MMU 中，这样，正常情况下，处理器就可以直接通过硬件，找出要访问的内存。</p><p>另外，我在 CPU 上下文切换的文章中曾经提到， TLB（Translation Lookaside Buffer，转译后备缓冲器）会影响 CPU 的内存访问性能，在这里其实就可以得到解释。</p><p>TLB 其实就是 MMU 中页表的高速缓存。由于进程的虚拟地址空间是独立的，而 TLB 的访问速度又比 MMU 快得多，所以，通过减少进程的上下文切换，减少 TLB 的刷新次数，就可以提高 TLB 缓存的使用率，进而提高 CPU 的内存访问性能。</p><p>不过要注意，MMU 并不以字节为单位来管理内存，而是规定了一个内存映射的最小单位，也就是页，通常是 4 KB 大小。这样，每一次内存映射，<strong>都需要关联 4 KB 或者 4KB 整数倍的内存空间</strong>。</p><p>页的大小只有 4 KB ，导致的另一个问题就是，整个页表会变得非常大。比方说，仅 32 位系统就需要 100 多万个页表项（4GB/4KB），才可以实现整个地址空间的映射。为了解决页表项过多的问题，Linux 提供了两种机制，也就<strong>是多级页表和大页（HugePage）</strong>。</p><p>Linux 用的正是四级页表来管理内存页，如下图所示，虚拟地址被分为 5 个部分，前 4 个表项用于选择页，而最后一个索引表示页内偏移。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3e7d01e421228e92.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>再看大页，顾名思义，就是比普通页更大的内存块，常见的大小有 2MB 和 1GB。大页通常用在使用大量内存的进程上，比如 Oracle、DPDK 等。</p><p>首先，我们需要进一步了解虚拟内存空间的分布情况。最上方的内核空间不用多讲，下方的用户空间内存，其实又被分成了多个不同的段。以 32 位系统为例，我画了一张图来表示它们的关系。</p><h3 id="内存分配与回收"><a href="#内存分配与回收" class="headerlink" title="内存分配与回收"></a>内存分配与回收</h3><p>malloc() 是 C 标准库提供的内存分配函数，对应到系统调用上，有两种实现方式，即 brk() 和 mmap()。</p><p>对小块内存（小于 128K），C 标准库使用 brk() 来分配，也就是通过移动堆顶的位置来分配内存。这些内存释放后并不会立刻归还系统，而是被缓存起来，这样就可以重复使用。</p><p>而大块内存（大于 128K），则直接使用内存映射 mmap() 来分配，也就是在文件映射段找一块空闲内存分配出去。</p><p>这两种方式，自然各有优缺点。</p><p>brk() 方式的缓存，可以减少缺页异常的发生，提高内存访问效率。不过，由于这些内存没有归还系统，在内存工作繁忙时，频繁的内存分配和释放会造成内存碎片。</p><p>而 mmap() 方式分配的内存，会在释放时直接归还系统，所以每次 mmap 都会发生缺页异常。在内存工作繁忙时，频繁的内存分配会导致大量的缺页异常，使内核的管理负担增大。这也是 malloc 只对大块内存使用 mmap 的原因。</p><p>了解这两种调用方式后，我们还需要清楚一点，那就是，当这两种调用发生后，其实并没有真正分配内存。<strong>这些内存，都只在首次访问时才分配，也就是通过缺页异常进入内核中，再由内核来分配内存</strong>。</p><p>实际系统运行中，确实有大量比页还小的对象，如果为它们也分配单独的页，那就太浪费内存了。</p><p>所以，在用户空间，malloc 通过 brk() 分配的内存，在释放时并不立即归还系统，而是缓存起来重复利用。在内核空间，Linux 则通过 slab 分配器来管理小内存。你可以把 slab 看成构建在伙伴系统上的一个缓存，主要作用就是分配并释放内核中的小对象。</p><p>对内存来说，如果只分配而不释放，就会造成内存泄漏，甚至会耗尽系统内存。所以，在应用程序用完内存后，还需要调用 free() 或 unmap() ，来释放这些不用的内存。</p><p>当然，系统也不会任由某个进程用完所有内存。在发现内存紧张时，系统就会通过一系列机制来回收内存，比如下面这三种方式：</p><ul><li>回收缓存，比如使用 LRU（Least Recently Used）算法，回收最近使用最少的内存页面；</li><li>回收不常访问的内存，把不常用的内存通过交换分区直接写到磁盘中；</li><li>杀死进程，内存紧张时系统还会通过 OOM（Out of Memory），直接杀掉占用大量内存的进程。</li></ul><p>其中，第二种方式回收不常访问的内存时，会用到交换分区（以下简称 Swap）。Swap 其实就是把一块磁盘空间当成内存来用。它可以把进程暂时不用的数据存储到磁盘中（这个过程称为换出），当进程访问这些内存时，再从磁盘读取这些数据到内存中（这个过程称为换入）。</p><p>所以，你可以发现，Swap 把系统的可用内存变大了。不过要注意，通常只在内存不足时，才会发生 Swap 交换。并且由于磁盘读写的速度远比内存慢，Swap 会导致严重的内存性能问题。</p><p>第三种方式提到的 OOM（Out of Memory），其实是内核的一种保护机制。它监控进程的内存使用情况，并且使用 oom_score 为每个进程的内存使用情况进行评分：</p><ul><li>一个进程消耗的内存越大，oom_score 就越大；</li><li>一个进程运行占用的 CPU 越多，oom_score 就越小。</li></ul><p>这样，进程的 oom_score 越大，代表消耗的内存越多，也就越容易被 OOM 杀死，从而可以更好保护系统。</p><p>当然，为了实际工作的需要，管理员可以通过 /proc 文件系统，手动设置进程的 oom_adj ，从而调整进程的 oom_score。</p><p>oom_adj 的范围是 [-17, 15]，数值越大，表示进程越容易被 OOM 杀死；数值越小，表示进程越不容易被 OOM 杀死，其中 -17 表示禁止 OOM。</p><p>比如用下面的命令，你就可以把 sshd 进程的 oom_adj 调小为 -16，这样， sshd 进程就不容易被 OOM 杀死。</p><pre><code>echo -16 &gt; /proc/$(pidof sshd)/oom_adj</code></pre><h3 id="如何查看内存使用情况"><a href="#如何查看内存使用情况" class="headerlink" title="如何查看内存使用情况"></a>如何查看内存使用情况</h3><p>其实前面 CPU 内容的学习中，我们也提到过一些相关工具。在这里，你第一个想到的应该是 free 工具吧。下面是一个 free 的输出示例：</p><pre><code># 注意不同版本的free输出可能会有所不同$ free              total        used        free      shared  buff/cache   availableMem:        8169348      263524     6875352         668     1030472     7611064Swap:             0           0           0</code></pre><p>你可以看到，free 输出的是一个表格，其中的数值都默认以字节为单位。表格总共有两行六列，这两行分别是物理内存 Mem 和交换分区 Swap 的使用情况，而六列中，每列数据的含义分别为：</p><ul><li>第一列，total 是总内存大小；</li><li>第二列，used 是已使用内存的大小，包含了共享内存；</li><li>第三列，free 是未使用内存的大小；</li><li>第四列，shared 是共享内存的大小；</li><li>第五列，buff/cache 是缓存和缓冲区的大小；</li><li>最后一列，available 是新进程可用内存的大小。</li></ul><p>从 free 的手册中，你可以看到 buffer 和 cache 的说明。</p><ul><li>Buffers 是内核缓冲区用到的内存，对应的是 /proc/meminfo 中的 Buffers 值。</li><li>Cache 是内核页缓存和 Slab 用到的内存，对应的是 /proc/meminfo 中的 Cached 与 SReclaimable 之和。</li><li>Buffers 是对原始磁盘块的临时存储，也就是用来缓存磁盘的数据，通常不会特别大（20MB 左右）。这样，内核就可以把分散的写集中起来，统一优化磁盘的写入，比如可以把多次小的写合并成单次大的写等等。</li></ul><p>继续说回/proc/meminfo，既然 Buffers、Cached、SReclaimable 这几个指标不容易理解，那我们还得继续查 proc 文件系统，获取它们的详细定义。</p><ul><li>Cached 是从磁盘读取文件的页缓存，也就是用来缓存从文件读取的数据。这样，下次访问这些文件数据时，就可以直接从内存中快速获取，而不需要再次访问缓慢的磁盘。</li><li>SReclaimable 是 Slab 的一部分。Slab 包括两部分，其中的可回收部分，用 SReclaimable 记录；而不可回收部分，用 SUnreclaim 记录。</li></ul><p>准备环节的最后一步，为了减少缓存的影响，记得在第一个终端中，运行下面的命令来清理系统缓存：</p><pre><code># 清理文件页、目录项、Inodes等各种缓存$ echo 3 &gt; /proc/sys/vm/drop_caches</code></pre><p>这里的 /proc/sys/vm/drop_caches ，就是通过 proc 文件系统修改内核行为的一个示例，写入 3 表示清理文件页、目录项、Inodes 等各种缓存。这几种缓存的区别你暂时不用管，后面我们都会讲到。</p><p><strong>场景 1：磁盘和文件写案例</strong></p><p>接下来，到第二个终端执行 dd 命令，通过读取随机设备，生成一个 500MB 大小的文件：</p><pre><code>$ dd if=/dev/urandom of=/tmp/file bs=1M count=500</code></pre><p>然后再回到第一个终端，观察 Buffer 和 Cache 的变化情况：</p><pre><code>procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st0  0      0 7499460   1344 230484    0    0     0     0   29  145  0  0 100  0  0 1  0      0 7338088   1752 390512    0    0   488     0   39  558  0 47 53  0  0 1  0      0 7158872   1752 568800    0    0     0     4   30  376  1 50 49  0  0 1  0      0 6980308   1752 747860    0    0     0     0   24  360  0 50 50  0  0 0  0      0 6977448   1752 752072    0    0     0     0   29  138  0  0 100  0  0 0  0      0 6977440   1760 752080    0    0     0   152   42  212  0  1 99  1  0... 0  1      0 6977216   1768 752104    0    0     4 122880   33  234  0  1 51 49  0 0  1      0 6977440   1768 752108    0    0     0 10240   38  196  0  0 50 50  0</code></pre><p>通过观察 vmstat 的输出，我们发现，在 dd 命令运行时， Cache 在不停地增长，而 Buffer 基本保持不变。</p><p>再进一步观察 I/O 的情况，你会看到，</p><ul><li>在 Cache 刚开始增长时，块设备 I/O 很少，bi 只出现了一次 488 KB/s，bo 则只有一次 4KB。而过一段时间后，才会出现大量的块设备写，比如 bo 变成了 122880。</li><li>当 dd 命令结束后，Cache 不再增长，但块设备写还会持续一段时间，并且，多次 I/O 写的结果加起来，才是 dd 要写的 500M 的数据。</li></ul><p>把这个结果，跟我们刚刚了解到的 Cache 的定义做个对比，你可能会有点晕乎。为什么前面文档上说 Cache 是文件读的页缓存，怎么现在写文件也有它的份？</p><p>下面的命令对环境要求很高，需要你的系统配置多块磁盘，并且磁盘分区 /dev/sdb1 还要处于未使用状态。如果你只有一块磁盘，千万不要尝试，否则将会对你的磁盘分区造成损坏。</p><p>如果你的系统符合标准，就可以继续在第二个终端中，运行下面的命令。清理缓存后，向磁盘分区 /dev/sdb1 写入 2GB 的随机数据：</p><pre><code># 首先清理缓存$ echo 3 &gt; /proc/sys/vm/drop_caches# 然后运行dd命令向磁盘分区/dev/sdb1写入2G数据$ dd if=/dev/urandom of=/dev/sdb1 bs=1M count=2048</code></pre><p>然后，再回到终端一，观察内存和 I/O 的变化情况：</p><pre><code>vmstat 1procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st1  0      0 7584780 153592  97436    0    0   684     0   31  423  1 48 50  2  0 1  0      0 7418580 315384 101668    0    0     0     0   32  144  0 50 50  0  0 1  0      0 7253664 475844 106208    0    0     0     0   20  137  0 50 50  0  0 1  0      0 7093352 631800 110520    0    0     0     0   23  223  0 50 50  0  0 1  1      0 6930056 790520 114980    0    0     0 12804   23  168  0 50 42  9  0 1  0      0 6757204 949240 119396    0    0     0 183804   24  191  0 53 26 21  0 1  1      0 6591516 1107960 123840    0    0     0 77316   22  232  0 52 16 33  0</code></pre><p>从这里你会看到，虽然同是写数据，写磁盘跟写文件的现象还是不同的。写磁盘时（也就是 bo 大于 0 时），Buffer 和 Cache 都在增长，但显然 Buffer 的增长快得多。</p><p>这说明，写磁盘用到了大量的 Buffer，这跟我们在文档中查到的定义是一样的。</p><p>对比两个案例，我们发现，写文件时会用到 Cache 缓存数据，<strong>而写磁盘则会用到 Buffer 来缓存数据</strong>。所以，回到刚刚的问题，虽然文档上只提到，<strong>Cache 是文件读的缓存，但实际上，Cache 也会缓存写文件时的数据</strong>。</p><p><strong>场景 2：磁盘和文件读案例</strong></p><p>我们回到第二个终端，运行下面的命令。清理缓存后，从文件 /tmp/file 中，读取数据写入空设备：</p><pre><code># 首先清理缓存$ echo 3 &gt; /proc/sys/vm/drop_caches# 运行dd命令读取文件数据$ dd if=/tmp/file of=/dev/null</code></pre><p>然后，再回到终端一，观察内存和 I/O 的变化情况：</p><pre><code>procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st 0  1      0 7724164   2380 110844    0    0 16576     0   62  360  2  2 76 21  0 0  1      0 7691544   2380 143472    0    0 32640     0   46  439  1  3 50 46  0 0  1      0 7658736   2380 176204    0    0 32640     0   54  407  1  4 50 46  0 0  1      0 7626052   2380 208908    0    0 32640    40   44  422  2  2 50 46  0</code></pre><p>观察 vmstat 的输出，你会发现读磁盘时（也就是 bi 大于 0 时），Buffer 和 Cache 都在增长，但显然 Buffer 的增长快很多。这说明读磁盘时，数据缓存到了 Buffer 中。</p><p>当然，我想，经过上一个场景中两个案例的分析，你自己也可以对比得出这个结论：读文件时数据会缓存到 Cache 中，而读磁盘时数据会缓存到 Buffer 中。</p><p>到这里你应该发现了，虽然文档提供了对 Buffer 和 Cache 的说明，但是仍不能覆盖到所有的细节。比如说，今天我们了解到的这两点：</p><ul><li>Buffer 既可以用作“将要写入磁盘数据的缓存”，也可以用作“从磁盘读取数据的缓存”。</li><li>Cache 既可以用作“从文件读取数据的页缓存”，也可以用作“写文件的页缓存”。</li></ul><p>简单来说，<strong>Buffer 是对磁盘数据的缓存，而 Cache 是文件数据的缓存，它们既会用在读请求中，也会用在写请求中</strong>。</p><h3 id="cachestat"><a href="#cachestat" class="headerlink" title="cachestat"></a>cachestat</h3><pre><code>sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDDecho "deb https://repo.iovisor.org/apt/xenial xenial main" | sudo tee /etc/apt/sources.list.d/iovisor.listsudo apt-get updatesudo apt-get install -y bcc-tools libbcc-examples linux-headers-$(uname -r)</code></pre><blockquote><p>注意：bcc-tools 需要内核版本为 4.1 或者更新的版本，如果你用的是 CentOS，那就需要手动升级内核版本后再安装。</p></blockquote><pre><code>$ cachestat 1 3   TOTAL   MISSES     HITS  DIRTIES   BUFFERS_MB  CACHED_MB       2        0        2        1           17        279       2        0        2        1           17        279       2        0        2        1           17        279    </code></pre><p> 你可以看到，cachestat 的输出其实是一个表格。每行代表一组数据，而每一列代表不同的缓存统计指标。这些指标从左到右依次表示：</p><ul><li>TOTAL ，表示总的 I/O 次数；</li><li>MISSES ，表示缓存未命中的次数；</li><li>HITS ，表示缓存命中的次数；</li><li>DIRTIES， 表示新增到缓存中的脏页数；</li><li>BUFFERS_MB 表示 Buffers 的大小，以 MB 为单位；</li><li>CACHED_MB 表示 Cache 的大小，以 MB 为单位。</li></ul><p> 接下来我们再来看一个 cachetop 的运行界面：</p><pre><code>$ cachetop11:58:50 Buffers MB: 258 / Cached MB: 347 / Sort: HITS / Order: ascendingPID      UID      CMD              HITS     MISSES   DIRTIES  READ_HIT%  WRITE_HIT%   13029 root     python                  1        0        0     100.0%       0.0%   </code></pre><p>它的输出跟 top 类似，默认按照缓存的命中次数（HITS）排序，展示了每个进程的缓存命中情况。具体到每一个指标，这里的 HITS、MISSES 和 DIRTIES ，跟 cachestat 里的含义一样，分别代表间隔时间内的缓存命中次数、未命中次数以及新增到缓存中的脏页数。</p><p><strong>案例一</strong></p><p>dd 作为一个磁盘和文件的拷贝工具，经常被拿来测试磁盘或者文件系统的读写性能。不过，既然缓存会影响到性能，如果用 dd 对同一个文件进行多次读取测试，测试的结果会怎么样呢？</p><p>使用 dd 命令生成一个临时文件，用于后面的文件读取测试：</p><pre><code># 生成一个512MB的临时文件$ dd if=/dev/sda1 of=file bs=1M count=512# 清理缓存$ echo 3 &gt; /proc/sys/vm/drop_caches</code></pre><p>继续在第一个终端，运行 pcstat 命令，确认刚刚生成的文件不在缓存中。如果一切正常，你会看到 Cached 和 Percent 都是 0:</p><pre><code>$ pcstat file+-------+----------------+------------+-----------+---------+| Name  | Size (bytes)   | Pages      | Cached    | Percent ||-------+----------------+------------+-----------+---------|| file  | 536870912      | 131072     | 0         | 000.000 |+-------+----------------+------------+-----------+---------+</code></pre><p>还是在第一个终端中，现在运行 cachetop 命令：</p><pre><code># 每隔5秒刷新一次数据$ cachetop 5</code></pre><p>这次是第二个终端，运行 dd 命令测试文件的读取速度：</p><pre><code>$ dd if=file of=/dev/null bs=1M512+0 records in512+0 records out536870912 bytes (537 MB, 512 MiB) copied, 16.0509 s, 33.4 MB/s</code></pre><p>从 dd 的结果可以看出，这个文件的读性能是 33.4 MB/s。由于在 dd 命令运行前我们已经清理了缓存，所以 dd 命令读取数据时，肯定要通过文件系统从磁盘中读取。</p><p>不过，这是不是意味着， dd 所有的读请求都能直接发送到磁盘呢？</p><p>我们再回到第一个终端， 查看 cachetop 界面的缓存命中情况：</p><pre><code>PID      UID      CMD              HITS     MISSES   DIRTIES  READ_HIT%  WRITE_HIT%\.\.\.    3264 root     dd                  37077    37330        0      49.8%      50.2%</code></pre><p>从 cachetop 的结果可以发现，并不是所有的读都落到了磁盘上，事实上读请求的缓存命中率只有 50% 。</p><pre><code>$ dd if=file of=/dev/null bs=1M512+0 records in512+0 records out536870912 bytes (537 MB, 512 MiB) copied, 0.118415 s, 4.5 GB/s</code></pre><p>看到这次的结果，有没有点小惊讶？磁盘的读性能居然变成了 4.5 GB/s，比第一次的结果明显高了太多。为什么这次的结果这么好呢？</p><p>不妨再回到第一个终端，看看 cachetop 的情况：</p><pre><code>10:45:22 Buffers MB: 4 / Cached MB: 719 / Sort: HITS / Order: ascendingPID      UID      CMD              HITS     MISSES   DIRTIES  READ_HIT%  WRITE_HIT%\.\.\.   32642 root     dd                 131637        0        0     100.0%       0.0%</code></pre><p>显然，cachetop 也有了不小的变化。你可以发现，这次的读的缓存命中率是 100.0%，也就是说这次的 dd 命令全部命中了缓存，所以才会看到那么高的性能。</p><p>然后，回到第二个终端，再次执行 pcstat 查看文件 file 的缓存情况：</p><pre><code>$ pcstat file+-------+----------------+------------+-----------+---------+| Name  | Size (bytes)   | Pages      | Cached    | Percent ||-------+----------------+------------+-----------+---------|| file  | 536870912      | 131072     | 131072    | 100.000 |+-------+----------------+------------+-----------+---------+</code></pre><p>从 pcstat 的结果你可以发现，测试文件 file 已经被全部缓存了起来，这跟刚才观察到的缓存命中率 100% 是一致的。</p><p>这两次结果说明，系统缓存对第二次 dd 操作有明显的加速效果，可以大大提高文件读取的性能。</p><p>但同时也要注意，如果我们把 dd 当成测试文件系统性能的工具，由于缓存的存在，就会导致测试结果严重失真。</p><h3 id="Swap"><a href="#Swap" class="headerlink" title="Swap"></a>Swap</h3><p>除了直接内存回收，还有一个专门的内核线程用来定期回收内存，也就是 kswapd0。为了衡量内存的使用情况，kswapd0 定义了三个内存阈值（watermark，也称为水位），分别是</p><p>页最小阈值（pages_min）、页低阈值（pages_low）和页高阈值（pages_high）。剩余内存，则使用 pages_free 表示。</p><ul><li>剩余内存小于页最小阈值，说明进程可用内存都耗尽了，只有内核才可以分配内存。</li><li>剩余内存落在页最小阈值和页低阈值中间，说明内存压力比较大，剩余内存不多了。这时 kswapd0 会执行内存回收，直到剩余内存大于高阈值为止。</li><li>剩余内存落在页低阈值和页高阈值中间，说明内存有一定压力，但还可以满足新内存请求。</li><li>剩余内存大于页高阈值，说明剩余内存比较多，没有内存压力。</li></ul><p>我们可以看到，一旦剩余内存小于页低阈值，就会触发内存的回收。这个页低阈值，其实可以通过内核选项 /proc/sys/vm/min_free_kbytes 来间接设置。min_free_kbytes 设置了页最小阈值，而其他两个阈值，都是根据页最小阈值计算生成的，计算方法如下 ：</p><pre><code>pages_low = pages_min*5/4pages_high = pages_min*3/2</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3ba6fe3cc72fb753.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>NUMA 与 Swap</strong></p><p>你可以通过 numactl 命令，来查看处理器在 Node 的分布情况，以及每个 Node 的内存使用情况。比如，下面就是一个 numactl 输出的示例：</p><pre><code>$ numactl --hardwareavailable: 1 nodes (0)node 0 cpus: 0 1node 0 size: 7977 MBnode 0 free: 4416 MB...</code></pre><p>这个界面显示，我的系统中只有一个 Node，也就是 Node 0 ，而且编号为 0 和 1 的两个 CPU， 都位于 Node 0 上。另外，Node 0 的内存大小为 7977 MB，剩余内存为 4416 MB。</p><p>比如，下面就是一个 /proc/zoneinfo 文件的内容示例：</p><pre><code>$ cat /proc/zoneinfo...Node 0, zone   Normal pages free     227894       min      14896       low      18620       high     22344...     nr_free_pages 227894     nr_zone_inactive_anon 11082     nr_zone_active_anon 14024     nr_zone_inactive_file 539024     nr_zone_active_file 923986...</code></pre><p>这个输出中有大量指标，我来解释一下比较重要的几个。</p><ul><li>pages 处的 min、low、high，就是上面提到的三个内存阈值，而 free 是剩余内存页数，它跟后面的 nr_free_pages 相同。</li><li>nr_zone_active_anon 和 nr_zone_inactive_anon，分别是活跃和非活跃的匿名页数。</li><li>nr_zone_active_file 和 nr_zone_inactive_file，分别是活跃和非活跃的文件页数。</li></ul><p>从这个输出结果可以发现，剩余内存远大于页高阈值，所以此时的 kswapd0 不会回收内存。</p><p>当然，某个 Node 内存不足时，系统可以从其他 Node 寻找空闲内存，也可以从本地内存中回收内存。具体选哪种模式，你可以通过 /proc/sys/vm/zone_reclaim_mode 来调整。它支持以下几个选项：</p><ul><li>默认的 0 ，也就是刚刚提到的模式，表示既可以从其他 Node 寻找空闲内存，也可以从本地回收内存。</li><li>1、2、4 都表示只回收本地内存，2 表示可以回写脏数据回收内存，4 表示可以用 Swap 方式回收内存。</li></ul><p><strong>swappiness</strong></p><p>到这里，我们就可以理解内存回收的机制了。这些回收的内存既包括了文件页，又包括了匿名页。</p><ul><li>对文件页的回收，当然就是直接回收缓存，或者把脏页写回磁盘后再回收。</li><li>而对匿名页的回收，其实就是通过 Swap 机制，把它们写入磁盘后再释放内存。</li></ul><p>不过，你可能还有一个问题。既然有两种不同的内存回收机制，那么在实际回收内存时，到底该先回收哪一种呢？</p><p>其实，Linux 提供了一个 /proc/sys/vm/swappiness 选项，用来调整使用 Swap 的积极程度。</p><p>swappiness 的范围是 0-100，数值越大，越积极使用 Swap，也就是更倾向于回收匿名页；数值越小，越消极使用 Swap，也就是更倾向于回收文件页。</p><p>虽然 swappiness 的范围是 0-100，不过要注意，这并不是内存的百分比，而是调整 Swap 积极程度的权重，即使你把它设置成 0，当剩余内存 + 文件页小于页高阈值时，还是会发生 Swap。</p><p><strong>案例</strong></p><pre><code>$ free             total        used        free      shared  buff/cache   availableMem:        8169348      331668     6715972         696     1121708     7522896Swap:             0           0           0</code></pre><p>从这个 free 输出你可以看到，Swap 的大小是 0，这说明我的机器没有配置 Swap。</p><p>要开启 Swap，我们首先要清楚，Linux 本身支持两种类型的 Swap，即 Swap 分区和 Swap 文件。以 Swap 文件为例，在第一个终端中运行下面的命令开启 Swap，我这里配置 Swap 文件的大小为 8GB：</p><pre><code># 创建Swap文件$ fallocate -l 8G /mnt/swapfile# 修改权限只有根用户可以访问$ chmod 600 /mnt/swapfile# 配置Swap文件$ mkswap /mnt/swapfile# 开启Swap$ swapon /mnt/swapfile</code></pre><p>然后，再执行 free 命令，确认 Swap 配置成功：</p><pre><code>$ free             total        used        free      shared  buff/cache   availableMem:        8169348      331668     6715972         696     1121708     7522896Swap:       8388604           0     8388604</code></pre><p>接下来，我们在第一个终端中，运行下面的 dd 命令，模拟大文件的读取：</p><pre><code># 写入空设备，实际上只有磁盘的读请求$ dd if=/dev/sda1 of=/dev/null bs=1G count=2048</code></pre><p>接着，在第二个终端中运行 sar 命令，查看内存各个指标的变化情况。你可以多观察一会儿，查看这些指标的变化情况。</p><pre><code># 间隔1秒输出一组数据# -r表示显示内存使用情况，-S表示显示Swap使用情况$ sar -r -S 104:39:56    kbmemfree   kbavail kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty04:39:57      6249676   6839824   1919632     23.50    740512     67316   1691736     10.22    815156    841868         404:39:56    kbswpfree kbswpused  %swpused  kbswpcad   %swpcad04:39:57      8388604         0      0.00         0      0.0004:39:57    kbmemfree   kbavail kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty04:39:58      6184472   6807064   1984836     24.30    772768     67380   1691736     10.22    847932    874224        2004:39:57    kbswpfree kbswpused  %swpused  kbswpcad   %swpcad04:39:58      8388604         0      0.00         0      0.00…04:44:06    kbmemfree   kbavail kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty04:44:07       152780   6525716   8016528     98.13   6530440     51316   1691736     10.22    867124   6869332         004:44:06    kbswpfree kbswpused  %swpused  kbswpcad   %swpcad04:44:07      8384508      4096      0.05        52      1.27</code></pre><p>我们可以看到，sar 的输出结果是两个表格，第一个表格表示内存的使用情况，第二个表格表示 Swap 的使用情况。其中，各个指标名称前面的 kb 前缀，表示这些指标的单位是 KB。</p><p>去掉前缀后，你会发现，大部分指标我们都已经见过了，剩下的几个新出现的指标，我来简单介绍一下。</p><ul><li>kbcommit，表示当前系统负载需要的内存。它实际上是为了保证系统内存不溢出，对需要内存的估计值。%commit，就是这个值相对总内存的百分比。</li><li>kbactive，表示活跃内存，也就是最近使用过的内存，一般不会被系统回收。</li><li>kbinact，表示非活跃内存，也就是不常访问的内存，有可能会被系统回收。</li></ul><p>清楚了界面指标的含义后，我们再结合具体数值，来分析相关的现象。你可以清楚地看到，总的内存使用率（%memused）在不断增长，从开始的 23% 一直长到了 98%，并且主要内存都被缓冲区（kbbuffers）占用。具体来说：</p><ul><li>刚开始，剩余内存（kbmemfree）不断减少，而缓冲区（kbbuffers）则不断增大，由此可知，剩余内存不断分配给了缓冲区。</li><li>一段时间后，剩余内存已经很小，而缓冲区占用了大部分内存。这时候，Swap 的使用开始逐渐增大，缓冲区和剩余内存则只在小范围内波动。</li></ul><p>显然，我们还得看看进程缓存的情况。在前面缓存的案例中我们学过， cachetop 正好能满足这一点。那我们就来 cachetop 一下。</p><p>在第二个终端中，按下 Ctrl+C 停止 sar 命令，然后运行下面的 cachetop 命令，观察缓存的使用情况：</p><pre><code>$ cachetop 512:28:28 Buffers MB: 6349 / Cached MB: 87 / Sort: HITS / Order: ascendingPID      UID      CMD              HITS     MISSES   DIRTIES  READ_HIT%  WRITE_HIT%   18280 root     python                 22        0        0     100.0%       0.0%   18279 root     dd                  41088    41022        0      50.0%      50.0%                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       </code></pre><p>通过 cachetop 的输出，我们看到，dd 进程的读写请求只有 50% 的命中率，并且未命中的缓存页数（MISSES）为 41022（单位是页）。这说明，正是案例开始时运行的 dd，导致了缓冲区使用升高。</p><p>你可能接着会问，为什么 Swap 也跟着升高了呢？直观来说，缓冲区占了系统绝大部分内存，还属于可回收内存，内存不够用时，不应该先回收缓冲区吗？</p><p>这种情况，我们还得进一步通过 /proc/zoneinfo ，观察剩余内存、内存阈值以及匿名页和文件页的活跃情况。</p><p>你可以在第二个终端中，按下 Ctrl+C，停止 cachetop 命令。然后运行下面的命令，观察 /proc/zoneinfo 中这几个指标的变化情况：</p><pre><code># -d 表示高亮变化的字段# -A 表示仅显示Normal行以及之后的15行输出$ watch -d grep -A 15 'Normal' /proc/zoneinfoNode 0, zone   Normal  pages free     21328        min      14896        low      18620        high     22344        spanned  1835008        present  1835008        managed  1796710        protection: (0, 0, 0, 0, 0)      nr_free_pages 21328      nr_zone_inactive_anon 79776      nr_zone_active_anon 206854      nr_zone_inactive_file 918561      nr_zone_active_file 496695      nr_zone_unevictable 2251      nr_zone_write_pending 0      </code></pre><p>你可以发现，剩余内存（pages_free）在一个小范围内不停地波动。当它小于页低阈值（pages_low) 时，又会突然增大到一个大于页高阈值（pages_high）的值。</p><p>再结合刚刚用 sar 看到的剩余内存和缓冲区的变化情况，我们可以推导出，剩余内存和缓冲区的波动变化，正是由于内存回收和缓存再次分配的循环往复。</p><ul><li>当剩余内存小于页低阈值时，系统会回收一些缓存和匿名内存，使剩余内存增大。其中，缓存的回收导致 sar 中的缓冲区减小，而匿名内存的回收导致了 Swap 的使用增大。</li><li>紧接着，由于 dd 还在继续，剩余内存又会重新分配给缓存，导致剩余内存减少，缓冲区增大。</li></ul><p>其实还有一个有趣的现象，如果多次运行 dd 和 sar，你可能会发现，在多次的循环重复中，有时候是 Swap 用得比较多，有时候 Swap 很少，反而缓冲区的波动更大。</p><p>最后，如果你在一开始配置了 Swap，不要忘记在案例结束后关闭。你可以运行下面的命令，关闭 Swap：</p><pre><code>$ swapoff -a</code></pre><p>实际上，关闭 Swap 后再重新打开，也是一种常用的 Swap 空间清理方法，比如：</p><pre><code>$ swapoff -a &amp;&amp; swapon -a </code></pre><p><strong>总结</strong></p><p>在内存资源紧张时，Linux 会通过 Swap ，把不常访问的匿名页换出到磁盘中，下次访问的时候再从磁盘换入到内存中来。你可以设置 /proc/sys/vm/min_free_kbytes，来调整系统定期回收内存的阈值；也可以设置 /proc/sys/vm/swappiness，来调整文件页和匿名页的回收倾向。</p><p>当 Swap 变高时，你可以用 sar、/proc/zoneinfo、/proc/pid/status 等方法，查看系统和进程的内存使用情况，进而找出 Swap 升高的根源和受影响的进程。</p><p>反过来说，通常，降低 Swap 的使用，可以提高系统的整体性能。要怎么做呢？这里，我也总结了几种常见的降低方法。</p><ul><li>禁止 Swap，现在服务器的内存足够大，所以除非有必要，禁用 Swap 就可以了。随着云计算的普及，大部分云平台中的虚拟机都默认禁止 Swap。</li><li>如果实在需要用到 Swap，可以尝试降低 swappiness 的值，减少内存回收时 Swap 的使用倾向。</li><li>响应延迟敏感的应用，如果它们可能在开启 Swap 的服务器中运行，你还可以用库函数 mlock() 或者 mlockall() 锁定内存，阻止它们的内存换出。</li></ul><h3 id="内存性能指标"><a href="#内存性能指标" class="headerlink" title="内存性能指标"></a>内存性能指标</h3><p>首先，你最容易想到的是系统内存使用情况，比如已用内存、剩余内存、共享内存、可用内存、缓存和缓冲区的用量等。</p><ul><li>已用内存和剩余内存很容易理解，就是已经使用和还未使用的内存。</li><li>共享内存是通过 tmpfs 实现的，所以它的大小也就是 tmpfs 使用的内存大小。tmpfs 其实也是一种特殊的缓存。</li><li>可用内存是新进程可以使用的最大内存，它包括剩余内存和可回收缓存。</li><li>缓存包括两部分，一部分是磁盘读取文件的页缓存，用来缓存从磁盘读取的数据，可以加快以后再次访问的速度。另一部分，则是 Slab 分配器中的可回收内存。</li><li>缓冲区是对原始磁盘块的临时存储，用来缓存将要写入磁盘的数据。这样，内核就可以把分散的写集中起来，统一优化磁盘写入。</li></ul><p>第二类很容易想到的，应该是进程内存使用情况，比如进程的虚拟内存、常驻内存、共享内存以及 Swap 内存等。</p><ul><li>虚拟内存，包括了进程代码段、数据段、共享内存、已经申请的堆内存和已经换出的内存等。这里要注意，已经申请的内存，即使还没有分配物理内存，也算作虚拟内存。</li><li>常驻内存是进程实际使用的物理内存，不过，它不包括 Swap 和共享内存。</li><li>共享内存，既包括与其他进程共同使用的真实的共享内存，还包括了加载的动态链接库以及程序的代码段等。</li><li>Swap 内存，是指通过 Swap 换出到磁盘的内存。</li></ul><p>除了这些很容易想到的指标外，我还想再强调一下，缺页异常。</p><p>在内存分配的原理中，我曾经讲到过，系统调用内存分配请求后，并不会立刻为其分配物理内存，而是在请求首次访问时，通过缺页异常来分配。缺页异常又分为下面两种场景。</p><ul><li>可以直接从物理内存中分配时，被称为次缺页异常。</li><li>需要磁盘 I/O 介入（比如 Swap）时，被称为主缺页异常。</li></ul><p>显然，主缺页异常升高，就意味着需要磁盘 I/O，那么内存访问也会慢很多。</p><p>除了系统内存和进程内存，第三类重要指标就是 Swap 的使用情况，比如 Swap 的已用空间、剩余空间、换入速度和换出速度等。</p><ul><li>已用空间和剩余空间很好理解，就是字面上的意思，已经使用和没有使用的内存空间。</li><li>换入和换出速度，则表示每秒钟换入和换出内存的大小。</li></ul><p><strong>总结</strong></p><p>常见的优化思路有这么几种。</p><ol><li>最好禁止 Swap。如果必须开启 Swap，降低 swappiness 的值，减少内存回收时 Swap 的使用倾向。</li><li>减少内存的动态分配。比如，可以使用内存池、大页（HugePage）等。</li><li>尽量使用缓存和缓冲区来访问数据。比如，可以使用堆栈明确声明内存空间，来存储需要缓存的数据；或者用 Redis 这类的外部缓存组件，优化数据的访问。</li><li>使用 cgroups 等方式限制进程的内存使用情况。这样，可以确保系统内存不会被异常进程耗尽。</li><li>通过 /proc/pid/oom_adj ，调整核心应用的 oom_score。这样，可以保证即使内存紧张，核心应用也不会被 OOM 杀死。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4ca1a243b3334aa2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9ceecdfaae45ffe0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c71609284b08fb12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0bae9d87bd7d11b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="答疑"><a href="#答疑" class="headerlink" title="答疑"></a>答疑</h3><p><strong>问题 1：内存回收与 OOM</strong></p><ul><li>怎么理解 LRU 内存回收？</li><li>回收后的内存又到哪里去了？</li><li>OOM 是按照虚拟内存还是实际内存来打分？</li><li>怎么估计应用程序的最小内存？</li></ul><p>其实在 Linux 内存的原理篇和 Swap 原理篇中我曾经讲到，一旦发现内存紧张，系统会通过三种方式回收内存。我们来复习一下，这三种方式分别是 ：</p><ul><li>基于 LRU（Least Recently Used）算法，回收缓存；</li><li>基于 Swap 机制，回收不常访问的匿名页；</li><li>基于 OOM（Out of Memory）机制，杀掉占用大量内存的进程。</li></ul><p>前两种方式，缓存回收和 Swap 回收，实际上都是基于 LRU 算法，也就是优先回收不常访问的内存。LRU 回收算法，实际上维护着 active 和 inactive 两个双向链表，其中：</p><ul><li>active 记录活跃的内存页；</li><li>inactive 记录非活跃的内存页。</li></ul><p>越接近链表尾部，就表示内存页越不常访问。这样，在回收内存时，系统就可以根据活跃程度，优先回收不活跃的内存。</p><p>活跃和非活跃的内存页，按照类型的不同，又分别分为文件页和匿名页，对应着缓存回收和 Swap 回收。</p><p>当然，你可以从 /proc/meminfo 中，查询它们的大小，比如：</p><pre><code># grep表示只保留包含active的指标（忽略大小写）# sort表示按照字母顺序排序$ cat /proc/meminfo | grep -i active | sortActive(anon):     167976 kBActive(file):     971488 kBActive:          1139464 kBInactive(anon):      720 kBInactive(file):  2109536 kBInactive:        2110256 kB</code></pre><p>第三种方式，OOM 机制按照 oom_score 给进程排序。oom_score 越大，进程就越容易被系统杀死。</p><p>当系统发现内存不足以分配新的内存请求时，就会尝试直接内存回收。这种情况下，如果回收完文件页和匿名页后，内存够用了，当然皆大欢喜，把回收回来的内存分配给进程就可以了。但如果内存还是不足，OOM 就要登场了。</p><p>OOM 发生时，你可以在 dmesg 中看到 Out of memory 的信息，从而知道是哪些进程被 OOM 杀死了。比如，你可以执行下面的命令，查询 OOM 日志：</p><pre><code>$ dmesg | grep -i "Out of memory"Out of memory: Kill process 9329 (java) score 321 or sacrifice child</code></pre><p>这三种方式，我们就复习完了。接下来，我们回到开始的四个问题，相信你自己已经有了答案。</p><ol><li>LRU 算法的原理刚才已经提到了，这里不再重复。</li><li>内存回收后，会被重新放到未使用内存中。这样，新的进程就可以请求、使用它们。</li><li>OOM 触发的时机基于虚拟内存。换句话说，进程在申请内存时，如果申请的虚拟内存加上服务器实际已用的内存之和，比总的物理内存还大，就会触发 OOM。</li><li>要确定一个进程或者容器的最小内存，最简单的方法就是让它运行起来，再通过 ps 或者 smap ，查看它的内存使用情况。不过要注意，进程刚启动时，可能还没开始处理实际业务，一旦开始处理实际业务，就会占用更多内存。所以，要记得给内存留一定的余量。</li></ol><p><strong>问题 2: 文件系统与磁盘的区别</strong></p><p>文件系统和磁盘的原理，我将在下一个模块中讲解，它们跟内存的关系也十分密切。不过，在学习 Buffer 和 Cache 的原理时，我曾提到，Buffer 用于磁盘，而 Cache 用于文件。因此，有不少同学困惑了，比如 JJ 留言中的这两个问题。</p><ul><li>读写文件最终也是读写磁盘，到底要怎么区分，是读写文件还是读写磁盘呢？</li><li>读写磁盘难道可以不经过文件系统吗？</li></ul><p>磁盘是一个存储设备（确切地说是块设备），可以被划分为不同的磁盘分区。而在磁盘或者磁盘分区上，还可以再创建文件系统，并挂载到系统的某个目录中。这样，系统就可以通过这个挂载目录，来读写文件。</p><p>换句话说，磁盘是存储数据的块设备，也是文件系统的载体。所以，文件系统确实还是要通过磁盘，来保证数据的持久化存储。</p><p>你在很多地方都会看到这句话， Linux 中一切皆文件。换句话说，你可以通过相同的文件接口，来访问磁盘和文件（比如 open、read、write、close 等）。</p><ul><li>我们通常说的“文件”，其实是指普通文件。</li><li>而磁盘或者分区，则是指块设备文件。</li></ul><p>你可以执行 “ls -l &lt; 路径 &gt;” 查看它们的区别。如果不懂 ls 输出的含义，别忘了 man 一下就可以。执行 man ls 命令，以及 info ‘(coreutils) ls invocation’ 命令，就可以查到了。</p><p>在读写普通文件时，I/O 请求会首先经过文件系统，然后由文件系统负责，来与磁盘进行交互。而在读写块设备文件时，会跳过文件系统，直接与磁盘交互，也就是所谓的“裸 I/O”。</p><p>这两种读写方式使用的缓存自然不同。文件系统管理的缓存，其实就是 Cache 的一部分。而裸磁盘的缓存，用的正是 Buffer。</p><h2 id="I-O-性能篇"><a href="#I-O-性能篇" class="headerlink" title="I/O 性能篇"></a>I/O 性能篇</h2><h3 id="索引节点和目录项"><a href="#索引节点和目录项" class="headerlink" title="索引节点和目录项"></a>索引节点和目录项</h3><p>为了方便管理，Linux 文件系统为每个文件都分配两个数据结构，索引节点（index node）和目录项（directory entry）。它们主要用来记录文件的元信息和目录结构。</p><ul><li>索引节点，简称为 inode，用来记录文件的元数据，比如 inode 编号、文件大小、访问权限、修改日期、数据的位置等。索引节点和文件一一对应，它跟文件内容一样，都会被持久化存储到磁盘中。所以记住，索引节点同样占用磁盘空间。</li><li>目录项，简称为 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的关联关系。多个关联的目录项，就构成了文件系统的目录结构。不过，不同于索引节点，目录项是由内核维护的一个内存数据结构，所以通常也被叫做目录项缓存。</li></ul><p>换句话说，索引节点是每个文件的唯一标志，而目录项维护的正是文件系统的树状结构。目录项和索引节点的关系是多对一，你可以简单理解为，一个文件可以有多个别名。</p><p>举个例子，通过硬链接为文件创建的别名，就会对应不同的目录项，不过这些目录项本质上还是链接同一个文件，所以，它们的索引节点相同。</p><p>索引节点和目录项纪录了文件的元数据，以及文件间的目录关系，那么具体来说，文件数据到底是怎么存储的呢？是不是直接写到磁盘中就好了呢？</p><p>实际上，磁盘读写的最小单位是扇区，然而扇区只有 512B 大小，如果每次都读写这么小的单位，效率一定很低。所以，文件系统又把连续的扇区组成了逻辑块，然后每次都以逻辑块为最小单元，来管理数据。常见的逻辑块大小为 4KB，也就是由连续的 8 个扇区组成。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4977cfb5e880bb8f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>第一，目录项本身就是一个内存缓存，而索引节点则是存储在磁盘中的数据。在前面的 Buffer 和 Cache 原理中，我曾经提到过，为了协调慢速磁盘与快速 CPU 的性能差异，文件内容会缓存到页缓存 Cache 中。</p><p>第二，磁盘在执行文件系统格式化时，会被分成三个存储区域，超级块、索引节点区和数据块区。其中，</p><ul><li>超级块，存储整个文件系统的状态。</li><li>索引节点区，用来存储索引节点。</li><li>数据块区，则用来存储文件数据。</li></ul><h3 id="虚拟文件系统"><a href="#虚拟文件系统" class="headerlink" title="虚拟文件系统"></a>虚拟文件系统</h3><p>目录项、索引节点、逻辑块以及超级块，构成了 Linux 文件系统的四大基本要素。不过，为了支持各种不同的文件系统，Linux 内核在用户进程和文件系统的中间，又引入了一个抽象层，也就是虚拟文件系统 VFS（Virtual File System）。</p><p>VFS 定义了一组所有文件系统都支持的数据结构和标准接口。这样，用户进程和内核中的其他子系统，只需要跟 VFS 提供的统一接口进行交互就可以了，而不需要再关心底层各种文件系统的实现细节。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3d39139211d51c0f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>通过这张图，你可以看到，在 VFS 的下方，Linux 支持各种各样的文件系统，如 Ext4、XFS、NFS 等等。按照存储位置的不同，这些文件系统可以分为三类。</p><ul><li>第一类是基于磁盘的文件系统，也就是把数据直接存储在计算机本地挂载的磁盘中。常见的 Ext4、XFS、OverlayFS 等，都是这类文件系统。</li><li>第二类是基于内存的文件系统，也就是我们常说的虚拟文件系统。这类文件系统，不需要任何磁盘分配存储空间，但会占用内存。我们经常用到的 /proc 文件系统，其实就是一种最常见的虚拟文件系统。此外，/sys 文件系统也属于这一类，主要向用户空间导出层次化的内核对象。</li><li>第三类是网络文件系统，也就是用来访问其他计算机数据的文件系统，比如 NFS、SMB、iSCSI 等。</li></ul><p>这些文件系统，要先挂载到 VFS 目录树中的某个子目录（称为挂载点），然后才能访问其中的文件。拿第一类，也就是基于磁盘的文件系统为例，在安装系统时，要先挂载一个根目录（/），在根目录下再把其他文件系统（比如其他的磁盘分区、/proc 文件系统、/sys 文件系统、NFS 等）挂载进来。</p><h3 id="文件系统-I-O"><a href="#文件系统-I-O" class="headerlink" title="文件系统 I/O"></a>文件系统 I/O</h3><p>把文件系统挂载到挂载点后，你就能通过挂载点，再去访问它管理的文件了。VFS 提供了一组标准的文件访问接口。这些接口以系统调用的方式，提供给应用程序使用。</p><p>就拿 cat 命令来说，它首先调用 open() ，打开一个文件；然后调用 read() ，读取文件的内容；最后再调用 write() ，把文件内容输出到控制台的标准输出中：</p><pre><code>int open(const char *pathname, int flags, mode_t mode); ssize_t read(int fd, void *buf, size_t count); ssize_t write(int fd, const void *buf, size_t count); </code></pre><p>文件读写方式的各种差异，导致 I/O 的分类多种多样。最常见的有，缓冲与非缓冲 I/O、直接与非直接 I/O、阻塞与非阻塞 I/O、同步与异步 I/O 等。 接下来，我们就详细看这四种分类。</p><p>第一种，根据是否利用标准库缓存，可以把文件 I/O 分为缓冲 I/O 与非缓冲 I/O。</p><ul><li>缓冲 I/O，是指利用标准库缓存来加速文件的访问，而标准库内部再通过系统调度访问文件。</li><li>非缓冲 I/O，是指直接通过系统调用来访问文件，不再经过标准库缓存。</li></ul><p>注意，这里所说的“缓冲”，是指标准库内部实现的缓存。比方说，你可能见到过，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来。</p><p>无论缓冲 I/O 还是非缓冲 I/O，它们最终还是要经过系统调用来访问文件。而根据上一节内容，我们知道，系统调用后，还会通过页缓存，来减少磁盘的 I/O 操作。</p><p>第二，根据是否利用操作系统的页缓存，可以把文件 I/O 分为直接 I/O 与非直接 I/O。</p><ul><li>直接 I/O，是指跳过操作系统的页缓存，直接跟文件系统交互来访问文件。</li><li>非直接 I/O 正好相反，文件读写时，先要经过系统的页缓存，然后再由内核或额外的系统调用，真正写入磁盘。</li></ul><p>想要实现直接 I/O，需要你在系统调用中，指定 O_DIRECT 标志。如果没有设置过，默认的是非直接 I/O。</p><p>不过要注意，直接 I/O、非直接 I/O，本质上还是和文件系统交互。如果是在数据库等场景中，你还会看到，跳过文件系统读写磁盘的情况，也就是我们通常所说的裸 I/O。</p><p>第三，根据应用程序是否阻塞自身运行，可以把文件 I/O 分为阻塞 I/O 和非阻塞 I/O：</p><ul><li>所谓阻塞 I/O，是指应用程序执行 I/O 操作后，如果没有获得响应，就会阻塞当前线程，自然就不能执行其他任务。</li><li>所谓非阻塞 I/O，是指应用程序执行 I/O 操作后，不会阻塞当前的线程，可以继续执行其他的任务，随后再通过轮询或者事件通知的形式，获取调用的结果。</li></ul><p>比方说，访问管道或者网络套接字时，设置 O_NONBLOCK 标志，就表示用非阻塞方式访问；而如果不做任何设置，默认的就是阻塞访问。</p><p>第四，根据是否等待响应结果，可以把文件 I/O 分为同步和异步 I/O：</p><ul><li>所谓同步 I/O，是指应用程序执行 I/O 操作后，要一直等到整个 I/O 完成后，才能获得 I/O 响应。</li><li>所谓异步 I/O，是指应用程序执行 I/O 操作后，不用等待完成和完成后的响应，而是继续执行就可以。等到这次 I/O 完成后，响应会用事件通知的方式，告诉应用程序。</li></ul><p>举个例子，在操作文件时，如果你设置了 O_SYNC 或者 O_DSYNC 标志，就代表同步 I/O。如果设置了 O_DSYNC，就要等文件数据写入磁盘后，才能返回；而 O_SYNC，则是在 O_DSYNC 基础上，要求文件元数据也要写入磁盘后，才能返回。</p><p>再比如，在访问管道或者网络套接字时，设置了 O_ASYNC 选项后，相应的 I/O 就是异步 I/O。这样，内核会再通过 SIGIO 或者 SIGPOLL，来通知进程文件是否可读写。</p><p>你可能发现了，这里的好多概念也经常出现在网络编程中。比如非阻塞 I/O，通常会跟 select/poll 配合，用在网络套接字的 I/O 中。</p><h3 id="df"><a href="#df" class="headerlink" title="df"></a>df</h3><p>对文件系统来说，最常见的一个问题就是空间不足。当然，你可能本身就知道，用 df 命令，就能查看文件系统的磁盘空间使用情况。比如：</p><pre><code>$ df /dev/sda1 Filesystem     1K-blocks    Used Available Use% Mounted on /dev/sda1       30308240 3167020  27124836  11% / </code></pre><p>你可以看到，我的根文件系统只使用了 11% 的空间。这里还要注意，总空间用 1K-blocks 的数量来表示，你可以给 df 加上 -h 选项，以获得更好的可读性：</p><pre><code>$ df -h /dev/sda1 Filesystem      Size  Used Avail Use% Mounted on /dev/sda1        29G  3.1G   26G  11% / </code></pre><p>不过有时候，明明你碰到了空间不足的问题，可是用 df 查看磁盘空间后，却发现剩余空间还有很多。这是怎么回事呢？</p><p>不知道你还记不记得，刚才我强调的一个细节。除了文件数据，索引节点也占用磁盘空间。你可以给 df 命令加上 -i 参数，查看索引节点的使用情况，如下所示：</p><pre><code>$ df -i /dev/sda1 Filesystem      Inodes  IUsed   IFree IUse% Mounted on /dev/sda1      3870720 157460 3713260    5% / </code></pre><p>索引节点的容量，（也就是 Inode 个数）是在格式化磁盘时设定好的，一般由格式化工具自动生成。当你发现索引节点空间不足，但磁盘空间充足时，很可能就是过多小文件导致的。</p><h3 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h3><p>在前面 Cache 案例中，我已经介绍过，可以用 free 或 vmstat，来观察页缓存的大小。复习一下，free 输出的 Cache，是页缓存和可回收 Slab 缓存的和，你可以从 /proc/meminfo ，直接得到它们的大小：</p><pre><code>$ cat /proc/meminfo | grep -E "SReclaimable|Cached" Cached:           748316 kB SwapCached:            0 kB SReclaimable:     179508 kB </code></pre><p>话说回来，文件系统中的目录项和索引节点缓存，又该如何观察呢？</p><p>实际上，内核使用 Slab 机制，管理目录项和索引节点的缓存。/proc/meminfo 只给出了 Slab 的整体大小，具体到每一种 Slab 缓存，还要查看 /proc/slabinfo 这个文件。</p><p>比如，运行下面的命令，你就可以得到，所有目录项和各种文件系统索引节点的缓存情况：</p><pre><code>$ cat /proc/slabinfo | grep -E '^#|dentry|inode' # name            &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;limit&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt; xfs_inode              0      0    960   17    4 : tunables    0    0    0 : slabdata      0      0      0 ... ext4_inode_cache   32104  34590   1088   15    4 : tunables    0    0    0 : slabdata   2306   2306      0hugetlbfs_inode_cache     13     13    624   13    2 : tunables    0    0    0 : slabdata      1      1      0 sock_inode_cache    1190   1242    704   23    4 : tunables    0    0    0 : slabdata     54     54      0 shmem_inode_cache   1622   2139    712   23    4 : tunables    0    0    0 : slabdata     93     93      0 proc_inode_cache    3560   4080    680   12    2 : tunables    0    0    0 : slabdata    340    340      0 inode_cache        25172  25818    608   13    2 : tunables    0    0    0 : slabdata   1986   1986      0 dentry             76050 121296    192   21    1 : tunables    0    0    0 : slabdata   5776   5776      0 </code></pre><p>这个界面中，dentry 行表示目录项缓存，inode_cache 行，表示 VFS 索引节点缓存，其余的则是各种文件系统的索引节点缓存。</p><p>/proc/slabinfo 的列比较多，具体含义你可以查询 man slabinfo。在实际性能分析中，我们更常使用 slabtop ，来找到占用内存最多的缓存类型。    </p><p>比如，下面就是我运行 slabtop 得到的结果：</p><pre><code># 按下c按照缓存大小排序，按下a按照活跃对象数排序 $ slabtop Active / Total Objects (% used)    : 277970 / 358914 (77.4%) Active / Total Slabs (% used)      : 12414 / 12414 (100.0%) Active / Total Caches (% used)     : 83 / 135 (61.5%) Active / Total Size (% used)       : 57816.88K / 73307.70K (78.9%) Minimum / Average / Maximum Object : 0.01K / 0.20K / 22.88K   OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME 69804  23094   0%    0.19K   3324       21     13296K dentry 16380  15854   0%    0.59K   1260       13     10080K inode_cache 58260  55397   0%    0.13K   1942       30      7768K kernfs_node_cache    485    413   0%    5.69K     97        5      3104K task_struct   1472   1397   0%    2.00K     92       16      2944K kmalloc-2048     </code></pre><h3 id="磁盘I-O"><a href="#磁盘I-O" class="headerlink" title="磁盘I/O"></a>磁盘I/O</h3><p>其实，无论机械磁盘，还是固态磁盘，相同磁盘的随机 I/O 都要比连续 I/O 慢很多，原因也很明显。</p><ul><li>对机械磁盘来说，我们刚刚提到过的，由于随机 I/O 需要更多的磁头寻道和盘片旋转，它的性能自然要比连续 I/O 慢。</li><li>而对固态磁盘来说，虽然它的随机性能比机械硬盘好很多，但同样存在“先擦除再写入”的限制。随机读写会导致大量的垃圾回收，所以相对应的，随机 I/O 的性能比起连续 I/O 来，也还是差了很多。</li><li>此外，连续 I/O 还可以通过预读的方式，来减少 I/O 请求的次数，这也是其性能优异的一个原因。很多性能优化的方案，也都会从这个角度出发，来优化 I/O 性能。</li></ul><p>此外，机械磁盘和固态磁盘还分别有一个最小的读写单位。</p><ul><li>机械磁盘的最小读写单位是扇区，一般大小为 512 字节。</li><li>而固态磁盘的最小读写单位是页，通常大小是 4KB、8KB 等。</li></ul><p><strong>通用块层</strong></p><p>通用块层，其实是处在文件系统和磁盘驱动中间的一个块设备抽象层。它主要有两个功能 。</p><ul><li>第一个功能跟虚拟文件系统的功能类似。向上，为文件系统和应用程序，提供访问块设备的标准接口；向下，把各种异构的磁盘设备抽象为统一的块设备，并提供统一框架来管理这些设备的驱动程序。</li><li>第二个功能，通用块层还会给文件系统和应用程序发来的 I/O 请求排队，并通过重新排序、请求合并等方式，提高磁盘读写的效率。</li></ul><p>其中，对 I/O 请求排序的过程，也就是我们熟悉的 I/O 调度。事实上，Linux 内核支持四种 I/O 调度算法，分别是 NONE、NOOP、CFQ 以及 DeadLine。这里我也分别介绍一下。</p><p>第一种 NONE ，更确切来说，并不能算 I/O 调度算法。因为它完全不使用任何 I/O 调度器，对文件系统和应用程序的 I/O 其实不做任何处理，常用在虚拟机中（此时磁盘 I/O 调度完全由物理机负责）。</p><p>第二种 NOOP ，是最简单的一种 I/O 调度算法。它实际上是一个先入先出的队列，只做一些最基本的请求合并，常用于 SSD 磁盘。</p><p>第三种 CFQ（Completely Fair Scheduler），也被称为完全公平调度器，是现在很多发行版的默认 I/O 调度器，它为每个进程维护了一个 I/O 调度队列，并按照时间片来均匀分布每个进程的 I/O 请求。</p><p>类似于进程 CPU 调度，CFQ 还支持进程 I/O 的优先级调度，所以它适用于运行大量进程的系统，像是桌面环境、多媒体应用等。</p><p>最后一种 DeadLine 调度算法，分别为读、写请求创建了不同的 I/O 队列，可以提高机械磁盘的吞吐量，并确保达到最终期限（deadline）的请求被优先处理。DeadLine 调度算法，多用在 I/O 压力比较重的场景，比如数据库等。</p><p><strong>I/O 栈</strong></p><p>我们可以把 Linux 存储系统的 I/O 栈，由上到下分为三个层次，分别是文件系统层、通用块层和设备层。这三个 I/O 层的关系如下图所示，这其实也是 Linux 存储系统的 I/O 栈全景图。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a0160fe4ce59c384.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>根据这张 I/O 栈的全景图，我们可以更清楚地理解，存储系统 I/O 的工作原理。</p><ul><li>文件系统层，包括虚拟文件系统和其他各种文件系统的具体实现。它为上层的应用程序，提供标准的文件访问接口；对下会通过通用块层，来存储和管理磁盘数据。</li><li>通用块层，包括块设备 I/O 队列和 I/O 调度器。它会对文件系统的 I/O 请求进行排队，再通过重新排序和请求合并，然后才要发送给下一级的设备层。</li><li>设备层，包括存储设备和相应的驱动程序，负责最终物理设备的 I/O 操作。</li></ul><p>存储系统的 I/O ，通常是整个系统中最慢的一环。所以， Linux 通过多种缓存机制来优化 I/O 效率。</p><p><strong>磁盘性能指标</strong></p><p>说到磁盘性能的衡量标准，必须要提到五个常见指标，也就是我们经常用到的，使用率、饱和度、IOPS、吞吐量以及响应时间等。这五个指标，是衡量磁盘性能的基本指标。</p><ul><li>使用率，是指磁盘处理 I/O 的时间百分比。过高的使用率（比如超过 80%），通常意味着磁盘 I/O 存在性能瓶颈。</li><li>饱和度，是指磁盘处理 I/O 的繁忙程度。过高的饱和度，意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I/O 请求。</li><li>IOPS（Input/Output Per Second），是指每秒的 I/O 请求数。</li><li>吞吐量，是指每秒的 I/O 请求大小。</li><li>响应时间，是指 I/O 请求从发出到收到响应的间隔时间。</li></ul><p>这里要注意的是，使用率只考虑有没有 I/O，而不考虑 I/O 的大小。换句话说，当使用率是 100% 的时候，磁盘依然有可能接受新的 I/O 请求。</p><h3 id="磁盘-I-O-观测"><a href="#磁盘-I-O-观测" class="headerlink" title="磁盘 I/O 观测"></a>磁盘 I/O 观测</h3><p>iostat 是最常用的磁盘 I/O 性能观测工具，它提供了每个磁盘的使用率、IOPS、吞吐量等各种常见的性能指标，当然，这些指标实际上来自 /proc/diskstats。</p><pre><code># -d -x表示显示所有磁盘I/O的指标$ iostat -d -x 1 Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util loop0            0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00 loop1            0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00 sda              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00 sdb              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00 </code></pre><p>这些指标中，你要注意：</p><ul><li>%util ，就是我们前面提到的磁盘 I/O 使用率；</li><li>r/s+ w/s ，就是 IOPS；</li><li>rkB/s+wkB/s ，就是吞吐量；</li><li>r_await+w_await ，就是响应时间。</li></ul><p>从这里你可以看到，iostat 提供了非常丰富的性能指标。第一列的 Device 表示磁盘设备的名字，其他各列指标，虽然数量较多，但是每个指标的含义都很重要。为了方便你理解，我把它们总结成了一个表格。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-70c7838144303f67.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>你可能注意到，从 iostat 并不能直接得到磁盘饱和度。事实上，饱和度通常也没有其他简单的观测方法，不过，你可以把观测到的，平均请求队列长度或者读写请求完成的等待时间，跟基准测试的结果（比如通过 fio）进行对比，综合评估磁盘的饱和情况。</p><p>上面提到的 iostat 只提供磁盘整体的 I/O 性能数据，缺点在于，并不能知道具体是哪些进程在进行磁盘读写。要观察进程的 I/O 情况，你还可以使用 pidstat 和 iotop 这两个工具。</p><pre><code>$ pidstat -d 1 13:39:51      UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s iodelay  Command 13:39:52      102       916      0.00      4.00      0.00       0  rsyslogd</code></pre><p>从 pidstat 的输出你能看到，它可以实时查看每个进程的 I/O 情况，包括下面这些内容。</p><ul><li>用户 ID（UID）和进程 ID（PID） 。</li><li>每秒读取的数据大小（kB_rd/s） ，单位是 KB。</li><li>每秒发出的写请求数据大小（kB_wr/s） ，单位是 KB。</li><li>每秒取消的写请求数据大小（kB_ccwr/s） ，单位是 KB。</li><li>块 I/O 延迟（iodelay），包括等待同步块 I/O 和换入块 I/O 结束的时间，单位是时钟周期。</li></ul><p>除了可以用 pidstat 实时查看，根据 I/O 大小对进程排序，也是性能分析中一个常用的方法。这一点，我推荐另一个工具， iotop。它是一个类似于 top 的工具，你可以按照 I/O 大小对进程排序，然后找到 I/O 较大的那些进程。</p><pre><code>$ iotopTotal DISK READ :       0.00 B/s | Total DISK WRITE :       7.85 K/s Actual DISK READ:       0.00 B/s | Actual DISK WRITE:       0.00 B/s   TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO&gt;    COMMAND 15055 be/3 root        0.00 B/s    7.85 K/s  0.00 %  0.00 % systemd-journald </code></pre><p>从这个输出，你可以看到，前两行分别表示，进程的磁盘读写大小总数和磁盘真实的读写大小总数。因为缓存、缓冲区、I/O 合并等因素的影响，它们可能并不相等。</p><p>剩下的部分，则是从各个角度来分别表示进程的 I/O 情况，包括线程 ID、I/O 优先级、每秒读磁盘的大小、每秒写磁盘的大小、换入和等待 I/O 的时钟百分比等。</p><p>接下来，我们在终端中运行下面的 lsof 命令，看看进程 18940 都打开了哪些文件：</p><pre><code>$ lsof -p 18940 COMMAND   PID USER   FD   TYPE DEVICE  SIZE/OFF    NODE NAME python  18940 root  cwd    DIR   0,50      4096 1549389 / python  18940 root  rtd    DIR   0,50      4096 1549389 / … python  18940 root    2u   CHR  136,0       0t0       3 /dev/pts/0 python  18940 root    3w   REG    8,1 117944320     303 /tmp/logtest.txt </code></pre><p>这里我给你介绍一个新工具， filetop。它是 bcc 软件包的一部分，基于 Linux 内核的 eBPF（extended Berkeley Packet Filters）机制，主要跟踪内核中文件的读写情况，并输出线程 ID（TID）、读写大小、读写类型以及文件名称。</p><pre><code># 切换到工具目录 $ cd /usr/share/bcc/tools # -C 选项表示输出新内容时不清空屏幕 $ ./filetop -C TID    COMM             READS  WRITES R_Kb    W_Kb    T FILE 514    python           0      1      0       2832    R 669.txt 514    python           0      1      0       2490    R 667.txt 514    python           0      1      0       2685    R 671.txt 514    python           0      1      0       2392    R 670.txt 514    python           0      1      0       2050    R 672.txt ...TID    COMM             READS  WRITES R_Kb    W_Kb    T FILE 514    python           2      0      5957    0       R 651.txt 514    python           2      0      5371    0       R 112.txt 514    python           2      0      4785    0       R 861.txt 514    python           2      0      4736    0       R 213.txt 514    python           2      0      4443    0       R 45.txt </code></pre><p>你会看到，filetop 输出了 8 列内容，分别是线程 ID、线程命令行、读写次数、读写的大小（单位 KB）、文件类型以及读写的文件名称。</p><p>接下来，在终端一中，运行下面的 opensnoop 命令：</p><pre><code>$ opensnoop 12280  python              6   0 /tmp/9046db9e-fe25-11e8-b13f-0242ac110002/650.txt 12280  python              6   0 /tmp/9046db9e-fe25-11e8-b13f-0242ac110002/651.txt 12280  python              6   0 /tmp/9046db9e-fe25-11e8-b13f-0242ac110002/652.txt </code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8c30aad41f63e73d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-022f0e298d7bb288.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d84ca1b04e088d3f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3559b52f9b291180.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="I-O-基准测试"><a href="#I-O-基准测试" class="headerlink" title="I/O 基准测试"></a>I/O 基准测试</h3><pre><code># Ubuntuapt-get install -y fio# CentOSyum install -y fio </code></pre><p>fio 的选项非常多， 我会通过几个常见场景的测试方法，介绍一些最常用的选项。这些常见场景包括随机读、随机写、顺序读以及顺序写等，你可以执行下面这些命令来测试：</p><pre><code># 随机读fio -name=randread -direct=1 -iodepth=64 -rw=randread -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb# 随机写fio -name=randwrite -direct=1 -iodepth=64 -rw=randwrite -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb# 顺序读fio -name=read -direct=1 -iodepth=64 -rw=read -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb# 顺序写fio -name=write -direct=1 -iodepth=64 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb </code></pre><p>在这其中，有几个参数需要你重点关注一下。</p><ul><li>direct，表示是否跳过系统缓存。上面示例中，我设置的 1 ，就表示跳过系统缓存。</li><li>iodepth，表示使用异步 I/O（asynchronous I/O，简称 AIO）时，同时发出的 I/O 请求上限。在上面的示例中，我设置的是 64。</li><li>rw，表示 I/O 模式。我的示例中， read/write 分别表示顺序读 / 写，而 randread/randwrite 则分别表示随机读 / 写。</li><li>ioengine，表示 I/O 引擎，它支持同步（sync）、异步（libaio）、内存映射（mmap）、网络（net）等各种 I/O 引擎。上面示例中，我设置的 libaio 表示使用异步 I/O。</li><li>bs，表示 I/O 的大小。示例中，我设置成了 4K（这也是默认值）。</li><li>filename，表示文件路径，当然，它可以是磁盘路径（测试磁盘性能），也可以是文件路径（测试文件系统性能）。示例中，我把它设置成了磁盘 /dev/sdb。不过注意，用磁盘路径测试写，会破坏这个磁盘中的文件系统，所以在使用前，你一定要事先做好数据备份。</li></ul><p>下面就是我使用 fio 测试顺序读的一个报告示例。</p><pre><code>read: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64fio-3.1Starting 1 processJobs: 1 (f=1): [R(1)][100.0%][r=16.7MiB/s,w=0KiB/s][r=4280,w=0 IOPS][eta 00m:00s]read: (groupid=0, jobs=1): err= 0: pid=17966: Sun Dec 30 08:31:48 2018   read: IOPS=4257, BW=16.6MiB/s (17.4MB/s)(1024MiB/61568msec)    slat (usec): min=2, max=2566, avg= 4.29, stdev=21.76    clat (usec): min=228, max=407360, avg=15024.30, stdev=20524.39     lat (usec): min=243, max=407363, avg=15029.12, stdev=20524.26    clat percentiles (usec):     |  1.00th=[   498],  5.00th=[  1020], 10.00th=[  1319], 20.00th=[  1713],     | 30.00th=[  1991], 40.00th=[  2212], 50.00th=[  2540], 60.00th=[  2933],     | 70.00th=[  5407], 80.00th=[ 44303], 90.00th=[ 45351], 95.00th=[ 45876],     | 99.00th=[ 46924], 99.50th=[ 46924], 99.90th=[ 48497], 99.95th=[ 49021],     | 99.99th=[404751]   bw (  KiB/s): min= 8208, max=18832, per=99.85%, avg=17005.35, stdev=998.94, samples=123   iops        : min= 2052, max= 4708, avg=4251.30, stdev=249.74, samples=123  lat (usec)   : 250=0.01%, 500=1.03%, 750=1.69%, 1000=2.07%  lat (msec)   : 2=25.64%, 4=37.58%, 10=2.08%, 20=0.02%, 50=29.86%  lat (msec)   : 100=0.01%, 500=0.02%  cpu          : usr=1.02%, sys=2.97%, ctx=33312, majf=0, minf=75  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%     issued rwt: total=262144,0,0, short=0,0,0, dropped=0,0,0     latency   : target=0, window=0, percentile=100.00%, depth=64Run status group 0 (all jobs):   READ: bw=16.6MiB/s (17.4MB/s), 16.6MiB/s-16.6MiB/s (17.4MB/s-17.4MB/s), io=1024MiB (1074MB), run=61568-61568msecDisk stats (read/write):  sdb: ios=261897/0, merge=0/0, ticks=3912108/0, in_queue=3474336, util=90.09% </code></pre><p>这个报告中，需要我们重点关注的是， slat、clat、lat ，以及 bw 和 iops 这几行。</p><p>先来看刚刚提到的前三个参数。事实上，slat、clat、lat 都是指 I/O 延迟（latency）。不同之处在于：</p><ul><li>slat ，是指从 I/O 提交到实际执行 I/O 的时长（Submission latency）；</li><li>clat ，是指从 I/O 提交到 I/O 完成的时长（Completion latency）；</li><li>而 lat ，指的是从 fio 创建 I/O 到 I/O 完成的总时长。</li></ul><p>这里需要注意的是，对同步 I/O 来说，由于 I/O 提交和 I/O 完成是一个动作，所以 slat 实际上就是 I/O 完成的时间，而 clat 是 0。而从示例可以看到，使用异步 I/O（libaio）时，lat 近似等于 slat + clat 之和。</p><p>最后的 iops ，其实就是每秒 I/O 的次数，上面示例中的平均 IOPS 为 4250。</p><p>通常情况下，应用程序的 I/O 都是读写并行的，而且每次的 I/O 大小也不一定相同。所以，刚刚说的这几种场景，并不能精确模拟应用程序的 I/O 模式。那怎么才能精确模拟应用程序的 I/O 模式呢？</p><p>幸运的是，fio 支持 I/O 的重放。借助前面提到过的 blktrace，再配合上 fio，就可以实现对应用程序 I/O 模式的基准测试。你需要先用 blktrace ，记录磁盘设备的 I/O 访问情况；然后使用 fio ，重放 blktrace 的记录。</p><p>比如你可以运行下面的命令来操作：</p><pre><code># 使用blktrace跟踪磁盘I/O，注意指定应用程序正在操作的磁盘$ blktrace /dev/sdb# 查看blktrace记录的结果# lssdb.blktrace.0  sdb.blktrace.1# 将结果转化为二进制文件$ blkparse sdb -d sdb.bin# 使用fio重放日志$ fio --name=replay --filename=/dev/sdb --direct=1 --read_iolog=sdb.bin </code></pre><h3 id="I-O-性能优化"><a href="#I-O-性能优化" class="headerlink" title="I/O 性能优化"></a>I/O 性能优化</h3><p><strong>应用程序优化</strong></p><p>应用程序处于整个 I/O 栈的最上端，它可以通过系统调用，来调整 I/O 模式（如顺序还是随机、同步还是异步）， 同时，它也是 I/O 数据的最终来源。在我看来，可以有这么几种方式来优化应用程序的 I/O 性能。</p><p>第一，可以用追加写代替随机写，减少寻址开销，加快 I/O 写的速度。</p><p>第二，可以借助缓存 I/O ，充分利用系统缓存，降低实际 I/O 的次数。</p><p>第三，可以在应用程序内部构建自己的缓存，或者用 Redis 这类外部缓存系统。这样，一方面，能在应用程序内部，控制缓存的数据和生命周期；另一方面，也能降低其他应用程序使用缓存对自身的影响。</p><p>比如，在前面的 MySQL 案例中，我们已经见识过，只是因为一个干扰应用清理了系统缓存，就会导致 MySQL 查询有数百倍的性能差距（0.1s vs 15s）。</p><p>第四，在需要频繁读写同一块磁盘空间时，可以用 mmap 代替 read/write，减少内存的拷贝次数。</p><p>第五，在需要同步写的场景中，尽量将写请求合并，而不是让每个请求都同步写入磁盘，即可以用 fsync() 取代 O_SYNC。</p><p>第六，在多个应用程序共享相同磁盘时，为了保证 I/O 不被某个应用完全占用，推荐你使用 cgroups 的 I/O 子系统，来限制进程 / 进程组的 IOPS 以及吞吐量。</p><p>最后，在使用 CFQ 调度器时，可以用 ionice 来调整进程的 I/O 调度优先级，特别是提高核心应用的 I/O 优先级。ionice 支持三个优先级类：Idle、Best-effort 和 Realtime。其中， Best-effort 和 Realtime 还分别支持 0-7 的级别，数值越小，则表示优先级别越高。</p><p><strong>文件系统优化</strong></p><p>应用程序访问普通文件时，实际是由文件系统间接负责，文件在磁盘中的读写。所以，跟文件系统中相关的也有很多优化 I/O 性能的方式。</p><p>第一，你可以根据实际负载场景的不同，选择最适合的文件系统。比如 Ubuntu 默认使用 ext4 文件系统，而 CentOS 7 默认使用 xfs 文件系统。</p><p>相比于 ext4 ，xfs 支持更大的磁盘分区和更大的文件数量，如 xfs 支持大于 16TB 的磁盘。但是 xfs 文件系统的缺点在于无法收缩，而 ext4 则可以。</p><p>第二，在选好文件系统后，还可以进一步优化文件系统的配置选项，包括文件系统的特性（如 ext_attr、dir_index）、日志模式（如 journal、ordered、writeback）、挂载选项（如 noatime）等等。</p><p>比如， 使用 tune2fs 这个工具，可以调整文件系统的特性（tune2fs 也常用来查看文件系统超级块的内容）。 而通过 /etc/fstab ，或者 mount 命令行参数，我们可以调整文件系统的日志模式和挂载选项等。</p><p>第三，可以优化文件系统的缓存。</p><p>比如，你可以优化 pdflush 脏页的刷新频率（比如设置 <code>dirty_expire_centisecs</code> 和 <code>dirty_writeback_centisecs</code>）以及脏页的限额（比如调整 <code>dirty_background_ratio</code> 和 dirty_ratio 等）。</p><p>再如，你还可以优化内核回收目录项缓存和索引节点缓存的倾向，即调整 <code>vfs_cache_pressure</code>（<code>/proc/sys/vm/vfs_cache_pressure</code>，默认值 100），数值越大，就表示越容易回收。</p><p>最后，在不需要持久化时，你还可以用内存文件系统 tmpfs，以获得更好的 I/O 性能 。tmpfs 把数据直接保存在内存中，而不是磁盘中。比如 /dev/shm/ ，就是大多数 Linux 默认配置的一个内存文件系统，它的大小默认为总内存的一半。</p><p><strong>磁盘优化</strong></p><p>数据的持久化存储，最终还是要落到具体的物理磁盘中，同时，磁盘也是整个 I/O 栈的最底层。从磁盘角度出发，自然也有很多有效的性能优化方法。</p><p>第一，最简单有效的优化方法，就是换用性能更好的磁盘，比如用 SSD 替代 HDD。</p><p>第二，我们可以使用 RAID ，把多块磁盘组合成一个逻辑磁盘，构成冗余独立磁盘阵列。这样做既可以提高数据的可靠性，又可以提升数据的访问性能。</p><p>第三，针对磁盘和应用程序 I/O 模式的特征，我们可以选择最适合的 I/O 调度算法。比方说，SSD 和虚拟机中的磁盘，通常用的是 noop 调度算法。而数据库应用，我更推荐使用 deadline 算法。</p><p>第四，我们可以对应用程序的数据，进行磁盘级别的隔离。比如，我们可以为日志、数据库等 I/O 压力比较重的应用，配置单独的磁盘。</p><p>第五，在顺序读比较多的场景中，我们可以增大磁盘的预读数据，比如，你可以通过下面两种方法，调整 /dev/sdb 的预读大小。</p><ul><li>调整内核选项 /sys/block/sdb/queue/read_ahead_kb，默认大小是 128 KB，单位为 KB。</li><li>使用 blockdev 工具设置，比如 blockdev –setra 8192 /dev/sdb，注意这里的单位是 512B（0.5KB），所以它的数值总是 read_ahead_kb 的两倍。</li></ul><p>第六，我们可以优化内核块设备 I/O 的选项。比如，可以调整磁盘队列的长度 /sys/block/sdb/queue/nr_requests，适当增大队列长度，可以提升磁盘的吞吐量（当然也会导致 I/O 延迟增大）。</p><p>最后，要注意，磁盘本身出现硬件错误，也会导致 I/O 性能急剧下降，所以发现磁盘性能急剧下降时，你还需要确认，磁盘本身是不是出现了硬件错误。</p><h2 id="网络性能篇"><a href="#网络性能篇" class="headerlink" title="网络性能篇"></a>网络性能篇</h2><h3 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-92069a9952cce335.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f175f974196dbd22.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="Linux-网络收发流程"><a href="#Linux-网络收发流程" class="headerlink" title="Linux 网络收发流程"></a>Linux 网络收发流程</h3><p><strong>网络包的接收流程</strong></p><p>当一个网络帧到达网卡后，网卡会通过 DMA 方式，把这个网络包放到收包队列中；然后通过硬中断，告诉中断处理程序已经收到了网络包。</p><p>接着，网卡中断处理程序会为网络帧分配内核数据结构（sk_buff），并将其拷贝到 sk_buff 缓冲区中；然后再通过软中断，通知内核收到了新的网络帧。</p><p>接下来，内核协议栈从缓冲区中取出网络帧，并通过网络协议栈，从下到上逐层处理这个网络帧。比如，</p><ul><li>在链路层检查报文的合法性，找出上层协议的类型（比如 IPv4 还是 IPv6），再去掉帧头、帧尾，然后交给网络层。</li><li>网络层取出 IP 头，判断网络包下一步的走向，比如是交给上层处理还是转发。当网络层确认这个包是要发送到本机后，就会取出上层协议的类型（比如 TCP 还是 UDP），去掉 IP 头，再交给传输层处理。</li><li>传输层取出 TCP 头或者 UDP 头后，根据 &lt; 源 IP、源端口、目的 IP、目的端口 &gt; 四元组作为标识，找出对应的 Socket，并把数据拷贝到 Socket 的接收缓存中。</li></ul><p>最后，应用程序就可以使用 Socket 接口，读取到新接收到的数据了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-65ce01477b3de01f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>网络包的发送流程</strong></p><p>了解网络包的接收流程后，就很容易理解网络包的发送流程。网络包的发送流程就是上图的右半部分，很容易发现，网络包的发送方向，正好跟接收方向相反。</p><p>首先，应用程序调用 Socket API（比如 sendmsg）发送网络包。</p><p>由于这是一个系统调用，所以会陷入到内核态的套接字层中。套接字层会把数据包放到 Socket 发送缓冲区中。</p><p>接下来，网络协议栈从 Socket 发送缓冲区中，取出数据包；再按照 TCP/IP 栈，从上到下逐层处理。比如，传输层和网络层，分别为其增加 TCP 头和 IP 头，执行路由查找确认下一跳的 IP，并按照 MTU 大小进行分片。</p><p>分片后的网络包，再送到网络接口层，进行物理地址寻址，以找到下一跳的 MAC 地址。然后添加帧头和帧尾，放到发包队列中。这一切完成后，会有软中断通知驱动程序：发包队列中有新的网络帧需要发送。</p><p>最后，驱动程序通过 DMA ，从发包队列中读出网络帧，并通过物理网卡把它发送出去。</p><h3 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a>性能指标</h3><p>实际上，我们通常用带宽、吞吐量、延时、PPS（Packet Per Second）等指标衡量网络的性能。</p><ul><li>带宽，表示链路的最大传输速率，单位通常为 b/s （比特 / 秒）。</li><li>吞吐量，表示单位时间内成功传输的数据量，单位通常为 b/s（比特 / 秒）或者 B/s（字节 / 秒）。吞吐量受带宽限制，而吞吐量 / 带宽，也就是该网络的使用率。</li><li>延时，表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟。在不同场景中，这一指标可能会有不同含义。比如，它可以表示，建立连接需要的时间（比如 TCP 握手延时），或一个数据包往返所需的时间（比如 RTT）。</li><li>PPS，是 Packet Per Second（包 / 秒）的缩写，表示以网络包为单位的传输速率。PPS 通常用来评估网络的转发能力，比如硬件交换机，通常可以达到线性转发（即 PPS 可以达到或者接近理论最大值）。而基于 Linux 服务器的转发，则容易受网络包大小的影响。</li></ul><p>除了这些指标，网络的可用性（网络能否正常通信）、并发连接数（TCP 连接数量）、丢包率（丢包百分比）、重传率（重新传输的网络包比例）等也是常用的性能指标。</p><p><strong>套接字信息</strong></p><pre><code># head -n 3 表示只显示前面3行# -l 表示只显示监听套接字# -n 表示显示数字地址和端口(而不是名字)# -p 表示显示进程信息$ netstat -nlp | head -n 3Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program nametcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      840/systemd-resolve# -l 表示只显示监听套接字# -t 表示只显示 TCP 套接字# -n 表示显示数字地址和端口(而不是名字)# -p 表示显示进程信息$ ss -ltnp | head -n 3State    Recv-Q    Send-Q        Local Address:Port        Peer Address:PortLISTEN   0         128           127.0.0.53%lo:53               0.0.0.0:*        users:(("systemd-resolve",pid=840,fd=13))LISTEN   0         128                 0.0.0.0:22               0.0.0.0:*        users:(("sshd",pid=1459,fd=3))</code></pre><p>netstat 和 ss 的输出也是类似的，都展示了套接字的状态、接收队列、发送队列、本地地址、远端地址、进程 PID 和进程名称等。</p><p>其中，接收队列（Recv-Q）和发送队列（Send-Q）需要你特别关注，它们通常应该是 0。当你发现它们不是 0 时，说明有网络包的堆积发生。当然还要注意，在不同套接字状态下，它们的含义不同。</p><p>当套接字处于连接状态（Established）时，</p><ul><li>Recv-Q 表示套接字缓冲还没有被应用程序取走的字节数（即接收队列长度）。</li><li>而 Send-Q 表示还没有被远端主机确认的字节数（即发送队列长度）。</li></ul><p>当套接字处于监听状态（Listening）时，</p><ul><li>Recv-Q 表示全连接队列的长度。</li><li>而 Send-Q 表示全连接队列的最大长度。</li></ul><p>所谓全连接，是指服务器收到了客户端的 ACK，完成了 TCP 三次握手，然后就会把这个连接挪到全连接队列中。这些全连接中的套接字，还需要被 accept() 系统调用取走，服务器才可以开始真正处理客户端的请求。</p><p>与全连接队列相对应的，还有一个半连接队列。所谓半连接是指还没有完成 TCP 三次握手的连接，连接只进行了一半。服务器收到了客户端的 SYN 包后，就会把这个连接放到半连接队列中，然后再向客户端发送 SYN+ACK 包。</p><p><strong>协议栈统计信息</strong></p><pre><code>$ netstat -s...Tcp:    3244906 active connection openings    23143 passive connection openings    115732 failed connection attempts    2964 connection resets received    1 connections established    13025010 segments received    17606946 segments sent out    44438 segments retransmitted    42 bad segments received    5315 resets sent    InCsumErrors: 42...$ ss -sTotal: 186 (kernel 1446)TCP:   4 (estab 1, closed 0, orphaned 0, synrecv 0, timewait 0/0), ports 0Transport Total     IP        IPv6*    1446      -         -RAW    2         1         1UDP    2         2         0TCP    4         3         1...</code></pre><p>这些协议栈的统计信息都很直观。ss 只显示已经连接、关闭、孤儿套接字等简要统计，而 netstat 则提供的是更详细的网络协议栈信息。</p><p>比如，上面 netstat 的输出示例，就展示了 TCP 协议的主动连接、被动连接、失败重试、发送和接收的分段数量等各种信息。</p><p><strong>网络吞吐和 PPS</strong></p><p>给 sar 增加 -n 参数就可以查看网络的统计信息，比如网络接口（DEV）、网络接口错误（EDEV）、TCP、UDP、ICMP<br>等等。执行下面的命令，你就可以得到网络接口统计信息：</p><pre><code># 数字1表示每隔1秒输出一组数据$ sar -n DEV 1Linux 4.15.0-1035-azure (ubuntu)   01/06/19   _x86_64_  (2 CPU)13:21:40        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil13:21:41         eth0     18.00     20.00      5.79      4.25      0.00      0.00      0.00      0.0013:21:41      docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.0013:21:41           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</code></pre><p>这儿输出的指标比较多，我来简单解释下它们的含义。</p><ul><li>rxpck/s 和 txpck/s 分别是接收和发送的 PPS，单位为包 / 秒。</li><li>rxkB/s 和 txkB/s 分别是接收和发送的吞吐量，单位是 KB/ 秒。</li><li>rxcmp/s 和 txcmp/s 分别是接收和发送的压缩数据包数，单位是包 / 秒。</li><li>%ifutil 是网络接口的使用率，即半双工模式下为 (rxkB/s+txkB/s)/Bandwidth，而全双工模式下为 max(rxkB/s, txkB/s)/Bandwidth。</li></ul><p>其中，Bandwidth 可以用 ethtool 来查询，它的单位通常是 Gb/s 或者 Mb/s，不过注意这里小写字母 b ，表示比特而不是字节。我们通常提到的千兆网卡、万兆网卡等，单位也都是比特。如下你可以看到，我的 eth0 网卡就是一个千兆网卡：</p><pre><code>$ ethtool eth0 | grep Speed  Speed: 1000Mb/s    </code></pre><h3 id="I-O-模型优化"><a href="#I-O-模型优化" class="headerlink" title="I/O 模型优化"></a>I/O 模型优化</h3><p>异步、非阻塞 I/O 的解决思路，你应该听说过，其实就是我们在网络编程中经常用到的 I/O 多路复用（I/O Multiplexing）。I/O 多路复用是什么意思呢？</p><p>别急，详细了解前，我先来讲两种 I/O 事件通知的方式：水平触发和边缘触发，它们常用在套接字接口的文件描述符中。</p><ul><li>水平触发：只要文件描述符可以非阻塞地执行 I/O ，就会触发通知。也就是说，应用程序可以随时检查文件描述符的状态，然后再根据状态，进行 I/O 操作。</li><li>边缘触发：只有在文件描述符的状态发生改变（也就是 I/O 请求达到）时，才发送一次通知。这时候，应用程序需要尽可能多地执行 I/O，直到无法继续读写，才可以停止。如果 I/O 没执行完，或者因为某种原因没来得及处理，那么这次通知也就丢失了。</li></ul><p>接下来，我们再回过头来看 I/O 多路复用的方法。这里其实有很多实现方法，我带你来逐个分析一下。</p><p><strong>第一种，使用非阻塞 I/O 和水平触发通知，比如使用 select 或者 poll。</strong></p><p>根据刚才水平触发的原理，select 和 poll 需要从文件描述符列表中，找出哪些可以执行 I/O ，然后进行真正的网络 I/O 读写。由于 I/O 是非阻塞的，一个线程中就可以同时监控一批套接字的文件描述符，这样就达到了单线程处理多请求的目的。</p><p>所以，这种方式的最大优点，是对应用程序比较友好，它的 API 非常简单。</p><p>但是，应用软件使用 select 和 poll 时，需要对这些文件描述符列表进行轮询，这样，请求数多的时候就会比较耗时。并且，select 和 poll 还有一些其他的限制。</p><p>select 使用固定长度的位相量，表示文件描述符的集合，因此会有最大描述符数量的限制。比如，在 32 位系统中，默认限制是 1024。并且，在 select 内部，检查套接字状态是用轮询的方法，处理耗时跟描述符数量是 O(N) 的关系。</p><p>而 poll 改进了 select 的表示方法，换成了一个没有固定长度的数组，这样就没有了最大描述符数量的限制（当然还会受到系统文件描述符限制）。但应用程序在使用 poll 时，同样需要对文件描述符列表进行轮询，这样，处理耗时跟描述符数量就是 O(N) 的关系。</p><p>除此之外，应用程序每次调用 select 和 poll 时，还需要把文件描述符的集合，从用户空间传入内核空间，由内核修改后，再传出到用户空间中。这一来一回的内核空间与用户空间切换，也增加了处理成本。</p><p>有没有什么更好的方式来处理呢？答案自然是肯定的。</p><p><strong>第二种，使用非阻塞 I/O 和边缘触发通知，比如 epoll。</strong></p><p>既然 select 和 poll 有那么多的问题，就需要继续对其进行优化，而 epoll 就很好地解决了这些问题。</p><ul><li>epoll 使用红黑树，在内核中管理文件描述符的集合，这样，就不需要应用程序在每次操作时都传入、传出这个集合。</li><li>epoll 使用事件驱动的机制，只关注有 I/O 事件发生的文件描述符，不需要轮询扫描整个集合。</li></ul><p>不过要注意，epoll 是在 Linux 2.6 中才新增的功能（2.4 虽然也有，但功能不完善）。由于边缘触发只在文件描述符可读或可写事件发生时才通知，那么应用程序就需要尽可能多地执行 I/O，并要处理更多的异常事件。</p><p><strong>第三种，使用异步 I/O（Asynchronous I/O，简称为 AIO）</strong>。在前面文件系统原理的内容中，我曾介绍过异步 I/O 与同步 I/O 的区别。异步 I/O 允许应用程序同时发起很多 I/O 操作，而不用等待这些操作完成。而在 I/O 完成后，系统会用事件通知（比如信号或者回调函数）的方式，告诉应用程序。这时，应用程序才会去查询 I/O 操作的结果。</p><p>异步 I/O 也是到了 Linux 2.6 才支持的功能，并<strong>且在很长时间里都处于不完善的状态</strong>，比如 glibc 提供的异步 I/O 库，就一直被社区诟病。同时，由于异步 I/O 跟我们的直观逻辑不太一样，想要使用的话，一定要小心设计，其使用难度比较高。</p><h3 id="工作模型优化"><a href="#工作模型优化" class="headerlink" title="工作模型优化"></a>工作模型优化</h3><p>第一种，主进程 + 多个 worker 子进程，这也是最常用的一种模型。这种方法的一个通用工作模式就是：</p><ul><li>主进程执行 bind() + listen() 后，创建多个子进程；</li><li>然后，在每个子进程中，都通过 accept() 或 epoll_wait() ，来处理相同的套接字。</li></ul><p>比如，最常用的反向代理服务器 Nginx 就是这么工作的。它也是由主进程和多个 worker 进程组成。主进程主要用来初始化套接字，并管理子进程的生命周期；而 worker 进程，则负责实际的请求处理。我画了一张图来表示这个关系。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-af10632dff0bbfcd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li>其中，accept() 的惊群问题，已经在 Linux 2.6 中解决了；</li><li>而 epoll 的问题，到了 Linux 4.5 ，才通过 EPOLLEXCLUSIVE 解决。</li></ul><p>为了避免惊群问题， Nginx 在每个 worker 进程中，都增加一个了全局锁（accept_mutex）。这些 worker 进程需要首先竞争到锁，只有竞争到锁的进程，才会加入到 epoll 中，这样就确保只有一个 worker 子进程被唤醒。</p><p>这里最主要的一个原因就是，这些 worker 进程，实际上并不需要经常创建和销毁，而是在没任务时休眠，有任务时唤醒。只有在 worker 由于某些异常退出时，主进程才需要创建新的进程来代替它。</p><p>当然，你也可以用线程代替进程：主线程负责套接字初始化和子线程状态的管理，而子线程则负责实际的请求处理。由于线程的调度和切换成本比较低，实际上你可以进一步把 epoll_wait() 都放到主线程中，保证每次事件都只唤醒主线程，而子线程只需要负责后续的请求处理。</p><p>第二种，监听到相同端口的多进程模型。在这种方式下，所有的进程都监听相同的接口，并且开启 SO_REUSEPORT 选项，由内核负责将请求负载均衡到这些监听进程中去。这一过程如下图所示。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-df5773d0fcb39be5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>由于内核确保了只有一个进程被唤醒，就不会出现惊群问题了。比如，Nginx 在 1.9.1 中就已经支持了这种模式。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4ae6d85735b55aaa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>C10M</strong></p><p>实际上，在 C1000K 问题中，各种软件、硬件的优化很可能都已经做到头了。特别是当升级完硬件（比如足够多的内存、带宽足够大的网卡、更多的网络功能卸载等）后，你可能会发现，无论你怎么优化应用程序和内核中的各种网络参数，想实现 1000 万请求的并发，都是极其困难的。</p><p>究其根本，还是 Linux 内核协议栈做了太多太繁重的工作。从网卡中断带来的硬中断处理程序开始，到软中断中的各层网络协议处理，最后再到应用程序，这个路径实在是太长了，就会导致网络包的处理优化，到了一定程度后，就无法更进一步了。</p><p>要解决这个问题，最重要就是跳过内核协议栈的冗长路径，把网络包直接送到要处理的应用程序那里去。这里有两种常见的机制，DPDK 和 XDP。</p><p>第一种机制，DPDK，是用户态网络的标准。它跳过内核协议栈，直接由用户态进程通过轮询的方式，来处理网络接收。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-09cee784e0ce740d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>说起轮询，你肯定会下意识认为它是低效的象征，但是进一步反问下自己，它的低效主要体现在哪里呢？是查询时间明显多于实际工作时间的情况下吧！那么，换个角度来想，如果每时每刻都有新的网络包需要处理，轮询的优势就很明显了。比如：</p><ul><li>在 PPS 非常高的场景中，查询时间比实际工作时间少了很多，绝大部分时间都在处理网络包；</li><li>而跳过内核协议栈后，就省去了繁杂的硬中断、软中断再到 Linux 网络协议栈逐层处理的过程，应用程序可以针对应用的实际场景，有针对性地优化网络包的处理逻辑，而不需要关注所有的细节。</li></ul><p>此外，DPDK 还通过大页、CPU 绑定、内存对齐、流水线并发等多种机制，优化网络包的处理效率。</p><p>第二种机制，XDP（eXpress Data Path），则是 Linux 内核提供的一种高性能网络数据路径。它允许网络包，在进入内核协议栈之前，就进行处理，也可以带来更高的性能。XDP 底层跟我们之前用到的 bcc-tools 一样，都是基于 Linux 内核的 eBPF 机制实现的。</p><p>XDP 的原理如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1af52d57e11ec45c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>你可以看到，XDP 对内核的要求比较高，需要的是 Linux 4.8 以上版本，并且它也不提供缓存队列。基于 XDP 的应用程序通常是专用的网络应用，常见的有 IDS（入侵检测系统）、DDoS 防御、 cilium 容器网络插件等。</p><h3 id="网络基准测试"><a href="#网络基准测试" class="headerlink" title="网络基准测试"></a>网络基准测试</h3><p>Linux 内核自带的高性能网络测试工具 pktgen。pktgen 支持丰富的自定义选项，方便你根据实际需要构造所需网络包，从而更准确地测试出目标服务器的性能。</p><p>不过，在 Linux 系统中，你并不能直接找到 pktgen 命令。因为 pktgen 作为一个内核线程来运行，需要你加载 pktgen 内核模块后，再通过 /proc 文件系统来交互。下面就是 pktgen 启动的两个内核线程和 /proc 文件系统的交互文件：</p><pre><code>$ modprobe pktgen$ ps -ef | grep pktgen | grep -v greproot     26384     2  0 06:17 ?        00:00:00 [kpktgend_0]root     26385     2  0 06:17 ?        00:00:00 [kpktgend_1]$ ls /proc/net/pktgen/kpktgend_0  kpktgend_1  pgctrl</code></pre><p>pktgen 在每个 CPU 上启动一个内核线程，并可以通过 /proc/net/pktgen 下面的同名文件，跟这些线程交互；而 pgctrl 则主要用来控制这次测试的开启和停止。</p><blockquote><p>如果 modprobe 命令执行失败，说明你的内核没有配置 CONFIG_NET_PKTGEN 选项。这就需要你配置 pktgen 内核模块（即 CONFIG_NET_PKTGEN=m）后，重新编译内核，才可以使用。</p></blockquote><p>接下来，就是一个发包测试的示例。</p><pre><code># 定义一个工具函数，方便后面配置各种测试选项function pgset() {    local result    echo $1 &gt; $PGDEV    result=`cat $PGDEV | fgrep "Result: OK:"`    if [ "$result" = "" ]; then         cat $PGDEV | fgrep Result:    fi}# 为0号线程绑定eth0网卡PGDEV=/proc/net/pktgen/kpktgend_0pgset "rem_device_all"   # 清空网卡绑定pgset "add_device eth0"  # 添加eth0网卡# 配置eth0网卡的测试选项PGDEV=/proc/net/pktgen/eth0pgset "count 1000000"    # 总发包数量pgset "delay 5000"       # 不同包之间的发送延迟(单位纳秒)pgset "clone_skb 0"      # SKB包复制pgset "pkt_size 64"      # 网络包大小pgset "dst 192.168.0.30" # 目的IPpgset "dst_mac 11:11:11:11:11:11"  # 目的MAC# 启动测试PGDEV=/proc/net/pktgen/pgctrlpgset "start"</code></pre><p>稍等一会儿，测试完成后，结果可以从 /proc 文件系统中获取。通过下面代码段中的内容，我们可以查看刚才的测试报告：</p><pre><code>$ cat /proc/net/pktgen/eth0Params: count 1000000  min_pkt_size: 64  max_pkt_size: 64     frags: 0  delay: 0  clone_skb: 0  ifname: eth0     flows: 0 flowlen: 0...Current:     pkts-sofar: 1000000  errors: 0     started: 1534853256071us  stopped: 1534861576098us idle: 70673us...Result: OK: 8320027(c8249354+d70673) usec, 1000000 (64byte,0frags)  120191pps 61Mb/sec (61537792bps) errors: 0</code></pre><p>你可以看到，测试报告主要分为三个部分：</p><ul><li>第一部分的 Params 是测试选项；</li><li>第二部分的 Current 是测试进度，其中， packts so far（pkts-sofar）表示已经发送了 100 万个包，也就表明测试已完成。</li><li>第三部分的 Result 是测试结果，包含测试所用时间、网络包数量和分片、PPS、吞吐量以及错误数。</li></ul><p>根据上面的结果，我们发现，PPS 为 12 万，吞吐量为 61 Mb/s，没有发生错误。那么，12 万的 PPS 好不好呢？</p><p>作为对比，你可以计算一下千兆交换机的 PPS。交换机可以达到线速（满负载时，无差错转发），它的 PPS 就是 1000Mbit 除以以太网帧的大小，即 1000Mbps/((64+20)*8bit) = 1.5 Mpps（其中，20B 为以太网帧前导和帧间距的大小）。</p><p>你看，即使是千兆交换机的 PPS，也可以达到 150 万 PPS，比我们测试得到的 12 万大多了。所以，看到这个数值你并不用担心，现在的多核服务器和万兆网卡已经很普遍了，稍做优化就可以达到数百万的 PPS。而且，如果你用了上节课讲到的 DPDK 或 XDP ，还能达到千万数量级。</p><p><strong>TCP/UDP 性能</strong></p><p>iperf 和 netperf 都是最常用的网络性能测试工具，测试 TCP 和 UDP 的吞吐量。它们都以客户端和服务器通信的方式，测试一段时间内的平均吞吐量。</p><p>接下来，我们就以 iperf 为例，看一下 TCP 性能的测试方法。目前，iperf 的最新版本为 iperf3，你可以运行下面的命令来安装：</p><pre><code># Ubuntuapt-get install iperf3# CentOSyum install iperf3</code></pre><p>然后，在目标机器上启动 iperf 服务端：</p><pre><code># -s表示启动服务端，-i表示汇报间隔，-p表示监听端口$ iperf3 -s -i 1 -p 10000</code></pre><p>接着，在另一台机器上运行 iperf 客户端，运行测试：</p><pre><code># -c表示启动客户端，192.168.0.30为目标服务器的IP# -b表示目标带宽(单位是bits/s)# -t表示测试时间# -P表示并发数，-p表示目标服务器监听端口$ iperf3 -c 192.168.0.30 -b 1G -t 15 -P 2 -p 10000</code></pre><p>稍等一会儿（15 秒）测试结束后，回到目标服务器，查看 iperf 的报告：</p><pre><code>[ ID] Interval           Transfer     Bandwidth...[SUM]   0.00-15.04  sec  0.00 Bytes  0.00 bits/sec                  sender[SUM]   0.00-15.04  sec  1.51 GBytes   860 Mbits/sec                  receiver</code></pre><p>最后的 SUM 行就是测试的汇总结果，包括测试时间、数据传输量以及带宽等。按照发送和接收，这一部分又分为了 sender 和 receiver 两行。</p><p>从测试结果你可以看到，这台机器 TCP 接收的带宽（吞吐量）为 860 Mb/s， 跟目标的 1Gb/s 相比，还是有些差距的。</p><p><strong>DNS解析过程</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-61db4e9e33da489b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="tcpdump"><a href="#tcpdump" class="headerlink" title="tcpdump"></a>tcpdump</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c0ad2cbdfefeeb5e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f8eb83dfcd68d2f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="网络性能排查工具"><a href="#网络性能排查工具" class="headerlink" title="网络性能排查工具"></a>网络性能排查工具</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8794f84e4ed7b7f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e68704d3dbd08601.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>套接字</strong></p><p>套接字可以屏蔽掉 Linux 内核中不同协议的差异，为应用程序提供统一的访问接口。每个套接字，都有一个读写缓冲区。</p><ul><li>读缓冲区，缓存了远端发过来的数据。如果读缓冲区已满，就不能再接收新的数据。</li><li>写缓冲区，缓存了要发出去的数据。如果写缓冲区已满，应用程序的写操作就会被阻塞。</li></ul><p>所以，为了提高网络的吞吐量，你通常需要调整这些缓冲区的大小。比如：</p><ul><li>增大每个套接字的缓冲区大小 net.core.optmem_max；</li><li>增大套接字接收缓冲区大小 net.core.rmem_max 和发送缓冲区大小 net.core.wmem_max；</li><li>增大 TCP 接收缓冲区大小 net.ipv4.tcp_rmem 和发送缓冲区大小 net.ipv4.tcp_wmem。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4763ab75c894a0a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li>tcp_rmem 和 tcp_wmem 的三个数值分别是 min，default，max，系统会根据这些设置，自动调整 TCP 接收 / 发送缓冲区的大小。</li><li>udp_mem 的三个数值分别是 min，pressure，max，系统会根据这些设置，自动调整 UDP 发送缓冲区的大小。</li></ul><p>当然，表格中的数值只提供参考价值，具体应该设置多少，还需要你根据实际的网络状况来确定。比如，发送缓冲区大小，理想数值是吞吐量 * 延迟，这样才可以达到最大网络利用率。</p><p>除此之外，套接字接口还提供了一些配置选项，用来修改网络连接的行为：</p><ul><li>为 TCP 连接设置 TCP_NODELAY 后，就可以禁用 Nagle 算法；</li><li>为 TCP 连接开启 TCP_CORK 后，可以让小包聚合成大包后再发送（注意会阻塞小包的发送）；</li><li>使用 SO_SNDBUF 和 SO_RCVBUF ，可以分别调整套接字发送缓冲区和接收缓冲区的大小。</li></ul><h3 id="网络性能优化"><a href="#网络性能优化" class="headerlink" title="网络性能优化"></a>网络性能优化</h3><p><strong>传输层</strong></p><p>第一类，在请求数比较大的场景下，你可能会看到大量处于 TIME_WAIT 状态的连接，它们会占用大量内存和端口资源。这时，我们可以优化与 TIME_WAIT 状态相关的内核选项，比如采取下面几种措施。</p><ul><li>增大处于 TIME_WAIT 状态的连接数量 net.ipv4.tcp_max_tw_buckets ，并增大连接跟踪表的大小 net.netfilter.nf_conntrack_max。</li><li>减小 net.ipv4.tcp_fin_timeout 和 net.netfilter.nf_conntrack_tcp_timeout_time_wait ，让系统尽快释放它们所占用的资源。</li><li>开启端口复用 net.ipv4.tcp_tw_reuse。这样，被 TIME_WAIT 状态占用的端口，还能用到新建的连接中。</li><li>增大本地端口的范围 net.ipv4.ip_local_port_range 。这样就可以支持更多连接，提高整体的并发能力。</li><li>增加最大文件描述符的数量。你可以使用 fs.nr_open 和 fs.file-max ，分别增大进程和系统的最大文件描述符数；或在应用程序的 systemd 配置文件中，配置 LimitNOFILE ，设置应用程序的最大文件描述符数。</li></ul><p>第二类，为了缓解 SYN FLOOD 等，利用 TCP 协议特点进行攻击而引发的性能问题，你可以考虑优化与 SYN 状态相关的内核选项，比如采取下面几种措施。</p><ul><li>增大 TCP 半连接的最大数量 net.ipv4.tcp_max_syn_backlog ，或者开启 TCP SYN Cookies net.ipv4.tcp_syncookies ，来绕开半连接数量限制的问题（注意，这两个选项不可同时使用）。</li><li>减少 SYN_RECV 状态的连接重传 SYN+ACK 包的次数 net.ipv4.tcp_synack_retries。</li></ul><p>第三类，在长连接的场景中，通常使用 Keepalive 来检测 TCP 连接的状态，以便对端连接断开后，可以自动回收。但是，系统默认的 Keepalive 探测间隔和重试次数，一般都无法满足应用程序的性能要求。所以，这时候你需要优化与 Keepalive 相关的内核选项，比如：</p><ul><li>缩短最后一次数据包到 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_time；</li><li>缩短发送 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_intvl；</li><li>减少 Keepalive 探测失败后，一直到通知应用程序前的重试次数 net.ipv4.tcp_keepalive_probes。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8ef6101d0feed93c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>UDP 提供了面向数据报的网络协议，它不需要网络连接，也不提供可靠性保障。所以，UDP 优化，相对于 TCP 来说，要简单得多。这里我也总结了常见的几种优化方案。</p><ul><li>跟上篇套接字部分提到的一样，增大套接字缓冲区大小以及 UDP 缓冲区范围；</li><li>跟前面 TCP 部分提到的一样，增大本地端口号的范围；</li><li>根据 MTU 大小，调整 UDP 数据包的大小，减少或者避免分片的发生。</li></ul><p><strong>网络层</strong></p><p>网络层，负责网络包的封装、寻址和路由，包括 IP、ICMP 等常见协议。在网络层，最主要的优化，其实就是对路由、 IP 分片以及 ICMP 等进行调优。</p><p>第一种，从路由和转发的角度出发，你可以调整下面的内核选项。</p><ul><li>在需要转发的服务器中，比如用作 NAT 网关的服务器或者使用 Docker 容器时，开启 IP 转发，即设置 net.ipv4.ip_forward = 1。</li><li>调整数据包的生存周期 TTL，比如设置 net.ipv4.ip_default_ttl = 64。注意，增大该值会降低系统性能。</li><li>开启数据包的反向地址校验，比如设置 net.ipv4.conf.eth0.rp_filter = 1。这样可以防止 IP 欺骗，并减少伪造 IP 带来的 DDoS 问题。</li></ul><p>第二种，从分片的角度出发，最主要的是调整 MTU（Maximum Transmission Unit）的大小。</p><p>通常，MTU 的大小应该根据以太网的标准来设置。以太网标准规定，一个网络帧最大为 1518B，那么去掉以太网头部的 18B 后，剩余的 1500 就是以太网 MTU 的大小。</p><p>在使用 VXLAN、GRE 等叠加网络技术时，要注意，网络叠加会使原来的网络包变大，导致 MTU 也需要调整。</p><p>比如，就以 VXLAN 为例，它在原来报文的基础上，增加了 14B 的以太网头部、 8B 的 VXLAN 头部、8B 的 UDP 头部以及 20B 的 IP 头部。换句话说，每个包比原来增大了 50B。</p><p>所以，我们就需要把交换机、路由器等的 MTU，增大到 1550， 或者把 VXLAN 封包前（比如虚拟化环境中的虚拟网卡）的 MTU 减小为 1450。</p><p>另外，现在很多网络设备都支持巨帧，如果是这种环境，你还可以把 MTU 调大为 9000，以提高网络吞吐量。</p><p>第三种，从 ICMP 的角度出发，为了避免 ICMP 主机探测、ICMP Flood 等各种网络问题，你可以通过内核选项，来限制 ICMP 的行为。</p><ul><li>比如，你可以禁止 ICMP 协议，即设置 net.ipv4.icmp_echo_ignore_all = 1。这样，外部主机就无法通过 ICMP 来探测主机。</li><li>或者，你还可以禁止广播 ICMP，即设置 net.ipv4.icmp_echo_ignore_broadcasts = 1。</li></ul><p><strong>链路层</strong></p><p>由于网卡收包后调用的中断处理程序（特别是软中断），需要消耗大量的 CPU。所以，将这些中断处理程序调度到不同的 CPU 上执行，就可以显著提高网络吞吐量。这通常可以采用下面两种方法。</p><ul><li>比如，你可以为网卡硬中断配置 CPU 亲和性（smp_affinity），或者开启 irqbalance 服务。</li><li>再如，你可以开启 RPS（Receive Packet Steering）和 RFS（Receive Flow Steering），将应用程序和软中断的处理，调度到相同 CPU 上，这样就可以增加 CPU 缓存命中率，减少网络延迟。</li></ul><p>另外，现在的网卡都有很丰富的功能，原来在内核中通过软件处理的功能，可以卸载到网卡中，通过硬件来执行。</p><ul><li>TSO（TCP Segmentation Offload）和 UFO（UDP Fragmentation Offload）：在 TCP/UDP 协议中直接发送大包；而 TCP 包的分段（按照 MSS 分段）和 UDP 的分片（按照 MTU 分片）功能，由网卡来完成 。</li><li>GSO（Generic Segmentation Offload）：在网卡不支持 TSO/UFO 时，将 TCP/UDP 包的分段，延迟到进入网卡前再执行。这样，不仅可以减少 CPU 的消耗，还可以在发生丢包时只重传分段后的包。</li><li>LRO（Large Receive Offload）：在接收 TCP 分段包时，由网卡将其组装合并后，再交给上层网络处理。不过要注意，在需要 IP 转发的情况下，不能开启 LRO，因为如果多个包的头部信息不一致，LRO 合并会导致网络包的校验错误。</li><li>GRO（Generic Receive Offload）：GRO 修复了 LRO 的缺陷，并且更为通用，同时支持 TCP 和 UDP。</li><li>RSS（Receive Side Scaling）：也称为多队列接收，它基于硬件的多个接收队列，来分配网络接收进程，这样可以让多个 CPU 来处理接收到的网络包。</li><li>VXLAN 卸载：也就是让网卡来完成 VXLAN 的组包功能。</li></ul><p>最后，对于网络接口本身，也有很多方法，可以优化网络的吞吐量。</p><ul><li>比如，你可以开启网络接口的多队列功能。这样，每个队列就可以用不同的中断号，调度到不同 CPU 上执行，从而提升网络的吞吐量。</li><li>再如，你可以增大网络接口的缓冲区大小，以及队列长度等，提升网络传输的吞吐量（注意，这可能导致延迟增大）。</li><li>你还可以使用 Traffic Control 工具，为不同网络流量配置 QoS。</li></ul><p>在单机并发 1000 万的场景中，对 Linux 网络协议栈进行的各种优化策略，基本都没有太大效果。因为这种情况下，网络协议栈的冗长流程，其实才是最主要的性能负担。</p><p>这时，我们可以用两种方式来优化。</p><p>第一种，使用 DPDK 技术，跳过内核协议栈，直接由用户态进程用轮询的方式，来处理网络请求。同时，再结合大页、CPU 绑定、内存对齐、流水线并发等多种机制，优化网络包的处理效率。</p><p>第二种，使用内核自带的 XDP 技术，在网络包进入内核协议栈前，就对其进行处理，这样也可以实现很好的性能。</p><h3 id="答疑-1"><a href="#答疑-1" class="headerlink" title="答疑"></a>答疑</h3><p><strong>问题 1：网络收发过程中缓冲区的位置</strong></p><p>在 关于 Linux 网络，你必须要知道这些 中，我曾介绍过 Linux 网络的收发流程。这个流程涉及到了多个队列和缓冲区，包括：</p><ul><li>网卡收发网络包时，通过 DMA 方式交互的环形缓冲区；</li><li>网卡中断处理程序为网络帧分配的，内核数据结构 sk_buff 缓冲区；</li><li>应用程序通过套接字接口，与网络协议栈交互时的套接字缓冲区。</li></ul><p>首先，这些缓冲区的位置在哪儿？是在网卡硬件中，还是在内存中？这个问题其实仔细想一下，就很容易明白——这些缓冲区都处于内核管理的内存中。</p><p>其中，环形缓冲区，由于需要 DMA 与网卡交互，理应属于网卡设备驱动的范围。</p><p>sk_buff 缓冲区，是一个维护网络帧结构的双向链表，链表中的每一个元素都是一个网络帧（Packet）。虽然 TCP/IP 协议栈分了好几层，但上下不同层之间的传递，实际上只需要操作这个数据结构中的指针，而无需进行数据复制。</p><p>套接字缓冲区，则允许应用程序，给每个套接字配置不同大小的接收或发送缓冲区。应用程序发送数据，实际上就是将数据写入缓冲区；而接收数据，其实就是从缓冲区中读取。至于缓冲区中数据的进一步处理，则由传输层的 TCP 或 UDP 协议来完成。</p><p>其次，这些缓冲区，跟前面内存部分讲到的 Buffer 和 Cache 有什么关联吗？</p><p>这个问题其实也不难回答。我在内存模块曾提到过，内存中提到的 Buffer ，都跟块设备直接相关；而其他的都是 Cache。</p><p>实际上，sk_buff、套接字缓冲、连接跟踪等，都通过 slab 分配器来管理。你可以直接通过 /proc/slabinfo，来查看它们占用的内存大小。</p><h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><p><strong>服务端丢包排查</strong></p><p>接着，我们切换到终端二中，执行下面的 hping3 命令，进一步验证 Nginx 是不是真的可以正常访问了。注意，这里我没有使用 ping，是因为 ping 基于 ICMP 协议，而 Nginx 使用的是 TCP 协议。</p><pre><code># -c表示发送10个请求，-S表示使用TCP SYN，-p指定端口为80$ hping3 -c 10 -S -p 80 192.168.0.30HPING 192.168.0.30 (eth0 192.168.0.30): S set, 40 headers + 0 data byteslen=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=3 win=5120 rtt=7.5 mslen=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=4 win=5120 rtt=7.4 mslen=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=5 win=5120 rtt=3.3 mslen=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=7 win=5120 rtt=3.0 mslen=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=6 win=5120 rtt=3027.2 ms--- 192.168.0.30 hping statistic ---10 packets transmitted, 5 packets received, 50% packet lossround-trip min/avg/max = 3.0/609.7/3027.2 ms</code></pre><p>从 hping3 的输出中，我们可以发现，发送了 10 个请求包，却只收到了 5 个回复，50% 的包都丢了。再观察每个请求的 RTT 可以发现，RTT 也有非常大的波动变化，小的时候只有 3ms，而大的时候则有 3s。</p><p>根据这些输出，我们基本能判断，已经发生了丢包现象。可以猜测，3s 的 RTT ，很可能是因为丢包后重传导致的。那到底是哪里发生了丢包呢？</p><p>排查之前，我们可以回忆一下 Linux 的网络收发流程，先从理论上分析，哪里有可能会发生丢包。你不妨拿出手边的笔和纸，边回忆边在纸上梳理，思考清楚再继续下面的内容。</p><p>在这里，为了帮你理解网络丢包的原理，我画了一张图，你可以保存并打印出来使用：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7b3b5c3da2804568.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>从图中你可以看出，可能发生丢包的位置，实际上贯穿了整个网络协议栈。换句话说，全程都有丢包的可能。比如我们从下往上看：</p><ul><li>在两台 VM 连接之间，可能会发生传输失败的错误，比如网络拥塞、线路错误等；</li><li>在网卡收包后，环形缓冲区可能会因为溢出而丢包；</li><li>在链路层，可能会因为网络帧校验失败、QoS 等而丢包；</li><li>在 IP 层，可能会因为路由失败、组包大小超过 MTU 等而丢包；</li><li>在传输层，可能会因为端口未监听、资源占用超过内核限制等而丢包；</li><li>在套接字层，可能会因为套接字缓冲区溢出而丢包；</li><li>在应用层，可能会因为应用程序异常而丢包；</li><li>此外，如果配置了 iptables 规则，这些网络包也可能因为 iptables 过滤规则而丢包。</li></ul><p>当然，上面这些问题，还有可能同时发生在通信的两台机器中。不过，由于我们没对 VM2 做任何修改，并且 VM2 也只运行了一个最简单的 hping3 命令，这儿不妨假设它是没有问题的。</p><p><strong>链路层</strong></p><p>首先，来看最底下的链路层。当缓冲区溢出等原因导致网卡丢包时，Linux 会在网卡收发数据的统计信息中，记录下收发错误的次数。</p><p>你可以通过 ethtool 或者 netstat ，来查看网卡的丢包记录。比如，可以在容器中执行下面的命令，查看丢包情况：</p><pre><code>root@nginx:/# netstat -iKernel Interface tableIface      MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flgeth0       100       31      0      0 0             8      0      0      0 BMRUlo       65536        0      0      0 0             0      0      0      0 LRU</code></pre><p>输出中的 RX-OK、RX-ERR、RX-DRP、RX-OVR ，分别表示接收时的总包数、总错误数、进入 Ring Buffer 后因其他原因（如内存不足）导致的丢包数以及 Ring Buffer 溢出导致的丢包数。</p><p>TX-OK、TX-ERR、TX-DRP、TX-OVR 也代表类似的含义，只不过是指发送时对应的各个指标。</p><blockquote><p>注意，由于 Docker 容器的虚拟网卡，实际上是一对 veth pair，一端接入容器中用作 eth0，另一端在主机中接入 docker0 网桥中。veth 驱动并没有实现网络统计的功能，所以使用 ethtool -S 命令，无法得到网卡收发数据的汇总信息。</p></blockquote><p>从这个输出中，我们没有发现任何错误，说明容器的虚拟网卡没有丢包。不过要注意，如果用 tc 等工具配置了 QoS，那么 tc 规则导致的丢包，就不会包含在网卡的统计信息中。</p><p>所以接下来，我们还要检查一下 eth0 上是否配置了 tc 规则，并查看有没有丢包。我们继续容器终端中，执行下面的 tc 命令，不过这次注意添加 -s 选项，以输出统计信息：</p><pre><code>root@nginx:/# tc -s qdisc show dev eth0qdisc netem 800d: root refcnt 2 limit 1000 loss 30% Sent 432 bytes 8 pkt (dropped 4, overlimits 0 requeues 0) backlog 0b 0p requeues 0 </code></pre><p>从 tc 的输出中可以看到， eth0 上面配置了一个网络模拟排队规则（qdisc netem），并且配置了丢包率为 30%（loss 30%）。再看后面的统计信息，发送了 8 个包，但是丢了 4 个。</p><p>看来，应该就是这里，导致 Nginx 回复的响应包，被 netem 模块给丢了。</p><p>既然发现了问题，解决方法也就很简单了，直接删掉 netem 模块就可以了。我们可以继续在容器终端中，执行下面的命令，删除 tc 中的 netem 模块：</p><pre><code>root@nginx:/# tc qdisc del dev eth0 root netem loss 30%</code></pre><p>删除后，问题到底解决了没？我们切换到终端二中，重新执行刚才的 hping3 命令，看看现在还有没有问题：</p><pre><code>$ hping3 -c 10 -S -p 80 192.168.0.30HPING 192.168.0.30 (eth0 192.168.0.30): S set, 40 headers + 0 data byteslen=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=0 win=5120 rtt=7.9 mslen=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=2 win=5120 rtt=1003.8 mslen=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=5 win=5120 rtt=7.6 mslen=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=6 win=5120 rtt=7.4 mslen=44 ip=192.168.0.30 ttl=63 DF id=0 sport=80 flags=SA seq=9 win=5120 rtt=3.0 ms--- 192.168.0.30 hping statistic ---10 packets transmitted, 5 packets received, 50% packet lossround-trip min/avg/max = 3.0/205.9/1003.8 ms    </code></pre><p>不幸的是，从 hping3 的输出中，我们可以看到，跟前面现象一样，还是 50% 的丢包；RTT 的波动也仍旧很大，从 3ms 到 1s。</p><p>显然，问题还是没解决，丢包还在继续发生。不过，既然链路层已经排查完了，我们就继续向上层分析，看看网络层和传输层有没有问题。</p><p><strong>网络层和传输层</strong></p><p>我们知道，在网络层和传输层中，引发丢包的因素非常多。不过，其实想确认是否丢包，是非常简单的事，因为 Linux 已经为我们提供了各个协议的收发汇总情况。</p><p>我们继续在容器终端中，执行下面的 netstat -s 命令，就可以看到协议的收发汇总，以及错误信息了：</p><pre><code>root@nginx:/# netstat -sIp:    Forwarding: 1          //开启转发    31 total packets received    //总收包数    0 forwarded            //转发包数    0 incoming packets discarded  //接收丢包数    25 incoming packets delivered  //接收的数据包数    15 requests sent out      //发出的数据包数Icmp:    0 ICMP messages received    //收到的ICMP包数    0 input ICMP message failed    //收到ICMP失败数    ICMP input histogram:    0 ICMP messages sent      //ICMP发送数    0 ICMP messages failed      //ICMP失败数    ICMP output histogram:Tcp:    0 active connection openings  //主动连接数    0 passive connection openings  //被动连接数    11 failed connection attempts  //失败连接尝试数    0 connection resets received  //接收的连接重置数    0 connections established    //建立连接数    25 segments received      //已接收报文数    21 segments sent out      //已发送报文数    4 segments retransmitted    //重传报文数    0 bad segments received      //错误报文数    0 resets sent          //发出的连接重置数Udp:    0 packets received    ...TcpExt:    11 resets received for embryonic SYN_RECV sockets  //半连接重置数    0 packet headers predicted    TCPTimeouts: 7    //超时数    TCPSynRetrans: 4  //SYN重传数  ...  </code></pre><p>netstat 汇总了 IP、ICMP、TCP、UDP 等各种协议的收发统计信息。不过，我们的目的是排查丢包问题，所以这里主要观察的是错误数、丢包数以及重传数。</p><p>根据上面的输出，你可以看到，只有 TCP 协议发生了丢包和重传，分别是：</p><ul><li>11 次连接失败重试（11 failed connection attempts）</li><li>4 次重传（4 segments retransmitted）</li><li>11 次半连接重置（11 resets received for embryonic SYN_RECV sockets）</li><li>4 次 SYN 重传（TCPSynRetrans）</li><li>7 次超时（TCPTimeouts）</li></ul><p>这个结果告诉我们，TCP 协议有多次超时和失败重试，并且主要错误是半连接重置。换句话说，主要的失败，都是三次握手失败。</p><p>接着，再来看 iptables。回顾一下 iptables 的原理，它基于 Netfilter 框架，通过一系列的规则，对网络数据包进行过滤（如防火墙）和修改（如 NAT）。</p><p>这些 iptables 规则，统一管理在一系列的表中，包括 filter（用于过滤）、nat（用于 NAT）、mangle（用于修改分组数据） 和 raw（用于原始数据包）等。而每张表又可以包括一系列的链，用于对 iptables 规则进行分组管理。</p><p>对于丢包问题来说，最大的可能就是被 filter 表中的规则给丢弃了。要弄清楚这一点，就需要我们确认，那些目标为 DROP 和 REJECT 等会弃包的规则，有没有被执行到。</p><p>你可以把所有的 iptables 规则列出来，根据收发包的特点，跟 iptables 规则进行匹配。不过显然，如果 iptables 规则比较多，这样做的效率就会很低。</p><p>当然，更简单的方法，就是直接查询 DROP 和 REJECT 等规则的统计信息，看看是否为 0。如果统计值不是 0 ，再把相关的规则拎出来进行分析。</p><p>我们可以通过 iptables -nvL 命令，查看各条规则的统计信息。比如，你可以执行下面的 docker exec 命令，进入容器终端；然后再执行下面的 iptables 命令，就可以看到 filter 表的统计数据了：</p><pre><code># 在主机中执行$ docker exec -it nginx bash# 在容器中执行root@nginx:/# iptables -t filter -nvLChain INPUT (policy ACCEPT 25 packets, 1000 bytes) pkts bytes target     prot opt in     out     source               destination    6   240 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.29999999981Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target     prot opt in     out     source               destinationChain OUTPUT (policy ACCEPT 15 packets, 660 bytes) pkts bytes target     prot opt in     out     source               destination    6   264 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.29999999981    </code></pre><p>从 iptables 的输出中，你可以看到，两条 DROP 规则的统计数值不是 0，它们分别在 INPUT 和 OUTPUT 链中。这两条规则实际上是一样的，指的是使用 statistic 模块，进行随机 30% 的丢包。</p><p>再观察一下它们的匹配规则。0.0.0.0/0 表示匹配所有的源 IP 和目的 IP，也就是会对所有包都进行随机 30% 的丢包。看起来，这应该就是导致部分丢包的“罪魁祸首”了。</p><p>既然找出了原因，接下来的优化就比较简单了。比如，把这两条规则直接删除就可以了。我们可以在容器终端中，执行下面的两条 iptables 命令，删除这两条 DROP 规则：</p><pre><code>root@nginx:/# iptables -t filter -D INPUT -m statistic --mode random --probability 0.30 -j DROProot@nginx:/# iptables -t filter -D OUTPUT -m statistic --mode random --probability 0.30 -j DROP</code></pre><p>接下来，我们切换回终端一，在容器终端中，执行下面的 tcpdump 命令，抓取 80 端口的包：</p><pre><code>root@nginx:/# tcpdump -i eth0 -nn port 80tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes</code></pre><p>然后，切换到终端二中，再次执行前面的 curl 命令：</p><pre><code>$ curl --max-time 3 http://192.168.0.30/curl: (28) Operation timed out after 3000 milliseconds with 0 bytes received</code></pre><p>等到 curl 命令结束后，再次切换回终端一，查看 tcpdump 的输出：</p><pre><code>14:40:00.589235 IP 10.255.255.5.39058 &gt; 172.17.0.2.80: Flags [S], seq 332257715, win 29200, options [mss 1418,sackOK,TS val 486800541 ecr 0,nop,wscale 7], length 014:40:00.589277 IP 172.17.0.2.80 &gt; 10.255.255.5.39058: Flags [S.], seq 1630206251, ack 332257716, win 4880, options [mss 256,sackOK,TS val 2509376001 ecr 486800541,nop,wscale 7], length 014:40:00.589894 IP 10.255.255.5.39058 &gt; 172.17.0.2.80: Flags [.], ack 1, win 229, options [nop,nop,TS val 486800541 ecr 2509376001], length 014:40:03.589352 IP 10.255.255.5.39058 &gt; 172.17.0.2.80: Flags [F.], seq 76, ack 1, win 229, options [nop,nop,TS val 486803541 ecr 2509376001], length 014:40:03.589417 IP 172.17.0.2.80 &gt; 10.255.255.5.39058: Flags [.], ack 1, win 40, options [nop,nop,TS val 2509379001 ecr 486800541,nop,nop,sack 1 {76:77}], length 0</code></pre><p>经过这么一系列的操作，从 tcpdump 的输出中，我们就可以看到：</p><ul><li>前三个包是正常的 TCP 三次握手，这没问题；</li><li>但第四个包却是在 3 秒以后了，并且还是客户端（VM2）发送过来的 FIN 包，也就说明，客户端的连接关闭了。</li></ul><p>我想，根据 curl 设置的 3 秒超时选项，你应该能猜到，这是因为 curl 命令超时后退出了。</p><p>我把这一过程，用 TCP 交互的流程图（实际上来自 Wireshark 的 Flow Graph）来表示，你可以更清楚地看到上面这个问题：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9deb6ef5cf0971c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这里比较奇怪的是，我们并没有抓取到 curl 发来的 HTTP GET 请求。那么，究竟是网卡丢包了，还是客户端压根儿就没发过来呢？</p><p>我们可以重新执行 netstat -i 命令，确认一下网卡有没有丢包问题：</p><pre><code>root@nginx:/# netstat -iKernel Interface tableIface      MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flgeth0       100      157      0    344 0            94      0      0      0 BMRUlo       65536        0      0      0 0             0      0      0      0 LRU</code></pre><p>从 netstat 的输出中，你可以看到，接收丢包数（RX-DRP）是 344，果然是在网卡接收时丢包了。不过问题也来了，为什么刚才用 hping3 时不丢包，现在换成 GET 就收不到了呢？</p><p>还是那句话，遇到搞不懂的现象，不妨先去查查工具和方法的原理。我们可以对比一下这两个工具：</p><ul><li>hping3 实际上只发送了 SYN 包；</li><li>而 curl 在发送 SYN 包后，还会发送 HTTP GET 请求。</li></ul><p>HTTP GET ，本质上也是一个 TCP 包，但跟 SYN 包相比，它还携带了 HTTP GET 的数据。</p><p>那么，通过这个对比，你应该想到了，这可能是 MTU 配置错误导致的。为什么呢？</p><p>其实，仔细观察上面 netstat 的输出界面，第二列正是每个网卡的 MTU 值。eth0 的 MTU 只有 100，而以太网的 MTU 默认值是 1500，这个 100 就显得太小了。</p><p>当然，MTU 问题是很好解决的，把它改成 1500 就可以了。我们继续在容器终端中，执行下面的命令，把容器 eth0 的 MTU 改成 1500：</p><pre><code>root@nginx:/# ifconfig eth0 mtu 1500</code></pre><p>修改完成后，再切换到终端二中，再次执行 curl 命令，确认问题是否真的解决了.</p><p><strong>内核线程</strong></p><p>我们知道，在 Linux 中，用户态进程的“祖先”，都是 PID 号为 1 的 init 进程。比如，现在主流的 Linux 发行版中，init 都是 systemd 进程；而其他的用户态进程，会通过 systemd 来进行管理。</p><p>稍微想一下 Linux 中的各种进程，除了用户态进程外，还有大量的内核态线程。按说内核态的线程，应该先于用户态进程启动，可是 systemd 只管理用户态进程。那么，内核态线程又是谁来管理的呢？</p><p>实际上，Linux 在启动过程中，有三个特殊的进程，也就是 PID 号最小的三个进程。</p><ul><li>0 号进程为 idle 进程，这也是系统创建的第一个进程，它在初始化 1 号和 2 号进程后，演变为空闲任务。当 CPU 上没有其他任务执行时，就会运行它。</li><li>1 号进程为 init 进程，通常是 systemd 进程，在用户态运行，用来管理其他用户态进程。</li><li>2 号进程为 kthreadd 进程，在内核态运行，用来管理内核线程。</li></ul><p>所以，要查找内核线程，我们只需要从 2 号进程开始，查找它的子孙进程即可。比如，你可以使用 ps 命令，来查找 kthreadd 的子进程：</p><pre><code>$ ps -f --ppid 2 -p 2UID         PID   PPID  C STIME TTY          TIME CMDroot          2      0  0 12:02 ?        00:00:01 [kthreadd]root          9      2  0 12:02 ?        00:00:21 [ksoftirqd/0]root         10      2  0 12:02 ?        00:11:47 [rcu_sched]root         11      2  0 12:02 ?        00:00:18 [migration/0]...root      11094      2  0 14:20 ?        00:00:00 [kworker/1:0-eve]root      11647      2  0 14:27 ?        00:00:00 [kworker/0:2-cgr]</code></pre><p>从上面的输出，你能够看到，内核线程的名称（CMD）都在中括号里（这一点，我们前面内容也有提到过）。所以，更简单的方法，就是直接查找名称包含中括号的进程。比如：</p><pre><code>$ ps -ef | grep "\[.*\]"root         2     0  0 08:14 ?        00:00:00 [kthreadd]root         3     2  0 08:14 ?        00:00:00 [rcu_gp]root         4     2  0 08:14 ?        00:00:00 [rcu_par_gp]...</code></pre><p>了解内核线程的基本功能，对我们排查问题有非常大的帮助。比如，我们曾经在软中断案例中提到过 ksoftirqd。它是一个用来处理软中断的内核线程，并且每个 CPU 上都有一个。</p><p>如果你知道了这一点，那么，以后遇到 ksoftirqd 的 CPU 使用高的情况，就会首先怀疑是软中断的问题，然后从软中断的角度来进一步分析。</p><p>其实，除了刚才看到的 kthreadd 和 ksoftirqd 外，还有很多常见的内核线程，我们在性能分析中都经常会碰到，比如下面这几个内核线程。</p><ul><li>kswapd0：用于内存回收。在 Swap 变高 案例中，我曾介绍过它的工作原理。</li><li>kworker：用于执行内核工作队列，分为绑定 CPU （名称格式为 kworker/CPU86330）和未绑定 CPU（名称格式为 kworker/uPOOL86330）两类。</li><li>migration：在负载均衡过程中，把进程迁移到 CPU 上。每个 CPU 都有一个 migration 内核线程。</li><li>jbd2/sda1-8：jbd 是 Journaling Block Device 的缩写，用来为文件系统提供日志功能，以保证数据的完整性；名称中的 sda1-8，表示磁盘分区名称和设备号。每个使用了 ext4 文件系统的磁盘分区，都会有一个 jbd2 内核线程。</li><li>pdflush：用于将内存中的脏页（被修改过，但还未写入磁盘的文件页）写入磁盘（已经在 3.10 中合并入了 kworker 中）。</li></ul><p>接着，还是在第二个终端中，运行 hping3 命令，模拟 Nginx 的客户端请求：</p><pre><code># -S参数表示设置TCP协议的SYN（同步序列号），-p表示目的端口为80# -i u10表示每隔10微秒发送一个网络帧# 注：如果你在实践过程中现象不明显，可以尝试把10调小，比如调成5甚至1$ hping3 -S -p 80 -i u10 192.168.0.30</code></pre><p>现在，我们再回到第一个终端，你应该就会发现异常——系统的响应明显变慢了。我们不妨执行 top，观察一下系统和进程的 CPU 使用情况：</p><pre><code>$ toptop - 08:31:43 up 17 min,  1 user,  load average: 0.00, 0.00, 0.02Tasks: 128 total,   1 running,  69 sleeping,   0 stopped,   0 zombie%Cpu0  :  0.3 us,  0.3 sy,  0.0 ni, 66.8 id,  0.3 wa,  0.0 hi, 32.4 si,  0.0 st%Cpu1  :  0.0 us,  0.3 sy,  0.0 ni, 65.2 id,  0.0 wa,  0.0 hi, 34.5 si,  0.0 stKiB Mem :  8167040 total,  7234236 free,   358976 used,   573828 buff/cacheKiB Swap:        0 total,        0 free,        0 used.  7560460 avail Mem  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND    9 root      20   0       0      0      0 S   7.0  0.0   0:00.48 ksoftirqd/0   18 root      20   0       0      0      0 S   6.9  0.0   0:00.56 ksoftirqd/1 2489 root      20   0  876896  38408  21520 S   0.3  0.5   0:01.50 docker-containe 3008 root      20   0   44536   3936   3304 R   0.3  0.0   0:00.09 top    1 root      20   0   78116   9000   6432 S   0.0  0.1   0:11.77 systemd ... </code></pre><p>从 top 的输出中，你可以看到，两个 CPU 的软中断使用率都超过了 30%；而 CPU 使用率最高的进程，正好是软中断内核线程 ksoftirqd/0 和 ksoftirqd/1。</p><p>虽然，我们已经知道了 ksoftirqd 的基本功能，可以猜测是因为大量网络收发，引起了 CPU 使用率升高；但它到底在执行什么逻辑，我们却并不知道。</p><p>对于普通进程，我们要观察其行为有很多方法，比如 strace、pstack、lsof 等等。但这些工具并不适合内核线程，比如，如果你用 pstack ，或者通过 /proc/pid/stack 查看 ksoftirqd/0（进程号为 9）的调用栈时，分别可以得到以下输出：</p><pre><code>$ pstack 9Could not attach to target 9: Operation not permitted.detach: No such process$ cat /proc/9/stack[&lt;0&gt;] smpboot_thread_fn+0x166/0x170[&lt;0&gt;] kthread+0x121/0x140[&lt;0&gt;] ret_from_fork+0x35/0x40[&lt;0&gt;] 0xffffffffffffffff</code></pre><p>显然，pstack 报出的是不允许挂载进程的错误；而 /proc/9/stack 方式虽然有输出，但输出中并没有详细的调用栈情况。</p><p>那还有没有其他方法，来观察内核线程 ksoftirqd 的行为呢？</p><p>既然是内核线程，自然应该用到内核中提供的机制。回顾一下我们之前用过的 CPU 性能工具，我想你肯定还记得 perf ，这个内核自带的性能剖析工具。</p><p>perf 可以对指定的进程或者事件进行采样，并且还可以用调用栈的形式，输出整个调用链上的汇总信息。 我们不妨就用 perf ，来试着分析一下进程号为 9 的 ksoftirqd。</p><p>继续在终端一中，执行下面的 perf record 命令；并指定进程号 9 ，以便记录 ksoftirqd 的行为:</p><pre><code># 采样30s后退出$ perf record -a -g -p 9 -- sleep 30</code></pre><p>稍等一会儿，在上述命令结束后，继续执行 perf report命令，你就可以得到 perf 的汇总报告。按上下方向键以及回车键，展开比例最高的 ksoftirqd 后，你就可以得到下面这个调用关系链图：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-eb25d9d7e2930e64.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>从这个图中，你可以清楚看到 ksoftirqd 执行最多的调用过程。虽然你可能不太熟悉内核源码，但通过这些函数，我们可以大致看出它的调用栈过程。</p><ul><li>net_rx_action 和 netif_receive_skb，表明这是接收网络包（rx 表示 receive）。</li><li>br_handle_frame ，表明网络包经过了网桥（br 表示 bridge）。</li><li>br_nf_pre_routing ，表明在网桥上执行了 netfilter 的 PREROUTING（nf 表示 netfilter）。而我们已经知道 PREROUTING 主要用来执行 DNAT，所以可以猜测这里有 DNAT 发生。</li><li>br_pass_frame_up，表明网桥处理后，再交给桥接的其他桥接网卡进一步处理。比如，在新的网卡上接收网络包、执行 netfilter 过滤规则等等。</li></ul><p>首先，我们需要生成火焰图。我们先下载几个能从 perf record 记录生成火焰图的工具，这些工具都放在 <a href="https://github.com/brendangregg/FlameGraph">https://github.com/brendangregg/FlameGraph</a> 上面。你可以执行下面的命令来下载：</p><pre><code>$ git clone https://github.com/brendangregg/FlameGraph$ cd FlameGraph</code></pre><p>安装好工具后，要生成火焰图，其实主要需要三个步骤：</p><ul><li>执行 perf script ，将 perf record 的记录转换成可读的采样记录；</li><li>执行 stackcollapse-perf.pl 脚本，合并调用栈信息；</li><li>执行 flamegraph.pl 脚本，生成火焰图。</li></ul><p>不过，在 Linux 中，我们可以使用管道，来简化这三个步骤的执行过程。假设刚才用 perf record 生成的文件路径为 /root/perf.data，执行下面的命令，你就可以直接生成火焰图：</p><pre><code>$ perf script -i /root/perf.data | ./stackcollapse-perf.pl --all |  ./flamegraph.pl &gt; ksoftirqd.svg</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-10afe2884d65fe0b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>根据刚刚讲过的火焰图原理，这个图应该从下往上看，沿着调用栈中最宽的函数来分析执行次数最多的函数。这儿看到的结果，其实跟刚才的 perf report 类似，但直观了很多，中间这一团火，很明显就是最需要我们关注的地方。</p><p>我们顺着调用栈由下往上看（顺着图中蓝色箭头），就可以得到跟刚才 perf report 中一样的结果：</p><ul><li>最开始，还是 net_rx_action 到 netif_receive_skb 处理网络收包；</li><li>然后， br_handle_frame 到 br_nf_pre_routing ，在网桥中接收并执行 netfilter 钩子函数；</li><li>再向上， br_pass_frame_up 到 netif_receive_skb ，从网桥转到其他网络设备又一次接收。</li></ul><p>不过最后，到了 ip_forward 这里，已经看不清函数名称了。所以我们需要点击 ip_forward，展开最上面这一块调用栈：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-bc37f50ba574aa0f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这样，就可以进一步看到 ip_forward 后的行为，也就是把网络包发送出去。根据这个调用过程，再结合我们前面学习的网络收发和 TCP/IP 协议栈原理，这个流程中的网络接收、网桥以及 netfilter 调用等，都是导致软中断 CPU 升高的重要因素，也就是影响网络性能的潜在瓶颈。</p><p>不过，回想一下网络收发的流程，你可能会觉得它缺了好多步骤。</p><p>比如，这个堆栈中并没有 TCP 相关的调用，也没有连接跟踪 conntrack 相关的函数。实际上，这些流程都在其他更小的火焰中，你可以点击上图左上角的“Reset Zoom”，回到完整火焰图中，再去查看其他小火焰的堆栈。</p><p>所以，在理解这个调用栈时要注意。从任何一个点出发、纵向来看的整个调用栈，其实只是最顶端那一个函数的调用堆栈，而非完整的内核网络执行流程。</p><p>另外，整个火焰图不包含任何时间的因素，所以并不能看出横向各个函数的执行次序。</p><p>到这里，我们就找出了内核线程 ksoftirqd 执行最频繁的函数调用堆栈，而这个堆栈中的各层级函数，就是潜在的性能瓶颈来源。这样，后面想要进一步分析、优化时，也就有了根据。</p><h3 id="动态追踪"><a href="#动态追踪" class="headerlink" title="动态追踪"></a>动态追踪</h3><p>其实，使用 perf 对系统内核线程进行分析时，内核线程依然还在正常运行中，所以这种方法也被称为动态追踪技术。</p><p>动态追踪技术，通过探针机制，来采集内核或者应用程序的运行信息，从而可以不用修改内核和应用程序的代码，就获得丰富的信息，帮你分析、定位想要排查的问题。</p><p>以往，在排查和调试性能问题时，我们往往需要先为应用程序设置一系列的断点（比如使用 GDB），然后以手动或者脚本（比如 GDB 的 Python 扩展）的方式，在这些断点处分析应用程序的状态。或者，增加一系列的日志，从日志中寻找线索。</p><p>同时，相比以往的进程级跟踪方法（比如 ptrace），动态追踪往往只会带来很小的性能损耗（通常在 5% 或者更少）。</p><p>说到动态追踪（Dynamic Tracing），就不得不提源于 Solaris 系统的 DTrace。DTrace 是动态追踪技术的鼻祖，它提供了一个通用的观测框架，并可以使用 D 语言进行自由扩展。</p><p>DTrace 的工作原理如下图所示。它的运行常驻在内核中，用户可以通过 dtrace 命令，把 D 语言编写的追踪脚本，提交到内核中的运行时来执行。DTrace 可以跟踪用户态和内核态的所有事件，并通过一些列的优化措施，保证最小的性能开销。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c82afe8049b2ba9b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>虽然直到今天，DTrace 本身依然无法在 Linux 中运行，但它同样对 Linux 动态追踪产生了巨大的影响。很多工程师都尝试过把 DTrace 移植到 Linux 中，这其中，最著名的就是 RedHat 主推的 SystemTap。</p><p>同 DTrace 一样，SystemTap 也定义了一种类似的脚本语言，方便用户根据需要自由扩展。不过，不同于 DTrace，SystemTap 并没有常驻内核的运行时，它需要先把脚本编译为内核模块，然后再插入到内核中执行。这也导致 SystemTap 启动比较缓慢，并且依赖于完整的调试符号表。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-70860c47ce91a892.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>总的来说，为了追踪内核或用户空间的事件，Dtrace 和 SystemTap 都会把用户传入的追踪处理函数（一般称为 Action），关联到被称为探针的检测点上。这些探针，实际上也就是各种动态追踪技术所依赖的事件源。</p><p><strong>动态追踪的事件源</strong></p><p>根据事件类型的不同，动态追踪所使用的事件源，可以分为<strong>静态探针、动态探针以及硬件事件</strong>等三类。它们的关系如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-24140fbff13e36f9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>其中，硬件事件通常由性能监控计数器 PMC（Performance Monitoring Counter）产生，包括了各种硬件的性能情况，比如 CPU 的缓存、指令周期、分支预测等等。</p><p>静态探针，是指事先在代码中定义好，并编译到应用程序或者内核中的探针。这些探针只有在开启探测功能时，才会被执行到；未开启时并不会执行。常见的静态探针包括内核中的跟踪点（tracepoints）和 USDT（Userland Statically Defined Tracing）探针。</p><ul><li>跟踪点（tracepoints），实际上就是在源码中插入的一些带有控制条件的探测点，这些探测点允许事后再添加处理函数。比如在内核中，最常见的静态跟踪方法就是 printk，即输出日志。Linux 内核定义了大量的跟踪点，可以通过内核编译选项，来开启或者关闭。</li><li>USDT 探针，全称是用户级静态定义跟踪，需要在源码中插入 DTRACE_PROBE() 代码，并编译到应用程序中。不过，也有很多应用程序内置了 USDT 探针，比如 MySQL、PostgreSQL 等。</li></ul><p>动态探针，则是指没有事先在代码中定义，但却可以在运行时动态添加的探针，比如函数的调用和返回等。动态探针支持按需在内核或者应用程序中添加探测点，具有更高的灵活性。常见的动态探针有两种，即用于内核态的 kprobes 和用于用户态的 uprobes。</p><ul><li>kprobes 用来跟踪内核态的函数，包括用于函数调用的 kprobe 和用于函数返回的 kretprobe。</li><li>uprobes 用来跟踪用户态的函数，包括用于函数调用的 uprobe 和用于函数返回的 uretprobe。</li></ul><blockquote><p>注意，kprobes 需要内核编译时开启 CONFIG_KPROBE_EVENTS；而 uprobes 则需要内核编译时开启 CONFIG_UPROBE_EVENTS。</p></blockquote><p><strong>动态追踪机制</strong></p><p>而在这些探针的基础上，Linux 也提供了一系列的动态追踪机制，比如 ftrace、perf、eBPF 等。</p><p>ftrace 最早用于函数跟踪，后来又扩展支持了各种事件跟踪功能。ftrace 的使用接口跟我们之前提到的 procfs 类似，它通过 debugfs（4.1 以后也支持 tracefs），以普通文件的形式，向用户空间提供访问接口。</p><p>这样，不需要额外的工具，你就可以通过挂载点（通常为 /sys/kernel/debug/tracing 目录）内的文件读写，来跟 ftrace 交互，跟踪内核或者应用程序的运行事件。</p><p>perf 是我们的老朋友了，我们在前面的好多案例中，都使用了它的事件记录和分析功能，这实际上只是一种最简单的静态跟踪机制。你也可以通过 perf ，来自定义动态事件（perf probe），只关注真正感兴趣的事件。</p><p>eBPF 则在 BPF（Berkeley Packet Filter）的基础上扩展而来，不仅支持事件跟踪机制，还可以通过自定义的 BPF 代码（使用 C 语言）来自由扩展。所以，eBPF 实际上就是常驻于内核的运行时，可以说就是 Linux 版的 DTrace。</p><p>除此之外，还有很多内核外的工具，也提供了丰富的动态追踪功能。最常见的就是前面提到的 SystemTap，我们之前多次使用过的 BCC（BPF Compiler Collection），以及常用于容器性能分析的 sysdig 等。</p><p><strong>ftrace</strong></p><p>我们先来看 ftrace。刚刚提到过，ftrace 通过 debugfs（或者 tracefs），为用户空间提供接口。所以使用 ftrace，往往是从切换到 debugfs 的挂载点开始。</p><pre><code>$ cd /sys/kernel/debug/tracing$ lsREADME                      instances            set_ftrace_notrace  trace_marker_rawavailable_events            kprobe_events        set_ftrace_pid      trace_options...</code></pre><p>如果这个目录不存在，则说明你的系统还没有挂载 debugfs，你可以执行下面的命令来挂载它：</p><pre><code>$ mount -t debugfs nodev /sys/kernel/debug</code></pre><p>ftrace 提供了多个跟踪器，用于跟踪不同类型的信息，比如函数调用、中断关闭、进程调度等。具体支持的跟踪器取决于系统配置，你可以执行下面的命令，来查询所有支持的跟踪器：</p><pre><code>$ cat available_tracershwlat blk mmiotrace function_graph wakeup_dl wakeup_rt wakeup function nop</code></pre><p>这其中，function 表示跟踪函数的执行，function_graph 则是跟踪函数的调用关系，也就是生成直观的调用关系图。这便是最常用的两种跟踪器。</p><p>除了跟踪器外，使用 ftrace 前，还需要确认跟踪目标，包括内核函数和内核事件。其中，</p><ul><li>函数就是内核中的函数名。</li><li>而事件，则是内核源码中预先定义的跟踪点。</li></ul><p>同样地，你可以执行下面的命令，来查询支持的函数和事件：</p><pre><code>$ cat available_filter_functions$ cat available_events</code></pre><p>明白了这些基本信息，接下来，我就以 ls 命令为例，带你一起看看 ftrace 的使用方法。</p><p>为了列出文件，ls 命令会通过 open 系统调用打开目录文件，而 open 在内核中对应的函数名为 do_sys_open。 所以，我们要做的第一步，就是把要跟踪的函数设置为 do_sys_open：</p><pre><code>$ echo do_sys_open &gt; set_graph_function</code></pre><p>接下来，第二步，配置跟踪选项，开启函数调用跟踪，并跟踪调用进程：</p><pre><code>$ echo function_graph &gt; current_tracer$ echo funcgraph-proc &gt; trace_options</code></pre><p>接着，第三步，也就是开启跟踪：</p><pre><code>$ echo 1 &gt; tracing_on</code></pre><p>第四步，执行一个 ls 命令后，再关闭跟踪：</p><pre><code>$ ls$ echo 0 &gt; tracing_on</code></pre><p>第五步，也是最后一步，查看跟踪结果：</p><pre><code>$ cat trace# tracer: function_graph## CPU  TASK/PID         DURATION                  FUNCTION CALLS# |     |    |           |   |                     |   |   |   | 0)    ls-12276    |               |  do_sys_open() { 0)    ls-12276    |               |    getname() { 0)    ls-12276    |               |      getname_flags() { 0)    ls-12276    |               |        kmem_cache_alloc() { 0)    ls-12276    |               |          _cond_resched() { 0)    ls-12276    |   0.049 us    |            rcu_all_qs(); 0)    ls-12276    |   0.791 us    |          } 0)    ls-12276    |   0.041 us    |          should_failslab(); 0)    ls-12276    |   0.040 us    |          prefetch_freepointer(); 0)    ls-12276    |   0.039 us    |          memcg_kmem_put_cache(); 0)    ls-12276    |   2.895 us    |        } 0)    ls-12276    |               |        __check_object_size() { 0)    ls-12276    |   0.067 us    |          __virt_addr_valid(); 0)    ls-12276    |   0.044 us    |          __check_heap_object(); 0)    ls-12276    |   0.039 us    |          check_stack_object(); 0)    ls-12276    |   1.570 us    |        } 0)    ls-12276    |   5.790 us    |      } 0)    ls-12276    |   6.325 us    |    }...</code></pre><p>在最后得到的输出中：</p><ul><li>第一列表示运行的 CPU；</li><li>第二列是任务名称和进程 PID；</li><li>第三列是函数执行延迟；</li><li>最后一列，则是函数调用关系图。</li></ul><p>当然，我想你应该也发现了 ftrace 的使用缺点——五个步骤实在是麻烦，用起来并不方便。不过，不用担心， trace-cmd 已经帮你把这些步骤给包装了起来。这样，你就可以在同一个命令行工具里，完成上述所有过程。</p><pre><code># Ubuntu$ apt-get install trace-cmd# CentOS$ yum install trace-cmd</code></pre><p>安装好后，原本的五步跟踪过程，就可以简化为下面这两步：</p><pre><code>$ trace-cmd record -p function_graph -g do_sys_open -O funcgraph-proc ls$ trace-cmd report...              ls-12418 [000] 85558.075341: funcgraph_entry:                   |  do_sys_open() {              ls-12418 [000] 85558.075363: funcgraph_entry:                   |    getname() {              ls-12418 [000] 85558.075364: funcgraph_entry:                   |      getname_flags() {              ls-12418 [000] 85558.075364: funcgraph_entry:                   |        kmem_cache_alloc() {              ls-12418 [000] 85558.075365: funcgraph_entry:                   |          _cond_resched() {              ls-12418 [000] 85558.075365: funcgraph_entry:        0.074 us   |            rcu_all_qs();              ls-12418 [000] 85558.075366: funcgraph_exit:         1.143 us   |          }              ls-12418 [000] 85558.075366: funcgraph_entry:        0.064 us   |          should_failslab();              ls-12418 [000] 85558.075367: funcgraph_entry:        0.075 us   |          prefetch_freepointer();              ls-12418 [000] 85558.075368: funcgraph_entry:        0.085 us   |          memcg_kmem_put_cache();              ls-12418 [000] 85558.075369: funcgraph_exit:         4.447 us   |        }              ls-12418 [000] 85558.075369: funcgraph_entry:                   |        __check_object_size() {              ls-12418 [000] 85558.075370: funcgraph_entry:        0.132 us   |          __virt_addr_valid();              ls-12418 [000] 85558.075370: funcgraph_entry:        0.093 us   |          __check_heap_object();              ls-12418 [000] 85558.075371: funcgraph_entry:        0.059 us   |          check_stack_object();              ls-12418 [000] 85558.075372: funcgraph_exit:         2.323 us   |        }              ls-12418 [000] 85558.075372: funcgraph_exit:         8.411 us   |      }              ls-12418 [000] 85558.075373: funcgraph_exit:         9.195 us   |    }...</code></pre><p><strong>perf</strong></p><p>在 Linux 系统中，常见的动态追踪方法包括 ftrace、perf、eBPF 以及 SystemTap 等。上节课，我们具体学习了 ftrace 的使用方法。今天，我们再来一起看看其他几种方法。</p><p>不过，我们前面使用 perf record/top 时，都是先对事件进行采样，然后再根据采样数，评估各个函数的调用频率。实际上，perf 的功能远不止于此。比如，</p><ul><li>perf 可以用来分析 CPU cache、CPU 迁移、分支预测、指令周期等各种硬件事件；</li><li>perf 也可以只对感兴趣的事件进行动态追踪。</li></ul><p>接下来，我们还是以内核函数 do_sys_open，以及用户空间函数 readline 为例，看一看 perf 动态追踪的使用方法。</p><p>同 ftrace 一样，你也可以通过 perf list ，查询所有支持的事件：</p><pre><code>$ perf list</code></pre><p>然后，在 perf 的各个子命令中添加 –event 选项，设置追踪感兴趣的事件。如果这些预定义的事件不满足实际需要，你还可以使用 perf probe 来动态添加。而且，除了追踪内核事件外，perf 还可以用来跟踪用户空间的函数。</p><p>我们先来看第一个 perf 示例，内核函数 do_sys_open 的例子。你可以执行 perf probe 命令，添加 do_sys_open 探针：</p><pre><code>$ perf probe --add do_sys_openAdded new event:  probe:do_sys_open    (on do_sys_open)You can now use it in all perf tools, such as:    perf record -e probe:do_sys_open -aR sleep 1        </code></pre><p>探针添加成功后，就可以在所有的 perf 子命令中使用。比如，上述输出就是一个 perf record 的示例，执行它就可以对 10s 内的 do_sys_open 进行采样：</p><pre><code>$ perf record -e probe:do_sys_open -aR sleep 10[ perf record: Woken up 1 times to write data ][ perf record: Captured and wrote 0.148 MB perf.data (19 samples) ]</code></pre><p>而采样成功后，就可以执行 perf script ，来查看采样结果了：</p><pre><code>$ perf script            perf 12886 [000] 89565.879875: probe:do_sys_open: (ffffffffa807b290)           sleep 12889 [000] 89565.880362: probe:do_sys_open: (ffffffffa807b290)           sleep 12889 [000] 89565.880382: probe:do_sys_open: (ffffffffa807b290)           sleep 12889 [000] 89565.880635: probe:do_sys_open: (ffffffffa807b290)           sleep 12889 [000] 89565.880669: probe:do_sys_open: (ffffffffa807b290)           </code></pre><p>输出中，同样也列出了调用 do_sys_open 的任务名称、进程 PID 以及运行的 CPU 等信息。不过，对于 open 系统调用来说，只知道它被调用了并不够，我们需要知道的是，进程到底在打开哪些文件。所以，实际应用中，我们还希望追踪时能显示这些函数的参数。</p><p>对于内核函数来说，你当然可以去查看内核源码，找出它的所有参数。不过还有更简单的方法，那就是直接从调试符号表中查询。执行下面的命令，你就可以知道 do_sys_open 的所有参数：</p><pre><code>$ perf probe -V do_sys_openAvailable variables at do_sys_open        @&lt;do_sys_open+0&gt;                char*   filename                int     dfd                int     flags                struct open_flags       op                umode_t mode</code></pre><p>从这儿可以看出，我们关心的文件路径，就是第一个字符指针参数（也就是字符串），参数名称为 filename。如果这个命令执行失败，就说明调试符号表还没有安装。那么，你可以执行下面的命令，安装调试信息后重试：</p><pre><code># Ubuntu$ apt-get install linux-image-`uname -r`-dbgsym# CentOS$ yum --enablerepo=base-debuginfo install -y kernel-debuginfo-$(uname -r)</code></pre><p>找出参数名称和类型后，就可以把参数加到探针中了。不过由于我们已经添加过同名探针，所以在这次添加前，需要先把旧探针给删掉：</p><pre><code># 先删除旧的探针perf probe --del probe:do_sys_open# 添加带参数的探针$ perf probe --add 'do_sys_open filename:string'Added new event:  probe:do_sys_open    (on do_sys_open with filename:string)You can now use it in all perf tools, such as:    perf record -e probe:do_sys_open -aR sleep 1    </code></pre><p>新的探针添加后，重新执行 record 和 script 子命令，采样并查看记录：</p><pre><code># 重新采样记录$ perf record -e probe:do_sys_open -aR ls# 查看结果$ perf script            perf 13593 [000] 91846.053622: probe:do_sys_open: (ffffffffa807b290) filename_string="/proc/13596/status"              ls 13596 [000] 91846.053995: probe:do_sys_open: (ffffffffa807b290) filename_string="/etc/ld.so.cache"              ls 13596 [000] 91846.054011: probe:do_sys_open: (ffffffffa807b290) filename_string="/lib/x86_64-linux-gnu/libselinux.so.1"              ls 13596 [000] 91846.054066: probe:do_sys_open: (ffffffffa807b290) filename_string="/lib/x86_64-linux-gnu/libc.so.6”              ...# 使用完成后不要忘记删除探针$ perf probe --del probe:do_sys_open</code></pre><p>现在，你就可以看到每次调用 open 时打开的文件了。不过，这个结果是不是看着很熟悉呢？</p><p>其实，在我们使用 strace 跟踪进程的系统调用时，也经常会看到这些动态库的影子。比如，使用 strace 跟踪 ls 时，你可以得到下面的结果：</p><pre><code>$ strace ls...access("/etc/ld.so.nohwcap", F_OK)      = -1 ENOENT (No such file or directory)access("/etc/ld.so.preload", R_OK)      = -1 ENOENT (No such file or directory)openat(AT_FDCWD, "/etc/ld.so.cache", O_RDONLY|O_CLOEXEC) = 3...access("/etc/ld.so.nohwcap", F_OK)      = -1 ENOENT (No such file or directory)openat(AT_FDCWD, "/lib/x86_64-linux-gnu/libselinux.so.1", O_RDONLY|O_CLOEXEC) = 3...    </code></pre><p>你估计在想，既然 strace 也能得到类似结果，本身又容易操作，为什么我们还要用 perf 呢？</p><p>实际上，很多人只看到了 strace 简单易用的好处，却忽略了它对进程性能带来的影响。从原理上来说，strace 基于系统调用 ptrace 实现，这就带来了两个问题。</p><ul><li>由于 ptrace 是系统调用，就需要在内核态和用户态切换。当事件数量比较多时，繁忙的切换必然会影响原有服务的性能；</li><li>ptrace 需要借助 SIGSTOP 信号挂起目标进程。这种信号控制和进程挂起，会影响目标进程的行为。</li></ul><p>所以，在性能敏感的应用（比如数据库）中，我并不推荐你用 strace （或者其他基于 ptrace 的性能工具）去排查和调试。</p><p>在 strace 的启发下，结合内核中的 utrace 机制， perf 也提供了一个 trace 子命令，是取代 strace 的首选工具。相对于 ptrace 机制来说，perf trace 基于内核事件，自然要比进程跟踪的性能好很多。</p><p>perf trace 的使用方法如下所示，跟 strace 其实很像：</p><pre><code>$ perf trace ls         ? (         ): ls/14234  ... [continued]: execve()) = 0     0.177 ( 0.013 ms): ls/14234 brk(                                                                  ) = 0x555d96be7000     0.224 ( 0.014 ms): ls/14234 access(filename: 0xad98082                                            ) = -1 ENOENT No such file or directory     0.248 ( 0.009 ms): ls/14234 access(filename: 0xad9add0, mode: R                                   ) = -1 ENOENT No such file or directory     0.267 ( 0.012 ms): ls/14234 openat(dfd: CWD, filename: 0xad98428, flags: CLOEXEC                  ) = 3     0.288 ( 0.009 ms): ls/14234 fstat(fd: 3&lt;/usr/lib/locale/C.UTF-8/LC_NAME&gt;, statbuf: 0x7ffd2015f230 ) = 0     0.305 ( 0.011 ms): ls/14234 mmap(len: 45560, prot: READ, flags: PRIVATE, fd: 3                    ) = 0x7efe0af92000     0.324 Dockerfile  test.sh( 0.008 ms): ls/14234 close(fd: 3&lt;/usr/lib/locale/C.UTF-8/LC_NAME&gt;                          ) = 0     ...     </code></pre><p>不过，perf trace 还可以进行系统级的系统调用跟踪（即跟踪所有进程），而 strace 只能跟踪特定的进程。</p><p><strong>第二个 perf 的例子是用户空间的库函数</strong>。以 bash 调用的库函数 readline 为例，使用类似的方法，可以跟踪库函数的调用（基于 uprobes）。</p><p>readline 的作用，是从终端中读取用户输入，并把这些数据返回调用方。所以，跟 open 系统调用不同的是，我们更关注 readline 的调用结果。</p><p>我们执行下面的命令，通过 -x 指定 bash 二进制文件的路径，就可以动态跟踪库函数。这其实就是跟踪了所有用户在 bash 中执行的命令：</p><pre><code># 为/bin/bash添加readline探针$ perf probe -x /bin/bash 'readline%return +0($retval):string’# 采样记录$ perf record -e probe_bash:readline__return -aR sleep 5# 查看结果$ perf script    bash 13348 [000] 93939.142576: probe_bash:readline__return: (5626ffac1610 &lt;- 5626ffa46739) arg1="ls"# 跟踪完成后删除探针$ perf probe --del probe_bash:readline__return</code></pre><p>当然，如果你不确定探针格式，也可以通过下面的命令，查询所有支持的函数和函数参数：</p><pre><code># 查询所有的函数$ perf probe -x /bin/bash —funcs# 查询函数的参数$ perf probe -x /bin/bash -V readlineAvailable variables at readline        @&lt;readline+0&gt;                char*   prompt                </code></pre><p>跟内核函数类似，如果你想要查看普通应用的函数名称和参数，那么在应用程序的二进制文件中，同样需要包含调试信息。</p><p><strong>eBPF 和 BCC</strong></p><p>ftrace 和 perf 的功能已经比较丰富了，不过，它们有一个共同的缺陷，那就是不够灵活，没法像 DTrace 那样通过脚本自由扩展。</p><p>而 eBPF 就是 Linux 版的 DTrace，可以通过 C 语言自由扩展（这些扩展通过 LLVM 转换为 BPF 字节码后，加载到内核中执行）。下面这张图，就表示了 eBPF 追踪的工作原理：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-522625bd0d7a4441.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>从图中你可以看到，eBPF 的执行需要三步：</p><ul><li>从用户跟踪程序生成 BPF 字节码；</li><li>加载到内核中运行；</li><li>向用户空间输出结果。</li></ul><p>所以，从使用上来说，eBPF 要比我们前面看到的 ftrace 和 perf ，都更加繁杂。</p><p>实际上，在 eBPF 执行过程中，编译、加载还有 maps 等操作，对所有的跟踪程序来说都是通用的。把这些过程通过 Python 抽象起来，也就诞生了 BCC（BPF Compiler Collection）。</p><p>BCC 把 eBPF 中的各种事件源（比如 kprobe、uprobe、tracepoint 等）和数据操作（称为 Maps），也都转换成了 Python 接口（也支持 lua）。这样，使用 BCC 进行动态追踪时，编写简单的脚本就可以了。</p><p>不过要注意，因为需要跟内核中的数据结构交互，真正核心的事件处理逻辑，还是需要我们用 C 语言来编写。</p><p>至于 BCC 的安装方法，在内存模块的缓存案例中，我就已经介绍过了。如果你还没有安装过，可以执行下面的命令来安装（其他系统的安装请参考这里）：</p><pre><code># Ubuntusudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDDecho "deb https://repo.iovisor.org/apt/$(lsb_release -cs) $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/iovisor.listsudo apt-get updatesudo apt-get install bcc-tools libbcc-examples linux-headers-$(uname -r)# REHL 7.6yum install bcc-tools</code></pre><p>安装后，BCC 会把所有示例（包括 Python 和 lua），放到 /usr/share/bcc/examples 目录中：</p><pre><code>$ ls /usr/share/bcc/exampleshello_world.py  lua  networking  tracing    </code></pre><p>接下来，还是以 do_sys_open 为例，我们一起来看看，如何用 eBPF 和 BCC 实现同样的动态跟踪。</p><p>通常，我们可以把 BCC 应用，拆分为下面这四个步骤。</p><p>第一，跟所有的 Python 模块使用方法一样，在使用之前，先导入要用到的模块：</p><pre><code>from bcc import BPF</code></pre><p>第二，需要定义事件以及处理事件的函数。这个函数需要用 C 语言来编写，作用是初始化刚才导入的 BPF 对象。这些用 C 语言编写的处理函数，要以字符串的形式送到 BPF 模块中处理：</p><pre><code># define BPF program (""" is used for multi-line string).# '#' indicates comments for python, while '//' indicates comments for C.prog = """#include &lt;uapi/linux/ptrace.h&gt;#include &lt;uapi/linux/limits.h&gt;#include &lt;linux/sched.h&gt;// define output data structure in Cstruct data_t {    u32 pid;    u64 ts;    char comm[TASK_COMM_LEN];    char fname[NAME_MAX];};BPF_PERF_OUTPUT(events);// define the handler for do_sys_open.// ctx is required, while other params depends on traced function.int hello(struct pt_regs *ctx, int dfd, const char __user *filename, int flags){    struct data_t data = {};    data.pid = bpf_get_current_pid_tgid();    data.ts = bpf_ktime_get_ns();    if (bpf_get_current_comm(&amp;data.comm, sizeof(data.comm)) == 0) {        bpf_probe_read(&amp;data.fname, sizeof(data.fname), (void *)filename);    }    events.perf_submit(ctx, &amp;data, sizeof(data));    return 0;}"""# load BPF programb = BPF(text=prog)# attach the kprobe for do_sys_open, and set handler to hellob.attach_kprobe(event="do_sys_open", fn_name="hello")</code></pre><p>第三步，是定义一个输出函数，并把输出函数跟 BPF 事件绑定：</p><pre><code># process eventstart = 0def print_event(cpu, data, size):    global start    # event’s type is data_t    event = b["events"].event(data)    if start == 0:            start = event.ts    time_s = (float(event.ts - start)) / 1000000000    print("%-18.9f %-16s %-6d %-16s" % (time_s, event.comm, event.pid, event.fname))# loop with callback to print_eventb["events"].open_perf_buffer(print_event)</code></pre><p>最后一步，就是执行事件循环，开始追踪 do_sys_open 的调用：</p><pre><code># print headerprint("%-18s %-16s %-6s %-16s" % ("TIME(s)", "COMM", "PID", "FILE”))# start the event polling loopwhile 1:    try:        b.perf_buffer_poll()    except KeyboardInterrupt:        exit()        </code></pre><p>我们把上面几个步骤的代码，保存到文件 trace-open.py 中，然后就可以用 Python 来运行了。如果一切正常，你可以看到如下输出：</p><pre><code>$ python trace-open.pyTIME(s)            COMM             PID    FILE0.000000000        irqbalance       1073   /proc/interrupts0.000175401        irqbalance       1073   /proc/stat0.000258802        irqbalance       1073   /proc/irq/9/smp_affinity0.000290102        irqbalance       1073   /proc/irq/0/smp_affinity </code></pre><p>从输出中，你可以看到 irqbalance 进程（你的环境中可能还会有其他进程）正在打开很多文件，而 irqbalance 依赖这些文件中读取的内容，来执行中断负载均衡。</p><p>通过这个简单的示例，你也可以发现，eBPF 和 BCC 的使用，其实比 ftrace 和 perf 有更高的门槛。想用 BCC 开发自己的动态跟踪程序，至少要熟悉 C 语言、Python 语言、被跟踪事件或函数的特征（比如内核函数的参数和返回格式）以及 eBPF 提供的各种数据操作方法。</p><p>不过，因为强大的灵活性，虽然 eBPF 在使用上有一定的门槛，却也无法阻止它成为目前最热门、最受关注的动态追踪技术。</p><p>当然，BCC 软件包也内置了很多已经开发好的实用工具，默认安装到 /usr/share/bcc/tools/ 目录中，它们的使用场景如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ed6b3d1eb593c0d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这些工具，一般都可以直接拿来用。而在编写其他的动态追踪脚本时，它们也是最好的参考资料。不过，有一点需要你特别注意，很多 eBPF 的新特性，都需要比较新的内核版本（如下图所示）。如果某些工具无法运行，很可能就是因为使用了当前内核不支持的特性。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5659e6440fd76bcd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>SystemTap 和 sysdig</strong></p><p>除了前面提到的 ftrace、perf、eBPF 和 BCC 外，SystemTap 和 sysdig 也是常用的动态追踪工具。</p><p>SystemTap 也是一种可以通过脚本进行自由扩展的动态追踪技术。在 eBPF 出现之前，SystemTap 是 Linux 系统中，功能最接近 DTrace 的动态追踪机制。不过要注意，SystemTap 在很长时间以来都游离于内核之外（而 eBPF 自诞生以来，一直根植在内核中）。</p><p>所以，从稳定性上来说，SystemTap 只在 RHEL 系统中好用，在其他系统中则容易出现各种异常问题。当然，反过来说，支持 3.x 等旧版本的内核，也是 SystemTap 相对于 eBPF 的一个巨大优势。</p><p>sysdig 则是随着容器技术的普及而诞生的，主要用于容器的动态追踪。sysdig 汇集了一些列性能工具的优势，可以说是集百家之所长。我习惯用这个公式来表示 sysdig 的特点： sysdig = strace + tcpdump + htop + iftop + lsof + docker inspect。</p><p>而在最新的版本中（内核版本 &gt;= 4.14），sysdig 还可以通过 eBPF 来进行扩展，所以，也可以用来追踪内核中的各种函数和事件。</p><h3 id="如何选择追踪工具"><a href="#如何选择追踪工具" class="headerlink" title="如何选择追踪工具"></a>如何选择追踪工具</h3><p>到这里，你可能又觉得头大了，这么多动态追踪工具，在实际场景中到底该怎么选择呢？还是那句话，具体性能工具的选择，就要从具体的工作原理来入手。</p><ul><li>在不需要很高灵活性的场景中，使用 perf 对性能事件进行采样，然后再配合火焰图辅助分析，就是最常用的一种方法；</li><li>而需要对事件或函数调用进行统计分析（比如观察不同大小的 I/O 分布）时，就要用 SystemTap 或者 eBPF，通过一些自定义的脚本来进行数据处理。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0e7088a34be0ef3d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="网络优化"><a href="#网络优化" class="headerlink" title="网络优化"></a>网络优化</h3><p><strong>连接数优化</strong></p><p>要查看 TCP 连接数的汇总情况，首选工具自然是 ss 命令。为了观察 wrk 测试时发生的问题，我们在终端二中再次启动 wrk，并且把总的测试时间延长到 30 分钟：</p><pre><code># 测试时间30分钟$ wrk --latency -c 1000 -d 1800 http://192.168.0.30</code></pre><p>然后，回到终端一中，观察 TCP 连接数：</p><pre><code>$ ss -sTotal: 177 (kernel 1565)TCP:   1193 (estab 5, closed 1178, orphaned 0, synrecv 0, timewait 1178/0), ports 0Transport Total     IP        IPv6*    1565      -         -RAW    1         0         1UDP    2         2         0TCP    15        12        3INET    18        14        4FRAG    0         0         0</code></pre><p>从这里看出，wrk 并发 1000 请求时，建立连接数只有 5，而 closed 和 timewait 状态的连接则有 1100 多 。其实从这儿你就可以发现两个问题：</p><ul><li>一个是建立连接数太少了；</li><li>另一个是 timewait 状态连接太多了。</li></ul><p>分析问题，自然要先从相对简单的下手。我们先来看第二个关于 timewait 的问题。在之前的 NAT 案例中，我已经提到过，内核中的连接跟踪模块，有可能会导致 timewait 问题。我们今天的案例还是基于 Docker 运行，而 Docker 使用的 <strong>iptables</strong> ，就会使用连接跟踪模块来管理 NAT。那么，怎么确认是不是连接跟踪导致的问题呢？</p><p>其实，最简单的方法，就是通过 dmesg 查看系统日志，如果有连接跟踪出了问题，应该会看到 nf_conntrack 相关的日志。</p><p>我们可以继续在终端一中，运行下面的命令，查看系统日志：</p><pre><code>$ dmesg | tail[88356.354329] nf_conntrack: nf_conntrack: table full, dropping packet[88356.354374] nf_conntrack: nf_conntrack: table full, dropping packet</code></pre><p>从日志中，你可以看到 <code>nf_conntrack</code>: table full, dropping packet 的错误日志。这说明，正是连接跟踪导致的问题。</p><p>这种情况下，我们应该想起前面学过的两个内核选项——连接跟踪数的最大限制 <code>nf_conntrack_max</code> ，以及当前的连接跟踪数 <code>nf_conntrack_count</code>。执行下面的命令，你就可以查询这两个选项：</p><pre><code>$ sysctl net.netfilter.nf_conntrack_maxnet.netfilter.nf_conntrack_max = 200$ sysctl net.netfilter.nf_conntrack_countnet.netfilter.nf_conntrack_count = 200</code></pre><p>这次的输出中，你可以看到最大的连接跟踪限制只有 200，并且全部被占用了。200 的限制显然太小，不过相应的优化也很简单，调大就可以了。</p><p>我们执行下面的命令，将 nf_conntrack_max 增大：</p><pre><code># 将连接跟踪限制增大到1048576$ sysctl -w net.netfilter.nf_conntrack_max=1048576</code></pre><p>连接跟踪限制增大后，对 Nginx 吞吐量的优化效果如何呢？我们不妨再来测试一下。你可以切换到终端二中，按下 Ctrl+C ；然后执行下面的 wrk 命令，重新测试 Nginx 的性能：</p><pre><code># 默认测试时间为10s，请求超时2s$ wrk --latency -c 1000 http://192.168.0.30...  54221 requests in 10.07s, 15.16MB read  Socket errors: connect 0, read 7, write 0, timeout 110  Non-2xx or 3xx responses: 45577Requests/sec:   5382.21Transfer/sec:      1.50MB</code></pre><p>从 wrk 的输出中，你可以看到，连接跟踪的优化效果非常好，吞吐量已经从刚才的 189 增大到了 5382。看起来性能提升了将近 30 倍，</p><p><strong>套接字优化</strong></p><p>回想一下网络性能的分析套路，以及 Linux 协议栈的原理，我们可以从从套接字、TCP 协议等逐层分析。而分析的第一步，自然还是要观察有没有发生丢包现象。</p><p>我们切换到终端二中，重新运行测试，这次还是要用 -d 参数延长测试时间，以便模拟性能瓶颈的现场：</p><pre><code># 测试时间30分钟$ wrk --latency -c 1000 -d 1800 http://192.168.0.30</code></pre><p>然后回到终端一中，观察有没有发生套接字的丢包现象：</p><pre><code># 只关注套接字统计$ netstat -s | grep socket    73 resets received for embryonic SYN_RECV sockets    308582 TCP sockets finished time wait in fast timer    8 delayed acks further delayed because of locked socket    290566 times the listen queue of a socket overflowed    290566 SYNs to LISTEN sockets dropped# 稍等一会，再次运行$ netstat -s | grep socket    73 resets received for embryonic SYN_RECV sockets    314722 TCP sockets finished time wait in fast timer    8 delayed acks further delayed because of locked socket    344440 times the listen queue of a socket overflowed    344440 SYNs to LISTEN sockets dropped    </code></pre><p>根据两次统计结果中 socket overflowed 和 sockets dropped 的变化，你可以看到，有大量的套接字丢包，并且丢包都是套接字队列溢出导致的。所以，接下来，我们应该分析连接队列的大小是不是有异常。</p><p>你可以执行下面的命令，查看套接字的队列大小：</p><pre><code>$ ss -ltnpState     Recv-Q     Send-Q            Local Address:Port            Peer Address:PortLISTEN    10         10                      0.0.0.0:80                   0.0.0.0:*         users:(("nginx",pid=10491,fd=6),("nginx",pid=10490,fd=6),("nginx",pid=10487,fd=6))LISTEN    7          10                            *:9000                       *:*         users:(("php-fpm",pid=11084,fd=9),...,("php-fpm",pid=10529,fd=7))</code></pre><p>这次可以看到，Nginx 和 php-fpm 的监听队列 （Send-Q）只有 10，而 nginx 的当前监听队列长度 （Recv-Q）已经达到了最大值，php-fpm 也已经接近了最大值。很明显，套接字监听队列的长度太小了，需要增大。</p><p>关于套接字监听队列长度的设置，既可以在应用程序中，通过套接字接口调整，也支持通过内核选项来配置。我们继续在终端一中，执行下面的命令，分别查询 Nginx 和内核选项对监听队列长度的配置：</p><pre><code># 查询nginx监听队列长度配置$ docker exec nginx cat /etc/nginx/nginx.conf | grep backlog        listen       80  backlog=10;# 查询php-fpm监听队列长度$ docker exec phpfpm cat /opt/bitnami/php/etc/php-fpm.d/www.conf | grep backlog; Set listen(2) backlog.;listen.backlog = 511# somaxconn是系统级套接字监听队列上限$ sysctl net.core.somaxconnnet.core.somaxconn = 10</code></pre><p>从输出中可以看到，Nginx 和 somaxconn 的配置都是 10，而 php-fpm 的配置也只有 511，显然都太小了。那么，优化方法就是增大这三个配置，比如，可以把 Nginx 和 php-fpm 的队列长度增大到 8192，而把 somaxconn 增大到 65536。</p><p>同样地，我也把这些优化后的 Nginx ，重新打包成了两个 Docker 镜像，你可以执行下面的命令来运行它：</p><pre><code># 停止旧的容器$ docker rm -f nginx phpfpm# 使用新镜像启动Nginx和PHP$ docker run --name nginx --network host --privileged -itd feisky/nginx-tp:2$ docker run --name phpfpm --network host --privileged -itd feisky/php-fpm-tp:2</code></pre><p>然后，切换到终端二中，重新测试 Nginx 的性能：</p><pre><code>$ wrk --latency -c 1000 http://192.168.0.30...  62247 requests in 10.06s, 18.25MB read  Non-2xx or 3xx responses: 62247Requests/sec:   6185.65Transfer/sec:      1.81MB</code></pre><p>现在的吞吐量已经增大到了 6185，并且在测试的时候，如果你在终端一中重新执行 netstat -s | grep socket，还会发现，现在已经没有套接字丢包问题了。</p><p>不过，这次 Nginx 的响应，再一次全部失败了，都是 Non-2xx or 3xx。这是怎么回事呢？我们再去终端一中，查看 Nginx 日志：</p><pre><code>$ docker logs nginx --tail 102019/03/15 16:52:39 [crit] 15#15: *999779 connect() to 127.0.0.1:9000 failed (99: Cannot assign requested address) while connecting to upstream, client: 192.168.0.2, server: localhost, request: "GET / HTTP/1.1", upstream: "fastcgi://127.0.0.1:9000", host: "192.168.0.30"</code></pre><p>你可以看到，Nginx 报出了无法连接 fastcgi 的错误，错误消息是 Connect 时， Cannot assign requested address。这个错误消息对应的错误代码为 EADDRNOTAVAIL，表示 IP 地址或者端口号不可用。</p><p>在这里，显然只能是端口号的问题。接下来，我们就来分析端口号的情况。</p><p><strong>端口号优化</strong></p><p>根据网络套接字的原理，当客户端连接服务器端时，需要分配一个临时端口号，而 Nginx 正是 PHP-FPM 的客户端。端口号的范围并不是无限的，最多也只有 6 万多。</p><p>我们执行下面的命令，就可以查询系统配置的临时端口号范围：</p><pre><code>$ sysctl net.ipv4.ip_local_port_rangenet.ipv4.ip_local_port_range=20000 20050</code></pre><p>你可以看到，临时端口的范围只有 50 个，显然太小了 。优化方法很容易想到，增大这个范围就可以了。比如，你可以执行下面的命令，把端口号范围扩展为 “10000 65535”：</p><p>优化完成后，我们再次切换到终端二中，测试性能：</p><pre><code>$ wrk --latency -c 1000 http://192.168.0.30/...  32308 requests in 10.07s, 6.71MB read  Socket errors: connect 0, read 2027, write 0, timeout 433  Non-2xx or 3xx responses: 30Requests/sec:   3208.58Transfer/sec:    682.15KB</code></pre><p>这次，异常的响应少多了 ，不过，吞吐量也下降到了 3208。并且，这次还出现了很多 Socket read errors。显然，还得进一步优化。</p><p><strong>火焰图</strong></p><p>我们不妨在终端二中，执行下面的命令，重新启动长时间测试：</p><pre><code># 测试时间30分钟$ wrk --latency -c 1000 -d 1800 http://192.168.0.30</code></pre><p>然后，切换回终端一中，执行 top ，观察 CPU 和内存的使用：</p><pre><code>$ top...%Cpu0  : 30.7 us, 48.7 sy,  0.0 ni,  2.3 id,  0.0 wa,  0.0 hi, 18.3 si,  0.0 st%Cpu1  : 28.2 us, 46.5 sy,  0.0 ni,  2.0 id,  0.0 wa,  0.0 hi, 23.3 si,  0.0 stKiB Mem :  8167020 total,  5867788 free,   490400 used,  1808832 buff/cacheKiB Swap:        0 total,        0 free,        0 used.  7361172 avail Mem  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND20379 systemd+  20   0   38068   8692   2392 R  36.1  0.1   0:28.86 nginx20381 systemd+  20   0   38024   8700   2392 S  33.8  0.1   0:29.29 nginx 1558 root      20   0 1118172  85868  39044 S  32.8  1.1  22:55.79 dockerd20313 root      20   0   11024   5968   3956 S  27.2  0.1   0:22.78 docker-containe13730 root      20   0       0      0      0 I   4.0  0.0   0:10.07 kworker/u4:0-ev</code></pre><p>从 top 的结果中可以看到，可用内存还是很充足的，但系统 CPU 使用率（sy）比较高，两个 CPU 的系统 CPU 使用率都接近 50%，且空闲 CPU 使用率只有 2%。再看进程部分，CPU 主要被两个 Nginx 进程和两个 docker 相关的进程占用，使用率都是 30% 左右。</p><p>CPU 使用率上升了，该怎么进行分析呢？我想，你已经还记得我们多次用到的 perf，再配合前两节讲过的火焰图，很容易就能找到系统中的热点函数。</p><p>我们保持终端二中的 wrk 继续运行；在终端一中，执行 perf 和 flamegraph 脚本，生成火焰图：</p><pre><code># 执行perf记录事件$ perf record -g# 切换到FlameGraph安装路径执行下面的命令生成火焰图$ perf script -i ~/perf.data | ./stackcollapse-perf.pl --all | ./flamegraph.pl &gt; nginx.svg</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5f03ffc4dd95d49e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>根据我们讲过的火焰图原理，这个图应该从下往上、沿着调用栈中最宽的函数，来分析执行次数最多的函数。</p><p>这儿中间的 do_syscall_64、tcp_v4_connect、inet_hash_connect 这个堆栈，很明显就是最需要关注的地方。inet_hash_connect() 是 Linux 内核中负责分配临时端口号的函数。所以，这个瓶颈应该还在临时端口的分配上。</p><p>在上一步的“端口号”优化中，临时端口号的范围，已经优化成了 “10000 65535”。这显然是一个非常大的范围，那么，端口号的分配为什么又成了瓶颈呢？</p><p>一时想不到也没关系，我们可以暂且放下，先看看其他因素的影响。再顺着 inet_hash_connect 往堆栈上面查看，下一个热点是 __init_check_established 函数。而这个函数的目的，是检查端口号是否可用。结合这一点，你应该可以想到，如果有大量连接占用着端口，那么检查端口号可用的函数，不就会消耗更多的 CPU 吗？</p><p>实际是否如此呢？我们可以继续在终端一中运行 ss 命令， 查看连接状态统计：</p><pre><code>$ ss -sTCP:   32775 (estab 1, closed 32768, orphaned 0, synrecv 0, timewait 32768/0), ports 0...</code></pre><p>这回可以看到，有大量连接（这儿是 32768）处于 timewait 状态，而 timewait 状态的连接，本身会继续占用端口号。如果这些端口号可以重用，那么自然就可以缩短 __init_check_established 的过程。而 Linux 内核中，恰好有一个 tcp_tw_reuse 选项，用来控制端口号的重用。</p><p>我们在终端一中，运行下面的命令，查询它的配置：</p><pre><code>$ sysctl net.ipv4.tcp_tw_reusenet.ipv4.tcp_tw_reuse = 0</code></pre><p>你可以看到，tcp_tw_reuse 是 0，也就是禁止状态。其实看到这里，我们就能理解，为什么临时端口号的分配会是系统运行的热点了。当然，优化方法也很容易，把它设置成 1 就可以开启了。</p><p>我把优化后的应用，也打包成了两个 Docker 镜像，你可以执行下面的命令来运行：</p><pre><code># 停止旧的容器$ docker rm -f nginx phpfpm# 使用新镜像启动Nginx和PHP$ docker run --name nginx --network host --privileged -itd feisky/nginx-tp:3$ docker run --name phpfpm --network host --privileged -itd feisky/php-fpm-tp:3</code></pre><p>容器启动后，切换到终端二中，再次测试优化后的效果：</p><pre><code>$ wrk --latency -c 1000 http://192.168.0.30/...  52119 requests in 10.06s, 10.81MB read  Socket errors: connect 0, read 850, write 0, timeout 0Requests/sec:   5180.48Transfer/sec:      1.07MB</code></pre><p>现在的吞吐量已经达到了 5000 多，并且只有少量的 Socket errors，也不再有 Non-2xx or 3xx 的响应了。说明一切终于正常了。</p><p><strong>系统监控综合思路</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-91c57b2ad677a106.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>监控系统 Prometheus</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-cacdae28d210cee6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="性能分析-总结"><a href="#性能分析-总结" class="headerlink" title="性能分析-总结"></a>性能分析-总结</h3><p><strong>CPU 性能分析</strong></p><p>还记得这张图吗？利用 top、vmstat、pidstat、strace 以及 perf 等几个最常见的工具，获取 CPU 性能指标后，再结合进程与 CPU 的工作原理，就可以迅速定位出 CPU 性能瓶颈的来源。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d5511898fbfb2ab8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>实际上，top、pidstat、vmstat 这类工具所汇报的 CPU 性能指标，都源自 /proc 文件系统（比如 /proc/loadavg、/proc/stat、/proc/softirqs 等）。这些指标，都应该通过监控系统监控起来。虽然并非所有指标都需要报警，但这些指标却可以加快性能问题的定位分析。</p><p>比如说，当你收到系统的用户 CPU 使用率过高告警时，从监控系统中直接查询到，导致 CPU 使用率过高的进程；然后再登录到进程所在的 Linux 服务器中，分析该进程的行为。</p><p>你可以使用 strace，查看进程的系统调用汇总；也可以使用 perf 等工具，找出进程的热点函数；甚至还可以使用动态追踪的方法，来观察进程的当前执行过程，直到确定瓶颈的根源。</p><p><strong>内存性能分析</strong></p><p>说完了 CPU 的性能分析，再来看看第二种系统资源，即内存。关于内存性能的分析方法，我在如何“快准狠”找到系统内存的问题中，也已经为你整理了一个快速分析的思路。</p><p>下面这张图，就是一个迅速定位内存瓶颈的流程。我们可以通过 free 和 vmstat 输出的性能指标，确认内存瓶颈；然后，再根据内存问题的类型，进一步分析内存的使用、分配、泄漏以及缓存等，最后找出问题的来源。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-aed72d20fefe2d5a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>同 CPU 性能一样，很多内存的性能指标，也来源于 /proc 文件系统（比如 /proc/meminfo、/proc/slabinfo 等），它们也都应该通过监控系统监控起来。这样，当你收到内存告警时，就可以从监控系统中，直接得到上图中的各项性能指标，从而加快性能问题的定位过程。</p><p><strong>磁盘和文件系统 I/O 性能分析</strong></p><p>接下来，我们再来看第三种系统资源，即磁盘和文件系统的 I/O。关于磁盘和文件系统的 I/O 性能分析方法，我在如何迅速分析出系统 I/O 的瓶颈中也已经为你整理了一个快速分析的思路。</p><p>我们来看下面这张图。当你使用 iostat ，发现磁盘 I/O 存在性能瓶颈（比如 I/O 使用率过高、响应时间过长或者等待队列长度突然增大等）后，可以再通过 pidstat、 vmstat 等，确认 I/O 的来源。接着，再根据来源的不同，进一步分析文件系统和磁盘的使用率、缓存以及进程的 I/O 等，从而揪出 I/O 问题的真凶。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-024f56c912d64287.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>同 CPU 和内存性能类似，很多磁盘和文件系统的性能指标，也来源于 /proc 和 /sys 文件系统（比如 /proc/diskstats、/sys/block/sda/stat 等）。自然，它们也应该通过监控系统监控起来。这样，当你收到 I/O 性能告警时，就可以从监控系统中，直接得到上图中的各项性能指标，从而加快性能定位的过程。</p><p>比如说，当你发现某块磁盘的 I/O 使用率为 100% 时，首先可以从监控系统中，找出 I/O 最多的进程。然后，再登录到进程所在的 Linux 服务器中，借助 strace、lsof、perf 等工具，分析该进程的 I/O 行为。最后，再结合应用程序的原理，找出大量 I/O 的原因</p><p><strong>网络性能分析</strong></p><p>而要分析网络的性能，自然也是要从这几个协议层入手，通过使用率、饱和度以及错误数这几类性能指标，观察是否存在性能问题。比如 ：</p><ul><li>在链路层，可以从网络接口的吞吐量、丢包、错误以及软中断和网络功能卸载等角度分析；</li><li>在网络层，可以从路由、分片、叠加网络等角度进行分析；</li><li>在传输层，可以从 TCP、UDP 的协议原理出发，从连接数、吞吐量、延迟、重传等角度进行分析；</li><li>在应用层，可以从应用层协议（如 HTTP 和 DNS）、请求数（QPS）、套接字缓存等角度进行分析。</li></ul><p>同前面几种资源类似，网络的性能指标也都来源于内核，包括 /proc 文件系统（如 /proc/net）、网络接口以及 conntrack 等内核模块。这些指标同样需要被监控系统监控。这样，当你收到网络告警时，就可以从监控系统中，查询这些协议层的各项性能指标，从而更快定位出性能问题。</p><p>比如，当你收到网络不通的告警时，就可以从监控系统中，查找各个协议层的丢包指标，确认丢包所在的协议层。然后，从监控系统的数据中，确认网络带宽、缓冲区、连接跟踪数等软硬件，是否存在性能瓶颈。最后，再登录到发生问题的 Linux 服务器中，借助 netstat、tcpdump、bcc 等工具，分析网络的收发数据，并且结合内核中的网络选项以及 TCP 等网络协议的原理，找出问题的来源。</p><p><strong>应用程序瓶颈</strong></p><p>如果这些手段过后还是无法找出瓶颈，你还可以用系统资源模块提到的各类进程分析工具，来进行分析定位。比如：</p><ul><li>你可以用 strace，观察系统调用；</li><li>使用 perf 和火焰图，分析热点函数；</li><li>甚至使用动态追踪技术，来分析进程的执行状态。</li></ul><h3 id="性能优化-总结"><a href="#性能优化-总结" class="headerlink" title="性能优化-总结"></a>性能优化-总结</h3><p><strong>CPU 优化</strong></p><p>首先来看 CPU 性能的优化方法。在CPU 性能优化的几个思路中，我曾经介绍过，CPU 性能优化的核心，在于排除所有不必要的工作、充分利用 CPU 缓存并减少进程调度对性能的影响。</p><p>从这几个方面出发，我相信你已经想到了很多的优化方法。这里，我主要强调一下，最典型的三种优化方法。</p><ul><li>第一种，把进程绑定到一个或者多个 CPU 上，充分利用 CPU 缓存的本地性，并减少进程间的相互影响。</li><li>第二种，为中断处理程序开启多 CPU 负载均衡，以便在发生大量中断时，可以充分利用多 CPU 的优势分摊负载。</li><li>第三种，使用 Cgroups 等方法，为进程设置资源限制，避免个别进程消耗过多的 CPU。同时，为核心应用程序设置更高的优先级，减少低优先级任务的影响。</li></ul><p><strong>内存优化</strong></p><p>说完了 CPU 的性能优化，我们再来看看，怎么优化内存的性能。在如何“快准狠”找到系统内存的问题中，我曾经为你梳理了常见的一些内存问题，比如可用内存不足、内存泄漏、Swap 过多、缺页异常过多以及缓存过多等等。所以，说白了，内存性能的优化，也就是要解决这些内存使用的问题。</p><p>在我看来，你可以通过以下几种方法，来优化内存的性能。</p><ul><li>第一种，除非有必要，Swap 应该禁止掉。这样就可以避免 Swap 的额外 I/O ，带来内存访问变慢的问题。</li><li>第二种，使用 Cgroups 等方法，为进程设置内存限制。这样就可以避免个别进程消耗过多内存，而影响了其他进程。对于核心应用，还应该降低 oom_score，避免被 OOM 杀死。</li><li>第三种，使用大页、内存池等方法，减少内存的动态分配，从而减少缺页异常。</li></ul><p><strong>磁盘和文件系统 I/O 优化</strong></p><p>接下来，我们再来看第三类系统资源，即磁盘和文件系统 I/O 的优化方法。在磁盘 I/O 性能优化的几个思路 中，我已经为你梳理了一些常见的优化思路，这其中有三种最典型的方法。</p><ul><li>第一种，也是最简单的方法，通过 SSD 替代 HDD、或者使用 RAID 等方法，提升 I/O 性能。</li><li>第二种，针对磁盘和应用程序 I/O 模式的特征，选择最适合的 I/O 调度算法。比如，SSD 和虚拟机中的磁盘，通常用的是 noop 调度算法；而数据库应用，更推荐使用 deadline 算法。</li><li>第三，优化文件系统和磁盘的缓存、缓冲区，比如优化脏页的刷新频率、脏页限额，以及内核回收目录项缓存和索引节点缓存的倾向等等。</li></ul><p><strong>网络优化</strong></p><p>最后一个是网络的性能优化。在网络性能优化的几个思路中，我也已经为你梳理了一些常见的优化思路。这些优化方法都是从 Linux 的网络协议栈出发，针对每个协议层的工作原理进行优化。这里，我同样强调一下，最典型的几种网络优化方法。</p><p>首先，从内核资源和网络协议的角度来说，我们可以对内核选项进行优化，比如：</p><ul><li>你可以增大套接字缓冲区、连接跟踪表、最大半连接数、最大文件描述符数、本地端口范围等内核资源配额；</li><li>也可以减少 TIMEOUT 超时时间、SYN+ACK 重传数、Keepalive 探测时间等异常处理参数；</li><li>还可以开启端口复用、反向地址校验，并调整 MTU 大小等降低内核的负担。</li></ul><p>这些都是内核选项优化的最常见措施。</p><p>其次，从网络接口的角度来说，我们可以考虑对网络接口的功能进行优化，比如：</p><ul><li>你可以将原来 CPU 上执行的工作，卸载到网卡中执行，即开启网卡的 GRO、GSO、RSS、VXLAN 等卸载功能；</li><li>也可以开启网络接口的多队列功能，这样，每个队列就可以用不同的中断号，调度到不同 CPU 上执行；</li><li>还可以增大网络接口的缓冲区大小以及队列长度等，提升网络传输的吞吐量。</li></ul><p>最后，在极限性能情况（比如 C10M）下，内核的网络协议栈可能是最主要的性能瓶颈，所以，一般会考虑绕过内核协议栈。</p><ul><li>你可以使用 DPDK 技术，跳过内核协议栈，直接由用户态进程用轮询的方式，来处理网络请求。同时，再结合大页、CPU 绑定、内存对齐、流水线并发等多种机制，优化网络包的处理效率。</li><li>你还可以使用内核自带的 XDP 技术，在网络包进入内核协议栈前，就对其进行处理。这样，也可以达到目的，获得很好的性能。</li></ul><p><strong>应用程序优化</strong></p><p>虽然系统的软硬件资源，是保证应用程序正常运行的基础，但你要知道，性能优化的最佳位置，还是应用程序内部。为什么这么说呢？我简单举两个例子你就明白了。</p><p>第一个例子，是系统 CPU 使用率（sys%）过高的问题。有时候出现问题，虽然表面现象是系统 CPU 使用率过高，但待你分析过后，很可能会发现，应用程序的不合理系统调用才是罪魁祸首。这种情况下，优化应用程序内部系统调用的逻辑，显然要比优化内核要简单也有用得多。</p><p>再比如说，数据库的 CPU 使用率高、I/O 响应慢，也是最常见的一种性能问题。这种问题，一般来说，并不是因为数据库本身性能不好，而是应用程序不合理的表结构或者 SQL 查询语句导致的。这时候，优化应用程序中数据库表结构的逻辑或者 SQL 语句，显然要比优化数据库本身，能带来更大的收益。</p><p>所以，在观察性能指标时，你应该先查看应用程序的响应时间、吞吐量以及错误率等指标，因为它们才是性能优化要解决的终极问题。以终为始，从这些角度出发，你一定能想到很多优化方法，而我比较推荐下面几种方法。</p><ul><li>第一，从 CPU 使用的角度来说，简化代码、优化算法、异步处理以及编译器优化等，都是常用的降低 CPU 使用率的方法，这样可以利用有限的 CPU 处理更多的请求。</li><li>第二，从数据访问的角度来说，使用缓存、写时复制、增加 I/O 尺寸等，都是常用的减少磁盘 I/O 的方法，这样可以获得更快的数据处理速度。</li><li>第三，从内存管理的角度来说，使用大页、内存池等方法，可以预先分配内存，减少内存的动态分配，从而更好地内存访问性能。</li><li>第四，从网络的角度来说，使用 I/O 多路复用、长连接代替短连接、DNS 缓存等方法，可以优化网络 I/O 并减少网络请求数，从而减少网络延时带来的性能问题。</li><li>第五，从进程的工作模型来说，异步处理、多线程或多进程等，可以充分利用每一个 CPU 的处理能力，从而提高应用程序的吞吐能力。</li></ul><p>除此之外，你还可以使用消息队列、CDN、负载均衡等各种方法，来优化应用程序的架构，将原来单机要承担的任务，调度到多台服务器中并行处理。这样也往往能获得更好的整体性能。</p><h3 id="工具速查-总结"><a href="#工具速查-总结" class="headerlink" title="工具速查-总结"></a>工具速查-总结</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2674e8f83d4f8859.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>CPU 性能工具</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9e93296133bffae5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-49fc3cb8685b4b22.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>内存性能工具</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-db047c3df08dc3ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fff71a4964939a56.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>磁盘 I/O 性能工具</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-dcbd6e63b32a0bf6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c7bce9b381a9a14b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>网络性能工具</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-be681c3a3d7e2564.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5c9e95fb5a8ebb84.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>基准测试工具</strong></p><p>除了性能分析外，很多时候，我们还需要对系统性能进行基准测试。比如，</p><ul><li>在文件系统和磁盘 I/O 模块中，我们使用 fio 工具，测试了磁盘 I/O 的性能。</li><li>在网络模块中，我们使用 iperf、pktgen 等，测试了网络的性能。</li><li>而在很多基于 Nginx 的案例中，我们则使用 ab、wrk 等，测试 Nginx 应用的性能。</li></ul><p>除了专栏里介绍过的这些工具外，对于 Linux 的各个子系统来说，还有很多其他的基准测试工具可能会用到。下面这张图，是 Brendan Gregg 整理的 Linux 基准测试工具图谱，你可以保存下来，在需要时参考。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c131651a7d139684.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>]]></content>
      
      
      <categories>
          
          <category> System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《趣谈网络协议》</title>
      <link href="/2020/08/25/note/happy-talk-net/"/>
      <url>/2020/08/25/note/happy-talk-net/</url>
      
        <content type="html"><![CDATA[<h2 id="通信协议综述"><a href="#通信协议综述" class="headerlink" title="通信协议综述"></a>通信协议综述</h2><h3 id="IP-amp-amp-MAC"><a href="#IP-amp-amp-MAC" class="headerlink" title="IP &amp;&amp; MAC"></a>IP &amp;&amp; MAC</h3><p>IP 地址是一个网卡在网络世界的通讯地址，相当于我们现实世界的门牌号码。既然是门牌号码，不能大家都一样，不然就会起冲突。比方说，假如大家都叫六单元 1001 号，那快递就找不到地方了。所以，有时候咱们的电脑弹出网络地址冲突，出现上不去网的情况，多半是 IP 地址冲突了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-90a27f35483d8a00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>下面这个表格，详细地展示了 A、B、C 三类地址所能包含的主机的数量。在后文中，我也会多次借助这个表格来讲解。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7d6a468a197f8230.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>无类型域间选路（CIDR）</strong></p><p>于是有了一个折中的方式叫作无类型域间选路，简称 CIDR。这种方式打破了原来设计的几类地址的做法，将 32 位的 IP 地址一分为二，前面是网络号，后面是主机号。从哪里分呢？你如果注意观察的话可以看到，10.100.122.2/24，这个 IP 地址中有一个斜杠，斜杠后面有个数字 24。这种地址表示形式，就是 CIDR。后面 24 的意思是，32 位中，前 24 位是网络号，后 8 位是主机号。</p><p>伴随着 CIDR 存在的，一个是<strong>广播地址</strong>，10.100.122.255。如果发送这个地址，所有 10.100.122 网络里面的机器都可以收到。另一个是子<strong>网掩码</strong>，255.255.255.0。</p><p>将子网掩码和 IP 地址进行 AND 计算。前面三个 255，转成二进制都是 1。1 和任何数值取 AND，都是原来数值，因而前三个数不变，为 10.100.122。后面一个 0，转换成二进制是 0，0 和任何数值取 AND，都是 0，因而最后一个数变为 0，合起来就是 10.100.122.0。这就是网络号。<strong>将子网掩码和 IP 地址按位计算 AND，就可得到网络号</strong>。</p><p><strong>MAC 地址</strong></p><p>在 IP 地址的上一行是 link/ether fa:16:3e:c7:79:75 brd ff:ff:ff:ff:ff:ff，这个被称为 MAC 地址，是一个网卡的物理地址，用十六进制，6 个 byte 表示。</p><p>MAC 地址是一个很容易让人“误解”的地址。因为 MAC 地址号称全局唯一，不会有两个网卡有相同的 MAC 地址，而且网卡自生产出来，就带着这个地址。很多人看到这里就会想，既然这样，整个互联网的通信，全部用 MAC 地址好了，只要知道了对方的 MAC 地址，就可以把信息传过去。</p><p>这样当然是不行的。 <strong>一个网络包要从一个地方传到另一个地方，除了要有确定的地址，还需要有定位功能</strong>。 而有门牌号码属性的 IP 地址，才是有远程定位功能的。</p><p><strong>网络设备的状态标识</strong></p><p>解析完了 MAC 地址，我们再来看 是干什么的？这个叫做 <code>net_device</code> flags，网络设备的状态标识。</p><p>UP 表示网卡处于启动的状态；BROADCAST 表示这个网卡有广播地址，可以发送广播包；MULTICAST 表示网卡可以发送多播包；LOWER_UP 表示 L1 是启动的，也即网线插着呢。MTU1500 是指什么意思呢？是哪一层的概念呢？最大传输单元 MTU 为 1500，这是以太网的默认值。</p><p>上一节，我们讲过网络包是层层封装的。MTU 是二层 MAC 层的概念。MAC 层有 MAC 的头，以太网规定正文部分不允许超过 1500 个字节。正文里面有 IP 的头、TCP 的头、HTTP 的头。如果放不下，就需要分片来传输。</p><p>qdisc <code>pfifo_fast</code> 是什么意思呢？<strong>qdisc 全称是 queueing discipline，中文叫排队规则。</strong>内核如果需要通过某个网络接口发送数据包，它都需要按照为这个接口配置的 qdisc（排队规则）把数据包加入队列。</p><p>最简单的 qdisc 是 pfifo，它不对进入的数据包做任何的处理，数据包采用先入先出的方式通过队列。pfifo_fast 稍微复杂一些，它的队列包括三个波段（band）。在每个波段里面，使用先进先出规则。</p><p>三个波段（band）的优先级也不相同。band 0 的优先级最高，band 2 的最低。如果 band 0 里面有数据包，系统就不会处理 band 1 里面的数据包，band 1 和 band 2 之间也是一样。</p><p>数据包是按照服务类型（Type of Service，TOS）被分配到三个波段（band）里面的。TOS 是 IP 头里面的一个字段，代表了当前的包是高优先级的，还是低优先级的。</p><h3 id="预启动执行环境（PXE）"><a href="#预启动执行环境（PXE）" class="headerlink" title="预启动执行环境（PXE）"></a>预启动执行环境（PXE）</h3><p>那我们安装操作系统的过程，只能插在 BIOS 启动之后了。因为没安装系统之前，连启动扇区都没有。因而这个过程叫做预启动执行环境（Pre-boot Execution Environment），简称 PXE。</p><p>PXE 协议分为客户端和服务器端，由于还没有操作系统，只能先把客户端放在 BIOS 里面。当计算机启动时，BIOS 把 PXE 客户端调入内存里面，就可以连接到服务端做一些操作了。</p><p>首先，PXE 客户端自己也需要有个 IP 地址。因为 PXE 的客户端启动起来，就可以发送一个 DHCP 的请求，让 DHCP Server 给它分配一个地址。PXE 客户端有了自己的地址，那它怎么知道 PXE 服务器在哪里呢？对于其他的协议，都好办，要有人告诉他。例如，告诉浏览器要访问的 IP 地址，或者在配置中告诉它；例如，微服务之间的相互调用。</p><p>但是 PXE 客户端启动的时候，啥都没有。好在 DHCP Server 除了分配 IP 地址以外，还可以做一些其他的事情。这里有一个 DHCP Server 的一个样例配置：</p><pre><code>ddns-update-style interim;ignore client-updates;allow booting;allow bootp;subnet 192.168.1.0 netmask 255.255.255.0{option routers 192.168.1.1;option subnet-mask 255.255.255.0;option time-offset -18000;default-lease-time 21600;max-lease-time 43200;range dynamic-bootp 192.168.1.240 192.168.1.250;filename "pxelinux.0";next-server 192.168.1.180;}</code></pre><p>按照上面的原理，默认的 DHCP Server 是需要配置的，无非是我们配置 IP 的时候所需要的 IP 地址段、子网掩码、网关地址、租期等。如果想使用 PXE，则需要配置 next-server，指向 PXE 服务器的地址，另外要配置初始启动文件 filename。</p><p>这样 PXE 客户端启动之后，发送 DHCP 请求之后，除了能得到一个 IP 地址，还可以知道 PXE 服务器在哪里，也可以知道如何从 PXE 服务器上下载某个文件，去初始化操作系统。</p><p><strong>解析 PXE 的工作过程</strong></p><p>首先，启动 PXE 客户端。第一步是通过 DHCP 协议告诉 DHCP Server，我刚来，一穷二白，啥都没有。DHCP Server 便租给它一个 IP 地址，同时也给它 PXE 服务器的地址、启动文件 pxelinux.0。</p><p>其次，PXE 客户端知道要去 PXE 服务器下载这个文件后，就可以初始化机器。于是便开始下载，下载的时候使用的是 TFTP 协议。所以 PXE 服务器上，往往还需要有一个 TFTP 服务器。PXE 客户端向 TFTP 服务器请求下载这个文件，TFTP 服务器说好啊，于是就将这个文件传给它。</p><p>然后，PXE 客户端收到这个文件后，就开始执行这个文件。这个文件会指示 PXE 客户端，向 TFTP 服务器请求计算机的配置信息 pxelinux.cfg。TFTP 服务器会给 PXE 客户端一个配置文件，里面会说内核在哪里、initramfs 在哪里。PXE 客户端会请求这些文件。</p><p>最后，启动 Linux 内核。一旦启动了操作系统，以后就啥都好办了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-23947aa222844158.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="物理层和数据链路层"><a href="#物理层和数据链路层" class="headerlink" title="物理层和数据链路层"></a>物理层和数据链路层</h2><h3 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h3><p><strong>第一层（物理层）</strong></p><p>我们要的是电脑连电脑。这种方式就是一根网线，有两个头。一头插在一台电脑的网卡上，另一头插在另一台电脑的网卡上。但是在当时，普通的网线这样是通不了的，所以水晶头要做交叉线，用的就是所谓的 1－3、2－6 交叉接法。</p><p>水晶头的第 1、2 和第 3、6 脚，它们分别起着收、发信号的作用。将一端的 1 号和 3 号线、2 号和 6 号线互换一下位置，就能够在物理层实现一端发送的信号，另一端能收到。</p><p>当然电脑连电脑，除了网线要交叉，还需要配置这两台电脑的 IP 地址、子网掩码和默认网关。这三个概念上一节详细描述过了。要想两台电脑能够通信，这三项必须配置成为一个网络，可以一个是 192.168.0.1/24，另一个是 192.168.0.2/24，否则是不通的。</p><p>到此为止，两台电脑已经构成了一个最小的局域网，也即 LAN。可以玩联机局域网游戏啦！</p><p>等到第三个哥们也买了一台电脑，怎么把三台电脑连在一起呢？</p><p>先别说交换机，当时交换机也贵。有一个叫做 Hub 的东西，也就是集线器。这种设备有多个口，可以将宿舍里的多台电脑连接起来。但是，和交换机不同，集线器没有大脑，它完全在物理层工作。它会将自己收到的每一个字节，都复制到其他端口上去。这是第一层物理层联通的方案。</p><p><strong>第二层（数据链路层）</strong></p><p>你可能已经发现问题了。Hub 采取的是广播的模式，如果每一台电脑发出的包，宿舍的每个电脑都能收到，那就麻烦了。这就需要解决几个问题：</p><ol><li>这个包是发给谁的？谁应该接收？</li><li>大家都在发，会不会产生混乱？有没有谁先发、谁后发的规则？</li><li>如果发送的时候出现了错误，怎么办？</li></ol><p>这几个问题，都是第二层，数据链路层，也即 MAC 层要解决的问题。MAC 的全称是 <strong>Medium Access Control</strong>，即媒体访问控制。控制什么呢？<strong>其实就是控制在往媒体上发数据的时候，谁先发、谁后发的问题。防止发生混乱。这解决的是第二个问题。这个问题中的规则，学名叫多路访问</strong>。有很多算法可以解决这个问题。就像车管所管束马路上跑的车，能想的办法都想过了。</p><p>比如接下来这三种方式：</p><ul><li>方式一：分多个车道。每个车一个车道，你走你的，我走我的。这在计算机网络里叫作信道划分；</li><li>方式二：今天单号出行，明天双号出行，轮着来。这在计算机网络里叫作轮流协议；</li><li>方式三：不管三七二十一，有事儿先出门，发现特堵，就回去。错过高峰再出。我们叫作随机接入协议。著名的以太网，用的就是这个方式。</li></ul><p>接下来要解决第一个问题：发给谁，谁接收？这里用到一个物理地址，叫作链路层地址。但是因为第二层主要解决媒体接入控制的问题，所以它常被称为MAC 地址。</p><p>解决第一个问题就牵扯到第二层的网络包格式。对于以太网，第二层的最开始，就是目标的 MAC 地址和源的 MAC 地址。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3e57880f718598e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来是类型，大部分的类型是 IP 数据包，然后 IP 里面包含 TCP、UDP，以及 HTTP 等，这都是里层封装的事情。</p><p>有了这个目标 MAC 地址，数据包在链路上广播，MAC 的网卡才能发现，这个包是给它的。MAC 的网卡把包收进来，然后打开 IP 包，发现 IP 地址也是自己的，再打开 TCP 包，发现端口是自己，也就是 80，而 nginx 就是监听 80。</p><p>于是将请求提交给 nginx，nginx 返回一个网页。然后将网页需要发回请求的机器。然后层层封装，最后到 MAC 层。因为来的时候有源 MAC 地址，返回的时候，源 MAC 就变成了目标 MAC，再返给请求的机器。</p><p>对于以太网，第二层的最后面是 <strong>CRC，也就是循环冗余检测</strong>。<strong>通过 XOR 异或的算法</strong>，来计算整个包是否在发送的过程中出现了错误，主要解决第三个问题。</p><p><strong>ARP</strong></p><p>这里还有一个没有解决的问题，当源机器知道目标机器的时候，可以将目标地址放入包里面，如果不知道呢？一个广播的网络里面接入了 N 台机器，我怎么知道每个 MAC 地址是谁呢？<strong>这就是 ARP 协议，也就是已知 IP 地址，求 MAC 地址的协议</strong>。</p><p>在一个局域网里面，当知道了 IP 地址，不知道 MAC 怎么办呢？靠“吼”。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-27e03169cd8f7f9d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>广而告之，发送一个广播包，谁是这个 IP 谁来回答。具体询问和回答的报文就像下面这样：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e4e518f4aafb6037.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>为了避免每次都用 ARP 请求，机器本地也会进行 ARP 缓存。当然机器会不断地上线下线，IP 也可能会变，所以 ARP 的 MAC 地址缓存过一段时间就会过期。</p><p><strong>交换机</strong></p><p>谁能知道目标 MAC 地址是否就是连接某个口的电脑的 MAC 地址呢？这就需要一个能把 MAC 头拿下来，检查一下目标 MAC 地址，然后根据策略转发的设备，按第二节课中讲过的，这个设备显然是个二层设备，我们称为交换机。</p><p>交换机怎么知道每个口的电脑的 MAC 地址呢？这需要交换机会学习。</p><p>一台 MAC1 电脑将一个包发送给另一台 MAC2 电脑，当这个包到达交换机的时候，一开始交换机也不知道 MAC2 的电脑在哪个口，所以没办法，它只能将包转发给除了来的那个口之外的其他所有的口。但是，这个时候，交换机会干一件非常聪明的事情，就是交换机会记住，MAC1 是来自一个明确的口。以后有包的目的地址是 MAC1 的，直接发送到这个口就可以了。</p><p>当交换机作为一个关卡一样，过了一段时间之后，就有了整个网络的一个结构了，这个时候，基本上不用广播了，全部可以准确转发。当然，每个机器的 IP 地址会变，所在的口也会变，因而交换机上的学习的结果，我们称为<strong>转发表</strong>，是有一个过期时间的。</p><h3 id="STP-协议"><a href="#STP-协议" class="headerlink" title="STP 协议"></a>STP 协议</h3><p>在数据结构中，有一个方法叫做最小生成树。有环的我们常称为图。将图中的环破了，就生成了树。在计算机网络中，生成树的算法叫作 STP，全称 Spanning Tree Protocol。</p><p>STP 协议比较复杂，一开始很难看懂，但是其实这是一场血雨腥风的武林比武或者华山论剑，最终决出五岳盟主的方式。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3abf8c836b540bf4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在 STP 协议里面有很多概念，译名就非常拗口，但是我一作比喻，你很容易就明白了。</p><ul><li>Root Bridge，也就是根交换机。这个比较容易理解，可以比喻为“掌门”交换机，是某棵树的老大，是掌门，最大的大哥。</li><li>Designated Bridges，有的翻译为指定交换机。这个比较难理解，可以想像成一个“小弟”，对于树来说，就是一棵树的树枝。所谓“指定”的意思是，我拜谁做大哥，其他交换机通过这个交换机到达根交换机，也就相当于拜他做了大哥。这里注意是树枝，不是叶子，因为叶子往往是主机。</li><li>Bridge Protocol Data Units （BPDU） ，网桥协议数据单元。可以比喻为“相互比较实力”的协议。行走江湖，比的就是武功，拼的就是实力。当两个交换机碰见的时候，也就是相连的时候，就需要互相比一比内力了。BPDU 只有掌门能发，已经隶属于某个掌门的交换机只能传达掌门的指示。</li><li>Priority Vector，优先级向量。可以比喻为实力 （值越小越牛）。实力是啥？就是一组 ID 数目，[Root Bridge ID, Root Path Cost, Bridge ID, and Port ID]。为什么这样设计呢？这是因为要看怎么来比实力。先看 Root Bridge ID。拿出老大的 ID 看看，发现掌门一样，那就是师兄弟；再比 Root Path Cost，也即我距离我的老大的距离，也就是拿和掌门关系比，看同一个门派内谁和老大关系铁；最后比 Bridge ID，比我自己的 ID，拿自己的本事比。</li></ul><p><strong>STP 的工作过程是怎样的？</strong></p><p>接下来，我们来看 STP 的工作过程。</p><p>一开始，江湖纷争，异常混乱。大家都觉得自己是掌门，谁也不服谁。于是，所有的交换机都认为自己是掌门，每个网桥都被分配了一个 ID。这个 ID 里有管理员分配的优先级，当然网络管理员知道哪些交换机贵，哪些交换机好，就会给它们分配高的优先级。这种交换机生下来武功就很高，起步就是乔峰。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1ffd84f6168f575b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>既然都是掌门，互相都连着网线，就互相发送 BPDU 来比功夫呗。这一比就发现，有人是岳不群，有人是封不平，赢的接着当掌门，输的就只好做小弟了。当掌门的还会继续发 BPDU，而输的人就没有机会了。它们只有在收到掌门发的 BPDU 的时候，转发一下，表示服从命令。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e718783d083612ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>数字表示优先级。就像这个图，5 和 6 碰见了，6 的优先级低，所以乖乖做小弟。于是一个小门派形成，5 是掌门，6 是小弟。其他诸如 1-7、2-8、3-4 这样的小门派，也诞生了。于是江湖出现了很多小的门派，小的门派，接着合并。</p><p>合并的过程会出现以下四种情形，我分别来介绍。</p><p><strong>情形一：掌门遇到掌门</strong></p><p>当 5 碰到了 1，掌门碰见掌门，1 觉得自己是掌门，5 也刚刚跟别人 PK 完成为掌门。这俩掌门比较功夫，最终 1 胜出。于是输掉的掌门 5 就会率领所有的小弟归顺。结果就是 1 成为大掌门。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9c95f4ea34e2ea3e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>情形二：同门相遇</strong></p><p>同门相遇可以是掌门与自己的小弟相遇，这说明存在“环”了。这个小弟已经通过其他门路拜在你门下，结果你还不认识，就 PK 了一把。结果掌门发现这个小弟功夫不错，不应该级别这么低，就把它招到门下亲自带，那这个小弟就相当于升职了。</p><p>我们再来看，假如 1 和 6 相遇。6 原来就拜在 1 的门下，只不过 6 的上司是 5，5 的上司是 1。1 发现，6 距离我才只有 2，比从 5 这里过来的 5（=4+1）近多了，那 6 就直接汇报给我吧。于是，5 和 6 分别汇报给 1。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-614064e9216835b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>情形三：掌门与其他帮派小弟相遇</strong></p><p>小弟拿本帮掌门和这个掌门比较，赢了，这个掌门拜入门来。输了，会拜入新掌门，并且逐渐拉拢和自己连接的兄弟，一起弃暗投明。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7fe558b98b8871ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>例如，2 和 7 相遇，虽然 7 是小弟，2 是掌门。就个人武功而言，2 比 7 强，但是 7 的掌门是 1，比 2 牛，所以没办法，2 要拜入 7 的门派，并且连同自己的小弟都一起拜入。</p><p><strong>情形四：不同门小弟相遇</strong></p><p>各自拿掌门比较，输了的拜入赢的门派，并且逐渐将与自己连接的兄弟弃暗投明。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-bdd547aaac15bd0d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>例如，5 和 4 相遇。虽然 4 的武功好于 5，但是 5 的掌门是 1，比 4 牛，于是 4 拜入 5 的门派。后来当 3 和 4 相遇的时候，3 发现 4 已经叛变了，4 说我现在老大是 1，比你牛，要不你也来吧，于是 3 也拜入 1。</p><p>最终，生成一棵树，武林一统，天下太平。但是天下大势，分久必合，合久必分，天下统一久了，也会有相应的问题。</p><p><strong>如何解决广播问题和安全问题？</strong></p><p>有两种分的方法，<strong>一个是物理隔离</strong>。每个部门设一个单独的会议室，对应到网络方面，就是每个部门有单独的交换机，配置单独的子网，这样部门之间的沟通就需要路由器了。路由器咱们还没讲到，以后再说。这样的问题在于，有的部门人多，有的部门人少。人少的部门慢慢人会变多，人多的部门也可能人越变越少。如果每个部门有单独的交换机，口多了浪费，少了又不够用。</p><p>另外一种方式是<strong>虚拟隔离</strong>，就是用我们常说的 VLAN，或者叫虚拟局域网。使用 VLAN，一个交换机上会连属于多个局域网的机器，那交换机怎么区分哪个机器属于哪个局域网呢？</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-71657eecc8bbd6b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们只需要在原来的二层的头上加一个 TAG，里面有一个 VLAN ID，一共 12 位。为什么是 12 位呢？因为 12 位可以划分 4096 个 VLAN。这样是不是还不够啊。现在的情况证明，目前云计算厂商里面绝对不止 4096 个用户。当然每个用户需要一个 VLAN 了啊，怎么办呢，这个我们在后面的章节再说。</p><p>如果我们买的交换机是支持 VLAN 的，当这个交换机把二层的头取下来的时候，就能够识别这个 VLAN ID。这样只有相同 VLAN 的包，才会互相转发，不同 VLAN 的包，是看不到的。这样广播问题和安全问题就都能够解决了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-16ef8140cccb19a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们可以设置交换机每个口所属的 VLAN。如果某个口坐的是程序员，他们属于 VLAN 10；如果某个口坐的是人事，他们属于 VLAN 20；如果某个口坐的是财务，他们属于 VLAN 30。这样，财务发的包，交换机只会转发到 VLAN 30 的口上。程序员啊，你就监听 VLAN 10 吧，里面除了代码，啥都没有。</p><p>而且对于交换机来讲，每个 VLAN 的口都是可以重新设置的。一个财务走了，把他所在座位的口从 VLAN 30 移除掉，来了一个程序员，坐在财务的位置上，就把这个口设置为 VLAN 10，十分灵活。</p><p>有人会问交换机之间怎么连接呢？将两个交换机连接起来的口应该设置成什么 VLAN 呢？对于支持 VLAN 的交换机，有一种口叫作 Trunk 口。它可以转发属于任何 VLAN 的口。交换机之间可以通过这种口相互连接。</p><h3 id="ICMP-协议的格式"><a href="#ICMP-协议的格式" class="headerlink" title="ICMP 协议的格式"></a>ICMP 协议的格式</h3><p>ping 是基于 ICMP 协议工作的。ICMP 全称 Internet Control Message Protocol，就是互联网控制报文协议。这里面的关键词是“控制”，那具体是怎么控制的呢？</p><p>网络包在异常复杂的网络环境中传输时，常常会遇到各种各样的问题。当遇到问题的时候，总不能“死个不明不白”，要传出消息来，报告情况，这样才可以调整传输策略。这就相当于我们经常看到的电视剧里，古代行军的时候，为将为帅者需要通过侦察兵、哨探或传令兵等人肉的方式来掌握情况，控制整个战局。</p><p>ICMP 报文是封装在 IP 包里面的。因为传输指令的时候，肯定需要源地址和目标地址。它本身非常简单。因为作为侦查兵，要轻装上阵，不能携带大量的包袱。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5a2efe4f45523fcd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>ICMP 报文有很多的类型，不同的类型有不同的代码。最常用的类型是主动请求为 8，主动请求的应答为 0。</p><p><strong>查询报文类型</strong></p><p>对应 ICMP 的查询报文类型。例如，常用的 ping 就是查询报文，是一种主动请求，并且获得主动应答的 ICMP 协议。所以，ping 发的包也是符合 ICMP 协议格式的，只不过它在后面增加了自己的格式。</p><p>对 ping 的主动请求，进行网络抓包，称为 ICMP ECHO REQUEST。同理主动请求的回复，称为ICMP ECHO REPLY。比起原生的 ICMP，这里面多了两个字段，一个是标识符。这个很好理解，你派出去两队侦查兵，一队是侦查战况的，一队是去查找水源的，要有个标识才能区分。另一个是序号，你派出去的侦查兵，都要编个号。如果派出去 10 个，回来 10 个，就说明前方战况不错；如果派出去 10 个，回来 2 个，说明情况可能不妙。</p><p><strong>差错报文类型</strong></p><p>我举几个 ICMP 差错报文的例子：终点不可达为 3，源抑制为 4，超时为 11，重定向为 5。这些都是什么意思呢？我给你具体解释一下。</p><p><strong>ping：查询报文类型的使用</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7826800d77d28cfa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>ping 命令执行的时候，源主机首先会构建一个 ICMP 请求数据包，ICMP 数据包内包含多个字段。最重要的是两个，第一个是类型字段，对于请求数据包而言该字段为 8；另外一个是顺序号，主要用于区分连续 ping 的时候发出的多个数据包。每发出一个请求数据包，顺序号会自动加 1。为了能够计算往返时间 RTT，它会在报文的数据部分插入发送时间。</p><p><strong>Traceroute：差错报文类型的使用</strong></p><p>所以，Traceroute 的第一个作用就是故意设置特殊的 TTL，来追踪去往目的地时沿途经过的路由器。Traceroute 的参数指向某个目的 IP 地址，它会发送一个 UDP 的数据包。将 TTL 设置成 1，也就是说一旦遇到一个路由器或者一个关卡，就表示它“牺牲”了。</p><p>Traceroute 还有一个作用是故意设置不分片，从而确定路径的 MTU。要做的工作首先是发送分组，并设置“不分片”标志。发送的第一个分组的长度正好与出口 MTU 相等。如果中间遇到窄的关口会被卡住，会发送 ICMP 网络差错包，类型为“需要进行分片但设置了不分片位”。其实，这是人家故意的好吧，每次收到 ICMP“不能分片”差错时就减小分组的长度，直到到达目标主机。</p><h3 id="MAC-头和-IP-头的细节"><a href="#MAC-头和-IP-头的细节" class="headerlink" title="MAC 头和 IP 头的细节"></a>MAC 头和 IP 头的细节</h3><p>一旦配置了 IP 地址和网关，往往就能够指定目标地址进行访问了。由于在跨网关访问的时候，牵扯到 MAC 地址和 IP 地址的变化，这里有必要详细描述一下 MAC 头和 IP 头的细节。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-01b82789d64b55af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在 MAC 头里面，先是目标 MAC 地址，然后是源 MAC 地址，然后有一个协议类型，用来说明里面是 IP 协议。IP 头里面的版本号，目前主流的还是 IPv4，服务类型 TOS 在第三节讲 ip addr 命令的时候讲过，TTL 在第 7 节讲 ICMP 协议的时候讲过。另外，还有 8 位标识协议。这里到了下一层的协议，也就是，是 TCP 还是 UDP。最重要的就是源 IP 和目标 IP。先是源 IP 地址，然后是目标 IP 地址。</p><p>在任何一台机器上，当要访问另一个 IP 地址的时候，都会先判断，这个目标 IP 地址，和当前机器的 IP 地址，是否在同一个网段。怎么判断同一个网段呢？需要 CIDR 和子网掩码，这个在第三节的时候也讲过了。</p><p>如果是同一个网段，例如，你访问你旁边的兄弟的电脑，那就没网关什么事情，直接将源地址和目标地址放入 IP 头中，然后通过 ARP 获得 MAC 地址，将源 MAC 和目的 MAC 放入 MAC 头中，发出去就可以了。</p><p>如果不是同一网段，例如，你要访问你们校园网里面的 BBS，该怎么办？这就需要发往默认网关 Gateway。Gateway 的地址一定是和源 IP 地址是一个网段的。往往不是第一个，就是第二个。例如 192.168.1.0/24 这个网段，Gateway 往往会是 192.168.1.1/24 或者 192.168.1.2/24。</p><p>如何发往默认网关呢？网关不是和源 IP 地址是一个网段的么？这个过程就和发往同一个网段的其他机器是一样的：将源地址和目标 IP 地址放入 IP 头中，通过 ARP 获得网关的 MAC 地址，将源 MAC 和网关的 MAC 放入 MAC 头中，发送出去。网关所在的端口，例如 192.168.1.1/24 将网络包收进来，然后接下来怎么做，就完全看网关的了。</p><p>网关往往是一个路由器，是一个三层转发的设备。啥叫三层设备？前面也说过了，就是把 MAC 头和 IP 头都取下来，然后根据里面的内容，看看接下来把包往哪里转发的设备。</p><p>很多情况下，人们把网关就叫做路由器。其实不完全准确，而另一种比喻更加恰当：路由器是一台设备，它有五个网口或者网卡，相当于有五只手，分别连着五个局域网。每只手的 IP 地址都和局域网的 IP 地址相同的网段，每只手都是它握住的那个局域网的网关。</p><p><strong>静态路由是什么？</strong></p><p>这个时候，问题来了，该选择哪一只手？IP 头和 MAC 头加什么内容，哪些变、哪些不变呢？这个问题比较复杂，大致可以分为两类，一个是静态路由，一个是动态路由。动态路由下一节我们详细地讲。这一节我们先说静态路由。</p><p><strong>静态路由，其实就是在路由器上</strong>，配置一条一条规则。这些规则包括：想访问 BBS 站（它肯定有个网段），从 2 号口出去，下一跳是 IP2；想访问教学视频站（它也有个自己的网段），从 3 号口出去，下一跳是 IP3，然后保存在路由器里。</p><p>从这个过程可以看出，IP 地址也会变。<strong>这个过程用英文说就是 Network Address Translation，简称 NAT</strong>。</p><p><strong>如何配置路由？</strong></p><p>通过上一节的内容，你应该已经知道，路由器就是一台网络设备，它有多张网卡。当一个入口的网络包送到路由器时，它会根据一个本地的转发信息库，来决定如何正确地转发流量。这个转发信息库通常被称为<strong>路由表</strong>。</p><p>一张路由表中会有多条路由规则。每一条规则至少包含这三项信息。</p><ul><li>目的网络：这个包想去哪儿？</li><li>出口设备：将包从哪个口扔出去？</li><li>下一跳网关：下一个路由器的地址。</li></ul><p>通过 route 命令和 ip route 命令都可以进行查询或者配置。</p><p>例如，我们设置 ip route add 10.176.48.0/20 via 10.173.32.1 dev eth0，就说明要去 10.176.48.0/20 这个目标网络，要从 eth0 端口出去，经过 10.173.32.1。</p><p>上一节的例子中，网关上的路由策略就是按照这三项配置信息进行配置的。这种配置方式的一个核心思想是：根据目的 IP 地址来配置路由。</p><p><strong>如何配置策略路由？</strong></p><p>当然，在真实的复杂的网络环境中，除了可以根据目的 ip 地址配置路由外，还可以根据多个参数来配置路由，这就称为<strong>策略路由</strong>。</p><p>可以配置多个路由表，可以根据源 IP 地址、入口设备、TOS 等选择路由表，然后在路由表中查找路由。这样可以使得来自不同来源的包走不同的路由。</p><pre><code>ip rule add from 192.168.1.0/24 table 10 ip rule add from 192.168.2.0/24 table 20</code></pre><p>表示从 192.168.1.10/24 这个网段来的，使用 table 10 中的路由表，而从 192.168.2.0/24 网段来的，使用 table20 的路由表。</p><pre><code>ip route add default scope global nexthop via 100.100.100.1 weight 1 nexthop via 200.200.200.1 weight 2    </code></pre><p>下一跳有两个地方，分别是 100.100.100.1 和 200.200.200.1，权重分别为 1 比 2。</p><p><strong>动态路由协议</strong></p><p><strong>1.基于链路状态路由算法的 OSPF</strong></p><p><strong>OSPF（Open Shortest Path First，开放式最短路径优先</strong>）就是这样一个基于链路状态路由协议，广泛应用在数据中心中的协议。由于主要用在数据中心内部，用于路由决策，<strong>因而称为内部网关协议（Interior Gateway Protocol，简称 IGP）</strong>。</p><p>内部网关协议的重点就是找到最短的路径。在一个组织内部，路径最短往往最优。当然有时候 OSPF 可以发现多个最短的路径，可以在这多个路径中进行负载均衡，这常常被称为<strong>等价路由</strong>。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fcef92c7d8f08878.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这一点非常重要。有了等价路由，到一个地方去可以有相同的两个路线，可以分摊流量，还可以当一条路不通的时候，走另外一条路。这个在后面我们讲数据中心的网络的时候，一般应用的接入层会有负载均衡 LVS。它可以和 OSPF 一起，实现高吞吐量的接入层设计。</p><p><strong>2.基于距离矢量路由算法的 BGP</strong></p><p>但是外网的路由协议，也即国家之间的，又有所不同。我们称为外网路由协议（Border Gateway Protocol，简称 BGP）。</p><p>在一个国家内部，有路当然选近的走。但是国家之间，不光远近的问题，还有政策的问题。例如，唐僧去西天取经，有的路近。但是路过的国家看不惯僧人，见了僧人就抓。例如灭法国，连光头都要抓。这样的情况即便路近，也最好绕远点走。</p><p>对于网络包同样，每个数据中心都设置自己的 Policy。例如，哪些外部的 IP 可以让内部知晓，哪些内部的 IP 可以让外部知晓，哪些可以通过，哪些不能通过。这就好比，虽然从我家里到目的地最近，但是不能谁都能从我家走啊！</p><p>在网络世界，这一个个国家成为自治系统 AS（Autonomous System）。自治系统分几种类型。</p><ul><li>Stub AS：对外只有一个连接。这类 AS 不会传输其他 AS 的包。例如，个人或者小公司的网络。</li><li>Multihomed AS：可能有多个连接连到其他的 AS，但是大多拒绝帮其他的 AS 传输包。例如一些大公司的网络。</li><li>Transit AS：有多个连接连到其他的 AS，并且可以帮助其他的 AS 传输包。例如主干网。</li></ul><p>每个自治系统都有边界路由器，通过它和外面的世界建立联系。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7a37b7a217618af8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>BGP 又分为两类，eBGP 和 iBGP</strong>。自治系统间，边界路由器之间使用 eBGP 广播路由。内部网络也需要访问其他的自治系统。边界路由器如何将 BGP 学习到的路由导入到内部网络呢？就是通过运行 iBGP，使得内部的路由器能够找到到达外网目的地的最好的边界路由器。</p><p>BGP 协议使用的算法是<strong>路径矢量路由协议（path-vector protocol）</strong>。它是距离矢量路由协议的升级版。</p><p>前面说了距离矢量路由协议的缺点。其中一个是收敛慢。在 BGP 里面，除了下一跳 hop 之外，还包括了自治系统 AS 的路径，从而可以避免坏消息传得慢的问题，也即上面所描述的，B 知道 C 原来能够到达 A，是因为通过自己，一旦自己都到达不了 A 了，就不用假设 C 还能到达 A 了。</p><h2 id="传输层"><a href="#传输层" class="headerlink" title="传输层"></a>传输层</h2><h3 id="TCP-和-UDP-有哪些区别？"><a href="#TCP-和-UDP-有哪些区别？" class="headerlink" title="TCP 和 UDP 有哪些区别？"></a>TCP 和 UDP 有哪些区别？</h3><p>一般面试的时候我问这两个协议的区别，大部分人会回答，TCP 是面向连接的，UDP 是面向无连接的。</p><p>什么叫面向连接，什么叫无连接呢？在互通之前，面向连接的协议会先建立连接。例如，TCP 会三次握手，而 UDP 不会。为什么要建立连接呢？你 TCP 三次握手，我 UDP 也可以发三个包玩玩，有什么区别吗？</p><p><strong>所谓的建立连接，是为了在客户端和服务端维护连接，而建立一定的数据结构来维护双方交互的状态，用这样的数据结构来保证所谓的面向连接的特性。</strong></p><p>例如，<strong>TCP 提供可靠交付</strong>。通过 TCP 连接传输的数据，无差错、不丢失、不重复、并且按序到达。我们都知道 IP 包是没有任何可靠性保证的，一旦发出去，就像西天取经，走丢了、被妖怪吃了，都只能随它去。但是 TCP 号称能做到那个连接维护的程序做的事情，这个下两节我会详细描述。<strong>而 UDP 继承了 IP 包的特性，不保证不丢失，不保证按顺序到达</strong>。</p><p>再如，<strong>TCP 是面向字节流的</strong>。发送的时候发的是一个流，没头没尾。IP 包可不是一个流，而是一个个的 IP 包。之所以变成了流，这也是 TCP 自己的状态维护做的事情。而 UDP 继承了 IP 的特性，基于数据报的，一个一个地发，一个一个地收。</p><p>还有 <strong>TCP 是可以有拥塞控制的</strong>。它意识到包丢弃了或者网络的环境不好了，就会根据情况调整自己的行为，看看是不是发快了，要不要发慢点。UDP 就不会，应用让我发，我就发，管它洪水滔天。</p><p>因而 <strong>TCP 其实是一个有状态服务</strong>，通俗地讲就是有脑子的，里面精确地记着发送了没有，接收到没有，发送到哪个了，应该接收哪个了，错一点儿都不行。而 UDP 则是无状态服务。通俗地说是没脑子的，天真无邪的，发出去就发出去了。</p><p><strong>UDP 包头是什么样的？</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4bf2df9c07db2bdb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>UDP 的三大特点</strong></p><p>第一，沟通简单，不需要一肚子花花肠子（大量的数据结构、处理逻辑、包头字段）。前提是它相信网络世界是美好的，秉承性善论，相信网络通路默认就是很容易送达的，不容易被丢弃的。</p><p>第二，轻信他人。它不会建立连接，虽然有端口号，但是监听在这个地方，谁都可以传给他数据，他也可以传给任何人数据，甚至可以同时传给多个人数据。</p><p>第三，愣头青，做事不懂权变。不知道什么时候该坚持，什么时候该退让。它不会根据网络的情况进行发包的拥塞控制，无论网络丢包丢成啥样了，它该怎么发还怎么发。</p><p><strong>UDP 的三大使用场景</strong></p><p>第一，<strong>需要资源少，在网络情况比较好的内网，或者对于丢包不敏感的应用</strong>。这很好理解，就像如果你是领导，你会让你们组刚毕业的小朋友去做一些没有那么难的项目，打一些没有那么难的客户，或者做一些失败了也能忍受的实验性项目。</p><p>我们在第四节讲的** DHCP 就是基于 UDP 协议的**。一般的获取 IP 地址都是内网请求，而且一次获取不到 IP 又没事，过一会儿还有机会。我们讲过 PXE 可以在启动的时候自动安装操作系统，操作系统镜像的下载使用的 TFTP，这个也是基于 UDP 协议的。在还没有操作系统的时候，客户端拥有的资源很少，不适合维护一个复杂的状态机，而且因为是内网，一般也没啥问题。</p><p>第二，<strong>不需要一对一沟通，建立连接，而是可以广播的应用</strong>。咱们小时候人都很简单，大家在班级里面，谁成绩好，谁写作好，应该表扬谁惩罚谁，谁得几个小红花都是当着全班的面讲的，公平公正公开。长大了人心复杂了，薪水、奖金要背靠背，和员工一对一沟通。</p><p>UDP 的不面向连接的功能，可以使得可以承载广播或者多播的协议。<strong>DHCP 就是一种广播的形式</strong>，就是基于 UDP 协议的，而广播包的格式前面说过了。</p><p>第三，<strong>需要处理速度快，时延低，可以容忍少数丢包，但是要求即便网络拥塞，也毫不退缩</strong>，一往无前的时候。记得曾国藩建立湘军的时候，专门招出生牛犊不怕虎的新兵，而不用那些“老油条”的八旗兵，就是因为八旗兵经历的事情多，遇到敌军不敢舍死忘生。</p><p><strong>TCP 包头格式</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-95834d01d22060a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>首先，源端口号和目标端口号是不可少的，这一点和 UDP 是一样的。如果没有这两个端口号。数据就不知道应该发给哪个应用。</p><p>通过对 TCP 头的解析，我们知道要掌握 TCP 协议，重点应该关注以下几个问题：</p><ul><li>顺序问题 ，稳重不乱；</li><li>丢包问题，承诺靠谱；</li><li>连接维护，有始有终；</li><li>流量控制，把握分寸；</li><li>拥塞控制，知进知退。</li></ul><p><strong>TCP 的三次握手</strong></p><p>三次握手除了双方建立连接外，主要还是为了沟通一件事情，<strong>就是 TCP 包的序号的问题</strong>。</p><p>A 要告诉 B，我这面发起的包的序号起始是从哪个号开始的，B 同样也要告诉 A，B 发起的包的序号起始是从哪个号开始的。为什么序号不能都从 1 开始呢？因为这样往往会出现冲突。</p><p>例如，A 连上 B 之后，发送了 1、2、3 三个包，但是发送 3 的时候，中间丢了，或者绕路了，于是重新发送，后来 A 掉线了，重新连上 B 后，序号又从 1 开始，然后发送 2，但是压根没想发送 3，但是上次绕路的那个 3 又回来了，发给了 B，B 自然认为，这就是下一个包，于是发生了错误。</p><p>因而，每个连接都要有不同的序号。这个序号的起始序号是随着时间变化的，可以看成一个 32 位的计数器，每 4 微秒加一，如果计算一下，如果到重复，需要 4 个多小时，那个绕路的包早就死翘翘了，因为我们都知道 IP 包头里面有个 TTL，也即生存时间。</p><p>好了，双方终于建立了信任，建立了连接。前面也说过，为了维护这个连接，双方都要维护一个状态机，在连接建立的过程中，双方的状态变化时序图就像这样。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a719890b32259f58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>一开始，客户端和服务端都处于 CLOSED 状态。先是服务端主动监听某个端口，处于 LISTEN 状态。然后客户端主动发起连接 SYN，之后处于 SYN-SENT 状态。服务端收到发起的连接，返回 SYN，并且 ACK 客户端的 SYN，之后处于 SYN-RCVD 状态。客户端收到服务端发送的 SYN 和 ACK 之后，发送 ACK 的 ACK，之后处于 ESTABLISHED 状态，因为它一发一收成功了。服务端收到 ACK 的 ACK 之后，处于 ESTABLISHED 状态，因为它也一发一收了。</p><p><strong>TCP 四次挥手</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f65755bc0fa20a3c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>断开的时候，我们可以看到，当 A 说“不玩了”，就进入 FIN_WAIT_1 的状态，B 收到“A 不玩”的消息后，发送知道了，就进入 CLOSE_WAIT 的状态。</p><p>A 收到“B 说知道了”，就进入 FIN_WAIT_2 的状态，如果这个时候 B 直接跑路，则 A 将永远在这个状态。TCP 协议里面并没有对这个状态的处理，但是 Linux 有，可以调整 tcp_fin_timeout 这个参数，设置一个超时时间。</p><p>如果 B 没有跑路，发送了“B 也不玩了”的请求到达 A 时，A 发送“知道 B 也不玩了”的 ACK 后，从 FIN_WAIT_2 状态结束，按说 A 可以跑路了，但是最后的这个 ACK 万一 B 收不到呢？则 B 会重新发一个“B 不玩了”，这个时候 A 已经跑路了的话，B 就再也收不到 ACK 了，因而 TCP 协议要求 A 最后等待一段时间 TIME_WAIT，这个时间要足够长，长到如果 B 没收到 ACK 的话，“B 说不玩了”会重发的，A 会重新发一个 ACK 并且足够时间到达 B。</p><p>A 直接跑路还有一个问题是，A 的端口就直接空出来了，但是 B 不知道，B 原来发过的很多包很可能还在路上，如果 A 的端口被一个新的应用占用了，这个新的应用会收到上个连接中 B 发过来的包，虽然序列号是重新生成的，但是这里要上一个双保险，防止产生混乱，因而也需要等足够长的时间，等到原来 B 发送的所有的包都死翘翘，再空出端口来。</p><p>等待的时间设为** 2MSL，MSL 是 Maximum Segment Lifetime，**报文最大生存时间，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为 TCP 报文基于是 IP 协议的，而 IP 头中有一个 TTL 域，是 IP 数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报文通知源主机。协议规定 MSL 为 2 分钟，实际应用中常用的是 30 秒，1 分钟和 2 分钟等。</p><p>还有一个异常情况就是，B 超过了 2MSL 的时间，依然没有收到它发的 FIN 的 ACK，怎么办呢？按照 TCP 的原理，B 当然还会重发 FIN，这个时候 A 再收到这个包之后，A 就表示，我已经在这里等了这么长时间了，已经仁至义尽了，之后的我就都不认了，于是就直接发送 RST，B 就知道 A 早就跑了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c7348afb80807800.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>如何实现一个靠谱的协议？</strong></p><p>TCP 协议使用的也是同样的模式。为了保证顺序性，每一个包都有一个 ID。在建立连接的时候，会商定起始的 ID 是什么，然后按照 ID 一个个发送。为了保证不丢包，对于发送的包都要进行应答，但是这个应答也不是一个一个来的，而是会应答某个之前的 ID，表示都收到了，这种模式称为累计确认或者累计应答（cumulative acknowledgment）。</p><p>接收端会给发送端报一个窗口的大小，叫 Advertised window。这个窗口的大小应该等于上面的第二部分加上第三部分，就是已经交代了没做完的加上马上要交代的。超过这个窗口的，接收端做不过来，就不能发送了。</p><p>于是，发送端需要保持下面的数据结构。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0d4b1e336fbf8b4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li>LastByteAcked：第一部分和第二部分的分界线</li><li>LastByteSent：第二部分和第三部分的分界线</li><li>LastByteAcked + AdvertisedWindow：第三部分和第四部分的分界线</li></ul><p>对于接收端来讲，它的缓存里记录的内容要简单一些。</p><ul><li>第一部分：接受并且确认过的。也就是我领导交代给我，并且我做完的。</li><li>第二部分：还没接收，但是马上就能接收的。也即是我自己的能够接受的最大工作量。</li><li>第三部分：还没接收，也没法接收的。也即超过工作量的部分，实在做不完。</li></ul><p>对应的数据结构就像这样。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3e7047c074ff2de3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li>MaxRcvBuffer：最大缓存的量；</li><li>LastByteRead 之后是已经接收了，但是还没被应用层读取的；</li><li>NextByteExpected 是第一部分和第二部分的分界线。</li></ul><p>第二部分的窗口有多大呢？</p><p>NextByteExpected 和 LastByteRead 的差其实是还没被应用层读取的部分占用掉的 MaxRcvBuffer 的量，我们定义为 A。</p><p>AdvertisedWindow 其实是 MaxRcvBuffer 减去 A。</p><p>也就是：AdvertisedWindow=MaxRcvBuffer-((NextByteExpected-1)-LastByteRead)。</p><p>那第二部分和第三部分的分界线在哪里呢？NextByteExpected 加 AdvertisedWindow 就是第二部分和第三部分的分界线，其实也就是 LastByteRead 加上 MaxRcvBuffer。</p><p>其中第二部分里面，由于受到的包可能不是顺序的，会出现空档，只有和第一部分连续的，可以马上进行回复，中间空着的部分需要等待，哪怕后面的已经来了。</p><p><strong>顺序问题与丢包问题</strong></p><p>还是刚才的图，在发送端来看，1、2、3 已经发送并确认；4、5、6、7、8、9 都是发送了还没确认；10、11、12 是还没发出的；13、14、15 是接收方没有空间，不准备发的。</p><p>在接收端来看，1、2、3、4、5 是已经完成 ACK，但是没读取的；6、7 是等待接收的；<strong>8、9 是已经接收，但是没有 ACK 的</strong>。</p><p>发送端和接收端当前的状态如下：</p><ul><li>1、2、3 没有问题，双方达成了一致。</li><li>4、5 接收方说 ACK 了，但是发送方还没收到，有可能丢了，有可能在路上。</li><li>6、7、8、9 肯定都发了，但是 8、9 已经到了，但是 6、7 没到，出现了乱序，缓存着但是没办法 ACK。</li></ul><p>根据这个例子，我们可以知道，顺序问题和丢包问题都有可能发生，所以我们先来看<strong>确认与重发的机制</strong>。</p><p>假设 4 的确认到了，不幸的是，5 的 ACK 丢了，6、7 的数据包丢了，这该怎么办呢？</p><p>一种方法就是<strong>超时重试</strong>，也即对每一个发送了，但是没有 ACK 的包，都有设一个定时器，超过了一定的时间，就重新尝试。但是这个超时的时间如何评估呢？这个时间不宜过短，时间必须大于往返时间 RTT，否则会引起不必要的重传。也不宜过长，这样超时时间变长，访问就变慢了。</p><p>估计往返时间，需要 TCP 通过采样 RTT 的时间，然后进行加权平均，算出一个值，而且这个值还是要不断变化的，因为网络状况不断地变化。除了采样 RTT，还要采样 RTT 的波动范围，计算出一个估计的超时时间。由于重传时间是不断变化的，我们称为<strong>自适应重传算法（Adaptive Retransmission Algorithm）</strong>。</p><p>如果过一段时间，5、6、7 都超时了，就会重新发送。接收方发现 5 原来接收过，于是丢弃 5；6 收到了，发送 ACK，要求下一个是 7，7 不幸又丢了。当 7 再次超时的时候，<strong>有需要重传的时候，TCP 的策略是超时间隔加倍。每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送</strong>。</p><p><strong>有一个可以快速重传的机制</strong>，当接收方收到一个序号大于下一个所期望的报文段时，就会检测到数据流中的一个间隔，于是它就会发送冗余的 ACK，仍然 ACK 的是期望接收的报文段。而当客户端收到三个冗余的 ACK 后，就会在定时器过期之前，重传丢失的报文段。</p><p>例如，接收方发现 6 收到了，8 也收到了，但是 7 还没来，那肯定是丢了，于是发送 6 的 ACK，要求下一个是 7。接下来，收到后续的包，仍然发送 6 的 ACK，要求下一个是 7。当客户端收到 3 个重复 ACK，就会发现 7 的确丢了，不等超时，马上重发。</p><p>还有一种方式称为 <strong>Selective Acknowledgment （SACK）</strong>。这种方式需要在 TCP 头里加一个 SACK 的东西，可以将缓存的地图发送给发送方。例如可以发送 ACK6、SACK8、SACK9，有了地图，发送方一下子就能看出来是 7 丢了。</p><p><strong>流量控制问题</strong></p><p>我们再来看流量控制机制，在对于包的确认中，同时会携带一个窗口的大小。</p><p>我们先假设窗口不变的情况，窗口始终为 9。4 的确认来的时候，会右移一个，这个时候第 13 个包也可以发送了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-86cd8f06222509dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个时候，假设发送端发送过猛，会将第三部分的 10、11、12、13 全部发送完毕，之后就停止发送了，未发送可发送部分为 0。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-14716c6df118113c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>当对于包 5 的确认到达的时候，在客户端相当于窗口再滑动了一格，这个时候，才可以有更多的包可以发送了，例如第 14 个包才可以发送。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ed0311b40be16f6f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果接收方实在处理的太慢，导致缓存中没有空间了，可以通过确认信息修改窗口的大小，甚至可以设置为 0，则发送方将暂时停止发送。</p><p>我们假设一个极端情况，接收端的应用一直不读取缓存中的数据，当数据包 6 确认后，窗口大小就不能再是 9 了，就要缩小一个变为 8。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ae038c6b1b767e56.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个新的窗口 8 通过 6 的确认消息到达发送端的时候，你会发现窗口没有平行右移，而是仅仅左面的边右移了，窗口的大小从 9 改成了 8。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d210aa6525c929dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果接收端还是一直不处理数据，则随着确认的包越来越多，窗口越来越小，直到为 0。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4938fc33ce0e2c42.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>当这个窗口通过包 14 的确认到达发送端的时候，发送端的窗口也调整为 0，停止发送。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c487e3f88ba63809.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果这样的话，发送方会定时发送窗口探测数据包，看是否有机会调整窗口的大小。当接收方比较慢的时候，<strong>要防止低能窗口综合征</strong>，别空出一个字节来就赶快告诉发送方，然后马上又填满了，可以当窗口太小的时候，不更新窗口，直到达到一定大小，或者缓冲区一半为空，才更新窗口。</p><p><strong>拥塞控制问题</strong></p><p>最后，我们看一下拥塞控制的问题，也是通过窗口的大小来控制的，前面的滑动窗口 rwnd 是怕发送方把接收方缓存塞满，而拥塞窗口 cwnd，是怕把网络塞满。</p><p>这里有一个公式 LastByteSent - LastByteAcked &lt;= min {cwnd, rwnd} ，是拥塞窗口和滑动窗口共同控制发送的速度。</p><p>那发送方怎么判断网络是不是慢呢？这其实是个挺难的事情，因为对于 TCP 协议来讲，他压根不知道整个网络路径都会经历什么，对他来讲就是一个黑盒。TCP 发送包常被比喻为往一个水管里面灌水，而 TCP 的拥塞控制就是在不堵塞，不丢包的情况下，尽量发挥带宽。</p><p>水管有粗细，网络有带宽，也即每秒钟能够发送多少数据；水管有长度，端到端有时延。在理想状态下，水管里面水的量 = 水管粗细 x 水管长度。对于到网络上，通道的容量 = 带宽 × 往返延迟。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-773e9e069426d2fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如图所示，假设往返时间为 8s，去 4s，回 4s，每秒发送一个包，每个包 1024byte。已经过去了 8s，则 8 个包都发出去了，其中前 4 个包已经到达接收端，但是 ACK 还没有返回，不能算发送成功。5-8 后四个包还在路上，还没被接收。这个时候，整个管道正好撑满，在发送端，已发送未确认的为 8 个包，正好等于带宽，也即每秒发送 1 个包，乘以来回时间 8s。</p><p>如果我们在这个基础上再调大窗口，使得单位时间内更多的包可以发送，会出现什么现象呢？</p><p>我们来想，原来发送一个包，从一端到达另一端，假设一共经过四个设备，每个设备处理一个包时间耗费 1s，所以到达另一端需要耗费 4s，如果发送的更加快速，则单位时间内，会有更多的包到达这些中间设备，这些设备还是只能每秒处理一个包的话，多出来的包就会被丢弃，这是我们不想看到的。</p><p>这个时候，我们可以想其他的办法，例如这个四个设备本来每秒处理一个包，但是我们在这些设备上加缓存，处理不过来的在队列里面排着，这样包就不会丢失，但是缺点是会增加时延，这个缓存的包，4s 肯定到达不了接收端了，如果时延达到一定程度，就会超时重传，也是我们不想看到的。</p><p>于是 TCP 的拥塞控制主要来避免两种现象，包丢失和超时重传。一旦出现了这些现象就说明，发送速度太快了，要慢一点。但是一开始我怎么知道速度多快呢，我怎么知道应该把窗口调整到多大呢？</p><p>如果我们通过漏斗往瓶子里灌水，我们就知道，不能一桶水一下子倒进去，肯定会溅出来，要一开始慢慢的倒，然后发现总能够倒进去，就可以越倒越快。这叫作<strong>慢启动</strong>。</p><p>一条 TCP 连接开始，cwnd 设置为一个报文段，一次只能发送一个；当收到这一个确认的时候，cwnd 加一，于是一次能够发送两个；当这两个的确认到来的时候，每个确认 cwnd 加一，两个确认 cwnd 加二，于是一次能够发送四个；当这四个的确认到来的时候，每个确认 cwnd 加一，四个确认 cwnd 加四，于是一次能够发送八个。可以看出这是<strong>指数性的增长</strong>。</p><p>涨到什么时候是个头呢？有一个值 ssthresh 为 65535 个字节，当超过这个值的时候，就要小心一点了，不能倒这么快了，可能快满了，再慢下来。</p><p>每收到一个确认后，cwnd 增加 1/cwnd，我们接着上面的过程来，一次发送八个，当八个确认到来的时候，每个确认增加 1/8，八个确认一共 cwnd 增加 1，于是一次能够发送九个，变成了线性增长。</p><p>但是线性增长还是增长，还是越来越多，直到有一天，水满则溢，出现了拥塞，这时候一般就会一下子降低倒水的速度，等待溢出的水慢慢渗下去。</p><p>拥塞的一种表现形式是丢包，需要超时重传，这个时候，将 sshresh 设为 cwnd/2，将 cwnd 设为 1，重新开始慢启动。这真是一旦超时重传，马上回到解放前。但是这种方式太激进了，将一个高速的传输速度一下子停了下来，会造成网络卡顿。</p><p>前面我们讲过快速重传算法。当接收端发现丢了一个中间包的时候，发送三次前一个包的 ACK，于是发送端就会快速地重传，不必等待超时再重传。TCP 认为这种情况不严重，因为大部分没丢，只丢了一小部分，cwnd 减半为 cwnd/2，然后 sshthresh = cwnd，当三个包返回的时候，cwnd = sshthresh + 3，也就是没有一夜回到解放前，而是还在比较高的值，呈线性增长。</p><p>就像前面说的一样，正是这种知进退，使得时延很重要的情况下，反而降低了速度。但是如果你仔细想一下，TCP 的拥塞控制主要来避免的两个现象都是有问题的。</p><p>第一个问题是丢包并不代表着通道满了，也可能是管子本来就漏水。例如公网上带宽不满也会丢包，这个时候就认为拥塞了，退缩了，其实是不对的。</p><p>第二个问题是 TCP 的拥塞控制要等到将中间设备都填充满了，才发生丢包，从而降低速度，这时候已经晚了。其实 TCP 只要填满管道就可以了，不应该接着填，直到连缓存也填满。</p><p>为了优化这两个问题，后来有了 <strong>TCP BBR 拥塞算法</strong>。它企图找到一个平衡点，就是通过不断地加快发送速度，将管道填满，但是不要填满中间设备的缓存，因为这样时延会增加，在这个平衡点可以很好的达到高带宽和低时延的平衡。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-39c1189f8da20ffd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>Socket底层实现</strong></p><p>说 TCP 的 Socket 就是一个文件流，是非常准确的。因为，Socket 在 Linux 中就是以文件的形式存在的。除此之外，还存在文件描述符。写入和读出，也是通过文件描述符。</p><p>在内核中，Socket 是一个文件，那对应就有文件描述符。每一个进程都有一个数据结构 task_struct，里面指向一个文件描述符数组，来列出这个进程打开的所有文件的文件描述符。文件描述符是一个整数，是这个数组的下标。</p><p>这个数组中的内容是一个指针，指向内核中所有打开的文件的列表。既然是一个文件，就会有一个 inode，只不过 Socket 对应的 inode 不像真正的文件系统一样，保存在硬盘上的，而是在内存中的。在这个 inode 中，指向了 Socket 在内核中的 Socket 结构。</p><p>在这个结构里面，主要的是两个队列，一个是发送队列，一个是接收队列。在这两个队列里面保存的是一个缓存 sk_buff。这个缓存里面能够看到完整的包的结构。看到这个，是不是能和前面讲过的收发包的场景联系起来了？</p><p>整个数据结构我也画了一张图。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-88d8b4eceb690a49.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>epoll实现</strong></p><p>上面 select 函数还是有问题的，因为每次 Socket 所在的文件描述符集合中有 Socket 发生变化的时候，都需要通过轮询的方式，也就是需要将全部项目都过一遍的方式来查看进度，这大大影响了一个项目组能够支撑的最大的项目数量。因而使用 select，能够同时盯的项目数量由 FD_SETSIZE 限制。</p><p>如果改成事件通知的方式，情况就会好很多，项目组不需要通过轮询挨个盯着这些项目，而是当项目进度发生变化的时候，主动通知项目组，然后项目组再根据项目进展情况做相应的操作。</p><p>能完成这件事情的函数叫 epoll，它在内核中的实现不是通过轮询的方式，而是通过注册 callback 函数的方式，当某个文件描述符发送变化的时候，就会主动通知。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-92e5e6e83886e9ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如图所示，假设进程打开了 Socket m, n, x 等多个文件描述符，现在需要通过 epoll 来监听是否这些 Socket 都有事件发生。其中 epoll_create 创建一个 epoll 对象，也是一个文件，也对应一个文件描述符，同样也对应着打开文件列表中的一项。在这项里面有一个红黑树，在红黑树里，要保存这个 epoll 要监听的所有 Socket。</p><p>当 epoll_ctl 添加一个 Socket 的时候，其实是加入这个红黑树，同时红黑树里面的节点指向一个结构，将这个结构挂在被监听的 Socket 的事件列表中。当一个 Socket 来了一个事件的时候，可以从这个列表中得到 epoll 对象，并调用 call back 通知它。</p><p>这种通知方式使得监听的 Socket 数据增加的时候，效率不会大幅度降低，能够同时监听的 Socket 的数目也非常的多了。上限就为系统定义的、进程打开的最大文件描述符个数。因而，epoll 被称为解决 C10K 问题的利器。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="HTTP"><a href="#HTTP" class="headerlink" title="HTTP"></a>HTTP</h3><ul><li>HTTP 协议虽然很常用，也很复杂，重点记住 GET、POST、 PUT、DELETE 这几个方法，以及重要的首部字段；</li><li>HTTP 2.0 通过头压缩、分帧、二进制编码、多路复用等技术提升性能；</li><li>QUIC 协议通过基于 UDP 自定义的类似 TCP 的连接、重试、多路复用、流量控制技术，进一步提升性能。</li></ul><p><strong>HTTPS</strong></p><p>我们可以知道，非对称加密在性能上不如对称加密，那是否能将两者结合起来呢？例如，公钥私钥主要用于传输对称加密的秘钥，而真正的双方大数据量的通信都是通过对称加密进行的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6ac3005fcdb0d7b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>HTTPS双向校验</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8de2a11e67a10ced.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d922eb32037d9faa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li>加密分对称加密和非对称加密。对称加密效率高，但是解决不了密钥传输问题；非对称加密可以解决这个问题，但是效率不高。</li><li>非对称加密需要通过证书和权威机构来验证公钥的合法性。</li><li>HTTPS 是综合了对称加密和非对称加密算法的 HTTP 协议。既保证传输安全，也保证传输效率。</li></ul><h3 id="流媒体协议"><a href="#流媒体协议" class="headerlink" title="流媒体协议"></a>流媒体协议</h3><p><strong>视频和图片的压缩过程有什么特点？</strong></p><ul><li>空间冗余：图像的相邻像素之间有较强的相关性，一张图片相邻像素往往是渐变的，不是突变的，没必要每个像素都完整地保存，可以隔几个保存一个，中间的用算法计算出来。</li><li>时间冗余：视频序列的相邻图像之间内容相似。一个视频中连续出现的图片也不是突变的，可以根据已有的图片进行预测和推断。</li><li>视觉冗余：人的视觉系统对某些细节不敏感，因此不会每一个细节都注意到，可以允许丢失一些数据。</li><li>编码冗余：不同像素值出现的概率不同，概率高的用的字节少，概率低的用的字节多，类似霍夫曼编码（Huffman Coding）的思路。</li></ul><p>总之，用于编码的算法非常复杂，而且多种多样，但是编码过程其实都是类似的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e52ffe0cd90fe940.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>视频编码的两大流派</strong></p><p>能不能形成一定的标准呢？要不然开发视频播放的人得累死了。当然能，我这里就给你介绍，视频编码的两大流派。</p><ul><li>流派一：ITU（International Telecommunications Union）的 VCEG（Video Coding Experts Group），这个称为国际电联下的 VCEG。既然是电信，可想而知，他们最初做视频编码，主要侧重传输。名词系列二，就是这个组织制定的标准。</li><li>流派二：ISO（International Standards Organization）的 MPEG（Moving Picture Experts Group），这个是 ISO 旗下的 MPEG，本来是做视频存储的。例如，编码后保存在 VCD 和 DVD 中。当然后来也慢慢侧重视频传输了。名词系列三，就是这个组织制定的标准。</li></ul><p>后来，ITU-T（国际电信联盟电信标准化部门，ITU Telecommunication Standardization Sector）与 MPEG 联合制定了 H.264/MPEG-4 AVC，这才是我们这一节要重点关注的。</p><p>经过编码之后，生动活泼的一帧一帧的图像，就变成了一串串让人看不懂的二进制，这个二进制可以放在一个文件里面，按照一定的格式保存起来，这就是名词系列一。</p><p>其实这些就是视频保存成文件的格式。例如，前几个字节是什么意义，后几个字节是什么意义，然后是数据，数据中保存的就是编码好的结果。</p><p>整个直播过程，可以用这个的图来描述。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6cb2c649430bbb18.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-33d9da1e4f18abc8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>编码：如何将丰富多彩的图片变成二进制流？</strong></p><p>虽然我们说视频是一张张图片的序列，但是如果每张图片都完整，就太大了，因而会将视频序列分成三种帧。</p><ul><li>I 帧，也称关键帧。里面是完整的图片，只需要本帧数据，就可以完成解码。</li><li>P 帧，前向预测编码帧。P 帧表示的是这一帧跟之前的一个关键帧（或 P 帧）的差别，解码时需要用之前缓存的画面，叠加上和本帧定义的差别，生成最终画面。</li><li>B 帧，双向预测内插编码帧。B 帧记录的是本帧与前后帧的差别。要解码 B 帧，不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面的数据与本帧数据的叠加，取得最终的画面。</li></ul><p>可以看出，I 帧最完整，B 帧压缩率最高，而压缩后帧的序列，应该是在 IBBP 的间隔出现的。这就是通过时序进行编码。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5283c8230944fb79.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在一帧中，分成多个片，每个片中分成多个宏块，每个宏块分成多个子块，这样将一张大的图分解成一个个小块，可以方便进行空间上的编码。</p><p>尽管时空非常立体地组成了一个序列，但是总归还是要压缩成一个二进制流。这个流是有结构的，是一个个的网络提取层单元（NALU，Network Abstraction Layer Unit）。变成这种格式就是为了传输，因为网络上的传输，默认的是一个个的包，因而这里也就分成了一个个的单元。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7790e8af6458d36e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>每一个 NALU 首先是一个起始标识符，用于标识 NALU 之间的间隔；然后是 NALU 的头，里面主要配置了 NALU 的类型；最终 Payload 里面是 NALU 承载的数据。</p><ul><li>0x07 表示 SPS，是序列参数集， 包括一个图像序列的所有信息，如图像尺寸、视频格式等。</li><li>0x08 表示 PPS，是图像参数集，包括一个图像的所有分片的所有相关信息，包括图像类型、序列号等。</li></ul><p>在传输视频流之前，必须要传输这两类参数，不然无法解码。为了保证容错性，每一个 I 帧前面，都会传一遍这两个参数集合。</p><p>如果 NALU Header 里面的表示类型是 SPS 或者 PPS，则 Payload 中就是真正的参数集的内容。</p><p>如果类型是帧，则 Payload 中才是正的视频数据，当然也是一帧一帧存放的，前面说了，一帧的内容还是挺多的，因而每一个 NALU 里面保存的是一片。对于每一片，到底是 I 帧，还是 P 帧，还是 B 帧，在片结构里面也有个 Header，这里面有个类型，然后是片的内容。</p><p>这样，整个格式就出来了，一个视频，可以拆分成一系列的帧，每一帧拆分成一系列的片，每一片都放在一个 NALU 里面，NALU 之间都是通过特殊的起始标识符分隔，在每一个 I 帧的第一片前面，要插入单独保存 SPS 和 PPS 的 NALU，最终形成一个长长的 NALU 序列。</p><p><strong>推流：如何把数据流打包传输到对端？</strong></p><p>那这个格式是不是就能够直接在网上传输到对端，开始直播了呢？其实还不是，还需要将这个二进制的流打包成网络包进行发送，这里我们使用 RTMP 协议。这就进入了第二个过程，推流。</p><p>RTMP 是基于 TCP 的，因而肯定需要双方建立一个 TCP 的连接。在有 TCP 的连接的基础上，还需要建立一个 RTMP 的连接，也即在程序里面，你需要调用 RTMP 类库的 Connect 函数，显示创建一个连接。</p><p>RTMP 为什么需要建立一个单独的连接呢？</p><p>因为它们需要商量一些事情，保证以后的传输能正常进行。主要就是两个事情，一个是版本号，如果客户端、服务器的版本号不一致，则不能工作。另一个就是时间戳，视频播放中，时间是很重要的，后面的数据流互通的时候，经常要带上时间戳的差值，因而一开始双方就要知道对方的时间戳。</p><p>未来沟通这些事情，需要发送六条消息：客户端发送 C0、C1、 C2，服务器发送 S0、 S1、 S2。</p><p>首先，客户端发送 C0 表示自己的版本号，不必等对方的回复，然后发送 C1 表示自己的时间戳。</p><p>服务器只有在收到 C0 的时候，才能返回 S0，表明自己的版本号，如果版本不匹配，可以断开连接。</p><p>服务器发送完 S0 后，也不用等什么，就直接发送自己的时间戳 S1。客户端收到 S1 的时候，发一个知道了对方时间戳的 ACK C2。同理服务器收到 C1 的时候，发一个知道了对方时间戳的 ACK S2。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7ddb00a525f70cd7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>握手之后，双方需要互相传递一些控制信息，例如 Chunk 块的大小、窗口大小等。</p><p>真正传输数据的时候，还是需要创建一个流 Stream，然后通过这个 Stream 来推流 publish。</p><p>推流的过程，就是将 NALU 放在 Message 里面发送，这个也称为 RTMP Packet 包。Message 的格式就像这样。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-837c327b07afcaaf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>发送的时候，去掉 NALU 的起始标识符。因为这部分对于 RTMP 协议来讲没有用。接下来，将 SPS 和 PPS 参数集封装成一个 RTMP 包发送，然后发送一个个片的 NALU。</p><p>RTMP 在收发数据的时候并不是以 Message 为单位的，而是把 Message 拆分成 Chunk 发送，而且必须在一个 Chunk 发送完成之后，才能开始发送下一个 Chunk。每个 Chunk 中都带有 Message ID，表示属于哪个 Message，接收端也会按照这个 ID 将 Chunk 组装成 Message。</p><p>前面连接的时候，设置的 Chunk 块大小就是指这个 Chunk。将大的消息变为小的块再发送，可以在低带宽的情况下，减少网络拥塞。</p><p>这有一个分块的例子，你可以看一下。</p><p>假设一个视频的消息长度为 307，但是 Chunk 大小约定为 128，于是会拆分为三个 Chunk。</p><p>第一个 Chunk 的 Type＝0，表示 Chunk 头是完整的；头里面 Timestamp 为 1000，总长度 Length 为 307，类型为 9，是个视频，Stream ID 为 12346，正文部分承担 128 个字节的 Data。</p><p>第二个 Chunk 也要发送 128 个字节，Chunk 头由于和第一个 Chunk 一样，因此采用 Chunk Type＝3，表示头一样就不再发送了。</p><p>第三个 Chunk 要发送的 Data 的长度为 307-128-128=51 个字节，还是采用 Type＝3。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8c60fe34259d9d17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>就这样数据就源源不断到达流媒体服务器，整个过程就像这样。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5a3d58c99186b045.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个时候，大量观看直播的观众就可以通过 RTMP 协议从流媒体服务器上拉取，但是这么多的用户量，都去同一个地方拉取，服务器压力会很大，而且用户分布在全国甚至全球，如果都去统一的一个地方下载，也会时延比较长，需要有分发网络。</p><p>分发网络分为中心和边缘两层。边缘层服务器部署在全国各地及横跨各大运营商里，和用户距离很近。中心层是流媒体服务集群，负责内容的转发。智能负载均衡系统，根据用户的地理位置信息，就近选择边缘服务器，为用户提供推 / 拉流服务。中心层也负责转码服务，例如，把 RTMP 协议的码流转换为 HLS 码流。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-07c0cfb9a761e1a8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>拉流：观众的客户端如何看到视频？</strong></p><p>接下来，我们再来看观众的客户端通过 RTMP 拉流的过程。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5a58874825586f71.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>先读到的是 H.264 的解码参数，例如 SPS 和 PPS，然后对收到的 NALU 组成的一个个帧，进行解码，交给播发器播放，一个绚丽多彩的视频画面就出来了。</p><h3 id="DNS-amp-amp-HTTPDNS"><a href="#DNS-amp-amp-HTTPDNS" class="headerlink" title="DNS &amp;&amp; HTTPDNS"></a>DNS &amp;&amp; HTTPDNS</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7124456107671b8a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>DNS 解析流程</strong></p><ol><li>电脑客户端会发出一个 DNS 请求，问 <a href="http://www.163.com/">www.163.com</a> 的 IP 是啥啊，并发给本地域名服务器 (本地 DNS)。那本地域名服务器 (本地 DNS) 是什么呢？如果是通过 DHCP 配置，本地 DNS 由你的网络服务商（ISP），如电信、移动等自动分配，它通常就在你网络服务商的某个机房。</li><li>本地 DNS 收到来自客户端的请求。你可以想象这台服务器上缓存了一张域名与之对应 IP 地址的大表格。如果能找到 <a href="http://www.163.com,它就直接返回/">www.163.com，它就直接返回</a> IP 地址。如果没有，本地 DNS 会去问它的根域名服务器：“老大，能告诉我 <a href="http://www.163.com/">www.163.com</a> 的 IP 地址吗？”根域名服务器是最高层次的，全球共有 13 套。它不直接用于域名解析，但能指明一条道路。</li><li>根 DNS 收到来自本地 DNS 的请求，发现后缀是 .com，说：“哦，<a href="http://www.163.com/">www.163.com</a> 啊，这个域名是由.com 区域管理，我给你它的顶级域名服务器的地址，你去问问它吧。”</li><li>本地 DNS 转向问顶级域名服务器：“老二，你能告诉我 <a href="http://www.163.com/">www.163.com</a> 的 IP 地址吗？”顶级域名服务器就是大名鼎鼎的比如 .com、.net、 .org 这些一级域名，它负责管理二级域名，比如 163.com，所以它能提供一条更清晰的方向。</li><li>顶级域名服务器说：“我给你负责 <a href="http://www.163.com/">www.163.com</a> 区域的权威 DNS 服务器的地址，你去问它应该能问到。”</li><li>本地 DNS 转向问权威 DNS 服务器：“您好，<a href="http://www.163.com/">www.163.com</a> 对应的 IP 是啥呀？”163.com 的权威 DNS 服务器，它是域名解析结果的原出处。为啥叫权威呢？就是我的域名我做主。</li><li>权威 DNS 服务器查询后将对应的 IP 地址 X.X.X.X 告诉本地 DNS。</li><li>本地 DNS 再将 IP 地址返回客户端，客户端和目标建立连接。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-52597e12ef0f54fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li>传统的 DNS 有很多问题，例如解析慢、更新不及时。因为缓存、转发、NAT 问题导致客户端误会自己所在的位置和运营商，从而影响流量的调度。</li><li>HttpDNS 通过客户端 SDK 和服务端，通过 HTTP 直接调用解析 DNS 的方式，绕过了传统 DNS 的这些缺点，实现了智能的调度。</li></ul><h3 id="CDN"><a href="#CDN" class="headerlink" title="CDN"></a>CDN</h3><p>全球有这么多的数据中心，无论在哪里上网，临近不远的地方基本上都有数据中心。是不是可以在这些数据中心里部署几台机器，形成一个缓存的集群来缓存部分数据，那么用户访问数据的时候，就可以就近访问了呢？</p><p>当然是可以的。这些分布在各个地方的各个数据中心的节点，就称为边缘节点。</p><p>由于边缘节点数目比较多，但是每个集群规模比较小，不可能缓存下来所有东西，因而可能无法命中，这样就会在边缘节点之上。有区域节点，规模就要更大，缓存的数据会更多，命中的概率也就更大。在区域节点之上是中心节点，规模更大，缓存数据更多。如果还不命中，就只好回源网站访问了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5fd40a6d57f58334.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这就是 CDN 分发系统的架构。CDN 系统的缓存，也是一层一层的，能不访问后端真正的源，就不打扰它。这也是电商网站物流系统的思路，北京局找不到，找华北局，华北局找不到，再找北方局。</p><p>有了这个分发系统之后，接下来，客户端如何找到相应的边缘节点进行访问呢？</p><p>还记得我们讲过的基于 DNS 的全局负载均衡吗？这个负载均衡主要用来选择一个就近的同样运营商的服务器进行访问。你会发现，CDN 分发网络也是一个分布在多个区域、多个运营商的分布式系统，也可以用相同的思路选择最合适的边缘节点。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1c23bcde8279a91e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在没有 CDN 的情况下，用户向浏览器输入 <a href="http://www.web.com/">www.web.com</a> 这个域名，客户端访问本地 DNS 服务器的时候，如果本地 DNS 服务器有缓存，则返回网站的地址；如果没有，递归查询到网站的权威 DNS 服务器，这个权威 DNS 服务器是负责 web.com 的，它会返回网站的 IP 地址。本地 DNS 服务器缓存下 IP 地址，将 IP 地址返回，然后客户端直接访问这个 IP 地址，就访问到了这个网站。</p><p>然而有了 CDN 之后，情况发生了变化。在 web.com 这个权威 DNS 服务器上，会设置一个 CNAME 别名，指向另外一个域名 <a href="http://www.web.cdn.com,返回给本地/">www.web.cdn.com，返回给本地</a> DNS 服务器。</p><p>当本地 DNS 服务器拿到这个新的域名时，需要继续解析这个新的域名。这个时候，再访问的就不是 web.com 的权威 DNS 服务器了，而是 web.cdn.com 的权威 DNS 服务器，这是 CDN 自己的权威 DNS 服务器。在这个服务器上，还是会设置一个 CNAME，指向另外一个域名，也即 CDN 网络的全局负载均衡器。</p><p>接下来，本地 DNS 服务器去请求 CDN 的全局负载均衡器解析域名，全局负载均衡器会为用户选择一台合适的缓存服务器提供服务，选择的依据包括：</p><ul><li>根据用户 IP 地址，判断哪一台服务器距用户最近；</li><li>用户所处的运营商；</li><li>根据用户所请求的 URL 中携带的内容名称，判断哪一台服务器上有用户所需的内容；</li><li>查询各个服务器当前的负载情况，判断哪一台服务器尚有服务能力。</li></ul><p>基于以上这些条件，进行综合分析之后，全局负载均衡器会返回一台缓存服务器的 IP 地址。</p><p>本地 DNS 服务器缓存这个 IP 地址，然后将 IP 返回给客户端，客户端去访问这个边缘节点，下载资源。缓存服务器响应用户请求，将用户所需内容传送到用户终端。如果这台缓存服务器上并没有用户想要的内容，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本地。</p><p><strong>动态 CDN，主要有两种模式</strong></p><ul><li>一种为生鲜超市模式，也即边缘计算的模式。既然数据是动态生成的，所以数据的逻辑计算和存储，也相应的放在边缘的节点。其中定时从源数据那里同步存储的数据，然后在边缘进行计算得到结果。就像对生鲜的烹饪是动态的，没办法事先做好缓存，因而将生鲜超市放在你家旁边，既能够送货上门，也能够现场烹饪，也是边缘计算的一种体现。</li><li>另一种是冷链运输模式，也即路径优化的模式。数据不是在边缘计算生成的，而是在源站生成的，但是数据的下发则可以通过 CDN 的网络，对路径进行优化。因为 CDN 节点较多，能够找到离源站很近的边缘节点，也能找到离用户很近的边缘节点。中间的链路完全由 CDN 来规划，选择一个更加可靠的路径，使用类似专线的方式进行访问。</li></ul><p>对于常用的 TCP 连接，在公网上传输的时候经常会丢数据，导致 TCP 的窗口始终很小，发送速度上不去。根据前面的 TCP 流量控制和拥塞控制的原理，在 CDN 加速网络中可以调整 TCP 的参数，使得 TCP 可以更加激进地传输数据。</p><ul><li>CDN 和电商系统的分布式仓储系统一样，分为中心节点、区域节点、边缘节点，而数据缓存在离用户最近的位置。</li><li>CDN 最擅长的是缓存静态数据，除此之外还可以缓存流媒体数据，这时候要注意使用防盗链。它也支持动态数据的缓存，一种是边缘计算的生鲜超市模式，另一种是链路优化的冷链运输模式。</li></ul><h3 id="数据中心"><a href="#数据中心" class="headerlink" title="数据中心"></a>数据中心</h3><p>前面讲办公室网络的时候，我们知道办公室里面有很多台电脑。如果要访问外网，需要经过一个叫网关的东西，而网关往往是一个路由器。</p><p>数据中心里面也有一大堆的电脑，但是它和咱们办公室里面的笔记本或者台式机不一样。数据中心里面是服务器。服务器被放在一个个叫作<strong>机架（Rack）</strong>的架子上面。</p><p>数据中心的入口和出口也是路由器，由于在数据中心的边界，就像在一个国家的边境，称为<strong>边界路由器（Border Router）</strong>。为了高可用，边界路由器会有多个。</p><p>一般家里只会连接一个运营商的网络，而为了高可用，为了当一个运营商出问题的时候，还可以通过另外一个运营商来提供服务，所以数据中心的边界路由器会连接多个运营商网络。</p><p>既然是路由器，就需要跑路由协议，数据中心往往就是路由协议中的自治区域（AS）。数据中心里面的机器要想访问外面的网站，数据中心里面也是有对外提供服务的机器，都可以通过 BGP 协议，获取内外互通的路由信息。这就是我们常听到的多线 BGP 的概念。</p><p>如果数据中心非常简单，没几台机器，那就像家里或者宿舍一样，所有的服务器都直接连到路由器上就可以了。但是数据中心里面往往有非常多的机器，当塞满一机架的时候，需要有交换机将这些服务器连接起来，可以互相通信。</p><p>这些交换机往往是放在机架顶端的，所以经常称为 <strong>TOR（Top Of Rack）交换机</strong>。这一层的交换机常常称为<strong>接入层（Access Layer）</strong>。注意这个接入层和原来讲过的应用的接入层不是一个概念。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-db67ce518b99ce95.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>当一个机架放不下的时候，就需要多个机架，还需要有交换机将多个机架连接在一起。这些交换机对性能的要求更高，带宽也更大。这些交换机称为<strong>汇聚层交换机（Aggregation Layer）</strong>。</p><p>数据中心里面的每一个连接都是需要考虑高可用的。这里首先要考虑的是，如果一台机器只有一个网卡，上面连着一个网线，接入到 TOR 交换机上。如果网卡坏了，或者不小心网线掉了，机器就上不去了。所以，需要至少两个网卡、两个网线插到 TOR 交换机上，但是两个网卡要工作得像一张网卡一样，这就是常说的<strong>网卡绑定（bond）</strong>。</p><p>这就需要服务器和交换机都支持一种协议 <strong>LACP（Link Aggregation Control Protocol）</strong>。它们互相通信，将多个网卡聚合称为一个网卡，多个网线聚合成一个网线，在网线之间可以进行负载均衡，也可以为了高可用作准备。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6b7df90adf1206c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>网卡有了高可用保证，但交换机还有问题。如果一个机架只有一个交换机，它挂了，那整个机架都不能上网了。因而 TOR 交换机也需要高可用，同理接入层和汇聚层的连接也需要高可用性，也不能单线连着。</p><p>最传统的方法是，部署两个接入交换机、两个汇聚交换机。服务器和两个接入交换机都连接，接入交换机和两个汇聚都连接，当然这样会形成环，所以需要启用 STP 协议，去除环，但是这样两个汇聚就只能一主一备了。STP 协议里我们学过，只有一条路会起作用。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c32067e3c1fb273d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>交换机有一种技术叫作堆叠，所以另一种方法是，将多个交换机形成一个逻辑的交换机，服务器通过多根线分配连到多个接入层交换机上，而接入层交换机多根线分别连接到多个交换机上，并且通过堆叠的私有协议，形成双活的连接方式。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4196a7a5364b1fb6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>由于对带宽要求更大，而且挂了影响也更大，所以两个堆叠可能就不够了，可以就会有更多的，比如四个堆叠为一个逻辑的交换机。</p><p>汇聚层将大量的计算节点相互连接在一起，形成一个集群。在这个集群里面，服务器之间通过二层互通，这个区域常称为一个 POD（Point Of Delivery），有时候也称为一个可用区（Available Zone）。</p><p>当节点数目再多的时候，一个可用区放不下，需要将多个可用区连在一起，连接多个可用区的交换机称为核心交换机。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-595457e25b844750.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个时候还存在一个问题，出现环路怎么办？</p><p>一种方式是，不同的可用区在不同的二层网络，需要分配不同的网段。汇聚和核心之间通过三层网络互通的，二层都不在一个广播域里面，不会存在二层环路的问题。三层有环是没有问题的，只要通过路由协议选择最佳的路径就可以了。那为啥二层不能有环路，而三层可以呢？你可以回忆一下二层环路的情况。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f7107da3c299ab7b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如图，核心层和汇聚层之间通过内部的路由协议 OSPF，找到最佳的路径进行访问，而且还可以通过 ECMP 等价路由，在多个路径之间进行负载均衡和高可用。</p><p>但是随着数据中心里面的机器越来越多，尤其是有了云计算、大数据，集群规模非常大，而且都要求在一个二层网络里面。这就需要二层互连从汇聚层上升为核心层，也即在核心以下，全部是二层互连，全部在一个广播域里面，这就是常说的大二层。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3d38a5f779deb433.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果大二层横向流量不大，核心交换机数目不多，可以做堆叠，但是如果横向流量很大，仅仅堆叠满足不了，就需要部署多组核心交换机，而且要和汇聚层进行全互连。由于堆叠只解决一个核心交换机组内的无环问题，而组之间全互连，还需要其他机制进行解决。</p><p>如果是 STP，那部署多组核心无法扩大横向流量的能力，因为还是只有一组起作用。</p><p>于是大二层就引入了 TRILL（Transparent Interconnection of Lots of Link），即多链接透明互联协议。它的基本思想是，二层环有问题，三层环没有问题，那就把三层的路由能力模拟在二层实现。</p><p>运行 TRILL 协议的交换机称为 RBridge，是具有路由转发特性的网桥设备，只不过这个路由是根据 MAC 地址来的，不是根据 IP 来的。</p><p>Rbridage 之间通过链路状态协议运作。记得这个路由协议吗？通过它可以学习整个大二层的拓扑，知道访问哪个 MAC 应该从哪个网桥走；还可以计算最短的路径，也可以通过等价的路由进行负载均衡和高可用性。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-12dfd3cb5d3852c6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>TRILL 协议在原来的 MAC 头外面加上自己的头，以及外层的 MAC 头。TRILL 头里面的 Ingress RBridge，有点像 IP 头里面的源 IP 地址，Egress RBridge 是目标 IP 地址，这两个地址是端到端的，在中间路由的时候，不会发生改变。而外层的 MAC，可以有下一跳的 Bridge，就像路由的下一跳，也是通过 MAC 地址来呈现的一样。</p><p>如图中所示的过程，有一个包要从主机 A 发送到主机 B，中间要经过 RBridge 1、RBridge 2、RBridge X 等等，直到 RBridge 3。在 RBridge 2 收到的包里面，分内外两层，内层就是传统的主机 A 和主机 B 的 MAC 地址以及内层的 VLAN。</p><p>在外层首先加上一个 TRILL 头，里面描述这个包从 RBridge 1 进来的，要从 RBridge 3 出去，并且像三层的 IP 地址一样有跳数。然后再外面，目的 MAC 是 RBridge 2，源 MAC 是 RBridge 1，以及外层的 VLAN。</p><p>当 RBridge 2 收到这个包之后，首先看 MAC 是否是自己的 MAC，如果是，要看自己是不是 Egress RBridge，也即是不是最后一跳；如果不是，查看跳数是不是大于 0，然后通过类似路由查找的方式找到下一跳 RBridge X，然后将包发出去。</p><p>RBridge 2 发出去的包，内层的信息是不变的，外层的 TRILL 头里面。同样，描述这个包从 RBridge 1 进来的，要从 RBridge 3 出去，但是跳数要减 1。外层的目标 MAC 变成 RBridge X，源 MAC 变成 RBridge 2。</p><p>如此一直转发，直到 RBridge 3，将外层解出来，发送内层的包给主机 B。</p><p>这个过程是不是和 IP 路由很像？</p><p>对于大二层的广播包，也需要通过分发树的技术来实现。我们知道 STP 是将一个有环的图，通过去掉边形成一棵树，而分发树是一个有环的图形成多棵树，不同的树有不同的 VLAN，有的广播包从 VLAN A 广播，有的从 VLAN B 广播，实现负载均衡和高可用。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-dd8ee7a6b157b972.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>核心交换机之外，就是边界路由器了。至此从服务器到数据中心边界的层次情况已经清楚了。</p><p>在核心交换上面，往往会挂一些安全设备，例如入侵检测、DDoS 防护等等。这是整个数据中心的屏障，防止来自外来的攻击。核心交换机上往往还有负载均衡器，原理前面的章节已经说过了。</p><p>在有的数据中心里面，对于存储设备，还会有一个存储网络，用来连接 SAN 和 NAS。但是对于新的云计算来讲，往往不使用传统的 SAN 和 NAS，而使用部署在 x86 机器上的软件定义存储，这样存储也是服务器了，而且可以和计算节点融合在一个机架上，从而更加有效率，也就没有了单独的存储网络了。</p><p>于是整个数据中心的网络如下图所示。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-bd4e6e05418536f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这是一个典型的三层网络结构。这里的三层不是指 IP 层，而是指接入层、汇聚层、核心层三层。这种模式非常有利于外部流量请求到内部应用。这个类型的流量，是从外到内或者从内到外，对应到上面那张图里，就是从上到下，从下到上，上北下南，所以称为南北流量。</p><p>但是随着云计算和大数据的发展，节点之间的交互越来越多，例如大数据计算经常要在不同的节点将数据拷贝来拷贝去，这样需要经过交换机，使得数据从左到右，从右到左，左西右东，所以称为东西流量。</p><p>为了解决东西流量的问题，演进出了叶脊网络（Spine/Leaf）。</p><ul><li>叶子交换机（leaf），直接连接物理服务器。L2/L3 网络的分界点在叶子交换机上，叶子交换机之上是三层网络。</li><li>脊交换机（spine switch），相当于核心交换机。叶脊之间通过 ECMP 动态选择多条路径。脊交换机现在只是为叶子交换机提供一个弹性的 L3 路由网络。南北流量可以不用直接从脊交换机发出，而是通过与 leaf 交换机并行的交换机，再接到边界路由器出去。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-11698430cfeddcbd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>传统的三层网络架构是垂直的结构，而叶脊网络架构是扁平的结构，更易于水平扩展。</p><h3 id="QoS"><a href="#QoS" class="headerlink" title="QoS"></a>QoS</h3><p>在云平台上，也有这种现象，好在有一种流量控制的技术，可以实现 QoS（Quality of Service），从而保障大多数用户的服务质量。</p><p>对于控制一台机器的网络的 QoS，分两个方向，一个是入方向，一个是出方向。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-99f6711c0759ffd9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>控制网络的 QoS 有哪些方式？</strong></p><p>在 Linux 下，可以通过 TC 控制网络的 QoS，主要就是通过队列的方式。</p><p><strong>无类别排队规则</strong></p><p>第一大类称为无类别排队规则（Classless Queuing Disciplines）。还记得我们讲ip addr的时候讲过的 pfifo_fast，这是一种不把网络包分类的技术。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8c0b448d4139c2a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>pfifo_fast 分为三个先入先出的队列，称为三个 Band。根据网络包里面 TOS，看这个包到底应该进入哪个队列。TOS 总共四位，每一位表示的意思不同，总共十六种类型。</p><p>通过命令行 tc qdisc show dev eth0，可以输出结果 priomap，也是十六个数字。在 0 到 2 之间，和 TOS 的十六种类型对应起来，表示不同的 TOS 对应的不同的队列。其中 Band 0 优先级最高，发送完毕后才轮到 Band 1 发送，最后才是 Band 2。</p><p><strong>另外一种无类别队列规则叫作随机公平队列（Stochastic Fair Queuing）。</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-04881dbaf43e8b4d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>会建立很多的 FIFO 的队列，TCP Session 会计算 hash 值，通过 hash 值分配到某个队列。在队列的另一端，网络包会通过轮询策略从各个队列中取出发送。这样不会有一个 Session 占据所有的流量。</p><p>当然如果两个 Session 的 hash 是一样的，会共享一个队列，也有可能互相影响。hash 函数会经常改变，从而 session 不会总是相互影响。</p><p><strong>还有一种无类别队列规则称为令牌桶规则（TBF，Token Bucket Filte）。</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-88b4fed9f57adbfd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>所有的网络包排成队列进行发送，但不是到了队头就能发送，而是需要拿到令牌才能发送。</p><p>令牌根据设定的速度生成，所以即便队列很长，也是按照一定的速度进行发送的。</p><p>当没有包在队列中的时候，令牌还是以既定的速度生成，但是不是无限累积的，而是放满了桶为止。设置桶的大小为了避免下面的情况：当长时间没有网络包发送的时候，积累了大量的令牌，突然来了大量的网络包，每个都能得到令牌，造成瞬间流量大增。</p><p><strong>基于类别的队列规则</strong></p><p>另外一大类是基于类别的队列规则（Classful Queuing Disciplines），其中典型的为分层令牌桶规则（HTB， Hierarchical Token Bucket）。</p><p>HTB 往往是一棵树，接下来我举个具体的例子，通过 TC 如何构建一棵 HTB 树来带你理解。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9e80bf9678f38952.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>使用 TC 可以为某个网卡 eth0 创建一个 HTB 的队列规则，需要付给它一个句柄为（1:）。</p><p>这是整棵树的根节点，接下来会有分支。例如图中有三个分支，句柄分别为（:10）、（:11）、（:12）。最后的参数 default 12，表示默认发送给 1:12，也即发送给第三个分支。</p><pre><code>tc qdisc add dev eth0 root handle 1: htb default 12</code></pre><p>对于这个网卡，需要规定发送的速度。一般有两个速度可以配置，一个是 rate，表示一般情况下的速度；一个是 ceil，表示最高情况下的速度。对于根节点来讲，这两个速度是一样的，于是创建一个 root class，速度为（rate=100kbps，ceil=100kbps）。</p><pre><code>tc class add dev eth0 parent 1: classid 1:1 htb rate 100kbps ceil 100kbps</code></pre><p>接下来要创建分支，也即创建几个子 class。每个子 class 统一有两个速度。三个分支分别为（rate=30kbps，ceil=100kbps）、（rate=10kbps，ceil=100kbps）、（rate=60kbps，ceil=100kbps）。</p><pre><code>tc class add dev eth0 parent 1:1 classid 1:10 htb rate 30kbps ceil 100kbpstc class add dev eth0 parent 1:1 classid 1:11 htb rate 10kbps ceil 100kbpstc class add dev eth0 parent 1:1 classid 1:12 htb rate 60kbps ceil 100kbps</code></pre><p>你会发现三个 rate 加起来，是整个网卡允许的最大速度。</p><p>HTB 有个很好的特性，同一个 root class 下的子类可以相互借流量，如果不直接在队列规则下面创建一个 root class，而是直接创建三个 class，它们之间是不能相互借流量的。借流量的策略，可以使得当前不使用这个分支的流量的时候，可以借给另一个分支，从而不浪费带宽，使带宽发挥最大的作用。</p><p>最后，创建叶子队列规则，分别为 fifo 和 sfq。</p><pre><code>tc qdisc add dev eth0 parent 1:10 handle 20: pfifo limit 5tc qdisc add dev eth0 parent 1:11 handle 30: pfifo limit 5tc qdisc add dev eth0 parent 1:12 handle 40: sfq perturb 10</code></pre><p>基于这个队列规则，我们还可以通过 TC 设定发送规则：从 1.2.3.4 来的，发送给 port 80 的包，从第一个分支 1:10 走；其他从 1.2.3.4 发送来的包从第二个分支 1:11 走；其他的走默认分支。</p><pre><code>tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src 1.2.3.4 match ip dport 80 0xffff flowid 1:10tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src 1.2.3.4 flowid 1:11</code></pre><h3 id="容器网络"><a href="#容器网络" class="headerlink" title="容器网络"></a>容器网络</h3><p><strong>Flannel</strong></p><p>这里我要说 Flannel 使用 UDP 实现 Overlay 网络的方案。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-59ffce6aeb00f213.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在物理机 A 上的容器 A 里面，能看到的容器的 IP 地址是 172.17.8.2/24，里面设置了默认的路由规则 default via 172.17.8.1 dev eth0。</p><p>如果容器 A 要访问 172.17.9.2，就会发往这个默认的网关 172.17.8.1。172.17.8.1 就是物理机上面 docker0 网桥的 IP 地址，这台物理机上的所有容器都是连接到这个网桥的。</p><p>在物理机上面，查看路由策略，会有这样一条 172.17.0.0/24 via 172.17.0.0 dev flannel.1，也就是说发往 172.17.9.2 的网络包会被转发到 flannel.1 这个网卡。</p><p>这个网卡是怎么出来的呢？在每台物理机上，都会跑一个 flanneld 进程，这个进程打开一个 /dev/net/tun 字符设备的时候，就出现了这个网卡。</p><p>你有没有想起 qemu-kvm，打开这个字符设备的时候，物理机上也会出现一个网卡，所有发到这个网卡上的网络包会被 qemu-kvm 接收进来，变成二进制串。只不过接下来 qemu-kvm 会模拟一个虚拟机里面的网卡，将二进制的串变成网络包，发给虚拟机里面的网卡。但是 flanneld 不用这样做，所有发到 flannel.1 这个网卡的包都会被 flanneld 进程读进去，接下来 flanneld 要对网络包进行处理。</p><p>物理机 A 上的 flanneld 会将网络包封装在 UDP 包里面，然后外层加上物理机 A 和物理机 B 的 IP 地址，发送给物理机 B 上的 flanneld。</p><p>为什么是 UDP 呢？因为不想在 flanneld 之间建立两两连接，而 UDP 没有连接的概念，任何一台机器都能发给另一台。</p><p>物理机 B 上的 flanneld 收到包之后，解开 UDP 的包，将里面的网络包拿出来，从物理机 B 的 flannel.1 网卡发出去。</p><p>在物理机 B 上，有路由规则 172.17.9.0/24 dev docker0 proto kernel scope link src 172.17.9.1。</p><p>将包发给 docker0，docker0 将包转给容器 B。通信成功。</p><p>上面的过程连通性没有问题，但是由于全部在用户态，所以性能差了一些。</p><p>跨物理机的连通性问题，在虚拟机那里有成熟的方案，就是 VXLAN，那能不能 Flannel 也用 VXLAN 呢？</p><p>当然可以了。如果使用 VXLAN，就不需要打开一个 TUN 设备了，而是要建立一个 VXLAN 的 VTEP。如何建立呢？可以通过 netlink 通知内核建立一个 VTEP 的网卡 flannel.1。在我们讲 OpenvSwitch 的时候提过，netlink 是一种用户态和内核态通信的机制。</p><p>当网络包从物理机 A 上的容器 A 发送给物理机 B 上的容器 B，在容器 A 里面通过默认路由到达物理机 A 上的 docker0 网卡，然后根据路由规则，在物理机 A 上，将包转发给 flannel.1。这个时候 flannel.1 就是一个 VXLAN 的 VTEP 了，它将网络包进行封装。</p><p>内部的 MAC 地址这样写：源为物理机 A 的 flannel.1 的 MAC 地址，目标为物理机 B 的 flannel.1 的 MAC 地址，在外面加上 VXLAN 的头。</p><p>外层的 IP 地址这样写：源为物理机 A 的 IP 地址，目标为物理机 B 的 IP 地址，外面加上物理机的 MAC 地址。</p><p>这样就能通过 VXLAN 将包转发到另一台机器，从物理机 B 的 flannel.1 上解包，变成内部的网络包，通过物理机 B 上的路由转发到 docker0，然后转发到容器 B 里面。通信成功。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-20f3ad4eb681115a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>Calico</strong></p><p>Calico 网络模型的设计思路</p><p>我们看图中的两台物理机。它们的物理网卡是同一个二层网络里面的。由于两台物理机的容器网段不同，我们完全可以将两台物理机配置成为路由器，并按照容器的网段配置路由表。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9da5027d65611db0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>例如，在物理机 A 中，我们可以这样配置：要想访问网段 172.17.9.0/24，下一跳是 192.168.100.101，也即到物理机 B 上去。</p><p>这样在容器 A 中访问容器 B，当包到达物理机 A 的时候，就能够匹配到这条路由规则，并将包发给下一跳的路由器，也即发给物理机 B。在物理机 B 上也有路由规则，要访问 172.17.9.0/24，从 docker0 的网卡进去即可。</p><p>当容器 B 返回结果的时候，在物理机 B 上，可以做类似的配置：要想访问网段 172.17.8.0/24，下一跳是 192.168.100.100，也即到物理机 A 上去。</p><p>当包到达物理机 B 的时候，能够匹配到这条路由规则，将包发给下一跳的路由器，也即发给物理机 A。在物理机 A 上也有路由规则，要访问 172.17.8.0/24，从 docker0 的网卡进去即可。</p><p>这就是 <strong>Calico 网络的大概思路，即不走 Overlay 网络，不引入另外的网络性能损耗，而是将转发全部用三层网络的路由转发来实现</strong>，只不过具体的实现和上面的过程稍有区别。</p><p>首先，如果全部走三层的路由规则，没必要每台机器都用一个 docker0，从而浪费了一个 IP 地址，而是可以直接用路由转发到 veth pair 在物理机这一端的网卡。同样，在容器内，路由规则也可以这样设定：把容器外面的 veth pair 网卡算作默认网关，下一跳就是外面的物理机。</p><p>于是，整个拓扑结构就变成了这个图中的样子。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9e900be9fccc2630.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>Calico 网络的转发细节</strong></p><p>我们来看其中的一些细节。</p><p>容器 A1 的 IP 地址为 172.17.8.2/32，这里注意，不是 /24，而是 /32，将容器 A1 作为一个单点的局域网了。</p><p>容器 A1 里面的默认路由，Calico 配置得比较有技巧。</p><pre><code>default via 169.254.1.1 dev eth0 169.254.1.1 dev eth0 scope link </code></pre><p>这个 IP 地址 169.254.1.1 是默认的网关，但是整个拓扑图中没有一张网卡是这个地址。那如何到达这个地址呢？</p><p>前面我们讲网关的原理的时候说过，当一台机器要访问网关的时候，首先会通过 ARP 获得网关的 MAC 地址，然后将目标 MAC 变为网关的 MAC，而网关的 IP 地址不会在任何网络包头里面出现，也就是说，没有人在乎这个地址具体是什么，只要能找到对应的 MAC，响应 ARP 就可以了。</p><p>ARP 本地有缓存，通过 ip neigh 命令可以查看。</p><pre><code>169.254.1.1 dev eth0 lladdr ee:ee:ee:ee:ee:ee STALE</code></pre><p>这个 MAC 地址是 Calico 硬塞进去的，但是没有关系，它能响应 ARP，于是发出的包的目标 MAC 就是这个 MAC 地址。</p><p>在物理机 A 上查看所有网卡的 MAC 地址的时候，我们会发现 veth1 就是这个 MAC 地址。所以容器 A1 里发出的网络包，第一跳就是这个 veth1 这个网卡，也就到达了物理机 A 这个路由器。</p><p>在物理机 A 上有三条路由规则，分别是去两个本机的容器的路由，以及去 172.17.9.0/24，下一跳为物理机 B。</p><pre><code>172.17.8.2 dev veth1 scope link 172.17.8.3 dev veth2 scope link 172.17.9.0/24 via 192.168.100.101 dev eth0 proto bird onlink</code></pre><p>同理，物理机 B 上也有三条路由规则，分别是去两个本机的容器的路由，以及去 172.17.8.0/24，下一跳为物理机 A。</p><pre><code>172.17.9.2 dev veth1 scope link 172.17.9.3 dev veth2 scope link 172.17.8.0/24 via 192.168.100.100 dev eth0 proto bird onlink</code></pre><p>如果你觉得这些规则过于复杂，我将刚才的拓扑图转换为这个更加容易理解的图。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-aa44dce0835b9858.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在这里，物理机化身为路由器，通过路由器上的路由规则，将包转发到目的地。在这个过程中，没有隧道封装解封装，仅仅是单纯的路由转发，性能会好很多。但是，这种模式也有很多问题。</p><p><strong>Calico 的架构</strong></p><p><strong>路由配置组件 Felix</strong></p><p>如果只有两台机器，每台机器只有两个容器，而且保持不变。我手动配置一下，倒也没啥问题。但是如果容器不断地创建、删除，节点不断地加入、退出，情况就会变得非常复杂。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-03fd15a20fa31945.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>就像图中，有三台物理机，两两之间都需要配置路由，每台物理机上对外的路由就有两条。如果有六台物理机，则每台物理机上对外的路由就有五条。新加入一个节点，需要通知每一台物理机添加一条路由。</p><p>这还是在物理机之间，一台物理机上，每创建一个容器，也需要多配置一条指向这个容器的路由。如此复杂，肯定不能手动配置，需要每台物理机上有一个 agent，当创建和删除容器的时候，自动做这件事情。这个 agent 在 Calico 中称为 Felix。</p><p><strong>路由广播组件 BGP Speaker</strong></p><p>当 Felix 配置了路由之后，接下来的问题就是，如何将路由信息，也即将“如何到达我这个节点，访问我这个节点上的容器”这些信息，广播出去。</p><p>能想起来吗？这其实就是路由协议啊！路由协议就是将“我能到哪里，如何能到我”的信息广播给全网传出去，从而客户端可以一跳一跳地访问目标地址的。路由协议有很多种，Calico 使用的是 BGP 协议。</p><p>在 Calico 中，每个 Node 上运行一个软件 BIRD，作为 BGP 的客户端，或者叫作 BGP Speaker，将“如何到达我这个 Node，访问我这个 Node 上的容器”的路由信息广播出去。所有 Node 上的 BGP Speaker 都互相建立连接，就形成了全互连的情况，这样每当路由有所变化的时候，所有节点就都能够收到了。</p><p><strong>安全策略组件</strong></p><p>Calico 中还实现了灵活配置网络策略 Network Policy，可以灵活配置两个容器通或者不通。这个怎么实现呢？</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c609fb301be517d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>虚拟机中的安全组，是用 iptables 实现的。Calico 中也是用 iptables 实现的。这个图里的内容是 iptables 在内核处理网络包的过程中可以嵌入的处理点。Calico 也是在这些点上设置相应的规则。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8cc21f579fb5cd52.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>当网络包进入物理机上的时候，进入 PREOUTING 规则，这里面有一个规则是 cali-fip-dnat，这是实现浮动 IP（Floating IP）的场景，主要将外网的 IP 地址 dnat 作为容器内的 IP 地址。在虚拟机场景下，路由器的网络 namespace 里面有一个外网网卡上，也设置过这样一个 DNAT 规则。</p><p>接下来可以根据路由判断，是到本地的，还是要转发出去的。</p><p>如果是本地的，走 INPUT 规则，里面有个规则是 cali-wl-to-host，wl 的意思是 workload，也即容器，也即这是用来判断从容器发到物理机的网络包是否符合规则的。这里面内嵌一个规则 cali-from-wl-dispatch，也是匹配从容器来的包。如果有两个容器，则会有两个容器网卡，这里面内嵌有详细的规则“cali-fw-cali 网卡 1”和“cali-fw-cali 网卡 2”，fw 就是 from workload，也就是匹配从容器 1 来的网络包和从容器 2 来的网络包。</p><p>如果是转发出去的，走 FORWARD 规则，里面有个规则 cali-FORWARD。这里面分两种情况，一种是从容器里面发出来，转发到外面的；另一种是从外面发进来，转发到容器里面的。</p><p>第一种情况匹配的规则仍然是 cali-from-wl-dispatch，也即 from workload。第二种情况匹配的规则是 cali-to-wl-dispatch，也即 to workload。如果有两个容器，则会有两个容器网卡，在这里面内嵌有详细的规则“cali-tw-cali 网卡 1”和“cali-tw-cali 网卡 2”，tw 就是 to workload，也就是匹配发往容器 1 的网络包和发送到容器 2 的网络包。</p><p>接下来是匹配 OUTPUT 规则，里面有 cali-OUTPUT。接下来是 POSTROUTING 规则，里面有一个规则是 cali-fip-snat，也即发出去的时候，将容器网络 IP 转换为浮动 IP 地址。在虚拟机场景下，路由器的网络 namespace 里面有一个外网网卡上，也设置过这样一个 SNAT 规则。</p><p>至此为止，Calico 的所有组件基本凑齐。来看看我汇总的图。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-05ce5453b73650d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>全连接复杂性与规模问题</strong></p><p>这里面还存在问题，就是 BGP 全连接的复杂性问题。</p><p>你看刚才的例子里只有六个节点，BGP 的互连已经如此复杂，如果节点数据再多，这种全互连的模式肯定不行，到时候都成蜘蛛网了。于是多出了一个组件 BGP Route Reflector，它也是用 BIRD 实现的。有了它，BGP Speaker 就不用全互连了，而是都直连它，它负责将全网的路由信息广播出去。</p><p>可是问题来了，规模大了，大家都连它，它受得了吗？这个 BGP Router Reflector 会不会成为瓶颈呢？</p><p>所以，肯定不能让一个 BGP Router Reflector 管理所有的路由分发，而是应该有多个 BGP Router Reflector，每个 BGP Router Reflector 管一部分。</p><p>多大算一部分呢？咱们讲述数据中心的时候，说服务器都是放在机架上的，每个机架上最顶端有个 TOR 交换机。那将机架上的机器连在一起，这样一个机架是不是可以作为一个单元，让一个 BGP Router Reflector 来管理呢？如果要跨机架，如何进行通信呢？这就需要 BGP Router Reflector 也直接进行路由交换。它们之间的交换和一个机架之间的交换有什么关系吗？</p><p>有没有觉得在这个场景下，一个机架就像一个数据中心，可以把它设置为一个 AS，而 BGP Router Reflector 有点儿像数据中心的边界路由器。在一个 AS 内部，也即服务器和 BGP Router Reflector 之间使用的是数据中心内部的路由协议 iBGP，BGP Router Reflector 之间使用的是数据中心之间的路由协议 eBGP。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a0215c267f0bdc74.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个图中，一个机架上有多台机器，每台机器上面启动多个容器，每台机器上都有可以到达这些容器的路由。每台机器上都启动一个 BGP Speaker，然后将这些路由规则上报到这个 Rack 上接入交换机的 BGP Route Reflector，将这些路由通过 iBGP 协议告知到接入交换机的三层路由功能。</p><p>在接入交换机之间也建立 BGP 连接，相互告知路由，因而一个 Rack 里面的路由可以告知另一个 Rack。有多个核心或者汇聚交换机将接入交换机连接起来，如果核心和汇聚起二层互通的作用，则接入和接入之间之间交换路由即可。如果核心和汇聚交换机起三层路由的作用，则路由需要通过核心或者汇聚交换机进行告知。</p><p><strong>跨网段访问问题</strong></p><p>上面的 Calico 模式还有一个问题，就是跨网段问题，这里的跨网段是指物理机跨网段。</p><p>前面我们说的那些逻辑成立的条件，是我们假设物理机可以作为路由器进行使用。例如物理机 A 要告诉物理机 B，你要访问 172.17.8.0/24，下一跳是我 192.168.100.100；同理，物理机 B 要告诉物理机 A，你要访问 172.17.9.0/24，下一跳是我 192.168.100.101。</p><p>之所以能够这样，是因为物理机 A 和物理机 B 是同一个网段的，是连接在同一个交换机上的。那如果物理机 A 和物理机 B 不是在同一个网段呢？</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f82840f86a5ab0e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>例如，物理机 A 的网段是 192.168.100.100/24，物理机 B 的网段是 192.168.200.101/24，这样两台机器就不能通过二层交换机连接起来了，需要在中间放一台路由器，做一次路由转发，才能跨网段访问。</p><p>本来物理机 A 要告诉物理机 B，你要访问 172.17.8.0/24，下一跳是我 192.168.100.100 的，但是中间多了一台路由器，下一跳不是我了，而是中间的这台路由器了，这台路由器的再下一跳，才是我。这样之前的逻辑就不成立了。</p><p>我们看刚才那张图的下半部分。物理机 B 上的容器要访问物理机 A 上的容器，第一跳就是物理机 B，IP 为 192.168.200.101，第二跳是中间的物理路由器右面的网口，IP 为 192.168.200.1，第三跳才是物理机 A，IP 为 192.168.100.100。</p><p>这是咱们通过拓扑图看到的，关键问题是，在系统中物理机 A 如何告诉物理机 B，怎么让它才能到我这里？物理机 A 根本不可能知道从物理机 B 出来之后的下一跳是谁，况且现在只是中间隔着一个路由器这种简单的情况，如果隔着多个路由器呢？谁能把这一串的路径告诉物理机 B 呢？</p><p>我们能想到的第一种方式是，让中间所有的路由器都来适配 Calico。本来它们互相告知路由，只互相告知物理机的，现在还要告知容器的网段。这在大部分情况下，是不可能的。</p><p>第二种方式，还是在物理机 A 和物理机 B 之间打一个隧道，这个隧道有两个端点，在端点上进行封装，将容器的 IP 作为乘客协议放在隧道里面，而物理主机的 IP 放在外面作为承载协议。这样不管外层的 IP 通过传统的物理网络，走多少跳到达目标物理机，从隧道两端看起来，物理机 A 的下一跳就是物理机 B，这样前面的逻辑才能成立。</p><p>这就是 Calico 的<strong>IPIP 模式</strong>。使用了 IPIP 模式之后，在物理机 A 上，我们能看到这样的路由表：</p><pre><code>172.17.8.2 dev veth1 scope link 172.17.8.3 dev veth2 scope link 172.17.9.0/24 via 192.168.200.101 dev tun0 proto bird onlink</code></pre><p>这和原来模式的区别在于，下一跳不再是同一个网段的物理机 B 了，IP 为 192.168.200.101，并且不是从 eth0 跳，而是建立一个隧道的端点 tun0，从这里才是下一跳。</p><p>如果我们在容器 A1 里面的 172.17.8.2，去 ping 容器 B1 里面的 172.17.9.2，首先会到物理机 A。在物理机 A 上根据上面的规则，会转发给 tun0，并在这里对包做封装：</p><ul><li>内层源 IP 为 172.17.8.2；</li><li>内层目标 IP 为 172.17.9.2；</li><li>外层源 IP 为 192.168.100.100；</li><li>外层目标 IP 为 192.168.200.101。</li></ul><p>将这个包从 eth0 发出去，在物理网络上会使用外层的 IP 进行路由，最终到达物理机 B。在物理机 B 上，tun0 会解封装，将内层的源 IP 和目标 IP 拿出来，转发给相应的容器。</p><p><strong>总结</strong></p><ul><li>Calico 推荐使用物理机作为路由器的模式，这种模式没有虚拟化开销，性能比较高。</li><li>Calico 的主要组件包括路由、iptables 的配置组件 Felix、路由广播组件 BGP Speaker，以及大规模场景下的 BGP Route Reflector。</li><li>为解决跨网段的问题，Calico 还有一种 IPIP 模式，也即通过打隧道的方式，从隧道端点来看，将本来不是邻居的两台机器，变成相邻的机器。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Net </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《深入浅出计算机组成原理》</title>
      <link href="/2020/07/16/note/computer-organization/"/>
      <url>/2020/07/16/note/computer-organization/</url>
      
        <content type="html"><![CDATA[<h2 id="基础篇"><a href="#基础篇" class="headerlink" title="基础篇"></a>基础篇</h2><h3 id="计算机的基本硬件组成"><a href="#计算机的基本硬件组成" class="headerlink" title="计算机的基本硬件组成"></a>计算机的基本硬件组成</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-661174ea232e96a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>第一，广。组成原理中的概念非常多，每个概念的信息量也非常大。比如想要理解 CPU 中的算术逻辑单元（也就是 ALU）是怎么实现加法的，需要牵涉到如何把整数表示成二进制，还需要了解这些表示背后的电路、逻辑门、CPU 时钟、触发器等知识。</p><p>第二，深。组成原理中的很多概念，阐述开来就是计算机学科的另外一门核心课程。比如，计算机的指令是怎么从你写的 C、Java 这样的高级语言，变成计算机可以执行的机器码的？如果我们展开并深入讲解这个问题，就会变成《编译原理》这样一门核心课程。</p><p>第三，学不能致用。学东西是要拿来用的，但因为这门课本身的属性，很多人在学习时，常常沉溺于概念和理论中，无法和自己日常的开发工作联系起来，以此来解决工作中遇到的问题，所以，学习往往没有成就感，就很难有动力坚持下去。</p><p>早年，要自己组装一台计算机，要先有三大件，CPU、内存和主板。</p><p>在这三大件中，我们首先要说的是 CPU，它是计算机最重要的核心配件，全名你肯定知道，叫中央处理器（Central Processing Unit）。为什么说 CPU 是“最重要”的呢？因为计算机的所有“计算”都是由 CPU 来进行的。自然，CPU 也是整台计算机中造价最昂贵的部分之一。</p><p>第二个重要的配件，就是内存（Memory）。你撰写的程序、打开的浏览器、运行的游戏，都要加载到内存里才能运行。程序读取的数据、计算得到的结果，也都要放在内存里。内存越大，能加载的东西自然也就越多。</p><p>存放在内存里的程序和数据，需要被 CPU 读取，CPU 计算完之后，还要把数据写回到内存。然而 CPU 不能直接插到内存上，反之亦然。于是，就带来了最后一个大件——主板（Motherboard）。</p><p>主板是一个有着各种各样，有时候多达数十乃至上百个插槽的配件。我们的 CPU 要插在主板上，内存也要插在主板上。主板的<strong>芯片组（Chipset）和总线（Bus）解决了 CPU 和内存之间如何通信的问题</strong>。芯片组控制了数据传输的流转，也就是数据从哪里到哪里的问题。总线则是实际数据传输的高速公路。因此，总线速度（Bus Speed）决定了数据能传输得多快。</p><p>有了三大件，只要配上电源供电，计算机差不多就可以跑起来了。但是现在还缺少各类输入（Input）/ 输出（Output）设备，也就是我们常说的 I/O 设备。如果你用的是自己的个人电脑，那显示器肯定必不可少，只有有了显示器我们才能看到计算机输出的各种图像、文字，这也就是所谓的输出设备。</p><p>同样的，鼠标和键盘也都是必不可少的配件。这样我才能输入文本，写下这篇文章。它们也就是所谓的输入设备。</p><p>还有一个很特殊的设备，就是显卡（Graphics Card）。现在，使用图形界面操作系统的计算机，无论是 Windows、Mac OS 还是 Linux，显卡都是必不可少的。有人可能要说了，我装机的时候没有买显卡，计算机一样可以正常跑起来啊！那是因为，现在的主板都带了内置的显卡。如果你用计算机玩游戏，做图形渲染或者跑深度学习应用，你多半就需要买一张单独的显卡，插在主板上。显卡之所以特殊，是因为显卡里有除了 CPU 之外的另一个“处理器”，也就是 GPU（Graphics Processing Unit，图形处理器），GPU 一样可以做各种“计算”的工作。</p><p>鼠标、键盘以及硬盘，这些都是插在主板上的。作为外部 I/O 设备，它们是通过主板上的南桥（SouthBridge）芯片组，来控制和 CPU 之间的通信的。“南桥”芯片的名字很直观，一方面，它在主板上的位置，通常在主板的“南面”。另一方面，它的作用就是作为“桥”，来连接鼠标、键盘以及硬盘这些外部设备和 CPU 之间的通信。</p><p>有了南桥，自然对应着也有“北桥”。是的，以前的主板上通常也有“北桥”芯片，用来作为“桥”，连接 CPU 和内存、显卡之间的通信。不过，随着时间的变迁，现在的主板上的“北桥”芯片的工作，已经被移到了 CPU 的内部，所以你在主板上，已经看不到北桥芯片了。</p><p><strong>冯·诺依曼体系结构</strong></p><p>我们手机里只有 SD 卡（Secure Digital Memory Card）这样类似硬盘功能的存储卡插槽，并没有内存插槽、CPU 插槽这些东西。没错，因为手机尺寸的原因，手机制造商们选择把 CPU、内存、网络通信，乃至摄像头芯片，都封装到一个芯片，然后再嵌入到手机主板上。这种方式叫 SoC，也就是 System on a Chip（系统芯片）。</p><p>而所有的计算机程序，也都可以抽象为从输入设备读取输入信息，通过运算器和控制器来执行存储在存储器里的程序，最终把结果输出到输出设备中。而我们所有撰写的无论高级还是低级语言的程序，也都是基于这样一个抽象框架来进行运作的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-bd51501b622cb40e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li>图灵机是一种思想模型（计算机的基本理论基础），是一种有穷的、构造性的问题的问题求解思路，图灵认为凡是能用算法解决的问题也一定能用图灵机解决；</li><li>冯诺依曼提出了“存储程序”的计算机设计思想，并“参照”图灵模型设计了历史上第一台电子计算机，即冯诺依曼机。</li></ul><p><strong>计算机组成原理知识地图</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-96e52cbe8310d3b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h3><p><strong>什么是性能？时间的倒数</strong></p><p>计算机的性能，其实和我们干体力劳动很像，好比是我们要搬东西。对于计算机的性能，我们需要有个标准来衡量。这个标准中主要有两个指标。</p><p>第一个是响应时间（Response time）或者叫执行时间（Execution time）。想要提升响应时间这个性能指标，你可以理解为让计算机“跑得更快”。</p><p>第二个是吞吐率（Throughput）或者带宽（Bandwidth），想要提升这个指标，你可以理解为让计算机“搬得更多”。</p><p>过去几年流行的手机跑分软件，就是把多个预设好的程序在手机上运行，然后根据运行需要的时间，算出一个分数来给出手机的性能评估。而在业界，各大 CPU 和服务器厂商组织了一个叫作 SPEC（Standard Performance Evaluation Corporation）的第三方机构，专门用来指定各种“跑分”的规则。</p><p>SPEC 提供的 CPU 基准测试程序，就好像 CPU 届的“高考”，通过数十个不同的计算程序，对于 CPU 的性能给出一个最终评分。这些程序丰富多彩，有编译器、解释器、视频压缩、人工智能国际象棋等等，涵盖了方方面面的应用场景。感兴趣的话，你可以点击这个链接看看。</p><p>其次，<strong>即使我们已经拿到了 CPU 时间，我们也不一定可以直接“比较”出两个程序的性能差异</strong>。即使在同一台计算机上，CPU 可能满载运行也可能降频运行，降频运行的时候自然花的时间会多一些。</p><p>除了 CPU 之外，时间这个性能指标还会受到主板、内存这些其他相关硬件的影响。所以，我们需要对“时间”这个我们可以感知的指标进行拆解，把程序的 CPU 执行时间变成 CPU 时钟周期数（CPU Cycles）和 时钟周期时间（Clock Cycle）的乘积。</p><p><strong>程序的 CPU 执行时间 =CPU 时钟周期数×时钟周期时间</strong></p><p>我们先来理解一下什么是时钟周期时间。你在买电脑的时候，一定关注过 CPU 的主频。比如我手头的这台电脑就是 Intel Core-i7-7700HQ 2.8GHz，这里的 2.8GHz 就是电脑的主频（Frequency/Clock Rate）。这个 2.8GHz，我们可以先粗浅地认为，<strong>CPU 在 1 秒时间内，可以执行的简单指令的数量是 2.8G 条</strong>。</p><p><strong>2.0GHz意味着每秒钟它会产生20亿个时钟脉冲信号，每个时钟信号周期为0.5纳秒</strong></p><p>如果想要更准确一点描述，这个 2.8GHz 就代表，我们 CPU 的一个“钟表”能够识别出来的最小的时间间隔。就像我们挂在墙上的挂钟，都是“滴答滴答”一秒一秒地走，所以通过墙上的挂钟能够识别出来的最小时间单位就是秒。</p><p>而在 CPU 内部，和我们平时戴的电子石英表类似，有一个<strong>叫晶体振荡器（Oscillator Crystal）的东西</strong>，简称为晶振。我们把晶振当成 CPU 内部的电子表来使用。<strong>晶振带来的每一次“滴答”，就是时钟周期时间</strong>。</p><p>对于 CPU 时钟周期数，我们可以再做一个分解，把它变成“指令数×每条指令的平均时钟周期数（Cycles Per Instruction，简称 CPI）”。不同的指令需要的 Cycles 是不同的，<strong>加法和乘法都对应着一条 CPU 指令</strong>，<strong>但是乘法需要的 Cycles 就比加法要多</strong>，自然也就慢。在这样拆分了之后，我们的程序的 CPU 执行时间就可以变成这样三个部分的乘积。</p><p>**程序的 CPU 执行时间 = 指令数 × CPI × Clock Cycle Time **</p><ul><li>Clock Cycle Time 一次晶振时间，时钟周期</li><li>CPI，每条指令的平均时钟周期数（Cycles Per Instruction，简称 CPI），加法和乘法都对应着一条 CPU 指令，乘法需要的 Cycles 就比加法要多</li></ul><p>因此，如果我们想要解决性能问题，其实就是要优化这三者。</p><ol><li>时钟周期时间，就是计算机主频，这个取决于计算机硬件。我们所熟知的摩尔定律就一直在不停地提高我们计算机的主频。比如说，我最早使用的 80386 主频只有 33MHz，现在手头的笔记本电脑就有 2.8GHz，在主频层面，就提升了将近 100 倍。</li><li>每条指令的平均时钟周期数 CPI，就是一条指令到底需要多少 CPU Cycle。在后面讲解 CPU 结构的时候，我们会看到，现代的 CPU 通过流水线技术（Pipeline），让一条指令需要的 CPU Cycle 尽可能地少。因此，对于 CPI 的优化，也是计算机组成和体系结构中的重要一环。</li><li>指令数，代表执行我们的程序到底需要多少条指令、用哪些指令。这个很多时候就把挑战交给了编译器。同样的代码，编译成计算机指令时候，就有各种不同的表示方式。</li></ol><p><strong>功耗：CPU 的“人体极限”</strong></p><p>于是，从 1978 年 Intel 发布的 8086 CPU 开始，计算机的主频从 5MHz 开始，不断提升。1980 年代中期的 80386 能够跑到 40MHz，1989 年的 486 能够跑到 100MHz，直到 2000 年的奔腾 4 处理器，主频已经到达了 1.4GHz。而消费者也在这 20 年里养成了“看主频”买电脑的习惯。当时已经基本垄断了桌面 CPU 市场的 Intel 更是夸下了海口，表示<strong>奔腾 4 所使用的 CPU 结构可以做到 10GHz</strong>，颇有一点“大力出奇迹”的意思。</p><p>然而，计算机科学界从来不相信“大力出奇迹”。奔腾 4 的 CPU 主频从来没有达到过 10GHz，最终它的主频上限定格在 3.8GHz。这还不是最糟的，更糟糕的事情是，大家发现，奔腾 4 的主频虽然高，但是它的实际性能却配不上同样的主频。想要用在笔记本上的奔腾 4 2.4GHz 处理器，其性能只和基于奔腾 3 架构的奔腾 M 1.6GHz 处理器差不多。</p><p>一个 3.8GHz 的奔腾 4 处理器，满载功率是 130 瓦。这个 130 瓦是什么概念呢？机场允许带上飞机的充电宝的容量上限是 100 瓦时。如果我们把这个 CPU 安在手机里面，不考虑屏幕内存之类的耗电，这个 CPU 满载运行 45 分钟，充电宝里面就没电了。而 iPhone X 使用 ARM 架构的 CPU，功率则只有 4.5 瓦左右。</p><p>我们的 CPU，一般都被叫作超大规模集成电路（Very-Large-Scale Integration，VLSI）。这些电路，实际上都是一个个晶体管组合而成的。CPU 在计算，其实就是让晶体管里面的“开关”不断地去“打开”和“关闭”，来组合完成各种运算和功能。</p><p>想要计算得快，一方面，我们要在 CPU 里，同样的面积里面，多放一些晶体管，<strong>也就是增加密度</strong>；另一方面，我们要让晶体管“打开”和“关闭”得更快一点，<strong>也就是提升主频</strong>。而这两者，都会增加功耗，带来耗电和散热的问题。</p><p>我们会在 CPU 上面抹硅脂、装风扇，乃至用上水冷或者其他更好的散热设备，就好像在工厂里面装风扇、空调，发冷饮一样。但是同样的空间下，装上风扇空调能够带来的散热效果也是有极限的。</p><p>因此，在 CPU 里面，能够放下的晶体管数量和晶体管的“开关”频率也都是有限的。一个 CPU 的功率，可以用这样一个公式来表示：</p><p><strong>功耗 ~= 1/2 ×负载电容×电压的平方×开关频率×晶体管数量</strong></p><ul><li>增加晶体管可以增加硬件能够支持的指令数量，增加数字通路的位数，以及利用好电路天然的并行性，从硬件层面更快地实现特定的指令，所以增加晶体管也是常见的提升cpu性能的一种手段。</li><li>电压的问题在于两个，一个是电压太低就会导致电路无法联通，因为不管用什么作为电路材料，都是有电阻的，所以没有办法无限制降低电压，另外一个是对于工艺的要求也变高了，成本也更贵啊。</li></ul><p>那么，为了要提升性能，我们需要不断地增加晶体管数量。同样的面积下，<strong>我们想要多放一点晶体管，就要把晶体管造得小一点</strong>。这个就是平时我们所说的提升“制程”。从 28nm 到 7nm，相当于晶体管本身变成了原来的 1/4 大小。这个就相当于我们在工厂里，同样的活儿，我们要找瘦小一点的工人，这样一个工厂里面就可以多一些人。我们还要提升主频，让开关的频率变快，也就是要找手脚更快的工人。</p><p>但是，功耗增加太多，就会导致 CPU 散热跟不上，这时，我们就需要降低电压。这里有一点非常关键，在整个功耗的公式里面，功耗和电压的平方是成正比的。这意味着电压下降到原来的 1/5，整个的功耗会变成原来的 1/25。</p><p>事实上，从 5MHz 主频的 8086 到 5GHz 主频的 Intel i9，CPU 的电压已经从 5V 左右下降到了 1V 左右。这也是为什么我们 CPU 的主频提升了 1000 倍，但是功耗只增长了 40 倍。比如说，我写这篇文章用的是 Surface Go，在这样的轻薄笔记本上，微软就是选择了把电压下降到 0.25V 的低电压 CPU，使得笔记本能有更长的续航时间。</p><p>这就引出了我们在进行性能优化中，常常用到的一个经验定律，<strong>阿姆达尔定律（Amdahl’s Law）</strong>。这个定律说的就是，对于一个程序进行优化之后，处理器并行运算之后效率提升的情况。具体可以用这样一个公式来表示：</p><p><strong>优化后的执行时间 = 受优化影响的执行时间 / 加速倍数 + 不受影响的执行时间</strong></p><p>在“摩尔定律”和“并行计算”之外，在整个计算机组成层面，还有这样几个原则性的性能提升方法。</p><ol><li>加速大概率事件。最典型的就是，过去几年流行的深度学习，整个计算过程中，99% 都是向量和矩阵计算，于是，工程师们通过用 GPU 替代 CPU，大幅度提升了深度学习的模型训练过程。本来一个 CPU 需要跑几小时甚至几天的程序，GPU 只需要几分钟就好了。Google 更是不满足于 GPU 的性能，进一步地推出了 TPU。后面的文章，我也会为你讲解 GPU 和 TPU 的基本构造和原理。</li><li>通过流水线提高性能。现代的工厂里的生产线叫“流水线”。我们可以把装配 iPhone 这样的任务拆分成一个个细分的任务，让每个人都只需要处理一道工序，最大化整个工厂的生产效率。类似的，我们的 CPU 其实就是一个“运算工厂”。我们把 CPU 指令执行的过程进行拆分，细化运行，也是现代 CPU 在主频没有办法提升那么多的情况下，性能仍然可以得到提升的重要原因之一。我们在后面也会讲到，现代 CPU 里是如何通过流水线来提升性能的，以及反面的，过长的流水线会带来什么新的功耗和效率上的负面影响。</li><li>通过预测提高性能。通过预先猜测下一步该干什么，而不是等上一步运行的结果，提前进行运算，也是让程序跑得更快一点的办法。典型的例子就是在一个循环访问数组的时候，凭经验，你也会猜到下一步我们会访问数组的下一项。后面要讲的“分支和冒险”、“局部性原理”这些 CPU 和存储系统设计方法，其实都是在利用我们对于未来的“预测”，提前进行相应的操作，来提升我们的程序性能。</li></ol><h2 id="指令和运算"><a href="#指令和运算" class="headerlink" title="指令和运算"></a>指令和运算</h2><h3 id="CPU指令"><a href="#CPU指令" class="headerlink" title="CPU指令"></a>CPU指令</h3><p>我们就从平时用的电脑、手机这些设备来说起。这些设备的 CPU 到底有哪些指令呢？这个还真有不少，我们日常用的 Intel CPU，有 2000 条左右的 CPU 指令，实在是太多了，所以我没法一一来给你讲解。不过一般来说，常见的指令可以分成五大类。</p><p>第一类是算术类指令。我们的加减乘除，在 CPU 层面，都会变成一条条算术类指令。</p><p>第二类是数据传输类指令。给变量赋值、在内存里读写数据，用的都是数据传输类指令。</p><p>第三类是逻辑类指令。逻辑上的与或非，都是这一类指令。</p><p>第四类是条件分支类指令。日常我们写的“if/else”，其实都是条件分支类指令。</p><p>最后一类是无条件跳转指令。写一些大一点的程序，我们常常需要写一些函数或者方法。在调用函数的时候，其实就是发起了一个无条件跳转指令。</p><p>你可能一下子记不住，或者对这些指令的含义还不能一下子掌握，这里我画了一个表格，给你举例子说明一下，帮你理解、记忆。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7b2214ad75601b86.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们说过，不同的 CPU 有不同的指令集，也就对应着不同的汇编语言和不同的机器码。为了方便你快速理解这个机器码的计算方式，我们选用最简单的 MIPS 指令集，来看看机器码是如何生成的。</p><p>MIPS 是一组由 MIPS 技术公司在 80 年代中期设计出来的 CPU 指令集。就在最近，MIPS 公司把整个指令集和芯片架构都完全开源了。想要深入研究 CPU 和指令集的同学，我这里推荐一些资料，你可以自己了解下。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-769066c26ebc1548.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>R 指令是一般用来做算术和逻辑操作，里面有读取和写入数据的寄存器的地址。如果是逻辑位移操作，后面还有位移操作的位移量，而最后的功能码，则是在前面的操作码不够的时候，扩展操作码表示对应的具体指令的。</p><p>I 指令，则通常是用在数据传输、条件分支，以及在运算的时候使用的并非变量还是常数的时候。这个时候，没有了位移量和操作码，也没有了第三个寄存器，而是把这三部分直接合并成了一个地址值或者一个常数。</p><p>J 指令就是一个跳转指令，高 6 位之外的 26 位都是一个跳转后的地址。</p><pre><code>add $t0,$s2,$s1</code></pre><p>我以一个简单的加法算术指令 add t0,s1, $s2, 为例，给你解释。为了方便，我们下面都用十进制来表示对应的代码。</p><p>对应的 MIPS 指令里 opcode 是 0，rs 代表第一个寄存器 s1 的地址是 17，rt 代表第二个寄存器 s2 的地址是 18，rd 代表目标的临时寄存器 t0 的地址，是 8。因为不是位移操作，所以位移量是 0。把这些数字拼在一起，就变成了一个 MIPS 的加法指令。</p><p>为了读起来方便，我们一般把对应的二进制数，用 16 进制表示出来。在这里，也就是 0X02324020。这个数字也就是这条指令对应的机器码。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3c886840aa1a66ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>回到开头我们说的打孔带。如果我们用打孔代表 1，没有打孔代表 0，用 4 行 8 列代表一条指令来打一个穿孔纸带，那么这条命令大概就长这样：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c6bdac9ea3027f0f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-491acb33247cedf0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>一个 CPU 里面会有很多种不同功能的寄存器。我这里给你介绍三种比较特殊的。</p><p>一个是 PC 寄存器（Program Counter Register），我们也叫指令地址寄存器（Instruction Address Register）。顾名思义，它就是用来存放下一条需要执行的计算机指令的内存地址。</p><p>第二个是指令寄存器（Instruction Register），用来存放当前正在执行的指令。</p><p>第三个是条件码寄存器（Status Register），用里面的一个一个标记位（Flag），存放 CPU 进行算术或者逻辑计算的结果。</p><p>除了这些特殊的寄存器，CPU 里面还有更多用来存储数据和内存地址的寄存器。这样的寄存器通常一类里面不止一个。我们通常根据存放的数据内容来给它们取名字，比如整数寄存器、浮点数寄存器、向量寄存器和地址寄存器等等。有些寄存器既可以存放数据，又能存放地址，我们就叫它通用寄存器。</p><p>实际上，一个程序执行的时候，CPU 会根据 PC 寄存器里的地址，从内存里面把需要执行的指令读取到指令寄存器里面执行，然后根据指令长度自增，开始顺序读取下一条指令。可以看到，一个程序的一条条指令，在内存里面是连续保存的，也会一条条顺序加载。</p><pre><code>$ gcc -g -c test.c$ objdump -d -M intel -S test.o </code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7bcbb4e28b5ee15e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="编译链接"><a href="#编译链接" class="headerlink" title="编译链接"></a>编译链接</h3><p>GCC 预处理、编译、汇编、链接.</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2cf1a8e01fdeac03.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="9EEB42F4-31F5-4A88-8D70-C17B3F0695B2.png"></p><p>实际上，“C 语言代码 - 汇编代码 - 机器码” 这个过程，在我们的计算机上进行的时候是由两部分组成的。</p><p>第一个部分由编译（Compile）、汇编（Assemble）以及链接（Link）三个阶段组成。在这三个阶段完成之后，我们就生成了一个可执行文件。</p><p>第二部分，我们通过装载器（Loader）把可执行文件装载（Load）到内存中。CPU 从内存中读取指令和数据，来开始真正执行程序。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d2d477df06520238.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>你会发现，可执行代码 dump 出来内容，和之前的目标代码长得差不多，但是长了很多。因为在 Linux 下，可执行文件和目标文件所使用的都是一种叫 ELF（Execuatable and Linkable File Format）的文件格式，中文名字叫可执行与可链接文件格式，这里面不仅存放了编译成的汇编指令，还保留了很多别的数据。</p><p>比如我们过去所有 objdump 出来的代码里，你都可以看到对应的函数名称，像 add、main 等等，乃至你自己定义的全局可以访问的变量名称，都存放在这个 ELF 格式文件里。这些名字和它们对应的地址，在 ELF 文件里面，<strong>存储在一个叫作符号表（Symbols Table）</strong>的位置里。符号表相当于一个地址簿，把名字和地址关联了起来。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f59f61949cff549d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>ELF 文件格式把各种信息，分成一个一个的 Section 保存起来。ELF 有一个基本的文件头（File Header），用来表示这个文件的基本属性，比如是否是可执行文件，对应的 CPU、操作系统等等。除了这些基本属性之外，大部分程序还有这么一些 Section：</p><ol><li>首先是.text Section，也叫作代码段或者指令段（Code Section），用来保存程序的代码和指令；</li><li>接着是.data Section，也叫作数据段（Data Section），用来保存程序里面设置好的初始化数据信息；</li><li>然后就是.rel.text Secion，叫作重定位表（Relocation Table）。重定位表里，保留的是当前的文件里面，哪些跳转地址其实是我们不知道的。比如上面的 link_example.o 里面，我们在 main 函数里面调用了 add 和 printf 这两个函数，但是在链接发生之前，我们并不知道该跳转到哪里，这些信息就会存储在重定位表里；</li><li>最后是.symtab Section，叫作符号表（Symbol Table）。符号表保留了我们所说的当前文件里面定义的函数名称和对应地址的地址簿。</li></ol><p>链接器会扫描所有输入的目标文件，然后把所有符号表里的信息收集起来，构成一个全局的符号表。然后再根据重定位表，把所有不确定要跳转地址的代码，根据符号表里面存储的地址，进行一次修正。最后，把所有的目标文件的对应段进行一次合并，变成了最终的可执行代码。这也是为什么，可执行文件里面的函数调用的地址都是正确的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e27bbef810131516.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>链接可以分动、静，共享运行省内存</strong></p><p>我们上一节解决程序装载到内存的时候，讲了很多方法。说起来，最根本的问题其实就是内存空间不够用。如果我们能够让同样功能的代码，在不同的程序里面，不需要各占一份内存空间，那该有多好啊！就好比，现在马路上的共享单车，我们并不需要给每个人都造一辆自行车，只要马路上有这些单车，谁需要的时候，直接通过手机扫码，都可以解锁骑行。</p><p>这个思路就引入一种新的链接方法，叫作动态链接（Dynamic Link）。相应的，我们之前说的合并代码段的方法，就是静态链接（Static Link）。</p><p>在动态链接的过程中，我们想要“链接”的，不是存储在硬盘上的目标文件代码，而是加载到内存中的共享库（Shared Libraries）。顾名思义，这里的共享库重在“共享“这两个字。</p><p>这个加载到内存中的共享库会被很多个程序的指令调用到。在 Windows 下，这些共享库文件就是.dll 文件，也就是 Dynamic-Link Libary（DLL，动态链接库）。在 Linux 下，这些共享库文件就是.so 文件，也就是 Shared Object（一般我们也称之为动态链接库）。这两大操作系统下的文件名后缀，一个用了“动态链接”的意思，另一个用了“共享”的意思，正好覆盖了两方面的含义。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-450a259a806ddd07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>不过，要想要在程序运行的时候共享代码，也有一定的要求，就是这些机器码必须是“地址无关”的。也就是说，我们编译出来的共享库文件的指令代码，是地址无关码（Position-Independent Code）。换句话说就是，这段代码，无论加载在哪个内存地址，都能够正常执行。如果不是这样的代码，就是地址相关的代码。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-460471b40943b9d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>对于所有动态链接共享库的程序来讲，虽然我们的共享库用的都是同一段物理内存地址，但是在不同的应用程序里，它所在的虚拟内存地址是不同的。我们没办法、也不应该要求动态链接同一个共享库的不同程序，必须把这个共享库所使用的虚拟内存地址变成一致。如果这样的话，我们写的程序就必须明确地知道内部的内存地址分配。</p><p>那么问题来了，我们要怎么样才能做到，动态共享库编译出来的代码指令，都是地址无关码呢？</p><p>动态代码库内部的变量和函数调用都很容易解决，我们只需要使用相对地址（Relative Address）就好了。各种指令中使用到的内存地址，给出的不是一个绝对的地址空间，而是一个相对于当前指令偏移量的内存地址。因为整个共享库是放在一段连续的虚拟内存地址中的，无论装载到哪一段地址，不同指令之间的相对地址都是不变的。</p><p><strong>PLT 和 GOT，动态链接的解决方案</strong></p><p>要实现动态链接共享库，也并不困难，和前面的静态链接里的符号表和重定向表类似，还是和前面一样，我们还是拿出一小段代码来看一看。</p><p>首先，lib.h 定义了动态链接库的一个函数 show_me_the_money。</p><pre><code>// lib.h#ifndef LIB_H#define LIB_Hvoid show_me_the_money(int money);#endif</code></pre><p>lib.c 包含了 lib.h 的实际实现。</p><pre><code>// lib.c#include &lt;stdio.h&gt;void show_me_the_money(int money){    printf("Show me USD %d from lib.c \n", money);}</code></pre><p>然后，show_me_poor.c 调用了 lib 里面的函数。</p><pre><code>// show_me_poor.c#include "lib.h"int main(){    int money = 5;    show_me_the_money(money);}</code></pre><p>最后，我们把 lib.c 编译成了一个动态链接库，也就是 .so 文件。</p><pre><code>$ gcc lib.c -fPIC -shared -o lib.so$ gcc -o show_me_poor show_me_poor.c ./lib.so</code></pre><p>你可以看到，在编译的过程中，我们指定了一个 -fPIC 的参数。这个参数其实就是 Position Independent Code 的意思，也就是我们要把这个编译成一个地址无关代码。</p><p>然后，我们再通过 gcc 编译 <code>show_me_poor</code> 动态链接了 lib.so 的可执行文件。在这些操作都完成了之后，我们把 <code>show_me_poor</code> 这个文件通过 objdump 出来看一下。</p><pre><code>$ objdump -d -M intel -S show_me_poor    ……0000000000400540 &lt;show_me_the_money@plt-0x10&gt;:  400540:       ff 35 12 05 20 00       push   QWORD PTR [rip+0x200512]        # 600a58 &lt;_GLOBAL_OFFSET_TABLE_+0x8&gt;  400546:       ff 25 14 05 20 00       jmp    QWORD PTR [rip+0x200514]        # 600a60 &lt;_GLOBAL_OFFSET_TABLE_+0x10&gt;  40054c:       0f 1f 40 00             nop    DWORD PTR [rax+0x0]0000000000400550 &lt;show_me_the_money@plt&gt;:  400550:       ff 25 12 05 20 00       jmp    QWORD PTR [rip+0x200512]        # 600a68 &lt;_GLOBAL_OFFSET_TABLE_+0x18&gt;  400556:       68 00 00 00 00          push   0x0  40055b:       e9 e0 ff ff ff          jmp    400540 &lt;_init+0x28&gt;……0000000000400676 &lt;main&gt;:  400676:       55                      push   rbp  400677:       48 89 e5                mov    rbp,rsp  40067a:       48 83 ec 10             sub    rsp,0x10  40067e:       c7 45 fc 05 00 00 00    mov    DWORD PTR [rbp-0x4],0x5  400685:       8b 45 fc                mov    eax,DWORD PTR [rbp-0x4]  400688:       89 c7                   mov    edi,eax  40068a:       e8 c1 fe ff ff          call   400550 &lt;show_me_the_money@plt&gt;  40068f:       c9                      leave    400690:       c3                      ret      400691:       66 2e 0f 1f 84 00 00    nop    WORD PTR cs:[rax+rax*1+0x0]  400698:       00 00 00   40069b:       0f 1f 44 00 00          nop    DWORD PTR [rax+rax*1+0x0]……</code></pre><p>我们还是只关心整个可执行文件中的一小部分内容。你应该可以看到，在 main 函数调用 show_me_the_money 的函数的时候，对应的代码是这样的：</p><pre><code>call   400550 &lt;show_me_the_money@plt&gt;</code></pre><p>这里后面有一个 @plt 的关键字，代表了我们需要从 PLT，也就是程序链接表（Procedure Link Table）里面找要调用的函数。对应的地址呢，则是 400550 这个地址。</p><p>那当我们把目光挪到上面的 400550 这个地址，你又会看到里面进行了一次跳转，这个跳转指定的跳转地址，你可以在后面的注释里面可以看到，GLOBAL_OFFSET_TABLE+0x18。这里的 GLOBAL_OFFSET_TABLE，就是我接下来要说的全局偏移表。</p><pre><code>  400550:       ff 25 12 05 20 00       jmp    QWORD PTR [rip+0x200512]        # 600a68 &lt;_GLOBAL_OFFSET_TABLE_+0x18&gt;  </code></pre><p>在动态链接对应的共享库，我们在共享库的 data section 里面，保存了一张全局偏移表（GOT，Global Offset Table）。虽然共享库的代码部分的物理内存是共享的，但是数据部分是各个动态链接它的应用程序里面各加载一份的。所有需要引用当前共享库外部的地址的指令，都会查询 GOT，来找到当前运行程序的虚拟内存里的对应位置。而 GOT 表里的数据，则是在我们加载一个个共享库的时候写进去的。</p><p>不同的进程，调用同样的 lib.so，各自 GOT 里面指向最终加载的动态链接库里面的虚拟内存地址是不同的。</p><p>这样，虽然不同的程序调用的同样的动态库，各自的内存地址是独立的，调用的又都是同一个动态库，但是不需要去修改动态库里面的代码所使用的地址，而是各个程序各自维护好自己的 GOT，能够找到对应的动态库就好了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c673082a9ab66e37.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们的 GOT 表位于共享库自己的数据段里。GOT 表在内存里和对应的代码段位置之间的偏移量，始终是确定的。这样，我们的共享库就是地址无关的代码，对应的各个程序只需要在物理内存里面加载同一份代码。而我们又要通过各个可执行程序在加载时，生成的各不相同的 GOT 表，来找到它需要调用到的外部变量和函数的地址。</p><p>这是一个典型的、不修改代码，而是通过修改“地址数据”来进行关联的办法。它有点像我们在 C 语言里面用函数指针来调用对应的函数，并不是通过预先已经确定好的函数名称来调用，而是利用当时它在内存里面的动态地址来调用。</p><h3 id="字符集和字符编码"><a href="#字符集和字符编码" class="headerlink" title="字符集和字符编码"></a>字符集和字符编码</h3><p>ASCII 码只表示了 128 个字符，一开始倒也堪用，毕竟计算机是在美国发明的。然而随着越来越多的不同国家的人都用上了计算机，想要表示譬如中文这样的文字，128 个字符显然是不太够用的。于是，计算机工程师们开始各显神通，给自己国家的语言创建了对应的字符集（Charset）和字符编码（Character Encoding）。</p><p>字符集，表示的可以是字符的一个集合。比如“中文”就是一个字符集，不过这样描述一个字符集并不准确。想要更精确一点，我们可以说，“第一版《新华字典》里面出现的所有汉字”，这是一个字符集。这样，我们才能明确知道，一个字符在不在这个集合里面。比如，我们日常说的 Unicode，其实就是一个字符集，包含了 150 种语言的 14 万个不同的字符。</p><p>而字符编码则是对于字符集里的这些字符，怎么一一用二进制表示出来的一个字典。我们上面说的 Unicode，就可以用 UTF-8、UTF-16，乃至 UTF-32 来进行编码，存储成二进制。所以，有了 Unicode，其实我们可以用不止 UTF-8 一种编码形式，我们也可以自己发明一套 GT-32 编码，比如就叫作 Geek Time 32 好了。只要别人知道这套编码规则，就可以正常传输、显示这段代码。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-faf0950f547f4859.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>同样的文本，采用不同的编码存储下来。如果另外一个程序，用一种不同的编码方式来进行解码和展示，就会出现乱码。这就好像两个军队用密语通信，如果用错了密码本，那看到的消息就会不知所云。在中文世界里，最典型的就是“手持两把锟斤拷，口中疾呼烫烫烫”的典故。</p><h3 id="电路与运算"><a href="#电路与运算" class="headerlink" title="电路与运算"></a>电路与运算</h3><p>可以说，电报是现代计算机的一个最简单的原型。它和我们现在使用的现代计算机有很多相似之处。我们通过电路的“开”和“关”，来表示“1”和“0”。就像晶体管在不同的情况下，表现为导电的“1”和绝缘的“0”的状态。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0983b241a778d792.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>其实加法器就是想一个办法把这三排开关电路连起来</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-27e05244eff120c2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>讲与、或、非门的时候，我们很容易就能和程序里面的“AND（通常是 &amp; 符号）”“ OR（通常是 | 符号）”和“ NOT（通常是 ! 符号）”对应起来。可能你没有想过，为什么我们会需要“异或（XOR）”，这样一个在逻辑运算里面没有出现的形式，作为一个基本电路。其实，<strong>异或门就是一个最简单的整数加法，所需要使用的基本门电路</strong>。</p><p>算完个位的输出还不算完，输入的两位都是 11 的时候，我们还需要向更左侧的一位进行进位。那这个就对应一个与门，也就是有且只有在加数和被加数都是 1 的时候，我们的进位才会是 1。</p><p>所以，通过一个异或门计算出个位，通过一个与门计算出是否进位，我们就通过电路算出了一个一位数的加法。于是，我们把两个门电路打包，给它取一个名字，就叫作半加器（Half Adder）。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-96ac78e98a83ae61.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>全加器</strong></p><p>你肯定很奇怪，为什么我们给这样的电路组合，取名叫半加器（Half Adder）？莫非还有一个全加器（Full Adder）么？你猜得没错。半加器可以解决个位的加法问题，但是如果放到二位上来说，就不够用了。我们这里的竖式是个二进制的加法，所以如果从右往左数，第二列不是十位，我称之为“二位”。对应的再往左，就应该分别是四位、八位。</p><p>二位用一个半加器不能计算完成的原因也很简单。因为二位除了一个加数和被加数之外，还需要加上来自个位的进位信号，一共需要三个数进行相加，才能得到结果。但是我们目前用到的，无论是最简单的门电路，还是用两个门电路组合而成的半加器，输入都只能是两个 bit，也就是两个开关。那我们该怎么办呢？</p><p>实际上，解决方案也并不复杂。我们用两个半加器和一个或门，就能组合成一个全加器。第一个半加器，我们用和个位的加法一样的方式，得到是否进位 X 和对应的二个数加和后的结果 Y，这样两个输出。然后，我们把这个加和后的结果 Y，和个位数相加后输出的进位信息 U，再连接到一个半加器上，就会再拿到一个是否进位的信号 V 和对应的加和后的结果 W。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-96ac78e98a83ae61.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f351984cd2c30046.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><pre><code>int add(int a,int b){    int carry,add;    do{        add=a^b;        carry=(a&amp;b)&lt;&lt;1;        a=add;        b=carry;    }while(carry!=0);    return add;}</code></pre><p>有了全加器，我们要进行对应的两个 8 bit 数的加法就很容易了。我们只要把 8 个全加器串联起来就好了。个位的全加器的进位信号作为二位全加器的输入信号，二位全加器的进位信号再作为四位的全加器的进位信号。这样一层层串接八层，我们就得到了一个支持 8 位数加法的算术单元。如果要扩展到 16 位、32 位，乃至 64 位，都只需要多串联几个输入位和全加器就好了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8760f04ba1a88e5e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-63bac21563f000d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>顺序乘法的实现过程</strong></p><p>十进制中的 13 乘以 9，计算的结果应该是 117。我们通过转换成二进制，然后列竖式的办法，来看看整个计算的过程是怎样的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-974eeb484b674d35.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在 13×9 这个例子里面，被乘数 13 表示成二进制是 1101，乘数 9 在二进制里面是 1001。最右边的个位是 1，所以个位乘以被乘数，就是把被乘数 1101 复制下来。因为二位和四位都是 0，所以乘以被乘数都是 0，那么保留下来的都是 0000。乘数的八位是 1，我们仍然需要把被乘数 1101 复制下来。不过这里和个位位置的单纯复制有一点小小的差别，那就是要把复制好的结果向左侧移三位，然后把四位单独进行乘法加位移的结果，再加起来，我们就得到了最终的计算结果。</p><p>对应到我们之前讲的数字电路和 ALU，你可以看到，最后一步的加法，我们可以用上一讲的加法器来实现。乘法因为只有“0”和“1”两种情况，所以可以做成输入输出都是 4 个开关，中间用 1 个开关，同时来控制这 8 个开关的方式，这就实现了二进制下的单位的乘法。</p><p>我们可以用一个开关来决定，下面的输出是完全复制输入，还是将输出全部设置为 0</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-80a12c47d507f345.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>至于位移也不麻烦，我们只要不是直接连线，把正对着的开关之间进行接通，而是斜着错开位置去接就好了。如果要左移一位，就错开一位接线；如果要左移两位，就错开两位接线。</p><p>把对应的线路错位连接，就可以起到位移的作用</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e2951a8b02e7f17b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这样，你会发现，我们并不需要引入任何新的、更复杂的电路，仍然用最基础的电路，只要用不同的接线方式，就能够实现一个“列竖式”的乘法。而且，因为二进制下，只有 0 和 1，也就是开关的开和闭这两种情况，所以我们的计算机也不需要去“背诵”九九乘法口诀表，不需要单独实现一个更复杂的电路，就能够实现乘法。</p><p>为了节约一点开关，也就是晶体管的数量。实际上，像 13×9 这样两个四位数的乘法，我们不需要把四次单位乘法的结果，用四组独立的开关单独都记录下来，然后再把这四个数加起来。因为这样做，需要很多组开关，如果我们计算一个 32 位的整数乘法，就要 32 组开关，太浪费晶体管了。如果我们顺序地来计算，只需要一组开关就好了。</p><p>我们先拿乘数最右侧的个位乘以被乘数，然后把结果写入用来存放计算结果的开关里面，然后，把被乘数左移一位，把乘数右移一位，仍然用乘数去乘以被乘数，然后把结果加到刚才的结果上。反复重复这一步骤，直到不能再左移和右移位置。这样，乘数和被乘数就像两列相向而驶的列车，仅仅需要简单的加法器、一个可以左移一位的电路和一个右移一位的电路，就能完成整个乘法。</p><p>乘法器硬件结构示意图</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-162408c1ff3de686.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>你看这里画的乘法器硬件结构示意图。这里的控制测试，其实就是通过一个时钟信号，来控制左移、右移以及重新计算乘法和加法的时机。我们还是以计算 13×9，也就是二进制的 1101×1001 来具体看。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9a9d5660140b58e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>这个计算方式虽然节约电路了，但是也有一个很大的缺点，那就是慢。</strong></p><p>你应该很容易就能发现，在这个乘法器的实现过程里，我们其实就是把乘法展开，变成了“加法 + 位移”来实现。我们用的是 4 位数，所以要进行 4 组“位移 + 加法”的操作。而且这 4 组操作还不能同时进行。因为下一组的加法要依赖上一组的加法后的计算结果，下一组的位移也要依赖上一组的位移的结果。这样，整个算法是“顺序”的，每一组加法或者位移的运算都需要一定的时间。</p><p>所以，最终这个乘法的计算速度，其实和我们要计算的数的位数有关。比如，这里的 4 位，就需要 4 次加法。而我们的现代 CPU 常常要用 32 位或者是 64 位来表示整数，那么对应就需要 32 次或者 64 次加法。比起 4 位数，要多花上 8 倍乃至 16 倍的时间。</p><p>换个我们在算法和数据结构中的术语来说就是，**这样的一个顺序乘法器硬件进行计算的时间复杂度是 O(N)**。这里的 N，就是乘法的数里面的位数。</p><p><strong>并行加速方法</strong></p><p>前面顺序乘法器硬件的实现办法，就好像体育比赛里面的单败淘汰赛。只有一个擂台会存下最新的计算结果。每一场新的比赛就来一个新的选手，实现一次加法，实现完了剩下的还是原来那个守擂的，直到其余 31 个选手都上来比过一场。如果一场比赛需要一天，那么一共要比 31 场，也就是 31 天。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-baf971187d00fad2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>加速的办法，就是把比赛变成像世界杯足球赛那样的淘汰赛，32 个球队捉对厮杀，同时开赛。这样一天一下子就淘汰了 16 支队，也就是说，32 个数两两相加后，你可以得到 16 个结果。后面的比赛也是一样同时开赛捉对厮杀。只需要 5 天，也就是 O(log2N) 的时间，就能得到计算的结果。但是这种方式要求我们得有 16 个球场。因为在淘汰赛的第一轮，我们需要 16 场比赛同时进行。对应到我们 CPU 的硬件上，就是需要更多的晶体管开关，来放下中间计算结果。</p><p>通过并联更多的 ALU，加上更多的寄存器，我们也能加速乘法</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-22cde17db84e7587.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>电路并行</strong></p><p>上面我们说的并行加速的办法，看起来还是有点儿笨。我们回头来做一个抽象的思考。之所以我们的计算会慢，核心原因其实是“顺序”计算，也就是说，要等前面的计算结果完成之后，我们才能得到后面的计算结果。</p><p>最典型的例子就是我们上一讲讲的加法器。每一个全加器，都要等待上一个全加器，把对应的进入输入结果算出来，才能算下一位的输出。位数越多，越往高位走，等待前面的步骤就越多，这个等待的时间有个专门的名词，叫作门延迟（Gate Delay）。</p><p>每通过一个门电路，我们就要等待门电路的计算结果，就是一层的门电路延迟，我们一般给它取一个“T”作为符号。一个全加器，其实就已经有了 3T 的延迟（进位需要经过 3 个门电路）。而 4 位整数，最高位的计算需要等待前面三个全加器的进位结果，也就是要等 9T 的延迟。如果是 64 位整数，那就要变成 63×3=189T 的延迟。这可不是个小数字啊！</p><p>除了门延迟之外，还有一个问题就是<strong>时钟频率</strong>。在上面的顺序乘法计算里面，如果我们想要用更少的电路，计算的中间结果需要保存在寄存器里面，然后等待下一个时钟周期的到来，控制测试信号才能进行下一次移位和加法，这个延迟比上面的门延迟更可观。</p><p>那么，我们有什么办法可以解决这个问题呢？实际上，在我们进行加法的时候，如果相加的两个数是确定的，那高位是否会进位其实也是确定的。对于我们人来说，我们本身去做计算都是顺序执行的，所以要一步一步计算进位。但是，计算机是连结的各种线路。我们不用让计算机模拟人脑的思考方式，来连结线路。</p><p>那怎么才能把线路连结得复杂一点，让高位和低位的计算同时出结果呢？怎样才能让高位不需要等待低位的进位结果，而是把低位的所有输入信号都放进来，直接计算出高位的计算结果和进位结果呢？</p><p>我们只要把进位部分的电路完全展开就好了。我们的半加器到全加器，再到加法器，都是用最基础的门电路组合而成的。门电路的计算逻辑，可以像我们做数学里面的多项式乘法一样完全展开。在展开之后呢，我们可以把原来需要较少的，但是有较多层前后计算依赖关系的门电路，展开成需要较多的，但是依赖关系更少的门电路。</p><p>我在这里画了一个示意图，展示了一下我们加法器。如果我们完全展开电路，高位的进位和计算结果，可以和低位的计算结果同时获得。这个的核心原因是电路是天然并行的，一个输入信号，可以同时传播到所有接通的线路当中。</p><p>C4 是前 4 位的计算结果是否进位的门电路表示</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0e5fce493122185b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果一个 4 位整数最高位是否进位，展开门电路图，你会发现，我们只需要 3T 的延迟就可以拿到是否进位的计算结果。而对于 64 位的整数，也不会增加门延迟，只是从上往下复制这个电路，接入更多的信号而已。看到没？我们通过把电路变复杂，就解决了延迟的问题。</p><p>这个优化，本质上是利用了电路天然的并行性。电路只要接通，输入的信号自动传播到了所有接通的线路里面，这其实也是硬件和软件最大的不同。</p><p>无论是这里把对应的门电路逻辑进行完全展开以减少门延迟，还是上面的乘法通过并行计算多个位的乘法，都是把我们完成一个计算的电路变复杂了。而电路变复杂了，也就意味着晶体管变多了。</p><p>之前很多同学在我们讨论计算机的性能问题的时候，都提到，为什么晶体管的数量增加可以优化计算机的计算性能。实际上，这里的门电路展开和上面的并行计算乘法都是很好的例子。<strong>我们通过更多的晶体管，就可以拿到更低的门延迟，以及用更少的时钟周期完成一个计算指令</strong>。</p><p><strong>浮点数的不精确性</strong></p><p>你可以在 Linux 下打开 Python 的命令行 Console，也可以在 Chrome 浏览器里面通过开发者工具，打开浏览器里的 Console，在里面输入“0.3 + 0.6”，然后看看你会得到一个什么样的结果。</p><pre><code>&gt;&gt;&gt; 0.3 + 0.60.8999999999999999</code></pre><p>不知道你有没有大吃一惊，这么简单的一个加法，无论是在 Python 还是在 JavaScript 里面，算出来的结果居然不是准确的 0.9，而是 0.8999999999999999 这么个结果。这是为什么呢？</p><p><strong>定点数的表示</strong></p><p>有一个很直观的想法，就是我们用 4 个比特来表示 0～9 的整数，那么 32 个比特就可以表示 8 个这样的整数。然后我们把最右边的 2 个 0～9 的整数，当成小数部分；把左边 6 个 0～9 的整数，当成整数部分。这样，我们就可以用 32 个比特，来表示从 0 到 999999.99 这样 1 亿个实数了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ca2e4495993558b1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这种用二进制来表示十进制的编码方式，叫作BCD 编码（Binary-Coded Decimal）。其实它的运用非常广泛，最常用的是在超市、银行这样需要用小数记录金额的情况里。在超市里面，我们的小数最多也就到分。这样的表示方式，比较直观清楚，也满足了小数部分的计算。</p><p>不过，这样的表示方式也有几个缺点。</p><p>第一，这样的表示方式有点“浪费”。本来 32 个比特我们可以表示 40 亿个不同的数，但是在 BCD 编码下，只能表示 1 亿个数，如果我们要精确到分的话，那么能够表示的最大金额也就是到 100 万。如果我们的货币单位是人民币或者美元还好，如果我们的货币单位变成了津巴布韦币，这个数量就不太够用了。</p><p>第二，这样的表示方式没办法同时表示很大的数字和很小的数字。我们在写程序的时候，实数的用途可能是多种多样的。有时候我们想要表示商品的金额，关心的是 9.99 这样小的数字；有时候，我们又要进行物理学的运算，需要表示光速，也就是 3×108 这样很大的数字。那么，我们有没有一个办法，既能够表示很小的数，又能表示很大的数呢？</p><p><strong>浮点数的表示</strong></p><p>答案当然是有的，就是你可能经常听说过的浮点数（Floating Point），也就是 float 类型。</p><p>在计算机里，我们也可以用一样的办法，用科学计数法来表示实数。浮点数的科学计数法的表示，有一个 IEEE 的标准，它定义了两个基本的格式。一个是用 32 比特表示单精度的浮点数，也就是我们常常说的 float 或者 float32 类型。另外一个是用 64 比特表示双精度的浮点数，也就是我们平时说的 double 或者 float64 类型。</p><p>双精度类型和单精度类型差不多，这里，我们来看单精度类型，双精度你自然也就明白了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-63d682e34c5229e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>第一部分是一个符号位，用来表示是正数还是负数。我们一般用 s 来表示。在浮点数里，我们不像正数分符号数还是无符号数，所有的浮点数都是有符号的。</p><p>接下来是一个 8 个比特组成的指数位。我们一般用 e 来表示。8 个比特能够表示的整数空间，就是 0～255。我们在这里用 1～254 映射到 -126～127 这 254 个有正有负的数上。因为我们的浮点数，不仅仅想要表示很大的数，还希望能够表示很小的数，所以指数位也会有负数。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-df7a04484879e3a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="7277BB65-A362-48BE-9E15-D7E497F5ED3B.png"></p><p>我们可以做一个简单的实验，用一个循环相加 2000 万个 1.0f，最终的结果会是 1600 万左右，而不是 2000 万。这是因为，加到 1600 万之后的加法因为精度丢失都没有了。这个代码比起上面的使用 2000 万来加 1.0 更具有现实意义。</p><pre><code>public class FloatPrecision {  public static void main(String[] args) {    float sum = 0.0f;    for (int i = 0; i &lt; 20000000; i++) {      float x = 1.0f;      sum += x;          }    System.out.println("sum is " + sum);     }  }</code></pre><p>对应的输出结果是： </p><pre><code>sum is 1.6777216E7</code></pre><p>面对这个问题，聪明的计算机科学家们也想出了具体的解决办法。他们发明了一种叫作Kahan Summation的算法来解决这个问题。算法的对应代码我也放在文稿中了。从中你可以看到，同样是 2000 万个 1.0f 相加，用这种算法我们得到了准确的 2000 万的结果。</p><pre><code>public class KahanSummation {  public static void main(String[] args) {    float sum = 0.0f;    float c = 0.0f;    for (int i = 0; i &lt; 20000000; i++) {      float x = 1.0f;      float y = x - c;      float t = sum + y;      c = (t-sum)-y;      sum = t;          }    System.out.println("sum is " + sum);     }  }</code></pre><p>其实这个算法的原理其实并不复杂，就是在每次的计算过程中，都用一次减法，把当前加法计算中损失的精度记录下来，然后在后面的循环中，把这个精度损失放在要加的小数上，再做一次运算。</p><h2 id="处理器"><a href="#处理器" class="headerlink" title="处理器"></a>处理器</h2><h3 id="指令-运算"><a href="#指令-运算" class="headerlink" title="指令 + 运算"></a>指令 + 运算</h3><p><strong>指令周期（Instruction Cycle）</strong></p><p>前面讲计算机机器码的时候，我向你介绍过 PC 寄存器、指令寄存器，还介绍过 MIPS 体系结构的计算机所用到的 R、I、J 类指令。如果我们仔细看一看，可以发现，计算机每执行一条指令的过程，可以分解成这样几个步骤。</p><ol><li>Fetch（取得指令），也就是从 PC 寄存器里找到对应的指令地址，根据指令地址从内存里把具体的指令，加载到指令寄存器中，然后把 PC 寄存器自增，好在未来执行下一条指令。</li><li>Decode（指令译码），也就是根据指令寄存器里面的指令，解析成要进行什么样的操作，是 R、I、J 中的哪一种指令，具体要操作哪些寄存器、数据或者内存地址。</li><li>Execute（执行指令），也就是实际运行对应的 R、I、J 这些特定的指令，进行算术逻辑操作、数据传输或者直接的地址跳转。</li></ol><p>这样的步骤，其实就是一个永不停歇的“Fetch - Decode - Execute”的循环，<strong>我们把这个循环称之为指令周期（Instruction Cycle</strong>）。</p><p>在取指令的阶段，我们的指令是放在存储器里的，实际上，通过 PC 寄存器和指令寄存器取出指令的过程，是由控制器（Control Unit）操作的。指令的解码过程，也是由控制器进行的。一旦到了执行指令阶段，无论是进行算术操作、逻辑操作的 R 型指令，还是进行数据传输、条件分支的 I 型指令，都是由算术逻辑单元（ALU）操作的，也就是由运算器处理的。不过，如果是一个简单的无条件地址跳转，那么我们可以直接在控制器里面完成，不需要用到运算器。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-bc98ed12599728ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>除了 Instruction Cycle 这个指令周期，在 CPU 里面我们还会提到另外两个常见的 Cycle。一个叫 <strong>Machine Cycle</strong>，机器周期或者 CPU 周期。CPU 内部的操作速度很快，但是访问内存的速度却要慢很多。每一条指令都需要从内存里面加载而来，<strong>所以我们一般把从内存里面读取一条指令的最短时间，称为 CPU 周期</strong>。</p><p>对于一个指令周期来说，我们取出一条指令，然后执行它，至少需要两个 CPU 周期。取出指令至少需要一个 CPU 周期，执行至少也需要一个 CPU 周期，复杂的指令则需要更多的 CPU 周期。</p><p><strong>建立数据通路</strong></p><p>第一类叫操作元件，也叫组合逻辑元件（Combinational Element），其实就是我们的 ALU。在前面讲 ALU 的过程中可以看到，它们的功能就是在特定的输入下，根据下面的组合电路的逻辑，生成特定的输出。</p><p>第二类叫存储元件，也有叫状态元件（State Element）的。比如我们在计算过程中需要用到的寄存器，无论是通用寄存器还是状态寄存器，其实都是存储元件。</p><p>我们通过数据总线的方式，把它们连接起来，就可以完成数据的存储、处理和传输了，这就是所谓的建立数据通路了。</p><p>下面我们来说控制器。它的逻辑就没那么复杂了。我们可以把它看成只是机械地重复“Fetch - Decode - Execute“循环中的前两个步骤，然后把最后一个步骤，通过控制器产生的控制信号，交给 ALU 去处理。</p><p><strong>组合逻辑电路和时序逻辑电路</strong></p><p>上一讲，我们看到，要能够实现一个完整的 CPU 功能，除了加法器这样的电路之外，我们还需要实现其他功能的电路。其中有一些电路，和我们实现过的加法器一样，只需要给定输入，就能得到固定的输出。这样的电路，我们称之为组合逻辑电路（Combinational Logic Circuit）。</p><p>但是，光有组合逻辑电路是不够的。你可以想一下，如果只有组合逻辑电路，我们的 CPU 会是什么样的？电路输入是确定的，对应的输出自然也就确定了。那么，我们要进行不同的计算，就要去手动拨动各种开关，来改变电路的开闭状态。这样的计算机，不像我们现在每天用的功能强大的电子计算机，反倒更像古老的计算尺或者机械计算机，干不了太复杂的工作，只能协助我们完成一些计算工作。</p><p>这样，我们就需要引入第二类的电路，也就是时序逻辑电路（Sequential Logic Circuit）。时序逻辑电路可以帮我们解决这样几个问题。</p><p>第一个就是自动运行的问题。时序电路接通之后可以不停地开启和关闭开关，进入一个自动运行的状态。这个使得我们上一讲说的，控制器不停地让 PC 寄存器自增读取下一条指令成为可能。</p><p>第二个是存储的问题。通过时序电路实现的触发器，能把计算结果存储在特定的电路里面，而不是像组合逻辑电路那样，一旦输入有任何改变，对应的输出也会改变。</p><p>第三个本质上解决了各个功能按照时序协调的问题。无论是程序实现的软件指令，还是到硬件层面，各种指令的操作都有先后的顺序要求。时序电路使得不同的事件按照时间顺序发生。</p><p><strong>时钟信号的硬件实现</strong></p><p>想要实现时序逻辑电路，第一步我们需要的就是一个时钟。我在第 3 讲说过，CPU 的主频是由一个晶体振荡器来实现的，而这个晶体振荡器生成的电路信号，就是我们的时钟信号。</p><p>实现这样一个电路，和我们之前讲的，通过电的磁效应产生开关信号的方法是一样的。只不过，这里的磁性开关，打开的不再是后续的线路，而是当前的线路。</p><p>在下面这张图里你可以看到，我们在原先一般只放一个开关的信号输入端，放上了两个开关。一个开关 A，一开始是断开的，由我们手工控制；另外一个开关 B，一开始是合上的，磁性线圈对准一开始就合上的开关 B。</p><p>于是，一旦我们合上开关 A，磁性线圈就会通电，产生磁性，开关 B 就会从合上变成断开。一旦这个开关断开了，电路就中断了，磁性线圈就失去了磁性。于是，开关 B 又会弹回到合上的状态。这样一来，电路接通，线圈又有了磁性。我们的电路就会来回不断地在开启、关闭这两个状态中切换。</p><p>开关 A 闭合（也就是相当于接通电路之后），开关 B 就会不停地在开和关之间切换，生成对应的时钟信号</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-10f4d488768ba255.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个不断切换的过程，对于下游电路来说，就是不断地产生新的 0 和 1 这样的信号。如果你在下游的电路上接上一个灯泡，就会发现这个灯泡在亮和暗之间不停切换。这个按照固定的周期不断在 0 和 1 之间切换的信号，就是我们的时钟信号（Clock Signal）。这个不断切换的过程，对于下游电路来说，就是不断地产生新的 0 和 1 这样的信号。如果你在下游的电路上接上一个灯泡，就会发现这个灯泡在亮和暗之间不停切换。这个按照固定的周期不断在 0 和 1 之间切换的信号，就是我们的<strong>时钟信号（Clock Signal）</strong>。</p><p>时钟信号示意图</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7f4ce7b0e8ef7ed1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这种电路，其实就相当于把电路的输出信号作为输入信号，再回到当前电路。这样的电路构造方式呢，我们叫作反馈电路（Feedback Circuit）。</p><p>接下来，我们还会看到更多的反馈电路。上面这个反馈电路一般可以用下面这个示意图来表示，其实就是一个输出结果接回输入的反相器（Inverter），也就是我们之前讲过的非门。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2a5ced9966633cf1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>通过 D 触发器实现存储功能</strong></p><p>有了时钟信号，我们的系统里就有了一个像“自动门”一样的开关。利用这个开关和相同的反馈电路，我们就可以构造出一个有“记忆”功能的电路。这个有记忆功能的电路，可以实现在 CPU 中用来存储计算结果的寄存器，也可以用来实现计算机五大组成部分之一的存储器。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-101ab8a66f6d5c34.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这样一个电路，我们称之为触发器（Flip-Flop）。接通开关 R，输出变为 1，即使断开开关，输出还是 1 不变。接通开关 S，输出变为 0，即使断开开关，输出也还是 0。<strong>也就是，当两个开关都断开的时候，最终的输出结果，取决于之前动作的输出结果，这个也就是我们说的记忆功能</strong>。</p><p>这里的这个电路是最简单的 RS 触发器，也就是所谓的复位置位触发器（Reset-Set Flip Flop) 。对应的输出结果的真值表，你可以看下面这个表格。可以看到，当两个开关都是 0 的时候，对应的输出不是 1 或者 0，而是和 Q 的上一个状态一致。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-da650e8548b7c074.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>再往这个电路里加两个与门和一个小小的时钟信号，我们就可以实现一个利用时钟信号来操作一个电路了。这个电路可以帮我们实现什么时候可以往 Q 里写入数据。</p><p>我们看看下面这个电路，这个在我们的上面的 R-S 触发器基础之上，在 R 和 S 开关之后，加入了两个与门，同时给这两个与门加入了一个时钟信号 CLK 作为电路输入。</p><p>这样，当时钟信号 CLK 在低电平的时候，与门的输入里有一个 0，两个实际的 R 和 S 后的与门的输出必然是 0。也就是说，无论我们怎么按 R 和 S 的开关，根据 R-S 触发器的真值表，对应的 Q 的输出都不会发生变化。</p><p>只有当时钟信号 CLK 在高电平的时候，与门的一个输入是 1，输出结果完全取决于 R 和 S 的开关。我们可以在这个时候，通过开关 R 和 S，来决定对应 Q 的输出。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-750a783007136134.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果这个时候，我们让 R 和 S 的开关，也用一个反相器连起来，也就是通过同一个开关控制 R 和 S。只要 CLK 信号是 1，R 和 S 就可以设置输出 Q。而当 CLK 信号是 0 的时候，无论 R 和 S 怎么设置，输出信号 Q 是不变的。这样，这个电路就成了我们最常用的 D 型触发器。用来控制 R 和 S 这两个开关的信号呢，我们视作一个输入的数据信号 D，也就是 Data，这就是 D 型触发器的由来。</p><p>把 R 和 S 两个信号通过一个反相器合并，我们可以通过一个数据信号 D 进行 Q 的写入操作</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-cde7eb6c77ff90a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>一个 D 型触发器，只能控制 1 个比特的读写，但是如果我们同时拿出多个 D 型触发器并列在一起，并且把用同一个 CLK 信号控制作为所有 D 型触发器的开关，这就变成了一个 N 位的 D 型触发器，也就可以同时控制 N 位的读写。</p><p>CPU 里面的寄存器可以直接通过 D 型触发器来构造。我们可以在 D 型触发器的基础上，加上更多的开关，来实现清 0 或者全部置为 1 这样的快捷操作。</p><p>电路的输出信号不单单取决于当前的输入信号，还要取决于输出信号之前的状态。最常见的这个电路就是我们的 D 触发器，它也是我们实际在 CPU 内实现存储功能的寄存器的实现方式。</p><p><strong>PC 寄存器所需要的计数器</strong></p><p>我们常说的 PC 寄存器，还有个名字叫程序计数器。下面我们就来看看，它为什么叫作程序计数器。</p><p>有了时钟信号，我们可以提供定时的输入；有了 D 型触发器，我们可以在时钟信号控制的时间点写入数据。我们把这两个功能组合起来，就可以实现一个自动的计数器了。</p><p>加法器的两个输入，一个始终设置成 1，另外一个来自于一个 D 型触发器 A。我们把加法器的输出结果，写到这个 D 型触发器 A 里面。于是，D 型触发器里面的数据就会在固定的时钟信号为 1 的时候更新一次。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d2ae57597b6e635a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这样，我们就有了一个每过一个时钟周期，就能固定自增 1 的自动计数器了。这个自动计数器，可以拿来当我们的 PC 寄存器。事实上，PC 寄存器的这个 PC，英文就是 Program Counter，也就是程序计数器的意思。</p><p>每次自增之后，我们可以去对应的 D 型触发器里面取值，这也是我们下一条需要运行指令的地址。前面第 5 讲我们讲过，同一个程序的指令应该要顺序地存放在内存里面。这里就和前面对应上了，顺序地存放指令，就是为了让我们通过程序计数器就能定时地不断执行新指令。</p><p>在最简单的情况下，我们需要让每一条指令，从程序计数，到获取指令、执行指令，都在一个时钟周期内完成。如果 PC 寄存器自增地太快，程序就会出错。因为前一次的运算结果还没有写回到对应的寄存器里面的时候，后面一条指令已经开始读取里面的数据来做下一次计算了。这个时候，如果我们的指令使用同样的寄存器，前一条指令的计算就会没有效果，计算结果就错了。</p><p>在这种设计下，我们需要在一个时钟周期里，确保执行完一条最复杂的 CPU 指令，也就是耗时最长的一条 CPU 指令。这样的 CPU 设计，我们称之为<strong>单指令周期处理器（Single Cycle Processor）</strong>。</p><p>很显然，这样的设计有点儿浪费。因为即便只调用一条非常简单的指令，我们也需要等待整个时钟周期的时间走完，才能执行下一条指令。在后面章节里我们会讲到，通过流水线技术进行性能优化，可以减少需要等待的时间，这里我们暂且说到这里。</p><p><strong>读写数据所需要的译码器</strong></p><p>现在，我们的数据能够存储在 D 型触发器里了。如果我们把很多个 D 型触发器放在一起，就可以形成一块很大的存储空间，甚至可以当成一块内存来用。像我现在手头这台电脑，有 16G 内存。那我们怎么才能知道，写入和读取的数据，是在这么大的内存的哪几个比特呢？</p><p>于是，我们就需要有一个电路，来完成“寻址”的工作。这个“寻址”电路，就是我们接下来要讲的译码器。</p><p>在现在实际使用的计算机里面，内存所使用的 DRAM，并不是通过上面的 D 型触发器来实现的，而是使用了一种 CMOS 芯片来实现的。不过，这并不影响我们从基础原理方面来理解译码器。在这里，我们还是可以把内存芯片，当成是很多个连在一起的 D 型触发器来实现的。</p><p>如果把“寻址”这件事情退化到最简单的情况，就是在两个地址中，去选择一个地址。这样的电路，我们叫作 2-1 选择器。我把它的电路实现画在了这里。</p><p>我们通过一个反相器、两个与门和一个或门，就可以实现一个 <strong>2-1 选择器</strong>。通过控制反相器的输入是 0 还是 1，能够决定对应的输出信号，是和地址 A，还是地址 B 的输入信号一致。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-dd198c81aee0ae06.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>一个反向器只能有 0 和 1 这样两个状态，所以我们只能从两个地址中选择一个。如果输入的信号有三个不同的开关，我们就能从 23，也就是 8 个地址中选择一个了。这样的电路，我们就叫 3-8 译码器。现代的计算机，如果 CPU 是 64 位的，就意味着我们的寻址空间也是 2^64，那么我们就需要一个有 64 个开关的译码器。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5f93ddfffb07e1ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>所以说，其实译码器的本质，就是从输入的多个位的信号中，根据一定的开关和电路组合，选择出自己想要的信号。除了能够进行“寻址”之外，我们还可以把对应的需要运行的指令码，同样通过译码器，找出我们期望执行的指令，也就是在之前我们讲到过的 opcode，以及后面对应的操作数或者寄存器地址。只是，这样的“译码器”，比起 2-1 选择器和 3-8 译码器，要复杂的多。</p><p><strong>建立数据通路，构造一个最简单的 CPU</strong></p><p>D 触发器、自动计数以及译码器，再加上一个我们之前说过的 ALU，我们就凑齐了一个拼装一个 CPU 必须要的零件了。下面，我们就来看一看，怎么把这些零件组合起来，才能实现指令执行和算术逻辑计算的 CPU。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0974426adac2951e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ol><li>首先，我们有一个自动计数器。这个自动计数器会随着时钟主频不断地自增，来作为我们的 PC 寄存器。</li><li>在这个自动计数器的后面，我们连上一个译码器。译码器还要同时连着我们通过大量的 D 触发器组成的内存。</li><li>自动计数器会随着时钟主频不断自增，从译码器当中，找到对应的计数器所表示的内存地址，然后读取出里面的 CPU 指令。</li><li>读取出来的 CPU 指令会通过我们的 CPU 时钟的控制，写入到一个由 D 触发器组成的寄存器，也就是指令寄存器当中。</li><li>在指令寄存器后面，我们可以再跟一个译码器。这个译码器不再是用来寻址的了，而是把我们拿到的指令，解析成 opcode 和对应的操作数。</li><li>当我们拿到对应的 opcode 和操作数，对应的输出线路就要连接 ALU，开始进行各种算术和逻辑运算。对应的计算结果，则会再写回到 D 触发器组成的寄存器或者内存当中。</li></ol><p>这样的一个完整的通路，也就完成了我们的 CPU 的一条指令的执行过程。在这个过程中，你会发现这样几个有意思的问题。</p><h3 id="CPU指令周期和流水线"><a href="#CPU指令周期和流水线" class="headerlink" title="CPU指令周期和流水线"></a>CPU指令周期和流水线</h3><p><strong>愿得一心人，白首不相离：单指令周期处理器</strong></p><p>快速执行完成的指令，需要等待满一个时钟周期，才能执行下一条指令</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-649bce62547a6d8d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>所以，在单指令周期处理器里面，无论是执行一条用不到 ALU 的无条件跳转指令，还是一条计算起来电路特别复杂的浮点数乘法运算，我们都等要等满一个时钟周期。在这个情况下，虽然 CPI 能够保持在 1，但是我们的时钟频率却没法太高。因为太高的话，有些复杂指令没有办法在一个时钟周期内运行完成。那么在下一个时钟周期到来，开始执行下一条指令的时候，前一条指令的执行结果可能还没有写入到寄存器里面。那下一条指令读取的数据就是不准确的，就会出现错误。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-408a2df056bcc35f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>到这里你会发现，这和我们之前第 3 讲和第 4 讲讲时钟频率时候的说法不太一样。当时我们说，一个 CPU 时钟周期，可以认为是完成一条简单指令的时间。为什么到了这里，单指令周期处理器，反而变成了执行一条最复杂的指令的时间呢？</p><p>这是因为，无论是 PC 上使用的 Intel CPU，还是手机上使用的 ARM CPU，都不是单指令周期处理器，而是采用了一种叫<strong>作指令流水线（Instruction Pipeline）</strong>的技术。</p><p><strong>无可奈何花落去，似曾相识燕归来：现代处理器的流水线设计</strong></p><p>其实，CPU 执行一条指令的过程和我们开发软件功能的过程很像。</p><p>如果我们想开发一个手机 App 上的功能，并不是找来一个工程师，告诉他“你把这个功能开发出来”，然后他就吭哧吭哧把功能开发出来。真实的情况是，无论只有一个工程师，还是有一个开发团队，我们都需要先对开发功能的过程进行切分，把这个过程变成“撰写需求文档、开发后台 API、开发客户端 App、测试、发布上线”这样多个独立的过程。每一个后面的步骤，都要依赖前面的步骤。</p><p>我们的指令执行过程也是一样的，它会拆分成“取指令、译码、执行”这样三大步骤。更细分一点的话，执行的过程，其实还包含从寄存器或者内存中读取数据，通过 ALU 进行运算，把结果写回到寄存器或者内存中。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8e65be8481509f4e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这样一来，我们就不用把时钟周期设置成整条指令执行的时间，而是拆分成完成这样的一个一个小步骤需要的时间。同时，每一个阶段的电路在完成对应的任务之后，也不需要等待整个指令执行完成，而是可以直接执行下一条指令的对应阶段。</p><p>这就好像我们的后端程序员不需要等待功能上线，就会从产品经理手中拿到下一个需求，开始开发 API。这样的协作模式，就是我们所说的<strong>指令流水线</strong>。这里面每一个独立的步骤，我们就称之为流水线阶段或者流水线级（Pipeline Stage）。</p><p>五级的流水线，就表示我们在同一个时钟周期里面，同时运行五条指令的不同阶段。这个时候，<strong>虽然执行一条指令的时钟周期变成了 5，但是我们可以把 CPU 的主频提得更高了</strong>。我们不需要确保最复杂的那条指令在时钟周期里面执行完成，而只要保障一个最复杂的流水线级的操作，在一个时钟周期内完成就好了。</p><p>如果某一个操作步骤的时间太长，我们就可以考虑把这个步骤，拆分成更多的步骤，让所有步骤需要执行的时间尽量都差不多长。这样，也就可以解决我们在单指令周期处理器中遇到的，性能瓶颈来自于最复杂的指令的问题。像我们现代的 ARM 或者 Intel 的 CPU，流水线级数都已经到了 14 级。</p><p>虽然我们不能通过流水线，来减少单条指令执行的“延时”这个性能指标，但是，通过同时在执行多条指令的不同阶段，我们提升了 CPU 的“吞吐率”。在外部看来，我们的 CPU 好像是“一心多用”，在同一时间，同时执行 5 条不同指令的不同阶段。在 CPU 内部，其实它就像生产线一样，不同分工的组件不断处理上游传递下来的内容，而不需要等待单件商品生产完成之后，再启动下一件商品的生产过程。</p><p><strong>超长流水线的性能瓶颈</strong></p><p>既然流水线可以增加我们的吞吐率，你可能要问了，为什么我们不把流水线级数做得更深呢？为什么不做成 20 级，乃至 40 级呢？这个其实有很多原因，我在之后几讲里面会详细讲解。这里，我先讲一个最基本的原因，就是增加流水线深度，其实是有性能成本的。</p><p>我们用来同步时钟周期的，不再是指令级别的，而是流水线阶段级别的。每一级流水线对应的输出，都要放到流水线寄存器（Pipeline Register）里面，然后在下一个时钟周期，交给下一个流水线级去处理。所以，每增加一级的流水线，就要多一级写入到流水线寄存器的操作。虽然流水线寄存器非常快，比如只有 20 皮秒（ps，10−12 秒）。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9f3c44c2b56e19cd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>但是，如果我们不断加深流水线，这些操作占整个指令的执行时间的比例就会不断增加。最后，我们的性能瓶颈就会出现在这些 overhead 上。如果我们指令的执行有 3 纳秒，也就是 3000 皮秒。我们需要 20 级的流水线，那流水线寄存器的写入就需要花费 400 皮秒，占了超过 10%。如果我们需要 50 级流水线，就要多花费 1 纳秒在流水线寄存器上，占到 25%。这也就意味着，单纯地增加流水线级数，不仅不能提升性能，反而会有更多的 overhead 的开销。所以，设计合理的流水线级数也是现代 CPU 中非常重要的一点。</p><p><strong>“主频战争”带来的超长流水线</strong></p><p>我们在第 3 讲里讲过，我们其实并不能简单地通过 CPU 的主频，就来衡量 CPU 乃至计算机整机的性能。因为不同的 CPU 实际的体系架构和实现都不一样。同样的 CPU 主频，实际的性能可能差别很大。所以，在工业界，更好的衡量方式通常是，用 SPEC 这样的跑分程序，从多个不同的实际应用场景，来衡量计算机的性能。</p><p>但是，跑分对于消费者来说还是太复杂了。在 Pentium 4 的 CPU 面世之前，绝大部分消费者并不是根据跑分结果来判断 CPU 的性能的。大家判断一个 CPU 的性能，通常只看 CPU 的主频。而 CPU 的厂商们也通过不停地提升主频，把主频当成技术竞赛的核心指标。</p><p>Intel 一向在“主频战争”中保持领先，但是到了世纪之交的 1999 年到 2000 年，情况发生了变化。</p><p>1999 年，AMD 发布了基于 K7 架构的 Athlon 处理器，其综合性能超越了当年的 Pentium III。2000 年，在大部分 CPU 还在 500～850MHz 的频率下运行的时候，AMD 推出了第一代 Athlon 1000 处理器，成为第一款 1GHz 主频的消费级 CPU。在 2000 年前后，AMD 的 CPU 不但性能和主频比 Intel 的要强，价格还往往只有 Intel 的 2/3。</p><p>在巨大的外部压力之下，Intel 在 2001 年推出了新一代的 NetBurst 架构 CPU，也就是 Pentium 4 和 Pentium D。Pentium 4 的 CPU 有个最大的特点，就是高主频。2000 年的 Athlon 1000 的主频在当时是最高的，1GHz，然而 Pentium 4 设计的目标最高主频是 10GHz。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-73e2d47146c891b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>为了达到这个 10GHz，Intel 的工程师做出了一个重大的错误决策，就是在 NetBurst 架构上，使用超长的流水线。这个超长流水线有多长呢？我们拿在 Pentium 4 之前和之后的 CPU 的数字做个比较，你就知道了。</p><p>Pentium 4 之前的 Pentium III CPU，流水线的深度是 11 级，也就是一条指令最多会拆分成 11 个更小的步骤来操作，而 CPU 同时也最多会执行 11 条指令的不同 Stage。随着技术发展到今天，你日常用的手机 ARM 的 CPU 或者 Intel i7 服务器的 CPU，流水线的深度是 14 级。</p><p>可以看到，差不多 20 年过去了，通过技术进步，现代 CPU 还是增加了一些流水线深度的。那 2000 年发布的 Pentium 4 的流水线深度是多少呢？答案是 20 级，比 Pentium III 差不多多了一倍，而到了代号为 Prescott 的 90 纳米工艺处理器 Pentium 4，Intel 更是把流水线深度增加到了 31 级。</p><p>要知道，增加流水线深度，在同主频下，其实是降低了 CPU 的性能。因为一个 Pipeline Stage，就需要一个时钟周期。那么我们把任务拆分成 31 个阶段，就需要 31 个时钟周期才能完成一个任务；而把任务拆分成 11 个阶段，就只需要 11 个时钟周期就能完成任务。在这种情况下，31 个 Stage 的 3GHz 主频的 CPU，其实和 11 个 Stage 的 1GHz 主频的 CPU，性能是差不多的。事实上，因为每个 Stage 都需要有对应的 Pipeline 寄存器的开销，这个时候，更深的流水线性能可能还会更差一些。</p><p>我在上一讲也说过，流水线技术并不能缩短单条指令的响应时间这个性能指标，但是可以增加在运行很多条指令时候的吞吐率。因为不同的指令，实际执行需要的时间是不同的。我们可以看这样一个例子。我们顺序执行这样三条指令。</p><ul><li>一条整数的加法，需要 200ps。</li><li>一条整数的乘法，需要 300ps。</li><li>一条浮点数的乘法，需要 600ps。</li></ul><p>如果我们是在单指令周期的 CPU 上运行，最复杂的指令是一条浮点数乘法，那就需要 600ps。那这三条指令，都需要 600ps。三条指令的执行时间，就需要 1800ps。</p><p>如果我们采用的是 6 级流水线 CPU，每一个 Pipeline 的 Stage 都只需要 100ps。那么，在这三个指令的执行过程中，在指令 1 的第一个 100ps 的 Stage 结束之后，第二条指令就开始执行了。在第二条指令的第一个 100ps 的 Stage 结束之后，第三条指令就开始执行了。这种情况下，这三条指令顺序执行所需要的总时间，就是 800ps。那么在 1800ps 内，使用流水线的 CPU 比单指令周期的 CPU 就可以多执行一倍以上的指令数。</p><p>虽然每一条指令从开始到结束拿到结果的时间并没有变化，也就是响应时间没有变化。但是同样时间内，完成的指令数增多了，也就是吞吐率上升了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fc77dd904b906d74.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>新的挑战：冒险和分支预测</strong></p><p>那到这里可能你就要问了，这样看起来不是很好么？Intel 的 CPU 支持的指令集很大，我们之前说过有 2000 多条指令。有些指令很简单，执行也很快，比如无条件跳转指令，不需要通过 ALU 进行任何计算，只要更新一下 PC 寄存器里面的内容就好了。而有些指令很复杂，比如浮点数的运算，需要进行指数位比较、对齐，然后对有效位进行移位，然后再进行计算。两者的执行时间相差二三十倍也很正常。</p><p>既然这样，Pentium 4 的超长流水线看起来很合理呀，为什么 Pentium 4 最终成为 Intel 在技术架构层面的大失败呢？</p><p>第一个，自然是我们在第 3 讲里讲过的功耗问题。提升流水线深度，必须要和提升 CPU 主频同时进行。因为在单个 Pipeline Stage 能够执行的功能变简单了，也就意味着单个时钟周期内能够完成的事情变少了。所以，只有提升时钟周期，CPU 在指令的响应时间这个指标上才能保持和原来相同的性能。</p><p>同时，由于流水线深度的增加，我们需要的电路数量变多了，也就是我们所使用的晶体管也就变多了。</p><p>主频的提升和晶体管数量的增加都使得我们 CPU 的功耗变大了。这个问题导致了 Pentium 4 在整个生命周期里，都成为了耗电和散热的大户。而 Pentium 4 是在 2000～2004 年作为 Intel 的主打 CPU 出现在市场上的。这个时间段，正是笔记本电脑市场快速发展的时间。在笔记本电脑上，功耗和散热比起台式机是一个更严重的问题了。即使性能更好，别人的笔记本可以用上 2 小时，你的只能用 30 分钟，那谁也不爱买啊！</p><p>更何况，Pentium 4 的性能还更差一些。这个就要我们说到第二点了，就是上面说的流水线技术带来的性能提升，是一个理想情况。在实际的程序执行中，并不一定能够做得到。</p><p>还回到我们刚才举的三条指令的例子。如果这三条指令，是下面这样的三条代码，会发生什么情况呢？</p><pre><code>int a = 10 + 5; // 指令1int b = a * 2; // 指令2float c = b * 1.0f; // 指令3</code></pre><p>我们会发现，指令 2，不能在指令 1 的第一个 Stage 执行完成之后进行。因为指令 2，依赖指令 1 的计算结果。同样的，指令 3 也要依赖指令 2 的计算结果。这样，即使我们采用了流水线技术，这三条指令执行完成的时间，也是 200 + 300 + 600 = 1100 ps，而不是之前说的 800ps。而如果指令 1 和 2 都是浮点数运算，需要 600ps。那这个依赖关系会导致我们需要的时间变成 1800ps，和单指令周期 CPU 所要花费的时间是一样的。</p><p>这个依赖问题，就是我们在计算机组成里面所说的冒险（Hazard）问题。这里我们只列举了在数据层面的依赖，也就是数据冒险。在实际应用中，还会有结构冒险、控制冒险等其他的依赖问题。</p><p>对应这些冒险问题，我们也有在乱序执行、分支预测等相应的解决方案。我们在后面的几讲里面，会详细讲解对应的知识。</p><p>但是，我们的流水线越长，这个冒险的问题就越难一解决。这是因为，同一时间同时在运行的指令太多了。如果我们只有 3 级流水线，我们可以把后面没有依赖关系的指令放到前面来执行。这个就是我们所说的乱序执行的技术。比方说，我们可以扩展一下上面的 3 行代码，再加上几行代码。</p><h3 id="CPU冒险"><a href="#CPU冒险" class="headerlink" title="CPU冒险"></a>CPU冒险</h3><p>任何一本讲解 CPU 的流水线设计的教科书，都会提到流水线设计需要解决的三大冒险，分别是<strong>结构冒险（Structural Hazard）、数据冒险（Data Hazard）以及控制冒险（Control Hazard</strong>）。</p><p>这三大冒险的名字很有意思，它们都叫作 hazard（冒险）。喜欢玩游戏的话，你应该知道一个著名的游戏，生化危机，英文名就叫 Biohazard。的确，hazard 还有一个意思就是“危机”。那为什么在流水线设计里，hazard 没有翻译成“危机”，而是要叫“冒险”呢？</p><p>在 CPU 的流水线设计里，固然我们会遇到各种“危险”情况，使得流水线里的下一条指令不能正常运行。但是，我们其实还是通过“抢跑”的方式，“冒险”拿到了一个提升指令吞吐率的机会。流水线架构的 CPU，是我们主动进行的冒险选择。我们期望能够通过冒险带来更高的回报，所以，这不是无奈之下的应对之举，自然也算不上什么危机了。</p><p>事实上，对于各种冒险可能造成的问题，我们其实都准备好了应对的方案。这一讲里，我们先从结构冒险和数据冒险说起，一起来看看这些冒险及其对应的应对方案。</p><p><strong>结构冒险：为什么工程师都喜欢用机械键盘？</strong></p><p>我们先来看一看结构冒险。结构冒险，本质上是一个硬件层面的资源竞争问题，也就是一个硬件电路层面的问题。</p><p>CPU 在同一个时钟周期，同时在运行两条计算机指令的不同阶段。但是这两个不同的阶段，可能会用到同样的硬件电路。</p><p>最典型的例子就是内存的数据访问。请你看看下面这张示意图，其实就是第 20 讲里对应的 5 级流水线的示意图。</p><p>可以看到，在第 1 条指令执行到访存（MEM）阶段的时候，流水线里的第 4 条指令，在执行取指令（Fetch）的操作。访存和取指令，都要进行内存数据的读取。我们的内存，只有一个地址译码器的作为地址输入，那就只能在一个时钟周期里面读取一条数据，没办法同时执行第 1 条指令的读取内存数据和第 4 条指令的读取指令代码。</p><p>同一个时钟周期，两个不同指令访问同一个资源</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-92e9d83a65b49a38.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>类似的资源冲突，其实你在日常使用计算机的时候也会遇到。最常见的就是薄膜键盘的“锁键”问题。常用的最廉价的薄膜键盘，并不是每一个按键的背后都有一根独立的线路，而是多个键共用一个线路。如果我们在同一时间，按下两个共用一个线路的按键，这两个按键的信号就没办法都传输出去。</p><p>这也是为什么，重度键盘用户，都要买贵一点儿的机械键盘或者电容键盘。因为这些键盘的每个按键都有独立的传输线路，可以做到“全键无冲”，这样，无论你是要大量写文章、写程序，还是打游戏，都不会遇到按下了键却没生效的情况。</p><p>“全键无冲”这样的资源冲突解决方案，其实本质就是增加资源。同样的方案，我们一样可以用在 CPU 的结构冒险里面。对于访问内存数据和取指令的冲突，一个直观的解决方案就是把我们的内存分成两部分，让它们各有各的地址译码器。这两部分分别是存放指令的程序内存和存放数据的数据内存。</p><p>这样把内存拆成两部分的解决方案，在计算机体系结构里叫作哈佛架构（Harvard Architecture），来自哈佛大学设计Mark I 型计算机时候的设计。对应的，我们之前说的冯·诺依曼体系结构，又叫作普林斯顿架构（Princeton Architecture）。从这些名字里，我们可以看到，早年的计算机体系结构的设计，其实产生于美国各个高校之间的竞争中。</p><p>不过，我们今天使用的 CPU，仍然是冯·诺依曼体系结构的，并没有把内存拆成程序内存和数据内存这两部分。因为如果那样拆的话，对程序指令和数据需要的内存空间，我们就没有办法根据实际的应用去动态分配了。虽然解决了资源冲突的问题，但是也失去了灵活性。</p><p>现代 CPU 架构，借鉴了哈佛架构，在高速缓存层面拆分成指令缓存和数据缓存</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5b3ae5c554df93c7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>不过，借鉴了哈佛结构的思路，现代的 CPU 虽然没有在内存层面进行对应的拆分，却在 CPU 内部的高速缓存部分进行了区分，把高速缓存分成了指令缓存（Instruction Cache）和数据缓存（Data Cache）两部分。</p><p>内存的访问速度远比 CPU 的速度要慢，所以现代的 CPU 并不会直接读取主内存。它会从主内存把指令和数据加载到高速缓存中，这样后续的访问都是访问高速缓存。而指令缓存和数据缓存的拆分，使得我们的 CPU 在进行数据访问和取指令的时候，不会再发生资源冲突的问题了。</p><p><strong>数据冒险：三种不同的依赖关系</strong></p><p>结构冒险是一个硬件层面的问题，我们可以靠增加硬件资源的方式来解决。然而还有很多冒险问题，是程序逻辑层面的事儿。其中，最常见的就是数据冒险。</p><p>数据冒险，其实就是同时在执行的多个指令之间，有数据依赖的情况。这些数据依赖，我们可以分成三大类，分别是先写后读（Read After Write，RAW）、先读后写（Write After Read，WAR）和写后再写（Write After Write，WAW）。下面，我们分别看一下这几种情况。</p><p><strong>先写后读（Read After Write）</strong></p><p>我们先来一起看看先写后读这种情况。这里有一段简单的 C 语言代码编译出来的汇编指令。这段代码简单地定义两个变量 a 和 b，然后计算 a = a + 2。再根据计算出来的结果，计算 b = a + 3。</p><pre><code>int main() {   0:   55                      push   rbp   1:   48 89 e5                mov    rbp,rsp  int a = 1;   4:   c7 45 fc 01 00 00 00    mov    DWORD PTR [rbp-0x4],0x1  int b = 2;   b:   c7 45 f8 02 00 00 00    mov    DWORD PTR [rbp-0x8],0x2  a = a + 2;  12:   83 45 fc 02             add    DWORD PTR [rbp-0x4],0x2  b = a + 3;  16:   8b 45 fc                mov    eax,DWORD PTR [rbp-0x4]  19:   83 c0 03                add    eax,0x3  1c:   89 45 f8                mov    DWORD PTR [rbp-0x8],eax}  1f:   5d                      pop    rbp  20:   c3                      ret  </code></pre><p>你可以看到，在内存地址为 12 的机器码，我们把 0x2 添加到 rbp-0x4 对应的内存地址里面。然后，在紧接着的内存地址为 16 的机器码，我们又要从 rbp-0x4 这个内存地址里面，把数据写入到 eax 这个寄存器里面。</p><p>所以，我们需要保证，在内存地址为 16 的指令读取 rbp-0x4 里面的值之前，内存地址 12 的指令写入到 rbp-0x4 的操作必须完成。这就是先写后读所面临的数据依赖。如果这个顺序保证不了，我们的程序就会出错。</p><p>这个先写后读的依赖关系，我们一般被称之为数据依赖，也就是 Data Dependency。</p><p><strong>再等等：通过流水线停顿解决数据冒险</strong></p><p>除了读之后再进行读，你会发现，对于同一个寄存器或者内存地址的操作，都有明确强制的顺序要求。而这个顺序操作的要求，也为我们使用流水线带来了很大的挑战。因为流水线架构的核心，就是在前一个指令还没有结束的时候，后面的指令就要开始执行。</p><p>所以，我们需要有解决这些数据冒险的办法。其中最简单的一个办法，不过也是最笨的一个办法，就是流水线停顿（Pipeline Stall），或者叫流水线冒泡（Pipeline Bubbling）。</p><p>流水线停顿的办法很容易理解。如果我们发现了后面执行的指令，会对前面执行的指令有数据层面的依赖关系，那最简单的办法就是“再等等”。我们在进行指令译码的时候，会拿到对应指令所需要访问的寄存器和内存地址。所以，在这个时候，我们能够判断出来，这个指令是否会触发数据冒险。如果会触发数据冒险，我们就可以决定，让整个流水线停顿一个或者多个周期。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0ba1b78da0012af8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我在前面说过，时钟信号会不停地在 0 和 1 之前自动切换。其实，我们并没有办法真的停顿下来。流水线的每一个操作步骤必须要干点儿事情。所以，在实践过程中，我们并不是让流水线停下来，而是在执行后面的操作步骤前面，插入一个 NOP 操作，也就是执行一个其实什么都不干的操作。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-87cb690f6463a5e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个插入的指令，就好像一个水管（Pipeline）里面，进了一个空的气泡。在水流经过的时候，没有传送水到下一个步骤，而是给了一个什么都没有的空气泡。这也是为什么，我们的流水线停顿，又被叫作流水线冒泡（Pipeline Bubble）的原因。</p><p><strong>NOP 操作和指令对齐</strong></p><p>要想理解操作数前推技术，我们先来回顾一下，第 5 讲讲过的，MIPS 体系结构下的 R、I、J 三类指令，以及第 20 讲里的五级流水线“取指令（IF）- 指令译码（ID）- 指令执行（EX）- 内存访问（MEM）- 数据写回（WB） ”。</p><p>在 MIPS 的体系结构下，不同类型的指令，会在流水线的不同阶段进行不同的操作。</p><p>我们以 MIPS 的 LOAD，这样从内存里读取数据到寄存器的指令为例，来仔细看看，它需要经历的 5 个完整的流水线。STORE 这样从寄存器往内存里写数据的指令，不需要有写回寄存器的操作，也就是没有数据写回的流水线阶段。至于像 ADD 和 SUB 这样的加减法指令，所有操作都在寄存器完成，所以没有实际的内存访问（MEM）操作。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-67593cd524e351b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>有些指令没有对应的流水线阶段，但是我们并不能跳过对应的阶段直接执行下一阶段。不然，如果我们先后执行一条 LOAD 指令和一条 ADD 指令，就会发生 LOAD 指令的 WB 阶段和 ADD 指令的 WB 阶段，在同一个时钟周期发生。这样，相当于触发了一个结构冒险事件，产生了资源竞争。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-07855aaab953716a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>所以，在实践当中，各个指令不需要的阶段，并不会直接跳过，而是会运行一次 NOP 操作。通过插入一个 NOP 操作，我们可以使后一条指令的每一个 Stage，一定不和前一条指令的同 Stage 在一个时钟周期执行。这样，就不会发生先后两个指令，在同一时钟周期竞争相同的资源，产生结构冒险了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4a111c7cf87d0bd6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>流水线里的接力赛：操作数前推</strong></p><p>通过 NOP 操作进行对齐，我们在流水线里，就不会遇到资源竞争产生的结构冒险问题了。除了可以解决结构冒险之外，这个 NOP 操作，也是我们之前讲的流水线停顿插入的对应操作。</p><p>但是，插入过多的 NOP 操作，意味着我们的 CPU 总是在空转，干吃饭不干活。那么，我们有没有什么办法，尽量少插入一些 NOP 操作呢？不要着急，下面我们就以两条先后发生的 ADD 指令作为例子，看看能不能找到一些好的解决方案。</p><pre><code>add $t0, $s2,$s1add $s2, $s1,$t0</code></pre><p>这两条指令很简单。</p><ol><li>第一条指令，把 s1 和 s2 寄存器里面的数据相加，存入到 t0 这个寄存器里面。</li><li>第二条指令，把 s1 和 t0 寄存器里面的数据相加，存入到 s2 这个寄存器里面。</li></ol><p>因为后一条的 add 指令，依赖寄存器 t0 里的值。而 t0 里面的值，又来自于前一条指令的计算结果。所以后一条指令，需要等待前一条指令的数据写回阶段完成之后，才能执行。就像上一讲里讲的那样，我们遇到了一个数据依赖类型的冒险。于是，我们就不得不通过流水线停顿来解决这个冒险问题。我们要在第二条指令的译码阶段之后，插入对应的 NOP 指令，直到前一天指令的数据写回完成之后，才能继续执行。</p><p>这样的方案，虽然解决了数据冒险的问题，但是也浪费了两个时钟周期。我们的第 2 条指令，其实就是多花了 2 个时钟周期，运行了两次空转的 NOP 操作。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9f4fe666e24b590a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>不过，其实我们第二条指令的执行，未必要等待第一条指令写回完成，才能进行。如果我们第一条指令的执行结果，能够直接传输给第二条指令的执行阶段，作为输入，那我们的第二条指令，就不用再从寄存器里面，把数据再单独读出来一次，才来执行代码。</p><p>我们完全可以在第一条指令的执行阶段完成之后，直接将结果数据传输给到下一条指令的 ALU。然后，下一条指令不需要再插入两个 NOP 阶段，就可以继续正常走到执行阶段。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-56c33a59b18fb104.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这样的解决方案，我们就叫作<strong>操作数前推</strong>（Operand Forwarding），或者操作数旁路（Operand Bypassing）。其实我觉得，更合适的名字应该叫操作数转发。这里的 Forward，其实就是我们写 Email 时的“转发”（Forward）的意思。不过现有的经典教材的中文翻译一般都叫“前推”，我们也就不去纠正这个说法了，你明白这个意思就好。</p><p>转发，其实是这个技术的逻辑含义，也就是在第 1 条指令的执行结果，直接“转发”给了第 2 条指令的 ALU 作为输入。另外一个名字，旁路（Bypassing），则是这个技术的硬件含义。为了能够实现这里的“转发”，我们在 CPU 的硬件里面，需要再单独拉一根信号传输的线路出来，使得 ALU 的计算结果，能够重新回到 ALU 的输入里来。这样的一条线路，就是我们的“旁路”。它越过（Bypass）了写入寄存器，再从寄存器读出的过程，也为我们节省了 2 个时钟周期。</p><p>操作数前推的解决方案不但可以单独使用，还可以和流水线冒泡一起使用。有的时候，虽然我们可以把操作数转发到下一条指令，但是下一条指令仍然需要停顿一个时钟周期。</p><p>比如说，我们先去执行一条 LOAD 指令，再去执行 ADD 指令。LOAD 指令在访存阶段才能把数据读取出来，所以下一条指令的执行阶段，需要在访存阶段完成之后，才能进行。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b87409cfef85c414.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>总的来说，操作数前推的解决方案，比流水线停顿更进了一步。流水线停顿的方案，有点儿像游泳比赛的接力方式。下一名运动员，需要在前一个运动员游玩了全程之后，触碰到了游泳池壁才能出发。而操作数前推，就好像短跑接力赛。后一个运动员可以提前抢跑，而前一个运动员会多跑一段主动把交接棒传递给他。</p><p><strong>填上空闲的 NOP：上菜的顺序不必是点菜的顺序</strong></p><p>但是这个“阻塞”很多时候是没有必要的。因为尽管你的代码生成的指令是顺序的，但是如果后面的指令不需要依赖前面指令的执行结果，完全可以不必等待前面的指令运算完成。</p><pre><code>a = b + cd = a * ex = y * z</code></pre><p>计算里面的 x ，却要等待 a 和 d 都计算完成，实在没啥必要。所以我们完全可以在 d 的计算等待 a 的计算的过程中，先把 x 的结果给算出来。</p><p>在流水线里，后面的指令不依赖前面的指令，那就不用等待前面的指令执行，它完全可以先执行。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-705a328881bceb49.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>可以看到，因为第三条指令并不依赖于前两条指令的计算结果，所以在第二条指令等待第一条指令的访存和写回阶段的时候，第三条指令就已经执行完成了。</p><p>这样的解决方案，在计算机组成里面，被称为乱序执行（Out-of-Order Execution，OoOE）。乱序执行，最早来自于著名的 IBM 360。相信你一定听说过《人月神话》这本软件工程届的经典著作，它讲的就是 IBM 360 开发过程中的“人生体会”。而 IBM 360 困难的开发过程，也少不了第一次引入乱序执行这个新的 CPU 技术。</p><p><strong>CPU 里的“线程池”：理解乱序执行</strong></p><p>使用乱序执行技术后，CPU 里的流水线就和我之前给你看的 5 级流水线不太一样了。我们一起来看一看下面这张图。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6cbd53f3fa9aa366.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ol><li>在取指令和指令译码的时候，乱序执行的 CPU 和其他使用流水线架构的 CPU 是一样的。它会一级一级顺序地进行取指令和指令译码的工作。</li><li>在指令译码完成之后，就不一样了。CPU 不会直接进行指令执行，而是进行一次指令分发，把指令发到一个叫作保留站（Reservation Stations）的地方。顾名思义，这个保留站，就像一个火车站一样。发送到车站的指令，就像是一列列的火车。</li><li>这些指令不会立刻执行，而要等待它们所依赖的数据，传递给它们之后才会执行。这就好像一列列的火车都要等到乘客来齐了才能出发。</li><li>一旦指令依赖的数据来齐了，指令就可以交到后面的功能单元（Function Unit，FU），其实就是 ALU，去执行了。我们有很多功能单元可以并行运行，但是不同的功能单元能够支持执行的指令并不相同。就和我们的铁轨一样，有些从上海北上，可以到北京和哈尔滨；有些是南下的，可以到广州和深圳。</li><li>指令执行的阶段完成之后，我们并不能立刻把结果写回到寄存器里面去，而是把结果再存放到一个叫作重排序缓冲区（Re-Order Buffer，ROB）的地方。</li><li>在重排序缓冲区里，我们的 CPU 会按照取指令的顺序，对指令的计算结果重新排序。只有排在前面的指令都已经完成了，才会提交指令，完成整个指令的运算结果。</li><li>实际的指令的计算结果数据，并不是直接写到内存或者高速缓存里，而是先写入存储缓冲区（Store Buffer 面，最终才会写入到高速缓存和内存里。</li></ol><p>可以看到，在乱序执行的情况下，只有 CPU 内部指令的执行层面，可能是“乱序”的。只要我们能在指令的译码阶段正确地分析出指令之间的数据依赖关系，这个“乱序”就只会在互相没有影响的指令之间发生。</p><p>有了乱序执行，我们重新去执行上面的 3 行代码。</p><pre><code>a = b + cd = a * ex = y * z</code></pre><p>里面的 d 依赖于 a 的计算结果，不会在 a 的计算完成之前执行。但是我们的 CPU 并不会闲着，因为 x = y * z 的指令同样会被分发到保留站里。因为 x 所依赖的 y 和 z 的数据是准备好的， 这里的乘法运算不会等待计算 d，而会先去计算 x 的值。</p><p>如果我们只有一个 FU 能够计算乘法，那么这个 FU 并不会因为 d 要等待 a 的计算结果，而被闲置，而是会先被拿去计算 x。</p><p>在 x 计算完成之后，d 也等来了 a 的计算结果。这个时候，我们的 FU 就会去计算出 d 的结果。然后在重排序缓冲区里，把对应的计算结果的提交顺序，仍然设置成 a -&gt; d -&gt; x，而计算完成的顺序是 x -&gt; a -&gt; d。</p><p>在这整个过程中，整个计算乘法的 FU 都没有闲置，这也意味着我们的 CPU 的吞吐率最大化了。</p><p>整个乱序执行技术，就好像在指令的执行阶段提供一个“线程池”。指令不再是顺序执行的，而是根据池里所拥有的资源，以及各个任务是否可以进行执行，进行动态调度。在执行完成之后，又重新把结果在一个队列里面，按照指令的分发顺序重新排序。即使内部是“乱序”的，但是在外部看起来，仍然是井井有条地顺序执行。</p><p>乱序执行，极大地提高了 CPU 的运行效率。核心原因是，现代 CPU 的运行速度比访问主内存的速度要快很多。如果完全采用顺序执行的方式，很多时间都会浪费在前面指令等待获取内存数据的时间里。CPU 不得不加入 NOP 操作进行空转。而现代 CPU 的流水线级数也已经相对比较深了，到达了 14 级。这也意味着，同一个时钟周期内并行执行的指令数是很多的。</p><p>而乱序执行，以及我们后面要讲的高速缓存，弥补了 CPU 和内存之间的性能差异。同样，也充分利用了较深的流水行带来的并发性，使得我们可以充分利用 CPU 的性能。</p><p><strong>控制冒险（Control Harzard）</strong></p><p>在遇到了控制冒险之后，我们的 CPU 具体会怎么应对呢？除了流水线停顿，等待前面的 jmp 指令执行完成之后，再去取最新的指令，还有什么好办法吗？当然是有的。我们一起来看一看。</p><p><strong>缩短分支延迟</strong></p><p>第一个办法，叫作缩短分支延迟。回想一下我们的条件跳转指令，条件跳转指令其实进行了两种电路操作。</p><p>第一种，是进行条件比较。这个条件比较，需要的输入是，根据指令的 opcode，就能确认的条件码寄存器。</p><p>第二种，是进行实际的跳转，也就是把要跳转的地址信息写入到 PC 寄存器。无论是 opcode，还是对应的条件码寄存器，还是我们跳转的地址，都是在指令译码（ID）的阶段就能获得的。而对应的条件码比较的电路，只要是简单的逻辑门电路就可以了，并不需要一个完整而复杂的 ALU。</p><p>所以，我们可以将条件判断、地址跳转，都提前到指令译码阶段进行，而不需要放在指令执行阶段。对应的，我们也要在 CPU 里面设计对应的旁路，在指令译码阶段，就提供对应的判断比较的电路。</p><p>这种方式，本质上和前面数据冒险的操作数前推的解决方案类似，就是在硬件电路层面，把一些计算结果更早地反馈到流水线中。这样反馈变得更快了，后面的指令需要等待的时间就变短了。</p><p>不过只是改造硬件，并不能彻底解决问题。跳转指令的比较结果，仍然要在指令执行的时候才能知道。在流水线里，第一条指令进行指令译码的时钟周期里，我们其实就要去取下一条指令了。这个时候，我们其实还没有开始指令执行阶段，自然也就不知道比较的结果。</p><p><strong>分支预测</strong></p><p>所以，这个时候，我们就引入了一个新的解决方案，叫作分支预测（Branch Prediction）技术，也就是说，让我们的 CPU 来猜一猜，条件跳转后执行的指令，应该是哪一条。</p><p>最简单的分支预测技术，叫作“假装分支不发生”。顾名思义，自然就是仍然按照顺序，把指令往下执行。其实就是 CPU 预测，条件跳转一定不发生。这样的预测方法，其实也是一种静态预测技术。就好像猜硬币的时候，你一直猜正面，会有 50% 的正确率。</p><p>如果分支预测是正确的，我们自然赚到了。这个意味着，我们节省下来本来需要停顿下来等待的时间。如果分支预测失败了呢？那我们就把后面已经取出指令已经执行的部分，给丢弃掉。这个丢弃的操作，在流水线里面，叫作 Zap 或者 Flush。CPU 不仅要执行后面的指令，对于这些已经在流水线里面执行到一半的指令，我们还需要做对应的清除操作。比如，清空已经使用的寄存器里面的数据等等，这些清除操作，也有一定的开销。</p><p>所以，CPU 需要提供对应的丢弃指令的功能，通过控制信号清除掉已经在流水线中执行的指令。只要对应的清除开销不要太大，我们就是划得来的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6715b9f1dd3cde80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>动态分支预测</strong></p><p>上面的静态预测策略，看起来比较简单，预测的准确率也许有 50%。但是如果运气不好，可能就会特别差。于是，工程师们就开始思考，我们有没有更好的办法呢？比如，根据之前条件跳转的比较结果来预测，是不是会更准一点？</p><p>而同样的策略，我们一样可以放在分支预测上。这种策略，我们叫一级分支预测（One Level Branch Prediction），或者叫 1 比特饱和计数（1-bit saturating counter）。这个方法，其实就是用一个比特，去记录当前分支的比较情况，直接用当前分支的比较情况，来预测下一次分支时候的比较情况。</p><p>只用一天下雨，就预测第二天下雨，这个方法还是有些“草率”，我们可以用更多的信息，而不只是一次的分支信息来进行预测。于是，我们可以引入一个状态机（State Machine）来做这个事情。</p><p>这个状态机里，我们一共有 4 个状态，所以我们需要 2 个比特来记录对应的状态。这样这整个策略，就可以叫作 2 比特饱和计数，或者叫双模态预测器（Bimodal Predictor）。</p><p>第一种方案，类似我们的操作数前推，其实是在改造我们的 CPU 功能，通过增加对应的电路的方式，来缩短分支带来的延迟。另外两种解决方案，无论是“假装分支不发生”，还是“动态分支预测”，其实都是在进行“分支预测”。只是，“假装分支不发生”是一种简单的静态预测方案而已。</p><p>在动态分支预测技术里，我给你介绍了一级分支预测，或者叫 1 比特饱和计数的方法。其实就是认为，预测结果和上一次的条件跳转是一致的。在此基础上，我还介绍了利用更多信息的，就是 2 比特饱和计数，或者叫双模态预测器的方法。这个方法其实也只是通过一个状态机，多看了一步过去的跳转比较结果。</p><p>这个方法虽然简单，但是却非常有效。在 SPEC 89 版本的测试当中，使用这样的饱和计数方法，预测的准确率能够高达 93.5%。Intel 的 CPU，一直到 Pentium 时代，在还没有使用 MMX 指令集的时候，用的就是这种分支预测方式。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-066387327ff198b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>多发射与超标量：同一时间执行的两条指令</strong></p><p>其实只要我们把取指令和指令译码，也一样通过增加硬件的方式，并行进行就好了。我们可以一次性从内存里面取出多条指令，然后分发给多个并行的指令译码器，进行译码，然后对应交给不同的功能单元去处理。这样，我们在一个时钟周期里，能够完成的指令就不只一条了。IPC 也就能做到大于 1 了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1d0426b91959d989.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这种 CPU 设计，我们叫作<strong>多发射（Mulitple Issue）</strong>和<strong>超标量（Superscalar）</strong>。</p><p>什么叫多发射呢？这个词听起来很抽象，其实它意思就是说，我们同一个时间，可能会同时把多条指令发射（Issue）到不同的译码器或者后续处理的流水线中去。</p><p>在超标量的 CPU 里面，有很多条并行的流水线，而不是只有一条流水线。“超标量“这个词是说，本来我们在一个时钟周期里面，只能执行一个标量（Scalar）的运算。在多发射的情况下，我们就能够超越这个限制，同时进行多次计算。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8881a1fe83153203.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>你可以看我画的这个超标量设计的流水线示意图。仔细看，你应该能看到一个有意思的现象，每一个功能单元的流水线的长度是不同的。事实上，不同的功能单元的流水线长度本来就不一样。我们平时所说的 14 级流水线，指的通常是进行整数计算指令的流水线长度。如果是浮点数运算，实际的流水线长度则会更长一些。</p><p><strong>超线程：Intel 多卖给你的那一倍 CPU</strong></p><p>不知道是不是因为当时面临的竞争太激烈了，为了让 Pentium 4 的 CPU 在性能上更有竞争力一点，2002 年底，Intel 在的 3.06GHz 主频的 Pentium 4 CPU 上，第一次引入了超线程（Hyper-Threading）技术。</p><p>什么是超线程技术呢？Intel 想，既然 CPU 同时运行那些在代码层面有前后依赖关系的指令，会遇到各种冒险问题，我们不如去找一些和这些指令完全独立，没有依赖关系的指令来运行好了。那么，这样的指令哪里来呢？自然同时运行在另外一个程序里了。</p><p>比如，在一个物理 CPU 核心内部，会有双份的 PC 寄存器、指令寄存器乃至条件码寄存器。这样，这个 CPU 核心就可以维护两条并行的指令的状态。在外面看起来，似乎有两个逻辑层面的 CPU 在同时运行。所以，超线程技术一般也被叫作同时多线程（Simultaneous Multi-Threading，简称 SMT）技术。</p><p>不过，在 CPU 的其他功能组件上，Intel 可不会提供双份。无论是指令译码器还是 ALU，一个 CPU 核心仍然只有一份。因为超线程并不是真的去同时运行两个指令，那就真的变成物理多核了。超线程的目的，是在一个线程 A 的指令，在流水线里停顿的时候，让另外一个线程去执行指令。因为这个时候，CPU 的译码器和 ALU 就空出来了，那么另外一个线程 B，就可以拿来干自己需要的事情。这个线程 B 可没有对于线程 A 里面指令的关联和依赖。</p><p><strong>SIMD：如何加速矩阵乘法？</strong></p><p>在上面的 CPU 信息的图里面，你会看到，中间有一组信息叫作 Instructions，里面写了有 MMX、SSE 等等。这些信息就是这个 CPU 所支持的指令集。这里的 MMX 和 SSE 的指令集，也就引出了我要给你讲的最后一个提升 CPU 性能的技术方案，SIMD，中文叫作单指令多数据流（Single Instruction Multiple Data）。</p><p>我们先来体会一下 SIMD 的性能到底怎么样。下面是两段示例程序，一段呢，是通过循环的方式，给一个 list 里面的每一个数加 1。另一段呢，是实现相同的功能，但是直接调用 NumPy 这个库的 add 方法。在统计两段程序的性能的时候，我直接调用了 Python 里面的 timeit 的库。</p><pre><code>$ python&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; import timeit&gt;&gt;&gt; a = list(range(1000))&gt;&gt;&gt; b = np.array(range(1000))&gt;&gt;&gt; timeit.timeit("[i + 1 for i in a]", setup="from __main__ import a", number=1000000)32.82800309999993&gt;&gt;&gt; timeit.timeit("np.add(1, b)", setup="from __main__ import np, b", number=1000000)0.9787889999997788&gt;&gt;&gt;</code></pre><p>从两段程序的输出结果来看，你会发现，两个功能相同的代码性能有着巨大的差异，足足差出了 30 多倍。也难怪所有用 Python 讲解数据科学的教程里，往往在一开始就告诉你不要使用循环，而要把所有的计算都向量化（Vectorize）。</p><p>有些同学可能会猜测，是不是因为 Python 是一门解释性的语言，所以这个性能差异会那么大。第一段程序的循环的每一次操作都需要 Python 解释器来执行，而第二段的函数调用是一次调用编译好的原生代码，所以才会那么快。如果你这么想，不妨试试直接用 C 语言实现一下 1000 个元素的数组里面的每个数加 1。你会发现，即使是 C 语言编译出来的代码，还是远远低于 NumPy。原因就是，NumPy 直接用到了 SIMD 指令，能够并行进行向量的操作。</p><p>而前面使用循环来一步一步计算的算法呢，一般被称为 SISD，也就是单指令单数据（Single Instruction Single Data）的处理方式。如果你手头的是一个多核 CPU 呢，那么它同时处理多个指令的方式可以叫作 MIMD，也就是多指令多数据（Multiple Instruction Multiple Dataa）。</p><p>为什么 SIMD 指令能快那么多呢？这是因为，SIMD 在获取数据和执行指令的时候，都做到了并行。一方面，在从内存里面读取数据的时候，SIMD 是一次性读取多个数据。</p><p>就以我们上面的程序为例，数组里面的每一项都是一个 integer，也就是需要 4 Bytes 的内存空间。Intel 在引入 SSE 指令集的时候，在 CPU 里面添上了 8 个 128 Bits 的寄存器。128 Bits 也就是 16 Bytes ，也就是说，一个寄存器一次性可以加载 4 个整数。比起循环分别读取 4 次对应的数据，时间就省下来了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7c06529ceb185f13.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在数据读取到了之后，在指令的执行层面，SIMD 也是可以并行进行的。4 个整数各自加 1，互相之前完全没有依赖，也就没有冒险问题需要处理。只要 CPU 里有足够多的功能单元，能够同时进行这些计算，这个加法就是 4 路同时并行的，自然也省下了时间。</p><p>所以，对于那些在计算层面存在大量“数据并行”（Data Parallelism）的计算中，使用 SIMD 是一个很划算的办法。在这个大量的“数据并行”，其实通常就是实践当中的向量运算或者矩阵运算。在实际的程序开发过程中，过去通常是在进行图片、视频、音频的处理。最近几年则通常是在进行各种机器学习算法的计算。</p><p>而基于 SIMD 的向量计算指令，也正是在 Intel 发布 Pentium 处理器的时候，被引入的指令集。当时的指令集叫作 MMX，也就是 Matrix Math eXtensions 的缩写，中文名字就是矩阵数学扩展。而 Pentium 处理器，也是 CPU 第一次有能力进行多媒体处理。这也正是拜 SIMD 和 MMX 所赐。</p><h3 id="异常的分类：中断、陷阱、故障和中止"><a href="#异常的分类：中断、陷阱、故障和中止" class="headerlink" title="异常的分类：中断、陷阱、故障和中止"></a>异常的分类：中断、陷阱、故障和中止</h3><p>我在前面说了，异常可以由硬件触发，也可以由软件触发。那我们平时会碰到哪些异常呢？下面我们就一起来看看。</p><p>第一种异常叫中断（Interrupt）。顾名思义，自然就是程序在执行到一半的时候，被打断了。这个打断执行的信号，来自于 CPU 外部的 I/O 设备。你在键盘上按下一个按键，就会对应触发一个相应的信号到达 CPU 里面。CPU 里面某个开关的值发生了变化，也就触发了一个中断类型的异常。</p><p>第二种异常叫陷阱（Trap）。陷阱，其实是我们程序员“故意“主动触发的异常。就好像你在程序里面打了一个断点，这个断点就是设下的一个”陷阱”。当程序的指令执行到这个位置的时候，就掉到了这个陷阱当中。然后，对应的异常处理程序就会来处理这个”陷阱”当中的猎物。</p><p>最常见的一类陷阱，发生在我们的应用程序调用系统调用的时候，也就是从程序的用户态切换到内核态的时候。我们在第 3 讲讲 CPU 性能的时候说过，可以用 Linux 下的 time 指令，去查看一个程序运行实际花费的时间，里面有在用户态花费的时间（user time），也有在内核态发生的时间（system time）。</p><p>我们的应用程序通过系统调用去读取文件、创建进程，其实也是通过触发一次陷阱来进行的。这是因为，我们用户态的应用程序没有权限来做这些事情，需要把对应的流程转交给有权限的异常处理程序来进行。</p><p>第三种异常叫故障（Fault）。它和陷阱的区别在于，陷阱是我们开发程序的时候刻意触发的异常，而故障通常不是。比如，我们在程序执行的过程中，进行加法计算发生了溢出，其实就是故障类型的异常。这个异常不是我们在开发的时候计划内的，也一样需要有对应的异常处理程序去处理。</p><p>故障和陷阱、中断的一个重要区别是，故障在异常程序处理完成之后，仍然回来处理当前的指令，而不是去执行程序中的下一条指令。因为当前的指令因为故障的原因并没有成功执行完成。</p><p>最后一种异常叫中止（Abort）。与其说这是一种异常类型，不如说这是故障的一种特殊情况。当 CPU 遇到了故障，但是恢复不过来的时候，程序就不得不中止了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a0ae77fcbd3c22e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="81E033D3-9A7F-45ED-A65D-6DC7815D3040.png"></p><h3 id="CISC和RISC"><a href="#CISC和RISC" class="headerlink" title="CISC和RISC"></a>CISC和RISC</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d1bc5ecd3e7457a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>于是，从 Pentium Pro 时代开始，Intel 就开始在处理器里引入了微指令（Micro-Instructions/Micro-Ops）架构。而微指令架构的引入，也让 CISC 和 RISC 的分界变得模糊了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d7b00dce2d051b21.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在微指令架构的 CPU 里面，编译器编译出来的机器码和汇编代码并没有发生什么变化。但在指令译码的阶段，指令译码器“翻译”出来的，不再是某一条 CPU 指令。译码器会把一条机器码，“翻译”成好几条“微指令”。这里的一条条微指令，就不再是 CISC 风格的了，而是变成了固定长度的 RISC 风格的了。</p><p>这些 RISC 风格的微指令，会被放到一个微指令缓冲区里面，然后再从缓冲区里面，分发给到后面的超标量，并且是乱序执行的流水线架构里面。不过这个流水线架构里面接受的，就不是复杂的指令，而是精简的指令了。在这个架构里，我们的指令译码器相当于变成了设计模式里的一个“适配器”（Adaptor）。这个适配器，填平了 CISC 和 RISC 之间的指令差异。</p><p>不过，凡事有好处就有坏处。这样一个能够把 CISC 的指令译码成 RISC 指令的指令译码器，比原来的指令译码器要复杂。这也就意味着更复杂的电路和更长的译码时间：本来以为可以通过 RISC 提升的性能，结果又有一部分浪费在了指令译码上。针对这个问题，我们有没有更好的办法呢？</p><p>我在前面说过，之所以大家认为 RISC 优于 CISC，来自于一个数字统计，那就是在实际的程序运行过程中，有 80% 运行的代码用着 20% 的常用指令。这意味着，CPU 里执行的代码有很强的局部性。而对于有着很强局部性的问题，常见的一个解决方案就是使用缓存。</p><p>所以，Intel 就在 CPU 里面加了一层 L0 Cache。这个 Cache 保存的就是指令译码器把 CISC 的指令“翻译”成 RISC 的微指令的结果。于是，在大部分情况下，CPU 都可以从 Cache 里面拿到译码结果，而不需要让译码器去进行实际的译码操作。这样不仅优化了性能，因为译码器的晶体管开关动作变少了，还减少了功耗。</p><p>因为“微指令”架构的存在，从 Pentium Pro 开始，Intel 处理器已经不是一个纯粹的 CISC 处理器了。它同样融合了大量 RISC 类型的处理器设计。不过，由于 Intel 本身在 CPU 层面做的大量优化，比如乱序执行、分支预测等相关工作，x86 的 CPU 始终在功耗上还是要远远超过 RISC 架构的 ARM，所以最终在智能手机崛起替代 PC 的时代，落在了 ARM 后面。</p><p><strong>ARM 和 RISC-V：CPU 的现在与未来</strong></p><p>2017 年，ARM 公司的 CEO Simon Segards 宣布，ARM 累积销售的芯片数量超过了 1000 亿。作为一个从 12 个人起步，在 80 年代想要获取 Intel 的 80286 架构授权来制造 CPU 的公司，ARM 是如何在移动端把自己的芯片塑造成了最终的霸主呢？</p><p>ARM 这个名字现在的含义，是“Advanced RISC Machines”。你从名字就能够看出来，ARM 的芯片是基于 RISC 架构的。不过，ARM 能够在移动端战胜 Intel，并不是因为 RISC 架构。</p><p>到了 21 世纪的今天，CISC 和 RISC 架构的分界已经没有那么明显了。Intel 和 AMD 的 CPU 也都是采用译码成 RISC 风格的微指令来运行。而 ARM 的芯片，一条指令同样需要多个时钟周期，有乱序执行和多发射。我甚至看到过这样的评价，“ARM 和 RISC 的关系，只有在名字上”。</p><p>ARM 真正能够战胜 Intel，我觉得主要是因为下面这两点原因。</p><p>第一点是功耗优先的设计。一个 4 核的 Intel i7 的 CPU，设计的时候功率就是 130W。而一块 ARM A8 的单个核心的 CPU，设计功率只有 2W。两者之间差出了 100 倍。在移动设备上，功耗是一个远比性能更重要的指标，毕竟我们不能随时在身上带个发电机。ARM 的 CPU，主频更低，晶体管更少，高速缓存更小，乱序执行的能力更弱。所有这些，都是为了功耗所做的妥协。</p><p>第二点则是低价。ARM 并没有自己垄断 CPU 的生产和制造，只是进行 CPU 设计，然后把对应的知识产权授权出去，让其他的厂商来生产 ARM 架构的 CPU。它甚至还允许这些厂商可以基于 ARM 的架构和指令集，设计属于自己的 CPU。像苹果、三星、华为，它们都是拿到了基于 ARM 体系架构设计和制造 CPU 的授权。ARM 自己只是收取对应的专利授权费用。多个厂商之间的竞争，使得 ARM 的芯片在市场上价格很便宜。所以，尽管 ARM 的芯片的出货量远大于 Intel，但是收入和利润却比不上 Intel。</p><p>不过，ARM 并不是开源的。所以，在 ARM 架构逐渐垄断移动端芯片市场的时候，“开源硬件”也慢慢发展起来了。一方面，MIPS 在 2019 年宣布开源；另一方面，从 UC Berkeley 发起的RISC-V项目也越来越受到大家的关注。而 RISC 概念的发明人，图灵奖的得主大卫·帕特森教授从伯克利退休之后，成了 RISC-V 国际开源实验室的负责人，开始推动 RISC-V 这个“CPU 届的 Linux”的开发。可以想见，未来的开源 CPU，也多半会像 Linux 一样，逐渐成为一个业界的主流选择。如果想要“打造一个属于自己 CPU”，不可不关注这个项目。</p><h3 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h3><p>这个对于图像进行实时渲染的过程，可以被分解成下面这样 5 个步骤：</p><ul><li>顶点处理（Vertex Processing）</li><li>图元处理（Primitive Processing）</li><li>栅格化（Rasterization）</li><li>片段处理（Fragment Processing）</li><li>像素操作（Pixel Operations）</li></ul><p><strong>解放图形渲染的 GPU</strong></p><p>我们可以想一想，如果用 CPU 来进行这个渲染过程，需要花上多少资源呢？我们可以通过一些数据来做个粗略的估算。</p><p>在上世纪 90 年代的时候，屏幕的分辨率还没有现在那么高。一般的 CRT 显示器也就是 640×480 的分辨率。这意味着屏幕上有 30 万个像素需要渲染。为了让我们的眼睛看到画面不晕眩，我们希望画面能有 60 帧。于是，每秒我们就要重新渲染 60 次这个画面。也就是说，每秒我们需要完成 1800 万次单个像素的渲染。从栅格化开始，每个像素有 3 个流水线步骤，即使每次步骤只有 1 个指令，那我们也需要 5400 万条指令，也就是 54M 条指令。</p><p>90 年代的 CPU 的性能是多少呢？93 年出货的第一代 Pentium 处理器，主频是 60MHz，后续逐步推出了 66MHz、75MHz、100MHz 的处理器。以这个性能来看，用 CPU 来渲染 3D 图形，基本上就要把 CPU 的性能用完了。因为实际的每一个渲染步骤可能不止一个指令，我们的 CPU 可能根本就跑不动这样的三维图形渲染。</p><p>也就是在这个时候，Voodoo FX 这样的图形加速卡登上了历史舞台。既然图形渲染的流程是固定的，那我们直接用硬件来处理这部分过程，不用 CPU 来计算是不是就好了？很显然，这样的硬件会比制造有同样计算性能的 CPU 要便宜得多。因为整个计算流程是完全固定的，不需要流水线停顿、乱序执行等等的各类导致 CPU 计算变得复杂的问题。我们也不需要有什么可编程能力，只要让硬件按照写好的逻辑进行运算就好了。</p><p>那个时候，整个顶点处理的过程还是都由 CPU 进行的，不过后续所有到图元和像素级别的处理都是通过 Voodoo FX 或者 TNT 这样的显卡去处理的。也就是从这个时代开始，我们能玩上“真 3D”的游戏了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-86ad4eea8cc42744.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>不过，无论是 Voodoo FX 还是 NVidia TNT。整个显卡的架构还不同于我们现代的显卡，也没有现代显卡去进行各种加速深度学习的能力。这个能力，要到 NVidia 提出 Unified Shader Archicture 才开始具备。这也是我们下一讲要讲的内容。</p><p><strong>Shader 的诞生和可编程图形处理器</strong></p><p>不知道你有没有发现，在 Voodoo 和 TNT 显卡的渲染管线里面，没有“顶点处理“这个步骤。在当时，把多边形的顶点进行线性变化，转化到我们的屏幕的坐标系的工作还是由 CPU 完成的。所以，CPU 的性能越好，能够支持的多边形也就越多，对应的多边形建模的效果自然也就越像真人。而 3D 游戏的多边形性能也受限于我们 CPU 的性能。无论你的显卡有多快，如果 CPU 不行，3D 画面一样还是不行。</p><p>所以，1999 年 NVidia 推出的 GeForce 256 显卡，就把顶点处理的计算能力，也从 CPU 里挪到了显卡里。不过，这对于想要做好 3D 游戏的程序员们还不够，即使到了 GeForce 256。整个图形渲染过程都是在硬件里面固定的管线来完成的。程序员们在加速卡上能做的事情呢，只有改配置来实现不同的图形渲染效果。如果通过改配置做不到，我们就没有什么办法了。</p><p>这个时候，程序员希望我们的 GPU 也能有一定的可编程能力。这个编程能力不是像 CPU 那样，有非常通用的指令，可以进行任何你希望的操作，而是在整个的渲染管线（Graphics Pipeline）的一些特别步骤，能够自己去定义处理数据的算法或者操作。于是，从 2001 年的 Direct3D 8.0 开始，微软第一次引入了可编程管线（Programable Function Pipeline）的概念。</p><p>早期的可编程管线的 GPU，提供了单独的顶点处理和片段处理（像素处理）的着色器</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-52e42f675197e417.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>一开始的可编程管线呢，仅限于顶点处理（Vertex Processing）和片段处理（Fragment Processing）部分。比起原来只能通过显卡和 Direct3D 这样的图形接口提供的固定配置，程序员们终于也可以开始在图形效果上开始大显身手了。</p><p>这些可以编程的接口，我们称之为 Shader，中文名称就是着色器。之所以叫“着色器”，是因为一开始这些“可编程”的接口，只能修改顶点处理和片段处理部分的程序逻辑。我们用这些接口来做的，也主要是光照、亮度、颜色等等的处理，所以叫着色器。</p><p>这个时候的 GPU，有两类 Shader，也就是 Vertex Shader 和 Fragment Shader。我们在上一讲看到，在进行顶点处理的时候，我们操作的是多边形的顶点；在片段操作的时候，我们操作的是屏幕上的像素点。对于顶点的操作，通常比片段要复杂一些。所以一开始，这两类 Shader 都是独立的硬件电路，也各自有独立的编程接口。因为这么做，硬件设计起来更加简单，一块 GPU 上也能容纳下更多的 Shader。</p><p>不过呢，大家很快发现，虽然我们在顶点处理和片段处理上的具体逻辑不太一样，但是里面用到的指令集可以用同一套。而且，虽然把 Vertex Shader 和 Fragment Shader 分开，可以减少硬件设计的复杂程度，但是也带来了一种浪费，有一半 Shader 始终没有被使用。在整个渲染管线里，Vertext Shader 运行的时候，Fragment Shader 停在那里什么也没干。Fragment Shader 在运行的时候，Vertext Shader 也停在那里发呆。</p><p>本来 GPU 就不便宜，结果设计的电路有一半时间是闲着的。喜欢精打细算抠出每一分性能的硬件工程师当然受不了了。于是，统一着色器架构（Unified Shader Architecture）就应运而生了。</p><p>既然大家用的指令集是一样的，那不如就在 GPU 里面放很多个一样的 Shader 硬件电路，然后通过统一调度，把顶点处理、图元处理、片段处理这些任务，都交给这些 Shader 去处理，让整个 GPU 尽可能地忙起来。这样的设计，就是我们现代 GPU 的设计，就是统一着色器架构。</p><p>有意思的是，这样的 GPU 并不是先在 PC 里面出现的，而是来自于一台游戏机，就是微软的 XBox 360。后来，这个架构才被用到 ATI 和 NVidia 的显卡里。这个时候的“着色器”的作用，其实已经和它的名字关系不大了，而是变成了一个通用的抽象计算模块的名字。</p><p>正是因为 Shader 变成一个“通用”的模块，才有了把 GPU 拿来做各种通用计算的用法，也就是 GPGPU（General-Purpose Computing on Graphics Processing Units，通用图形处理器）。而正是因为 GPU 可以拿来做各种通用的计算，才有了过去 10 年深度学习的火热。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7d0dc58239e7e478.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>现代 GPU 的三个核心创意</strong></p><p><strong>芯片瘦身</strong></p><p>我们先来回顾一下，之前花了很多讲仔细讲解的现代 CPU。现代 CPU 里的晶体管变得越来越多，越来越复杂，其实已经不是用来实现“计算”这个核心功能，而是拿来实现处理乱序执行、进行分支预测，以及我们之后要在存储器讲的高速缓存部分。</p><p>而在 GPU 里，这些电路就显得有点多余了，GPU 的整个处理过程是一个流式处理（Stream Processing）的过程。因为没有那么多分支条件，或者复杂的依赖关系，我们可以把 GPU 里这些对应的电路都可以去掉，做一次小小的瘦身，只留下取指令、指令译码、ALU 以及执行这些计算需要的寄存器和缓存就好了。一般来说，我们会把这些电路抽象成三个部分，就是下面图里的取指令和指令译码、ALU 和执行上下文。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3bd4664c71dfe7d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>多核并行和 SIMT</strong></p><p>这样一来，我们的 GPU 电路就比 CPU 简单很多了。于是，我们就可以在一个 GPU 里面，塞很多个这样并行的 GPU 电路来实现计算，就好像 CPU 里面的多核 CPU 一样。和 CPU 不同的是，我们不需要单独去实现什么多线程的计算。因为 GPU 的运算是天然并行的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5888ab1211d752bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们在上一讲里面其实已经看到，无论是对多边形里的顶点进行处理，还是屏幕里面的每一个像素进行处理，每个点的计算都是独立的。所以，简单地添加多核的 GPU，就能做到并行加速。不过光这样加速还是不够，工程师们觉得，性能还有进一步被压榨的空间。</p><p>我们在第 27 讲里面讲过，CPU 里有一种叫作 SIMD 的处理技术。这个技术是说，在做向量计算的时候，我们要执行的指令是一样的，只是同一个指令的数据有所不同而已。在 GPU 的渲染管线里，这个技术可就大有用处了。</p><p>无论是顶点去进行线性变换，还是屏幕上临近像素点的光照和上色，都是在用相同的指令流程进行计算。所以，GPU 就借鉴了 CPU 里面的 SIMD，用了一种叫作SIMT（Single Instruction，Multiple Threads）的技术。SIMT 呢，比 SIMD 更加灵活。在 SIMD 里面，CPU 一次性取出了固定长度的多个数据，放到寄存器里面，用一个指令去执行。而 SIMT，可以把多条数据，交给不同的线程去处理。</p><p>各个线程里面执行的指令流程是一样的，但是可能根据数据的不同，走到不同的条件分支。这样，相同的代码和相同的流程，可能执行不同的具体的指令。这个线程走到的是 if 的条件分支，另外一个线程走到的就是 else 的条件分支了。</p><p>于是，我们的 GPU 设计就可以进一步进化，也就是在取指令和指令译码的阶段，取出的指令可以给到后面多个不同的 ALU 并行进行运算。这样，我们的一个 GPU 的核里，就可以放下更多的 ALU，同时进行更多的并行运算了。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-aa28adf00fbd67d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>GPU 里的“超线程”</strong></p><p>虽然 GPU 里面的主要以数值计算为主。不过既然已经是一个“通用计算”的架构了，GPU 里面也避免不了会有 if…else 这样的条件分支。但是，在 GPU 里我们可没有 CPU 这样的分支预测的电路。这些电路在上面“芯片瘦身”的时候，就已经被我们砍掉了。</p><p>所以，GPU 里的指令，可能会遇到和 CPU 类似的“流水线停顿”问题。想到流水线停顿，你应该就能记起，我们之前在 CPU 里面讲过超线程技术。在 GPU 上，我们一样可以做类似的事情，也就是遇到停顿的时候，调度一些别的计算任务给当前的 ALU。</p><p>和超线程一样，既然要调度一个不同的任务过来，我们就需要针对这个任务，提供更多的执行上下文。所以，一个 Core 里面的执行上下文的数量，需要比 ALU 多。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0d3938a8ab77f544.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们去看 NVidia 2080 显卡的技术规格，就可以算出，它到底有多大的计算能力。</p><p>2080 一共有 46 个 SM（Streaming Multiprocessor，流式处理器），这个 SM 相当于 GPU 里面的 GPU Core，所以你可以认为这是一个 46 核的 GPU，有 46 个取指令指令译码的渲染管线。每个 SM 里面有 64 个 Cuda Core。你可以认为，这里的 Cuda Core 就是我们上面说的 ALU 的数量或者 Pixel Shader 的数量，46x64 呢一共就有 2944 个 Shader。然后，还有 184 个 TMU，TMU 就是 Texture Mapping Unit，也就是用来做纹理映射的计算单元，它也可以认为是另一种类型的 Shader。</p><p>2080 Super 显卡有 48 个 SM，比普通版的 2080 多 2 个。每个 SM（SM 也就是 GPU Core）里有 64 个 Cuda Core，也就是 Shader</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0267467f026edcd8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>2080 的主频是 1515MHz，如果自动超频（Boost）的话，可以到 1700MHz。而 NVidia 的显卡，根据硬件架构的设计，每个时钟周期可以执行两条指令。所以，能做的浮点数运算的能力，就是：</p><p>（2944 + 184）× 1700 MHz × 2 = 10.06 TFLOPS</p><p>那么，最新的 Intel i9 9900K 的性能是多少呢？不到 1TFLOPS。而 2080 显卡和 9900K 的价格却是差不多的。所以，在实际进行深度学习的过程中，用 GPU 所花费的时间，往往能减少一到两个数量级。而大型的深度学习模型计算，往往又是多卡并行，要花上几天乃至几个月。这个时候，用 CPU 显然就不合适了。</p><p>今天，随着 GPGPU 的推出，GPU 已经不只是一个图形计算设备，更是一个用来做数值计算的好工具了。同样，也是因为 GPU 的快速发展，带来了过去 10 年深度学习的繁荣。</p><h3 id="FPGA和ASIC"><a href="#FPGA和ASIC" class="headerlink" title="FPGA和ASIC"></a>FPGA和ASIC</h3><p><strong>FPGA</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e18e8a2b125dca9d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p> 这个，就是我们接下来要说的 FPGA，也就是现场可编程门阵列（Field-Programmable Gate Array）。看到这个名字，你可能要说了，这里面每个单词单独我都认识，放到一起就不知道是什么意思了。</p><p> 没关系，我们就从 FPGA 里面的每一个字符，一个一个来看看它到底是什么意思。</p><ul><li>P 代表 Programmable，这个很容易理解。也就是说这是一个可以通过编程来控制的硬件。</li><li>G 代表 Gate 也很容易理解，它就代表芯片里面的门电路。我们能够去进行编程组合的就是这样一个一个门电路。</li><li>A 代表的 Array，叫作阵列，说的是在一块 FPGA 上，密密麻麻列了大量 Gate 这样的门电路。</li><li>最后一个 F，不太容易理解。它其实是说，一块 FPGA 这样的板子，可以在“现场”多次进行编程。它不像 PAL（Programmable Array Logic，可编程阵列逻辑）这样更古老的硬件设备，只能“编程”一次，把预先写好的程序一次性烧录到硬件里面，之后就不能再修改了。</li></ul><p> 这么看来，其实“FPGA”这样的组合，基本上解决了我们前面说的想要设计硬件的问题。我们可以像软件一样对硬件编程，可以反复烧录，还有海量的门电路，可以组合实现复杂的芯片功能。</p><p> FPGA 的解决方案很精巧，我把它总结为这样三个步骤。</p><p> 第一，用存储换功能实现组合逻辑。在实现 CPU 的功能的时候，我们需要完成各种各样的电路逻辑。在 FPGA 里，这些基本的电路逻辑，不是采用布线连接的方式进行的，而是预先根据我们在软件里面设计的逻辑电路，算出对应的真值表，然后直接存到一个叫作 LUT（Look-Up Table，查找表）的电路里面。这个 LUT 呢，其实就是一块存储空间，里面存储了“特定的输入信号下，对应输出 0 还是 1”。</p><p> <img src="https://upload-images.jianshu.io/upload_images/12321605-db108369a1d903f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这里面的关键就在于，这个查表的办法，不只能够提供斐波那契数列。如果我们要有一个获得 N 的 5 次方的函数，一样可以先计算好，放在表里面进行查询。这个“查表”的方法，其实就是 FPGA 通过 LUT 来实现各种组合逻辑的办法。</p><p>第二，对于需要实现的时序逻辑电路，我们可以在 FPGA 里面直接放上 D 触发器，作为寄存器。这个和 CPU 里的触发器没有什么本质不同。不过，我们会把很多个 LUT 的电路和寄存器组合在一起，变成一个叫作逻辑簇（Logic Cluster）的东西。在 FPGA 里，这样组合了多个 LUT 和寄存器的设备，也被叫做 CLB（Configurable Logic Block，可配置逻辑块）。</p><p>我们通过配置 CLB 实现的功能有点儿像我们前面讲过的全加器。它已经在最基础的门电路上做了组合，能够提供更复杂一点的功能。更复杂的芯片功能，我们不用再从门电路搭起，可以通过 CLB 组合搭建出来。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-58037d8118411269.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>第三，FPGA 是通过可编程逻辑布线，来连接各个不同的 CLB，最终实现我们想要实现的芯片功能。这个可编程逻辑布线，你可以把它当成我们的铁路网。整个铁路系统已经铺好了，但是整个铁路网里面，设计了很多个道岔。我们可以通过控制道岔，来确定不同的列车线路。在可编程逻辑布线里面，“编程”在做的，就是拨动像道岔一样的各个电路开关，最终实现不同 CLB 之间的连接，完成我们想要的芯片功能。</p><p>于是，通过 LUT 和寄存器，我们能够组合出很多 CLB，而通过连接不同的 CLB，最终有了我们想要的芯片功能。最关键的是，这个组合过程是可以“编程”控制的。而且这个编程出来的软件，还可以后续改写，重新写入到硬件里。让同一个硬件实现不同的芯片功能。从这个角度来说，FPGA 也是“软件吞噬世界”的一个很好的例子。</p><p><strong>ASIC</strong></p><p>于是，我们就考虑为这些有专门用途的场景，单独设计一个芯片。这些专门设计的芯片呢，我们称之为 ASIC（Application-Specific Integrated Circuit），也就是专用集成电路。事实上，过去几年，ASIC 发展得特别快。因为 ASIC 是针对专门用途设计的，所以它的电路更精简，单片的制造成本也比 CPU 更低。而且，因为电路精简，所以通常能耗要比用来做通用计算的 CPU 更低。而我们上一讲所说的早期的图形加速卡，其实就可以看作是一种 ASIC。</p><p>因为 ASIC 的生产制造成本，以及能耗上的优势，过去几年里，有不少公司设计和开发 ASIC 用来“挖矿”。这个“挖矿”，说的其实就是设计专门的数值计算芯片，用来“挖”比特币、ETH 这样的数字货币。</p><p>那么，我们能不能用刚才说的 FPGA 来做 ASIC 的事情呢？当然是可以的。我们对 FPGA 进行“编程”，其实就是把 FPGA 的电路变成了一个 ASIC。这样的芯片，往往在成本和功耗上优于需要做通用计算的 CPU 和 GPU。</p><p>那你可能又要问了，那为什么我们干脆不要用 ASIC 了，全都用 FPGA 不就好了么？你要知道，其实 FPGA 一样有缺点，那就是它的硬件上有点儿“浪费”。这个很容易理解，我一说你就明白了。</p><p>每一个 LUT 电路，其实都是一个小小的“浪费”。一个 LUT 电路设计出来之后，既可以实现与门，又可以实现或门，自然用到的晶体管数量，比单纯连死的与门或者或门的要多得多。同时，因为用的晶体管多，它的能耗也比单纯连死的电路要大，单片 FPGA 的生产制造的成本也比 ASIC 要高不少。</p><p>当然，有缺点就有优点，FPGA 的优点在于，它没有硬件研发成本。ASIC 的电路设计，需要仿真、验证，还需要经过流片（Tape out），变成一个印刷的电路版，最终变成芯片。这整个从研发到上市的过程，最低花费也要几万美元，高的话，会在几千万乃至数亿美元。更何况，整个设计还有失败的可能。所以，如果我们设计的专用芯片，只是要制造几千片，那买几千片现成的 FPGA，可能远比花上几百万美元，来设计、制造 ASIC 要经济得多。</p><p>实际上，到底使用 ASIC 这样的专用芯片，还是采用 FPGA 这样可编程的通用硬件，核心的决策因素还是成本。不过这个成本，不只是单个芯片的生产制造成本，还要考虑总体拥有成本（Total Cost of Ownership），也就是说，除了生产成本之外，我们要把研发成本也算进去。如果我们只制造了一片芯片，那么成本就是“这枚芯片的成本 + 为了这枚芯片建的生产线的成本 + 芯片的研发成本”，而不只是“芯片的原材料沙子的成本 + 生产的电费”。</p><p>单个 ASIC 的生产制造成本比 FPGA 低，ASIC 的能耗也比能实现同样功能的 FPGA 要低。能耗低，意味着长时间运行这些芯片，所用的电力成本也更低。</p><p>但是，ASIC 有一笔很高的 NRE（Non-Recuring Engineering Cost，一次性工程费用）成本。这个成本，就是 ASIC 实际“研发”的成本。只有需要大量生产 ASIC 芯片的时候，我们才能摊薄这份研发成本。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-817a1af98eb0ed99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>其实，在我们的日常软件开发过程中，也需要做同样的决策。很多我们需要的功能，可能在市面上已经有开源的软件可以实现。我们可以在开源的软件之上做配置或者开发插件，也可以选择自己从头开始写代码。</p><h2 id="存储与I-O系统"><a href="#存储与I-O系统" class="headerlink" title="存储与I/O系统"></a>存储与I/O系统</h2><h3 id="存储器层次结构全景"><a href="#存储器层次结构全景" class="headerlink" title="存储器层次结构全景"></a>存储器层次结构全景</h3><p>而我们大脑中的记忆，就好比 CPU Cache（CPU 高速缓存，我们常常简称为“缓存”）。CPU Cache 用的是一种叫作 SRAM（Static Random-Access Memory，静态随机存取存储器）的芯片。</p><p><strong>SRAM</strong></p><p>SRAM 之所以被称为“静态”存储器，是因为只要处在通电状态，里面的数据就可以保持存在。而一旦断电，里面的数据就会丢失了。在 SRAM 里面，一个比特的数据，需要 6～8 个晶体管。所以 SRAM 的存储密度不高。同样的物理空间下，能够存储的数据有限。不过，因为 SRAM 的电路简单，所以访问速度非常快。</p><p>在 CPU 里，通常会有 L1、L2、L3 这样三层高速缓存。每个 CPU 核心都有一块属于自己的 L1 高速缓存，通常分成指令缓存和数据缓存，分开存放 CPU 使用的指令和数据。</p><p>L2 的 Cache 同样是每个 CPU 核心都有的，不过它往往不在 CPU 核心的内部。所以，L2 Cache 的访问速度会比 L1 稍微慢一些。而 L3 Cache，则通常是多个 CPU 核心共用的，尺寸会更大一些，访问速度自然也就更慢一些。</p><p>你可以把 CPU 中的 L1 Cache 理解为我们的短期记忆，把 L2/L3 Cache 理解成长期记忆，把内存当成我们拥有的书架或者书桌。 当我们自己记忆中没有资料的时候，可以从书桌或者书架上拿书来翻阅。这个过程中就相当于，数据从内存中加载到 CPU 的寄存器和 Cache 中，然后通过“大脑”，也就是 CPU，进行处理和运算。</p><p><strong>DRAM</strong></p><p>内存用的芯片和 Cache 有所不同，它用的是一种叫作 DRAM（Dynamic Random Access Memory，动态随机存取存储器）的芯片，比起 SRAM 来说，它的密度更高，有更大的容量，而且它也比 SRAM 芯片便宜不少。</p><p>DRAM 被称为“动态”存储器，是因为 DRAM 需要靠不断地“刷新”，才能保持数据被存储起来。DRAM 的一个比特，只需要一个晶体管和一个电容就能存储。所以，DRAM 在同样的物理空间下，能够存储的数据也就更多，也就是存储的“密度”更大。但是，因为数据是存储在电容里的，电容会不断漏电，所以需要定时刷新充电，才能保持数据不丢失。DRAM 的数据访问电路和刷新电路都比 SRAM 更复杂，所以访问延时也就更长。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f7c096b3c0f15abc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>为了弥补两者之间的性能差异，我们能真实地把 CPU 的性能提升用起来，而不是让它在那儿空转，我们在现代 CPU 中引入了高速缓存。</p><p>从 CPU Cache 被加入到现有的 CPU 里开始，内存中的指令、数据，会被加载到 L1-L3 Cache 中，而不是直接由 CPU 访问内存去拿。在 95% 的情况下，CPU 都只需要访问 L1-L3 Cache，从里面读取指令和数据，而无需访问内存。要注意的是，这里我们说的 CPU Cache 或者 L1/L3 Cache，不是一个单纯的、概念上的缓存（比如之前我们说的拿内存作为硬盘的缓存），而是指特定的由 SRAM 组成的物理芯片。</p><p>这里是一张 Intel CPU 的放大照片。这里面大片的长方形芯片，就是这个 CPU 使用的 20MB 的 L3 Cache。</p><p>现代 CPU 中大量的空间已经被 SRAM 占据，图中用红色框出的部分就是 CPU 的 L3 Cache 芯片</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0b6e86113fa6ace2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在这一讲一开始的程序里，运行程序的时间主要花在了将对应的数据从内存中读取出来，加载到 CPU Cache 里。CPU 从内存中读取数据到 CPU Cache 的过程中，是一小块一小块来读取数据的，而不是按照单个数组元素来读取数据的。这样一小块一小块的数据，在 CPU Cache 里面，我们把它叫作 Cache Line（缓存块）。</p><p>在我们日常使用的 Intel 服务器或者 PC 里，Cache Line 的大小通常是 64 字节。而在上面的循环 2 里面，我们每隔 16 个整型数计算一次，16 个整型数正好是 64 个字节。于是，循环 1 和循环 2，需要把同样数量的 Cache Line 数据从内存中读取到 CPU Cache 中，最终两个程序花费的时间就差别不大了。</p><p>总结一下，一个内存的访问地址，最终包括高位代表的组标记、低位代表的索引，以及在对应的 Data Block 中定位对应字的位置偏移量。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e34483e4164caad8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而内存地址对应到 Cache 里的数据结构，则多了一个有效位和对应的数据，由“索引 + 有效位 + 组标记 + 数据”组成。如果内存中的数据已经在 CPU Cache 里了，那一个内存地址的访问，就会经历这样 4 个步骤：</p><ol><li>根据内存地址的低位，计算在 Cache 中的索引；</li><li>判断有效位，确认 Cache 中的数据是有效的；</li><li>对比内存访问地址的高位，和 Cache 中的组标记，确认 Cache 中的数据就是我们要访问的内存数据，从 Cache Line 中读取到对应的数据块（Data Block）</li><li>根据内存地址的 Offset 位，从 Data Block 中，读取希望读取到的字。</li></ol><h3 id="CPU-读取数据过程"><a href="#CPU-读取数据过程" class="headerlink" title="CPU 读取数据过程"></a>CPU 读取数据过程</h3><p>TLB 和我们前面讲的 CPU 的高速缓存类似，可以分成指令的 TLB 和数据的 TLB，也就是 ITLB 和 DTLB。同样的，我们也可以根据大小对它进行分级，变成 L1、L2 这样多层的 TLB。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-29f955fbc93f8c4a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="安全性与内存保护"><a href="#安全性与内存保护" class="headerlink" title="安全性与内存保护"></a>安全性与内存保护</h3><p>进程的程序也好，数据也好，都要存放在内存里面。实际程序指令的执行，也是通过程序计数器里面的地址，去读取内存内的内容，然后运行对应的指令，使用相应的数据。</p><p>虽然我们现代的操作系统和 CPU，已经做了各种权限的管控。正常情况下，我们已经通过虚拟内存地址和物理内存地址的区分，隔离了各个进程。但是，无论是 CPU 这样的硬件，还是操作系统这样的软件，都太复杂了，难免还是会被黑客们找到各种各样的漏洞。</p><p>就像我们在软件开发过程中，常常会有一个“兜底”的错误处理方案一样，在对于内存的管理里面，计算机也有一些最底层的安全保护机制。这些机制统称为<strong>内存保护（Memory Protection）</strong>。我这里就为你简单介绍两个。</p><p><strong>可执行空间保护</strong></p><p>第一个常见的安全机制，叫可执行空间保护（Executable Space Protection）。</p><p>这个机制是说，我们对于一个进程使用的内存，只把其中的指令部分设置成“可执行”的，对于其他部分，比如数据部分，不给予“可执行”的权限。因为无论是指令，还是数据，在我们的 CPU 看来，都是二进制的数据。我们直接把数据部分拿给 CPU，如果这些数据解码后，也能变成一条合理的指令，其实就是可执行的。</p><p>这个时候，黑客们想到了一些搞破坏的办法。我们在程序的数据区里，放入一些要执行的指令编码后的数据，然后找到一个办法，让 CPU 去把它们当成指令去加载，那 CPU 就能执行我们想要执行的指令了。对于进程里内存空间的执行权限进行控制，可以使得 CPU 只能执行指令区域的代码。对于数据区域的内容，即使找到了其他漏洞想要加载成指令来执行，也会因为没有权限而被阻挡掉。</p><p>这个时候，黑客们想到了一些搞破坏的办法。我们在程序的数据区里，放入一些要执行的指令编码后的数据，然后找到一个办法，让 CPU 去把它们当成指令去加载，那 CPU 就能执行我们想要执行的指令了。对于进程里内存空间的执行权限进行控制，可以使得 CPU 只能执行指令区域的代码。对于数据区域的内容，即使找到了其他漏洞想要加载成指令来执行，也会因为没有权限而被阻挡掉。</p><p><strong>地址空间布局随机化</strong></p><p>为了防止入侵者通过缓冲区溢出进行攻击，linux系统实现了栈随机化技术。</p><p>第二个常见的安全机制，叫地址空间布局随机化（Address Space Layout Randomization）。</p><p>内存层面的安全保护核心策略，是在可能有漏洞的情况下进行安全预防。上面的可执行空间保护就是一个很好的例子。但是，内存层面的漏洞还有其他的可能性。</p><p>这里的核心问题是，其他的人、进程、程序，会去修改掉特定进程的指令、数据，然后，让当前进程去执行这些指令和数据，造成破坏。要想修改这些指令和数据，我们需要知道这些指令和数据所在的位置才行。</p><p>原先我们一个进程的内存布局空间是固定的，所以任何第三方很容易就能知道指令在哪里，程序栈在哪里，数据在哪里，堆又在哪里。这个其实为想要搞破坏的人创造了很大的便利。而地址空间布局随机化这个机制，就是让这些区域的位置不再固定，在内存空间随机去分配这些进程里不同部分所在的内存空间地址，让破坏者猜不出来。猜不出来呢，自然就没法找到想要修改的内容的位置。如果只是随便做点修改，程序只会 crash 掉，而不会去执行计划之外的代码。</p><p>关闭Linux 内存地址随机化机制, 禁用进程地址空间随机化.可以将进程的mmap的基址，stack和vdso页面地址固定下来. 可以通过设置kernel.randomize_va_space内核参数来设置内存地址随机化的行为.<br>目前randomize_va_space的值有三种，分别是[0,1,2]</p><ul><li>0 - 表示关闭进程地址空间随机化。</li><li>1 - 表示将mmap的基址，stack和vdso页面随机化。</li><li>2 - 表示在1的基础上增加栈（heap）的随机化。</li></ul><h3 id="总线"><a href="#总线" class="headerlink" title="总线"></a>总线</h3><p>首先，CPU 和内存以及高速缓存通信的总线，这里面通常有两种总线。这种方式，我们称之为双独立总线（Dual Independent Bus，缩写为 DIB）。CPU 里，有一个快速的本地总线（Local Bus），以及一个速度相对较慢的前端总线（Front-side Bus）。</p><p>我们在前面几讲刚刚讲过，现代的 CPU 里，通常有专门的高速缓存芯片。这里的高速本地总线，就是用来和高速缓存通信的。而前端总线，则是用来和主内存以及输入输出设备通信的。有时候，我们会把本地总线也叫作后端总线（Back-side Bus），和前面的前端总线对应起来。而前端总线也有很多其他名字，比如处理器总线（Processor Bus）、内存总线（Memory Bus）。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4e41a906fba428db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>CPU 里面的北桥芯片，把我们上面说的前端总线，一分为二，变成了三个总线。</p><p>我们的前端总线，其实就是系统总线。CPU 里面的内存接口，直接和系统总线通信，然后系统总线再接入一个 I/O 桥接器（I/O Bridge）。这个 I/O 桥接器，一边接入了我们的内存总线，使得我们的 CPU 和内存通信；另一边呢，又接入了一个 I/O 总线，用来连接 I/O 设备。</p><p>事实上，真实的计算机里，这个总线层面拆分得更细。根据不同的设备，还会分成独立的 PCI 总线、ISA 总线等等。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9546f0f629c99ea5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在物理层面，其实我们完全可以把总线看作一组“电线”。不过呢，这些电线之间也是有分工的，我们通常有三类线路。</p><ol><li>数据线（Data Bus），用来传输实际的数据信息，也就是实际上了公交车的“人”。</li><li>地址线（Address Bus），用来确定到底把数据传输到哪里去，是内存的某个位置，还是某一个 I/O 设备。这个其实就相当于拿了个纸条，写下了上面的人要下车的站点。</li><li>控制线（Control Bus），用来控制对于总线的访问。虽然我们把总线比喻成了一辆公交车。那么有人想要做公交车的时候，需要告诉公交车司机，这个就是我们的控制信号。</li></ol><p>尽管总线减少了设备之间的耦合，也降低了系统设计的复杂度，但同时也带来了一个新问题，那就是总线不能同时给多个设备提供通信功能。</p><p>我们的总线是很多个设备公用的，那多个设备都想要用总线，我们就需要有一个机制，去决定这种情况下，到底把总线给哪一个设备用。这个机制，就叫作总线裁决（Bus Arbitraction）。总线裁决的机制有很多种不同的实现，如果你对这个实现的细节感兴趣，可以去看一看 Wiki 里面关于裁决器的对应条目，这里我们就不多说了。</p><h3 id="CPU-是如何控制-I-O-设备的？"><a href="#CPU-是如何控制-I-O-设备的？" class="headerlink" title="CPU 是如何控制 I/O 设备的？"></a>CPU 是如何控制 I/O 设备的？</h3><p>无论是内置在主板上的接口，还是集成在设备上的接口，除了三类寄存器之外，还有对应的控制电路。正是通过这个控制电路，CPU 才能通过向这个接口电路板传输信号，来控制实际的硬件。</p><p>我们先来看一看，硬件设备上的这些寄存器有什么用。这里，我拿我们平时用的打印机作为例子。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-caae569d0149c779.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ol><li>首先是数据寄存器（Data Register）。CPU 向 I/O 设备写入需要传输的数据，比如要打印的内容是“GeekTime”，我们就要先发送一个“G”给到对应的 I/O 设备。</li><li>然后是命令寄存器（Command Register）。CPU 发送一个命令，告诉打印机，要进行打印工作。这个时候，打印机里面的控制电路会做两个动作。第一个，是去设置我们的状态寄存器里面的状态，把状态设置成 not-ready。第二个，就是实际操作打印机进行打印。</li><li>而状态寄存器（Status Register），就是告诉了我们的 CPU，现在设备已经在工作了，所以这个时候，CPU 你再发送数据或者命令过来，都是没有用的。直到前面的动作已经完成，状态寄存器重新变成了 ready 状态，我们的 CPU 才能发送下一个字符和命令。</li></ol><p>当然，在实际情况中，打印机里通常不只有数据寄存器，还会有数据缓冲区。我们的 CPU 也不是真的一个字符一个字符这样交给打印机去打印的，而是一次性把整个文档传输到打印机的内存或者数据缓冲区里面一起打印的。不过，通过上面这个例子，相信你对 CPU 是怎么操作 I/O 设备的，应该有所了解了。</p><p><strong>信号和地址：发挥总线的价值</strong></p><p>搞清楚了实际的 I/O 设备和接口之间的关系，一个新的问题就来了。那就是，我们的 CPU 到底要往总线上发送一个什么样的命令，才能和 I/O 接口上的设备通信呢？</p><p>CPU 和 I/O 设备的通信，一样是通过 CPU 支持的机器指令来执行的。</p><p>如果你回头去看一看第 5 讲，MIPS 的机器指令的分类，你会发现，我们并没有一种专门的和 I/O 设备通信的指令类型。那么，MIPS 的 CPU 到底是通过什么样的指令来和 I/O 设备来通信呢？</p><p>答案就是，和访问我们的主内存一样，使用“内存地址”。为了让已经足够复杂的 CPU 尽可能简单，计算机会把 I/O 设备的各个寄存器，以及 I/O 设备内部的内存地址，都映射到主内存地址空间里来。主内存的地址空间里，会给不同的 I/O 设备预留一段一段的内存地址。CPU 想要和这些 I/O 设备通信的时候呢，就往这些地址发送数据。这些地址信息，就是通过上一讲的地址线来发送的，而对应的数据信息呢，自然就是通过数据线来发送的了。</p><p>而我们的 I/O 设备呢，就会监控地址线，并且在 CPU 往自己地址发送数据的时候，把对应的数据线里面传输过来的数据，接入到对应的设备里面的寄存器和内存里面来。CPU 无论是向 I/O 设备发送命令、查询状态还是传输数据，都可以通过这样的方式。这种方式呢，叫作内存映射IO（Memory-Mapped I/O，简称 MMIO）。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-99024a65ad597cbf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>那么，MMIO 是不是唯一一种 CPU 和设备通信的方式呢？答案是否定的。精简指令集 MIPS 的 CPU 特别简单，所以这里只有 MMIO。而我们有 2000 多个指令的 Intel X86 架构的计算机，自然可以设计专门的和 I/O 设备通信的指令，也就是 in 和 out 指令。</p><p>Intel CPU 虽然也支持 MMIO，不过它还可以通过特定的指令，来支持端口映射 I/O（Port-Mapped I/O，简称 PMIO）或者也可以叫独立输入输出（Isolated I/O）。</p><p>其实 PMIO 的通信方式和 MMIO 差不多，核心的区别在于，PMIO 里面访问的设备地址，不再是在内存地址空间里面，而是一个专门的端口（Port）。这个端口并不是指一个硬件上的插口，而是和 CPU 通信的一个抽象概念。</p><p>无论是 PMIO 还是 MMIO，CPU 都会传送一条二进制的数据，给到 I/O 设备的对应地址。设备自己本身的接口电路，再去解码这个数据。解码之后的数据呢，就会变成设备支持的一条指令，再去通过控制电路去操作实际的硬件设备。对于 CPU 来说，它并不需要关心设备本身能够支持哪些操作。它要做的，只是在总线上传输一条条数据就好了。</p><p>这个，其实也有点像我们在设计模式里面的 Command 模式。我们在总线上传输的，是一个个数据对象，然后各个接受这些对象的设备，再去根据对象内容，进行实际的解码和命令执行。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-934db1cff5a21552.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这是一张我自己的显卡，在设备管理器里面的资源（Resource）信息。你可以看到，里面既有 Memory Range，这个就是设备对应映射到的内存地址，也就是我们上面所说的 MMIO 的访问方式。同样的，里面还有 I/O Range，这个就是我们上面所说的 PMIO，也就是通过端口来访问 I/O 设备的地址。最后，里面还有一个 IRQ，也就是会来自于这个设备的中断信号了。</p><h3 id="理解-DMA，一个协处理器"><a href="#理解-DMA，一个协处理器" class="headerlink" title="理解 DMA，一个协处理器"></a>理解 DMA，一个协处理器</h3><p>其实 DMA 技术很容易理解，本质上，DMA 技术就是我们在主板上放一块独立的芯片。在进行内存和 I/O 设备的数据传输的时候，我们不再通过 CPU 来控制数据传输，而直接通过 DMA 控制器（DMA Controller，简称 DMAC）。这块芯片，我们可以认为它其实就是一个协处理器（Co-Processor）。</p><p>DMAC 最有价值的地方体现在，当我们要传输的数据特别大、速度特别快，或者传输的数据特别小、速度特别慢的时候。</p><p>比如说，我们用千兆网卡或者硬盘传输大量数据的时候，如果都用 CPU 来搬运的话，肯定忙不过来，所以可以选择 DMAC。而当数据传输很慢的时候，DMAC 可以等数据到齐了，再发送信号，给到 CPU 去处理，而不是让 CPU 在那里忙等待。</p><p>好了，现在你应该明白 DMAC 的价值，知道了它适合用在什么情况下。那我们现在回过头来看。我们上面说，DMAC 是一块“协处理器芯片”，这是为什么呢？</p><p>注意，这里面的“协”字。DMAC 是在“协助”CPU，完成对应的数据传输工作。在 DMAC 控制数据传输的过程中，我们还是需要 CPU 的。</p><p>除此之外，DMAC 其实也是一个特殊的 I/O 设备，它和 CPU 以及其他 I/O 设备一样，通过连接到总线来进行实际的数据传输。总线上的设备呢，其实有两种类型。一种我们称之为主设备（Master），另外一种，我们称之为从设备（Slave）。</p><p>想要主动发起数据传输，必须要是一个主设备才可以，CPU 就是主设备。而我们从设备（比如硬盘）只能接受数据传输。所以，如果通过 CPU 来传输数据，要么是 CPU 从 I/O 设备读数据，要么是 CPU 向 I/O 设备写数据。</p><p>这个时候你可能要问了，那我们的 I/O 设备不能向主设备发起请求么？可以是可以，不过这个发送的不是数据内容，而是控制信号。I/O 设备可以告诉 CPU，我这里有数据要传输给你，但是实际数据是 CPU 拉走的，而不是 I/O 设备推给 CPU 的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8dee3b5c8cfa6f8e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>不过，DMAC 就很有意思了，它既是一个主设备，又是一个从设备。对于 CPU 来说，它是一个从设备；对于硬盘这样的 IO 设备来说呢，它又变成了一个主设备。那使用 DMAC 进行数据传输的过程究竟是什么样的呢？下面我们来具体看看。</p><ol><li>首先，CPU 还是作为一个主设备，向 DMAC 设备发起请求。这个请求，其实就是在 DMAC 里面修改配置寄存器。</li><li>CPU 修改 DMAC 的配置的时候，会告诉 DMAC 这样几个信息：<ul><li>首先是源地址的初始值以及传输时候的地址增减方式。所谓源地址，就是数据要从哪里传输过来。如果我们要从内存里面写入数据到硬盘上，那么就是要读取的数据在内存里面的地址。如果是从硬盘读取数据到内存里，那就是硬盘的 I/O 接口的地址。</li><li>我们讲过总线的时候说过，I/O 的地址可以是一个内存地址，也可以是一个端口地址。而地址的增减方式就是说，数据是从大的地址向小的地址传输，还是从小的地址往大的地址传输。</li><li>其次是目标地址初始值和传输时候的地址增减方式。目标地址自然就是和源地址对应的设备，也就是我们数据传输的目的地。</li><li>第三个自然是要传输的数据长度，也就是我们一共要传输多少数据。</li></ul></li><li>设置完这些信息之后，DMAC 就会变成一个空闲的状态（Idle）。</li><li>如果我们要从硬盘上往内存里面加载数据，这个时候，硬盘就会向 DMAC 发起一个数据传输请求。这个请求并不是通过总线，而是通过一个额外的连线。</li><li>然后，我们的 DMAC 需要再通过一个额外的连线响应这个申请。</li><li>于是，DMAC 这个芯片，就向硬盘的接口发起要总线读的传输请求。数据就从硬盘里面，读到了 DMAC 的控制器里面。</li><li>然后，DMAC 再向我们的内存发起总线写的数据传输请求，把数据写入到内存里面。</li><li>DMAC 会反复进行上面第 6、7 步的操作，直到 DMAC 的寄存器里面设置的数据长度传输完成。</li><li>数据传输完成之后，DMAC 重新回到第 3 步的空闲状态。</li></ol><p>所以，整个数据传输的过程中，我们不是通过 CPU 来搬运数据，而是由 DMAC 这个芯片来搬运数据。但是 CPU 在这个过程中也是必不可少的。因为传输什么数据，从哪里传输到哪里，其实还是由 CPU 来设置的。这也是为什么，DMAC 被叫作“协处理器”。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5260d3f08a44d73e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>最早，计算机里是没有 DMAC 的，所有数据都是由 CPU 来搬运的。随着人们对于数据传输的需求越来越多，先是出现了主板上独立的 DMAC 控制器。到了今天，各种 I/O 设备越来越多，数据传输的需求越来越复杂，使用的场景各不相同。加之显示器、网卡、硬盘对于数据传输的需求都不一样，所以各个设备里面都有自己的 DMAC 芯片了。</p><p><strong>kafka - sendfile</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-05c72910fc665a2e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-dd4fbd4a7de3e0b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="应用篇"><a href="#应用篇" class="headerlink" title="应用篇"></a>应用篇</h2><h3 id="KV-vs-MQ-VS-数据仓库"><a href="#KV-vs-MQ-VS-数据仓库" class="headerlink" title="KV vs MQ VS 数据仓库"></a>KV vs MQ VS 数据仓库</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3f235a830cbe6dad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="Cassandra：顺序写和随机读"><a href="#Cassandra：顺序写和随机读" class="headerlink" title="Cassandra：顺序写和随机读"></a>Cassandra：顺序写和随机读</h3><p>作为一个分布式的 KV 数据库，Cassandra 的键一般被称为 Row Key。其实就是一个 16 到 36 个字节的字符串。每一个 Row Key 对应的值其实是一个哈希表，里面可以用键值对，再存入很多你需要的数据。</p><p>Cassandra 本身不像关系型数据库那样，有严格的 Schema，在数据库创建的一开始就定义好了有哪些列（Column）。但是，它设计了一个叫作列族（Column Family）的概念，我们需要把经常放在一起使用的字段，放在同一个列族里面。比如，DMP 里面的人口属性信息，我们可以把它当成是一个列族。用户的兴趣信息，可以是另外一个列族。这样，既保持了不需要严格的 Schema 这样的灵活性，也保留了可以把常常一起使用的数据存放在一起的空间局部性。</p><p>往 Cassandra 的里面读写数据，其实特别简单，就好像是在一个巨大的分布式的哈希表里面写数据。我们指定一个 Row Key，然后插入或者更新这个 Row Key 的数据就好了。</p><p><strong>Cassandra 的写操作</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-177eecf560fcdc84.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Cassandra 解决随机写入数据的解决方案，简单来说，就叫作“不随机写，只顺序写”。对于 Cassandra 数据库的写操作，通常包含两个动作。第一个是往磁盘上写入一条提交日志（Commit Log）。另一个操作，则是直接在内存的数据结构上去更新数据。后面这个往内存的数据结构里面的数据更新，只有在提交日志写成功之后才会进行。每台机器上，都有一个可靠的硬盘可以让我们去写入提交日志。写入提交日志都是顺序写（Sequential Write），而不是随机写（Random Write），这使得我们最大化了写入的吞吐量。</p><p>内存的空间比较有限，一旦内存里面的数据量或者条目超过一定的限额，Cassandra 就会把内存里面的数据结构 dump 到硬盘上。这个 Dump 的操作，也是顺序写而不是随机写，所以性能也不会是一个问题。除了 Dump 的数据结构文件，Cassandra 还会根据 row key 来生成一个索引文件，方便后续基于索引来进行快速查询。</p><p>随着硬盘上的 Dump 出来的文件越来越多，Cassandra 会在后台进行文件的对比合并。在很多别的 KV 数据库系统里面，也有类似这种的合并动作，比如 AeroSpike 或者 Google 的 BigTable。这些操作我们一般称之为 Compaction。合并动作同样是顺序读取多个文件，在内存里面合并完成，再 Dump 出来一个新的文件。整个操作过程中，在硬盘层面仍然是顺序读写。</p><p><strong>Cassandra 的读操作</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c7e821f19c1c743c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>当我们要从 Cassandra 读数据的时候，会从内存里面找数据，再从硬盘读数据，然后把两部分的数据合并成最终结果。这些硬盘上的文件，在内存里面会有对应的 Cache，只有在 Cache 里面找不到，我们才会去请求硬盘里面的数据。</p><p>如果不得不访问硬盘，因为硬盘里面可能 Dump 了很多个不同时间点的内存数据的快照。所以，找数据的时候，我们也是按照时间从新的往旧的里面找。</p><p>这也就带来另外一个问题，我们可能要查询很多个 Dump 文件，才能找到我们想要的数据。所以，Cassandra 在这一点上又做了一个优化。那就是，它会为每一个 Dump 的文件里面所有 Row Key 生成一个 BloomFilter，然后把这个 BloomFilter 放在内存里面。这样，如果想要查询的 Row Key 在数据文件里面不存在，那么 99% 以上的情况下，它会被 BloomFilter 过滤掉，而不需要访问硬盘。</p><p>这样，只有当数据在内存里面没有，并且在硬盘的某个特定文件上的时候，才会触发一次对于硬盘的读请求。</p>]]></content>
      
      
      <categories>
          
          <category> System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Golang Memory Model</title>
      <link href="/2020/06/09/golang-memory-model/"/>
      <url>/2020/06/09/golang-memory-model/</url>
      
        <content type="html"><![CDATA[<img alt="cover" src="https://upload-images.jianshu.io/upload_images/12321605-0feac2eb12658cfb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><h3 id="1-1-一个-Code-Review-引发的思考"><a href="#1-1-一个-Code-Review-引发的思考" class="headerlink" title="1.1 一个 Code Review 引发的思考"></a>1.1 一个 Code Review 引发的思考</h3><p>一个同学在 <code>Golang</code> 项目里面用 <code>Double Check</code>（不清楚的同学可以去百度搜下，<code>Java</code>中比较常见）的方式实现了一个单例。具体实现如下：</p><pre><code>var (    lock     sync.Mutex    instance *UserInfo)func getInstance() (*UserInfo, error) {    if instance == nil {        //---Lock        lock.Lock()        defer lock.Unlock()        if instance == nil {            instance = &amp;UserInfo{                Name: "fan",            }        }    }//---Unlock()    return instance, nil}</code></pre><p>这个代码第一眼看上去好像是标准的<code>Double Check</code>的写法，确没有什么问题，但是大佬<code>Review</code>代码的时候，指出这里会发生<code>Data Race</code>。我用 <code>go run -race go_race2.go</code> 检查的确有<code>Data Race</code>的警告：</p><pre><code>==================WARNING: DATA RACERead at 0x00000120d9c0 by goroutine 8:  main.getInstance()      /Users/fl/go/src/github.com/fl/GolangDemo/GoTest/go_race2.go:42 +0x4f  main.main.func1()      /Users/fl/go/src/github.com/fl/GolangDemo/GoTest/go_race2.go:24 +0x44Previous write at 0x00000120d9c0 by goroutine 7:  main.getInstance()      /Users/fl/go/src/github.com/fl/GolangDemo/GoTest/go_race2.go:49 +0x169  main.main.func1()      /Users/fl/go/src/github.com/fl/GolangDemo/GoTest/go_race2.go:24 +0x44Goroutine 8 (running) created at:  main.main()      /Users/fl/go/src/github.com/fl/GolangDemo/GoTest/go_race2.go:23 +0xabGoroutine 7 (finished) created at:  main.main()      /Users/fl/go/src/github.com/fl/GolangDemo/GoTest/go_race2.go:23 +0xab==================</code></pre><p>警告中指明在多线程执行<code>getInstance</code>这个方法的时候，在<code>if instance == nil {</code> 这一行会发生<code>data race</code>。这个为什么在这一行会有<code>data race</code>，我们后面再说。</p><p>其实搞Java的同学很快可以看出这段代码的问题，Java中的<code>Double Check </code>来实现单例模式的时候都会使用<code>volatile</code>来修饰变量，<code>volatile</code>主要做的事就是<code>保证线程可见性</code>和<code>禁止指令重排序</code>。其实这两个主要就是<code>Memory Model</code>描述的事情。</p><h3 id="1-2-什么是-Memory-Model"><a href="#1-2-什么是-Memory-Model" class="headerlink" title="1.2 什么是 Memory Model"></a>1.2 什么是 Memory Model</h3><p><code>Java</code>和<code>C++</code>的同学可能会熟悉一些，其他很多同学第一次听到这个单词的时候，潜意识翻译成中文就是<code>内存模型</code>，好像是讲的一个内存数据结构相关的东西。某度上搜索<code>Memory Model</code>出来更多是不相干东西。我们看下<a href="https://en.wikipedia.org/wiki/Memory_model_(programming)#:~:text=In%20computing%2C%20a%20memory%20model,shared%20use%20of%20the%20data.">wikipedia</a>上的解释：</p><blockquote><p>In computing, a memory model describes the interactions of threads through memory and their shared use of the data.</p></blockquote><p>直接翻译过来就是：<code>在计算中，内存模型描述了多线程如何通过内存的交互来共享数据</code></p><p>看下<code>Golang</code>官方文档的解释：<a href="https://golang.org/ref/mem">The Go Memory Model</a></p><blockquote><p>The Go memory model specifies the conditions under which reads of a variable in one goroutine can be guaranteed to observe values produced by writes to the same variable in a different goroutine.</p></blockquote><p>翻译下就是：<code>Go内存模型指定了某些条件，在这种条件下，可以保证一个共享变量，在一个goroutine（线程）中写入，可以在另外一个线程被观察到。</code></p><p><strong><code>Memory Model</code> 其实是一个概念，表示在多线程场景下，如何保证数据同步的正确性。</strong> 为什么多线程读取共享内存变量的时候会有<code>数据同步正确性</code>问题呢，这里主要涉及到**<code>CPU缓存一致性问题</code><strong>和</strong><code>CPU乱序执行的问题</code>**，</p><p>各个语言对<code>Memory Model</code>实现方式各不相同，对其他语言感兴趣的同学可以去搜索下相关资料。</p><p><a href="https://en.wikipedia.org/wiki/Java_memory_model">Java Memory Model</a> </p><p><a href="https://en.cppreference.com/w/cpp/language/memory_model#:~:text=Defines%20the%20semantics%20of%20computer,memory%20has%20a%20unique%20address.">C++ Memory Model</a> </p><p><a href="https://book.douban.com/subject/27036085/">《C++ Concurrency in Action》第五章</a> （大佬推荐的书）</p><h2 id="二、CPU的高速缓存和流水线架构"><a href="#二、CPU的高速缓存和流水线架构" class="headerlink" title="二、CPU的高速缓存和流水线架构"></a>二、CPU的高速缓存和流水线架构</h2><h3 id="2-1-CPU-缓存一致性"><a href="#2-1-CPU-缓存一致性" class="headerlink" title="2.1 CPU 缓存一致性"></a>2.1 CPU 缓存一致性</h3><h4 id="2-1-1-线程可见性问题"><a href="#2-1-1-线程可见性问题" class="headerlink" title="2.1.1 线程可见性问题"></a>2.1.1 线程可见性问题</h4><p>我们先看一下这个线程可见性的测试<code>DEMO</code>（这里其实不是线程可见性的问题。是编译器优化的问题。更多可以看<a href="https://fanlv.wiki/2021/12/18/golang-complier-optimize/">Golang编译器优化哪些事</a> 这边文章）</p><pre><code>func main() {    running := true    go func() {        println("start thread1")        count := 1        for running {            count++        }        println("end thread1: count =", count) // 这句代码永远执行不到为什么？    }()    go func() {        println("start thread2")        for {            running = false        }    }()    time.Sleep(time.Hour)}</code></pre><p>为什么在一个线程修改了共享变量，另外一个线程感知不到呢？这里我们需要了解下CPU的Cache结构。</p><p><img src="/2020/06/09/golang-memory-model/cpu_cache.png"></p><p>现代多核<code>CPU</code>的<code>Cache </code>模型基本都跟上图所示一样，<code>L1 L2 cache</code>是每个核独占的，只有<code>L3</code>是共享的，当多个<code>CPU</code>读、写同一个变量时，就需要在多个<code>CPU</code>的<code>Cache</code>之间同步数据，跟分布式系统一样，必然涉及到一致性的问题。</p><h4 id="2-1-2-CPU-缓存一致性协议"><a href="#2-1-2-CPU-缓存一致性协议" class="headerlink" title="2.1.2 CPU 缓存一致性协议"></a>2.1.2 CPU 缓存一致性协议</h4><p>在<code>MESI</code>（还有<code>MSI</code>、<code>MESIF</code>等等）协议中，每个<code>Cache line</code>（<code>Cache Line</code>的概念后面会补充介绍）有<code>4</code>个状态，可用<code>2个bit</code>表示，它们分别是： </p><ul><li>失效（<code>Invalid</code>）缓存段，要么已经不在缓存中，要么它的内容已经过时。为了达到缓存的目的，这种状态的段将会被忽略。一旦缓存段被标记为失效，那效果就等同于它从来没被加载到缓存中。</li><li>共享（<code>Shared</code>）缓存段，它是和主内存内容保持一致的一份拷贝，在这种状态下的缓存段只能被读取，不能被写入。多组缓存可以同时拥有针对同一内存地址的共享缓存段，这就是名称的由来。</li><li>独占（<code>Exclusive</code>）缓存段，和 <code>S</code> 状态一样，也是和主内存内容保持一致的一份拷贝。区别在于，如果一个处理器持有了某个 <code>E</code> 状态的缓存段，那其他处理器就不能同时持有它，所以叫“独占”。这意味着，如果其他处理器原本也持有同一缓存段，那么它会马上变成“失效”状态。</li><li>已修改（<code>Modified</code>）缓存段，属于脏段，它们已经被所属的处理器修改了。如果一个段处于已修改状态，那么它在其他处理器缓存中的拷贝马上会变成失效状态，这个规律和 <code>E</code> 状态一样。此外，已修改缓存段如果被丢弃或标记为失效，那么先要把它的内容回写到内存中——这和回写模式下常规的脏段处理方式一样。</li></ul><p>只有<code>Core 0</code>访问变量x，它的<code>Cache line</code>状态为<code>E(Exclusive)</code>:</p><p><img src="/2020/06/09/golang-memory-model/mesi1.png"></p><p><code>3</code>个<code>Core</code>都访问变量<code>x</code>，它们对应的<code>Cache line</code>为<code>S(Shared)</code>状态:</p><p><img src="/2020/06/09/golang-memory-model/mesi2.png"></p><p><code>Core 0</code>修改了<code>x</code>的值之后，这个<code>Cache line</code>变成了<code>M(Modified)</code>状态，其他<code>Core</code>对应的<code>Cache line</code>变成了<code>I(Invalid)</code>状态 :</p><p><img src="/2020/06/09/golang-memory-model/mesi3.png"></p><p>在<code>MESI</code>协议中，每个<code>Cache</code>的<code>Cache</code>控制器不仅知道自己的读写操作，而且也监听(<code>snoop</code>)其它<code>Cache</code>的读写操作。每个<code>Cache line</code>所处的状态根据本核和其它核的读写操作在<code>4</code>个状态间进行迁移。</p><p>更多可以看 <a href="https://blog.csdn.net/muxiqingyang/article/details/6615199">《大话处理器》Cache一致性协议之MESI</a> 这篇文章介绍。</p><h4 id="2-1-3-为什么有-MESI-协议还会有缓存一致性问题"><a href="#2-1-3-为什么有-MESI-协议还会有缓存一致性问题" class="headerlink" title="2.1.3 为什么有 MESI 协议还会有缓存一致性问题"></a>2.1.3 为什么有 MESI 协议还会有缓存一致性问题</h4><p>由上面的<code>MESI</code>协议，我们可以知道如果满足下面两个条件，你就可以得到完全的顺序一致性：</p><ol><li>缓存一收到总线事件，就可以在当前指令周期中迅速做出响应.</li><li>处理器如实地按程序的顺序，把内存操作指令送到缓存，并且等前一条执行完后才能发送下一条。</li></ol><p>当然，实际上现代处理器一般都无法满足以上条件，主要原因有：</p><ul><li>缓存不会及时响应总线事件。如果总线上发来一条消息，要使某个缓存段失效，但是如果此时缓存正在处理其他事情（比如和 <code>CPU</code> 传输数据），那这个消息可能无法在当前的指令周期中得到处理，而会进入所谓的“失效队列（<code>invalidation queue</code>）”，这个消息等在队列中直到缓存有空为止。</li><li>处理器一般不会严格按照程序的顺序向缓存发送内存操作指令。当然，有乱序执行（<code>Out-of-Order execution</code>）功能的处理器肯定是这样的。顺序执行（<code>in-order execution</code>）的处理器有时候也无法完全保证内存操作的顺序（比如想要的内存不在缓存中时，<code>CPU</code> 就不能为了载入缓存而停止工作）。</li><li>写操作尤其特殊，因为它分为两阶段操作：在写之前我们先要得到缓存段的独占权。如果我们当前没有独占权，我们先要和其他处理器协商，这也需要一些时间。同理，在这种场景下让处理器闲着无所事事是一种资源浪费。实际上，写操作首先发起获得独占权的请求，然后就进入所谓的由“写缓冲（<code>store buffer</code>）”组成的队列（有些地方使用“写缓冲”指代整个队列，我这里使用它指代队列的一条入口）。写操作在队列中等待，直到缓存准备好处理它，此时写缓冲就被“清空（<code>drained</code>）”了，缓冲区被回收用于处理新的写操作。<br>这些特性意味着，默认情况下，读操作有可能会读到过时的数据（如果对应失效请求还等在队列中没执行），写操作真正完成的时间有可能比它们在代码中的位置晚，一旦牵涉到乱序执行，一切都变得模棱两可。</li></ul><p>可以看 <a href="https://www.infoq.cn/article/cache-coherency-primer">缓存一致性（Cache Coherency）入门</a> 这篇文章了解更多。</p><h3 id="2-2-CPU-指令乱序执行"><a href="#2-2-CPU-指令乱序执行" class="headerlink" title="2.2 CPU 指令乱序执行"></a>2.2 CPU 指令乱序执行</h3><h4 id="2-2-1-CPU-指令乱序执行验证-Demo"><a href="#2-2-1-CPU-指令乱序执行验证-Demo" class="headerlink" title="2.2.1 CPU 指令乱序执行验证 Demo"></a>2.2.1 CPU 指令乱序执行验证 Demo</h4><pre><code>func main() {    count := 0    for {        x, y, a, b := 0, 0, 0, 0        count++        var wg sync.WaitGroup        wg.Add(2)        go func() {            a = 1            x = b            println("thread1 done ", count)            wg.Done()        }()        go func() {            b = 1            y = a            println("thread2 done ", count)            wg.Done()        }()        wg.Wait()        if x == 0 &amp;&amp; y == 0 {            println("执行次数 ：", count)            break        }    }}...thread2 done  11061thread1 done  11061执行次数 ： 11061 // 执行了11061次以后出现了 x=0、y=0的情况</code></pre><p>上面<code>demo</code>中我在线程1和线程2中，先分别让<code>a = 1</code>、<code>b = 1</code>，再让<code>x = b</code>、<code>y = a</code>，如果CPU是按顺序执行这些指令的话，无论线程一和线程二中的如何而组合先后执行，永远也不会得到 <code>x = 0</code>、 <code>y = 0</code>的情况。CPU 为什么会发生乱序呢？我们先了解下<code>CPU 指令流水线</code></p><h4 id="2-2-2-为什么CPU会乱序执行"><a href="#2-2-2-为什么CPU会乱序执行" class="headerlink" title="2.2.2 为什么CPU会乱序执行"></a>2.2.2 为什么CPU会乱序执行</h4><p>我们知道对于CPU性能有以下公式</p><p><code>CPU</code>性能＝<code>IPC</code>(CPU每一时钟周期内所执行的指令多少)×频率(MHz时钟速度)</p><p>由上述公式我们可以知道，提高<code>CPU</code>性能要么就提高主频，要么就提高<code>IPC</code>(每周期执行的指令数).提升<code>IPC</code>有两种做法，一个是增加单核并行的度，一个是加多几个核。<strong>单核CPU增加并行度的主要方式是采用流水线设计</strong>。</p><h4 id="2-2-3-什么是指令流水线"><a href="#2-2-3-什么是指令流水线" class="headerlink" title="2.2.3 什么是指令流水线"></a>2.2.3 什么是指令流水线</h4><p>先看 <a href="https://zh.wikipedia.org/wiki/%E6%8C%87%E4%BB%A4%E7%AE%A1%E7%B7%9A%E5%8C%96">wikipedia</a> 解释</p><blockquote><p>指令流水线（英语：Instruction pipeline）是为了让计算机和其它数字电子设备能够加速指令的通过速度（单位时间内被运行的指令数量）而设计的技术。</p></blockquote><blockquote><p>流水线在处理器的内部被组织成层级，各个层级的流水线能半独立地单独运作。每一个层级都被管理并且链接到一条“链”，因而每个层级的输出被送到其它层级直至任务完成。 处理器的这种组织方式能使总体的处理时间显著缩短。</p></blockquote><p><img src="/2020/06/09/golang-memory-model/cpu_pipline.png"></p><p>流水线化则是实现各个工位不间断执行各自的任务，例如同样的四工位设计，指令拾取无需等待下一工位完成就进行下一条指令的拾取，其余工位亦然。</p><p>理想很丰满，现实很骨感，上述图示中的状态只是极为理想中的情况。流水线在运作过程中会遇到以下的问题：</p><ul><li>RISC&nbsp;指令集具备指令编码格式统一、指令都在一周期内完成等特点，在流水线设计设计上有得天独厚的优势。但是非等长不定周期的&nbsp;CISC（例如&nbsp;x86&nbsp;的指令长度为&nbsp;1&nbsp;个字节到&nbsp;17&nbsp;个字节不等）想要达到上图中紧凑高效的流水线形式就比较困难了，在执行的过程中肯定会存在气泡（存在空闲的流水线工位）。</li><li>如果连续指令之间存在依赖关系（如&nbsp;a=1,b=a）那么这两条指令不能使用流水线，必须等&nbsp;a=1执行完毕后才能执行&nbsp;b=a。在这里也产生了很大的一个气泡。</li><li>如果指令存在条件分支，那么CPU不知道要往哪里执行，那么流水线也要停掉，等条件分支的判断结果出来。大气泡~&nbsp;</li></ul><p>为了解决上述的问题，工程师们设计了以下的技术：</p><ul><li><strong>乱序执行</strong></li><li><strong>分支预测</strong></li></ul><h4 id="2-3-4-CPU-乱序执行"><a href="#2-3-4-CPU-乱序执行" class="headerlink" title="2.3.4 CPU 乱序执行"></a>2.3.4 CPU 乱序执行</h4><p>乱序执行就是说把原来有序执行的指令列表，在<strong>保证执行结果一致的情况下根据指令依赖关系及指令执行周期重新安排执行顺序</strong>。例如以下指令（a = 1; b = a; c = 2; d = c）在CPU中就很可能被重排序成为以下的执行顺序（a = 1;c = 2;b = a;d = c;），这样的话，4条指令都可以高效的在流水线中运转了。</p><p><strong>注意：CPU只会没有依赖关系的语句进行乱序执行。并不会对 x = 1 ，y = x 这种语句进行乱序</strong></p><p>虽然乱序执行提高了CPU的执行效率，但是却带来了另外一个问题。就是在多核多线程环境中，若线程A执行（a = 1，x = b）优化成了（x = b, a= 1）的话，线程B执行（b = 1，y = a）被优化成了（y = 1, b = 1）, 所以就得到了（x = 0，y = 0）这种结果</p><h4 id="2-3-5-分支预测"><a href="#2-3-5-分支预测" class="headerlink" title="2.3.5 分支预测"></a>2.3.5 分支预测</h4><p>分支预测(<code>Branch predictor</code>):当处理一个分支指令时,有可能会产生跳转,从而打断流水线指令的处理,因为处理器无法确定该指令的下一条指令,直到分支指令执行完毕。流水线越长,处理器等待时间便越长,分支预测技术就是为了解决这一问题而出现的。因此,分支预测是处理器在程序分支指令执行前预测其结果的一种机制。</p><p>采用分支预测，处理器猜测进入哪个分支，并且基于预测结果来取指、译码。如果猜测正确，就能节省时间，如果猜测错误，大不了从头再来，刷新流水线，在新的地址处取指、译码。</p><p>分支预测有很多方式，<a href="https://zh.wikipedia.org/wiki/%E5%88%86%E6%94%AF%E9%A0%90%E6%B8%AC%E5%99%A8">详见Wikipedia</a></p><h5 id="2-3-5-1-分支预测-Demo"><a href="#2-3-5-1-分支预测-Demo" class="headerlink" title="2.3.5.1 分支预测 Demo"></a>2.3.5.1 分支预测 Demo</h5><pre><code>func main() {    data := make([]int, 32678)    for i := 0; i &lt; len(data); i++ {        data[i] = rand.Intn(256)    }    sort.Sort(sort.IntSlice(data))// Sort和非Sort    now := time.Now()    count := 0    for i := 0; i &lt; len(data); i++ {        if data[i] &gt; 128 {            count += data[i]        }    }    end := time.Since(now)    fmt.Println("time : ", end.Microseconds(), "ms count = ", count)}sort ：time :  112 ms count =  3101495非Sort：time :  290 ms count =  3101495</code></pre><p>简单地分析一下：<br>有序数组的分支预测流程：</p><pre><code>T = 分支命中N = 分支没有命中 data[] = 0, 1, 2, 3, 4, ... 126, 127, 128, 129, 130, ... 250, 251, 252, ...branch = N  N  N  N  N  ...   N    N    T    T    T  ...   T    T    T  ...        = NNNNNNNNNNNN ... NNNNNNNTTTTTTTTT ... TTTTTTTTTT  (非常容易预测)</code></pre><p>无序数组的分支预测流程：</p><pre><code>data[] = 226, 185, 125, 158, 198, 144, 217, 79, 202, 118,  14, 150, 177, 182, 133, ...branch =   T,   T,   N,   T,   T,   T,   T,  N,   T,   N,   N,   T,   T,   T,   N  ...        = TTNTTTTNTNNTTTN ...   (完全随机--无法预测)</code></pre><h4 id="2-3-6-如何解决CPU会乱序执行"><a href="#2-3-6-如何解决CPU会乱序执行" class="headerlink" title="2.3.6 如何解决CPU会乱序执行"></a>2.3.6 如何解决CPU会乱序执行</h4><h5 id="2-3-6-1-内存屏障（Memory-barrier）"><a href="#2-3-6-1-内存屏障（Memory-barrier）" class="headerlink" title="2.3.6.1 内存屏障（Memory barrier）"></a>2.3.6.1 内存屏障（Memory barrier）</h5><p>内存屏障（<code>Memory barrier</code>），也称内存栅栏，内存栅障，屏障指令等，是一类同步屏障指令，它使得 <code>CPU</code> 或编译器在对内存进行操作的时候, 严格按照一定的顺序来执行, 也就是说在<code>memory barrier</code> 之前的指令和<code>memory barrier</code>之后的指令不会由于系统优化等原因而导致乱序。</p><p>内存屏障是底层原语，是内存排序的一部分，在不同体系结构下变化很大而不适合推广。需要认真研读硬件的手册以确定内存屏障的办法。x86指令集中的内存屏障指令是：</p><pre><code>lfence (asm), void _mm_lfence (void) 读操作屏障sfence (asm), void _mm_sfence (void)[1] 写操作屏障mfence (asm), void _mm_mfence (void)[2] 读写操作屏障</code></pre><p>有部分语言的一致性原语是通过上面几个命令来保证的。</p><h2 id="三、-Golang-一致性原语"><a href="#三、-Golang-一致性原语" class="headerlink" title="三、 Golang 一致性原语"></a>三、 Golang 一致性原语</h2><h3 id="3-1-什么是Happens-Before"><a href="#3-1-什么是Happens-Before" class="headerlink" title="3.1 什么是Happens Before"></a>3.1 什么是Happens Before</h3><p><strong>Happens Before 是<code>Memory Model</code>中一个通用的概念</strong>。主要是用来保证内存操作的可见性。如果要保证E1的内存写操作能够被<code>E2</code>读到，那么需要满足：</p><ul><li>E1 Happens Before E2；</li><li>其他所有针对此内存的写操作，要么Happens Before E1，要么Happens After E2。也就是说不能存在其他的一个写操作E3，这个E3 Happens Concurrently E1/E2。</li></ul><p>让我们再回头来看下官方文档 <a href="https://golang.org/ref/mem">The Go Memory Model</a>，里面讲到， golang 中有数个地方实现了 Happens Before 语义，分别是 <code>init函数</code>、<code>goruntine 的创建</code>、<code>goruntine 的销毁</code>、<code>channel 通讯</code>、<code>锁</code>、<code>sync</code>、<code>sync/atomic</code>.</p><h4 id="Init-函数"><a href="#Init-函数" class="headerlink" title="Init 函数"></a>Init 函数</h4><ul><li>如果包<code>P1</code>中导入了包<code>P2</code>，则<code>P2</code>中的<code>init</code>函数<code>Happens Before</code>所有<code>P1</code>中的操作</li><li><code>main</code>函数<code>Happens After</code>所有的init函数</li></ul><h4 id="Goroutine"><a href="#Goroutine" class="headerlink" title="Goroutine"></a>Goroutine</h4><ul><li><code>Goroutine</code> 的创建 <code>Happens Before</code> 所有此 <code>Goroutine</code> 中的操作</li><li><code>Goroutine</code> 的销毁 <code>Happens After</code> 所有此 <code>Goroutine</code> 中的操作</li></ul><h4 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h4><ul><li><code>channel</code>底层实现主要是由<code>ringbuf</code>、<code>sendqueue</code>、<code>recequeue</code>、<code>mutex</code>组成。</li><li>内部实现主要是使用锁来保证一致性，但这把锁并不是标准库里的锁，而是在 runtime 中自己实现的一把更加简单、高效的锁。</li></ul><h4 id="Lock"><a href="#Lock" class="headerlink" title="Lock"></a>Lock</h4><p><code>Go</code>里面有<code>Mutex</code>和<code>RWMutex</code>两种锁，<code>RWMutex</code>是在<code>Mutex</code>基础上实现的。所以这里主要说下<code>Mutex</code>。</p><p><code>Mutex</code>是一个公平锁，有正常模式和饥饿模式两种状态。看下<code>mutex</code>结构体</p><pre><code>type Mutex struct {    // 第0位:表示是否加锁，第1位:表示有 goroutine被唤醒，尝试获取锁； 第2位:是否为饥饿状态。    state int32    // semaphore，锁的信号量。    // 一般通过runtime_SemacquireMutex来获取、runtime_Semrelease来释放    sema  uint32 }</code></pre><p>在看下<code>Mutex</code>加锁是怎么实现的</p><pre><code>func (m *Mutex) Lock() {    // 先CAS判断是否加锁成功，成就返回    if atomic.CompareAndSwapInt32(&amp;m.state, 0, mutexLocked) {        return    }    // lockSlow 里面主要是尝试自旋、正常模式、饥饿模式切换    m.lockSlow()}</code></pre><p><code>sync.Mutex</code>底层都是使用<code>Atomic</code>来读写锁的状态。所以我们可以理解为，<code>Mutex</code>都是基于<code>Atomic</code>来实现<code>Happens Before</code>语义。我们下面来看下<code>Atomic</code>是如何实现的。</p><h4 id="Atomic"><a href="#Atomic" class="headerlink" title="Atomic"></a>Atomic</h4><p><code>Golang</code>中的<code>Atomic</code>主要保证了三件事，<strong>原子性、可见性、有序性</strong>。</p><p>我们先看下<a href="https://github.com/golang/go/blob/master/src/sync/atomic/doc.go">Go的源码里面Atomic</a> 的<code>API</code>，主要包括<code>Swap</code>、<code>CAS</code>、<code>Add</code>、<code>Load</code>、<code>Store</code>、<code>Pointer</code>几类，在<a href="https://github.com/golang/go/blob/master/src/runtime/internal/atomic/asm_amd64.s">IA64 CPU</a>上对应的汇编指令如下：</p><ul><li><code>Swap</code> : 主要是<a href="https://github.com/golang/go/blob/master/src/runtime/internal/atomic/asm_amd64.s#L110">XCHGQ</a>指令</li><li><code>CAS</code> : 主要是 <a href="https://github.com/golang/go/blob/master/src/runtime/internal/atomic/asm_amd64.s#L22">LOCK CMPXCHGQ</a>指令</li><li><code>Add</code> : 主要是 <a href="https://github.com/golang/go/blob/master/src/runtime/internal/atomic/asm_amd64.s#L99">LOCK XADDQ</a>指令</li><li><code>Load</code> : 主要是 <a href="https://github.com/golang/go/blob/master/src/runtime/internal/atomic/asm_386.s#L202">MOVQ（Load64）</a>指令</li><li><code>Store</code> : 主要是 <a href="https://github.com/golang/go/blob/master/src/runtime/internal/atomic/asm_amd64.s#L133">XCHGQ</a>指令</li><li><code>Pointer</code> : <a href="https://github.com/golang/go/blob/master/src/runtime/internal/atomic/asm_amd64.s#L50">主要当做64位int</a>，调用上述相关方法。</li></ul><p><strong>关于LOCK prefix和XCHG指令</strong>在 <a href="https://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-software-developer-vol-3a-part-1-manual.html">英特尔开发人员手册</a> section 8.2.5中，我们找到了如下的解释：</p><blockquote><p>For the Intel486 and Pentium processors, the LOCK# signal is always asserted on the bus during a LOCK operation, even if the area of memory being locked is cached in the processor.</p></blockquote><blockquote><p>For the P6 and more recent processor families, if the area of memory being locked during a LOCK operation is cached in the processor that is performing the LOCK operation as write-back memory and is completely contained in a cache line, the processor may not assert the LOCK# signal on the bus. Instead, it will modify the memory location internally and allow it’s cache coherency mechanism to ensure that the operation is carried out atomically. This operation is called “cache locking.” The cache coherency mechanism automatically prevents two or more processors that have cached the same area of memory from simultaneously modifying data in that area.</p></blockquote><blockquote><p>The I/O instructions, locking instructions, the LOCK prefix, and <strong>serializing instructions force stronger orderingon the processor</strong>.</p></blockquote><blockquote><p>Synchronization mechanisms in multiple-processor systems may depend upon a strong memory-ordering model. Here, a program can use a locking instruction such as the XCHG instruction or the LOCK prefix to ensure that a read-modify-write operation on memory is carried out atomically. Locking operations typically operate like I/O operations in that they wait for all previous instructions to complete and for all buffered writes to drain to memory (see Section 8.1.2, “Bus Locking”).</p></blockquote><p><strong>从描述中，我们了解到：LOCK prefix和XCHG 指令前缀提供了强一致性的内(缓)存读写保证，可以保证 LOCK 之后的指令在带 LOCK 前缀的指令执行之后才会执行。同时，我们在手册中还了解到，现代的 CPU 中的 LOCK 操作并不是简单锁 CPU 和主存之间的通讯总线， Intel 在 cache 层实现了这个 LOCK 操作，此因此我们也无需为 LOCK 的执行效率担忧。</strong></p><p>PS：<code>Java</code>中的<code>volatile</code>关键字也是基于 <code>Lock prefix</code> 实现的。</p><p>从上面可以看到<code>Swap</code>、<code>CAS</code>、<code>Add</code>、<code>Store</code> 都是基于<code>LOCK prefix</code>和<code>XCHG</code>指令实现的，他能保证缓存读写的强一致性。</p><p>我们单独来看下<code>Load</code>指令，在<a href="https://github.com/golang/go/blob/master/src/runtime/internal/atomic/atomic_386.go#L18">IA32</a>、<a href="https://github.com/golang/go/blob/master/src/runtime/internal/atomic/atomic_amd64.go#L17">IA64</a>、<a href="https://github.com/golang/go/blob/master/src/runtime/internal/atomic/atomic_arm64.s#L11">Arm</a>的<code>CPU</code>架构下就是对应<code>MOV</code>指令。我们写个简单<code>demo</code>验证下。测试代码如下：</p><pre><code>var numB uint32func main() {   numB = 8   fmt.Println(normalLoad())   fmt.Println(atomicLoad())}func normalLoad() uint32 {   a := numB   return a}func atomicLoad() uint32 {   a := atomic.LoadUint32(&amp;numB)   return a}</code></pre><p>我们<code>go build -gcflags "-N -l" atomic.go</code> 编译以后再<code>objdump -d atomic</code>导出对应的汇编代码。我们看到<code>normalLoad()</code>和<code>atomicLoad()</code> 对应的汇编代码是一样的，也印证了，我们上面说的atomic.Load方法在<a href="https://github.com/golang/go/blob/master/src/runtime/internal/atomic/atomic_amd64.go#L17">IA64</a> 就是简单的MOV指令。</p><p><img src="/2020/06/09/golang-memory-model/diff.png"></p><p>再回来看，我们知道Golang的Atomic方法保证了三件事，原子性、可见性、有序性。</p><p>可见性和有序性Store方法保证了，Load方法使用MOV指令只要能保证原子性就行了。我们知道golang里面是内存对齐的，所以能保证MOV指令是原子的。</p><p>更多可以参考<a href="https://wweir.cc/post/%E6%8E%A2%E7%B4%A2-golang-%E4%B8%80%E8%87%B4%E6%80%A7%E5%8E%9F%E8%AF%AD/">探索 Golang 一致性原语</a></p><h3 id="3-2-Golang-Happen-Before-语义继承图"><a href="#3-2-Golang-Happen-Before-语义继承图" class="headerlink" title="3.2 Golang Happen Before 语义继承图"></a>3.2 Golang Happen Before 语义继承图</h3><pre><code>                +----------+ +-----------+   +---------+                | sync.Map | | sync.Once |   | channel |                ++---------+++---------+-+   +----+----+                 |          |          |          |                 |          |          |          |+------------+   | +-----------------+ |          ||            |   | |       +v--------+ |          ||  WaitGroup +---+ | RwLock|  Mutex  | |   +------v-------++------------+   | +-------+---------+ |   | runtime lock |                 |                     |   +------+-------+                 |                     |          |                 |                     |          |                 |                     |          |         +------+v---------------------v   +------v-------+         | LOAD | other atomic action  |   |runtime atomic|         +------+--------------+-------+   +------+-------+                               |                  |                               |                  |                  +------------v------------------v+                  |           LOCK prefix          |                  +--------------------------------+                                    </code></pre><h3 id="3-3-如何解决上面Golang-Double-Check的问题"><a href="#3-3-如何解决上面Golang-Double-Check的问题" class="headerlink" title="3.3 如何解决上面Golang Double Check的问题"></a>3.3 如何解决上面Golang Double Check的问题</h3><pre><code>var (    lock     sync.Mutex    instance *UserInfo)func getInstance() (*UserInfo, error) {    if instance == nil {        //---Lock        lock.Lock()        defer lock.Unlock()        if instance == nil {            instance = &amp;UserInfo{                Name: "fan",            }        }    }//---Unlock()    return instance, nil}</code></pre><p>再看下这段有问题的代码，由上面的<code>Golang Happy Before</code>一致性原语我们知道，<code>instance</code>修改在<code>lock</code>临界区里面，其他的线程是可见的。那为什么在 <code>if instance == nil </code>还是会发生<code>Data Race</code>呢？</p><p>真正的原因是是在<code>instance = &amp;UserInfo{Name: "fan"}</code>这句代码，这句代码并不是原子操作，这个赋值可能是会有几步指令，比如</p><ol><li>先<code>new</code>一个<code>UserInfo</code></li><li>然后设置<code>Name=fan</code></li><li>最后把了<code>new</code>的对象赋值给<code>instance</code></li></ol><p>如果这个时候发生了乱序，可能会变成</p><ol><li>先了<code>new</code>一个<code>UserInfo</code></li><li>然后再赋值给<code>instance</code></li><li>最后再设置<code>Name=fan</code></li></ol><p>A进程进来的时候拿到锁，然后对<code>instance</code>进行赋值，这个时候<code>instance</code>对象是一个半初始化状态的数据，线程B来的时候判断<code>if instance == nil</code>发现不为<code>nil</code>就直接吧半初始化状态的数据返回了，所以会有问题。</p><p>下面是<code>instance = &amp;UserInfo{Name: "fan"}</code>这个代码对应的Golang的汇编指令</p><pre><code>0x00bc 00188 (main2.go:29)    LEAQ    type."".UserInfo(SB), AX0x00c3 00195 (main2.go:29)    PCDATA    $0, $00x00c3 00195 (main2.go:29)    MOVQ    AX, (SP)0x00c7 00199 (main2.go:29)    CALL    runtime.newobject(SB)0x00cc 00204 (main2.go:29)    PCDATA    $0, $20x00cc 00204 (main2.go:29)    MOVQ    8(SP), DI0x00d1 00209 (main2.go:29)    PCDATA    $1, $20x00d1 00209 (main2.go:29)    MOVQ    DI, ""..autotmp_2+72(SP)0x00d6 00214 (main2.go:29)    MOVQ    $3, 8(DI)0x00de 00222 (main2.go:29)    PCDATA    $0, $-20x00de 00222 (main2.go:29)    PCDATA    $1, $-20x00de 00222 (main2.go:29)    CMPL    runtime.writeBarrier(SB), $00x00e5 00229 (main2.go:29)    JEQ    2330x00e7 00231 (main2.go:29)    JMP    2860x00e9 00233 (main2.go:29)    LEAQ    go.string."fan"(SB), CX0x00f0 00240 (main2.go:29)    MOVQ    CX, (DI)0x00f3 00243 (main2.go:29)    JMP    2450x00f5 00245 (main2.go:28)    PCDATA    $0, $10x00f5 00245 (main2.go:28)    PCDATA    $1, $10x00f5 00245 (main2.go:28)    MOVQ    ""..autotmp_2+72(SP), AX0x00fa 00250 (main2.go:28)    PCDATA    $0, $-20x00fa 00250 (main2.go:28)    PCDATA    $1, $-20x00fa 00250 (main2.go:28)    CMPL    runtime.writeBarrier(SB), $00x0101 00257 (main2.go:28)    JEQ    2610x0103 00259 (main2.go:28)    JMP    2720x0105 00261 (main2.go:28)    MOVQ    AX, "".instance(SB)0x010c 00268 (main2.go:28)    JMP    2700x010e 00270 (main2.go:29)    JMP    1860x0110 00272 (main2.go:28)    LEAQ    "".instance(SB), DI0x0117 00279 (main2.go:28)    CALL    runtime.gcWriteBarrier(SB)0x011c 00284 (main2.go:28)    JMP    2700x011e 00286 (main2.go:29)    LEAQ    go.string."fan"(SB), AX0x0125 00293 (main2.go:29)    CALL    runtime.gcWriteBarrier(SB)0x012a 00298 (main2.go:29)    JMP    245</code></pre><p>知道了原因，我们可以直接用Atomic.Value来保证可见性和原子性就行了，改造代码如下：</p><pre><code>var flag uint32func getInstance() (*UserInfo, error) {   if atomic.LoadUint32(&amp;flag) != 1 {      lock.Lock()      defer lock.Unlock()      if instance == nil {         // 其他初始化错误，如果有错误可以直接返回     instance = &amp;UserInfo{            Age: 18,         }         atomic.StoreUint32(&amp;flag, 1)      }   }   return instance, nil}</code></pre><p>再次用<code>go run -race go_race2.go</code> 检查发现已经没有警告了。</p><p>这里，我们主要是通过<code>atomic.store</code>和<code>lock</code>来保证<code>flag</code>和<code>instance</code>的修改对其他线程可见。通过<code>atomic.LoadUint32(&amp;flag)</code>和<code>double check</code>来保证<code>instance</code>只会初始化一次。</p><h2 id="四、CPU-Cache-扩展知识"><a href="#四、CPU-Cache-扩展知识" class="headerlink" title="四、CPU Cache 扩展知识"></a>四、CPU Cache 扩展知识</h2><h3 id="4-1-CPU-Cache-的产生背景"><a href="#4-1-CPU-Cache-的产生背景" class="headerlink" title="4.1 CPU Cache 的产生背景"></a>4.1 CPU Cache 的产生背景</h3><p>　　计算机中的所有运算操作都是由<code>CPU</code>的寄存器来完成的，<code>CPU</code>指令的执行过程需要涉及数据的读取和写入，这些数据只能来自于计算机主存（通常指<code>RAM</code>）。</p><p>　　<code>CPU</code>的处理速度和内存的访问速度差距巨大，直连内存的访问方式使得<code>CPU</code>资源没有得到充分合理的利用，于是产生了在<code>CPU</code>与主存之间增加高速缓存<code>CPU Cache</code>的设计。 </p><h3 id="4-2-CPU-Cache-模型"><a href="#4-2-CPU-Cache-模型" class="headerlink" title="4.2 CPU Cache 模型"></a>4.2 CPU Cache 模型</h3><p>　　<code>CPU Cache</code>模型，缓存分为三级<code>L1/L2/L3</code>，由于指令和数据的行为和热点分布差异很大，因此将L1按照用途划分为<code>L1i（instruction）</code>和<code>L1d（data</code>）.</p><p>　　在多核<code>CPU</code>的结构中，<code>L1</code>和<code>L2</code>是<code>CPU</code>私有的，<code>L3</code>则是所有<code>CPU</code>共享的。</p><h3 id="4-3-什么是-Cache-Line"><a href="#4-3-什么是-Cache-Line" class="headerlink" title="4.3 什么是 Cache Line"></a>4.3 什么是 Cache Line</h3><p><code>Cache line</code> 是 <code>Cache</code> 和 <code>RAM</code> 交换数据的最小单位，通常为 <code>64 Byte</code>。当 CPU 把内存的数据载入 <code>Cache</code> 时，会把临近的共 <code>64 Byte</code> 的数据一同放入同一个<code>Cache line</code>，因为空间局部性：临近的数据在将来被访问的可能性大。</p><p>由于<code>CPU Cache</code>缓存数据最小的单位是一个<code>Cache Line（64节）</code>，如果两个<code>Core</code>读取了同一个<code>Cache Line</code>，并对<code>Cache Line</code>中的数据频繁读写，就会有<code>Flase Sharing</code>的问题。</p><h3 id="4-4-Flase-Sharing-问题"><a href="#4-4-Flase-Sharing-问题" class="headerlink" title="4.4 Flase Sharing 问题"></a>4.4 Flase Sharing 问题</h3><p><img src="/2020/06/09/golang-memory-model/false_sharding.png"></p><p>上图中 <code>thread1</code> 位于 <code>core1</code> ，而 <code>thread2</code> 位于 <code>core2</code> ，二者均想更新彼此独立的两个变量，但是由于两个变量位于不同核心中的同一个 <code>L1</code> 缓存行中，此时可知的是两个缓存行的状态应该都是 <code>Shared</code> ，而对于同一个缓存行的操作，不同的 <code>core</code> 间必须通过发送 <code>RFO</code> 消息来争夺所有权 (<code>ownership</code>) ，如果 <code>core1</code> 抢到了， <code>thread1</code> 因此去更新该缓存行，把状态变成 <code>Modified</code> ，那就会导致 <code>core2</code> 中对应的缓存行失效变成 <code>Invalid</code> ，当 <code>thread2</code> 取得所有权之后再去更新该缓存行时必须先让 <code>core1</code> 把对应的缓存行刷回 <code>L3</code> 缓存/主存，然后它再从 <code>L3</code> 缓存/主存中加载该缓存行进 <code>L1</code> 之后才能进行修改。然而，这个过程又会导致 <code>core1</code> 对应的缓存行失效变成 <code>Invalid</code> ，这个过程将会一直循环发生，从而导致 <code>L1</code> 高速缓存并未起到应有的作用，反而会降低性能；轮番夺取所有权不但带来大量的 <code>RFO</code> 消息，而且如果某个线程需要读此行数据时，<code>L1</code> 和 <code>L2</code> 缓存上都是失效数据，只有 <code>L3</code> 缓存上是同步好的数据，而从前面的内容可以知道，<code>L3</code> 的读取速度相比 <code>L1/L2</code> 要慢了数十倍，性能下降很大；更坏的情况是跨槽读取，<code>L3</code> 都不能命中，只能从主存上加载，那就更慢了。</p><p><strong>CPU 缓存的最小的处理单位永远是缓存行 (Cache Line)，所以当某个核心发送 RFO 消息请求把其他核心对应的缓存行设置成Invalid 从而使得 var1 缓存失效的同时，也会导致同在一个缓存行里的 var2 失效，反之亦然。</strong></p><p><code>Cache Line</code>缓存测试</p><pre><code>func main() {    arr := make([][]int, 64*1024)    for i := 0; i &lt; len(arr); i++ {        arr[i] = make([]int, 1024)    }    now := time.Now()    for i := 0; i &lt; len(arr); i++ {        for j := 0; j &lt; 1024; j++ {            arr[i][j]++        }    }    timeSpan := time.Since(now).Microseconds()    fmt.Println("横向遍历耗时：", timeSpan)    now = time.Now()    for j := 0; j &lt; 1024; j++ {        for i := 0; i &lt; len(arr); i++ {            arr[i][j]++        }    }    timeSpan = time.Since(now).Microseconds()    fmt.Println("纵向遍历耗时：", timeSpan)}横向遍历耗时： 485995  //因为横向写数据的时候，会一直命中CPU缓存，所以比纵向更快一些纵向遍历耗时： 1705150</code></pre><h3 id="4-5-如何解决False-Sharding问题"><a href="#4-5-如何解决False-Sharding问题" class="headerlink" title="4.5 如何解决False Sharding问题"></a>4.5 如何解决False Sharding问题</h3><p>对一些热点数据，如果想避免<code>cache line</code>被其他<code>Core</code>设置为失效，可以通过Pading的方式把每个项凑齐<code>cache line</code>的长度，即可实现隔离，虽然这不可避免的会浪费一些内存。</p><p>我们可以看到<code>golang</code>的源码里面 <code>p struct</code>的也用了<code>CacheLinePad</code>的方式来避免了<code>False Sharding</code>的问题</p><pre><code>type p struct {    上面省略    .....            runSafePointFn uint32 // if 1, run sched.safePointFn at next safe point    pad cpu.CacheLinePad}</code></pre><p><code>CacheLinePad</code> 是<code>cpu</code>包下面定义的一个64字节的数组</p><pre><code>const CacheLinePadSize = 64// CacheLinePad is used to pad structs to avoid false sharing.type CacheLinePad struct{ _ [CacheLinePadSize]byte }</code></pre><p>这样能保证<code>p</code>的数据不管怎么拼接都不会跟其他数据在同一个<code>cache line</code>中。</p><h3 id="4-6-CPU-Cache-是如何存放数据的"><a href="#4-6-CPU-Cache-是如何存放数据的" class="headerlink" title="4.6 CPU Cache 是如何存放数据的"></a>4.6 CPU Cache 是如何存放数据的</h3><p><img src="/2020/06/09/golang-memory-model/cpu_cache_struct.jpg"></p><p>由上图可以知<code>Cache</code>是由<code>Set</code>组成，<code>Set</code>由<code>Cache Line</code>组成，<code>Cache Line</code>由<code>Valid Bit</code>（MESI协议中这个是2个字节），<code>Tag</code>和<code>Data</code>组成。其中<code>Data</code>是真正要缓存的内存地址中的数据，而<code>Tag</code>是用来搜索<code>Cache Line</code>的标签。</p><p>假设<code>L1 Cache</code>总大小为<code>32KB</code>，8路组相连（<code>每个Set有8个Cache Line</code>），每个<code>Cache Line</code>的大小为<code>64Byte</code>。</p><p>我们可以得到一个 </p><p>Set大小 = 8 * Cache Line = 512Byte</p><p>Set个数 = 32*1024 /512 = 64</p><p>Cache Line Count = 32*1024 / 64 = 512个</p><h3 id="4-7-CPU-Cache-寻址过程"><a href="#4-7-CPU-Cache-寻址过程" class="headerlink" title="4.7 CPU Cache 寻址过程"></a>4.7 CPU Cache 寻址过程</h3><p>先看下内存地址表示的含义</p><p><img src="/2020/06/09/golang-memory-model/mem_address.jpg"></p><p>内存被分成了<code>TAG</code>、Set Index、<code>Block Offset</code> 三部分。</p><ol><li>根据地址中的<code>Set Index</code>找到对应的缓存中对应的Set</li><li>根据<code>Tag</code>与<code>Set</code>中所有<code>CacheLine</code>的<code>Tag</code>一一对比，遇到相等的表示找到缓存。</li><li>查看<code>Cache Line</code> 的<code>Validate Bit</code>是不是有效的。有效的表示命中<code>Cache</code>。</li><li>根据<code>Block Offset</code>读取<code>Cache Line</code>中<code>Block Data</code>对应的值。</li></ol><p><img src="/2020/06/09/golang-memory-model/hit_cache1.jpg"></p><p><img src="/2020/06/09/golang-memory-model/hit_cache2.jpg"></p><p><img src="/2020/06/09/golang-memory-model/hit_cache3.jpg"></p><h3 id="4-8-CPU-Cache-三种寻址方式"><a href="#4-8-CPU-Cache-三种寻址方式" class="headerlink" title="4.8 CPU Cache 三种寻址方式"></a>4.8 CPU Cache 三种寻址方式</h3><ul><li>直接映射（<code>direct mapped cache</code>），相当于每个set只有1个<code>cache line</code>（E=1）。那么相隔2^(s+b)个单元的2个内存单元，会被映射到同一个<code>cache line</code>中。</li><li>组关联（<code>set associative cache</code>），多个<code>set</code>，每个<code>set</code>多个<code>cache line</code>。一般每个<code>set</code>有<code>n</code>个<code>cache line</code>，就说<code>n-ways associative cache</code>。</li><li>全相联（<code>fully associative cache</code>），相当于只有1个<code>set</code>，每个内存单元都能映射到任意的<code>cache line</code>。带有这样<code>cache</code>的处理器几乎没有。可以想见这种方式不适合大的缓存。想想看，如果4M 的大缓存　<code>linesize</code>为<code>32Byte</code>，采用全相联的话，就意味着4 * 1024 * 1024/32 = 128K 个<code>line</code>挨个比较，来确定是否命中，这是多要命的事情。</li></ul><h3 id="4-9-CPU-Cache-的组织方式"><a href="#4-9-CPU-Cache-的组织方式" class="headerlink" title="4.9 CPU Cache 的组织方式"></a>4.9 CPU Cache 的组织方式</h3><h4 id="VIVT-Virtual-Index-Virtual-Tag"><a href="#VIVT-Virtual-Index-Virtual-Tag" class="headerlink" title="VIVT(Virtual Index Virtual Tag)"></a>VIVT(Virtual Index Virtual Tag)</h4><p>使用虚拟地址做索引，虚拟地址做<code>Tag</code>。早期的<code>ARM</code>处理器一般采用这种方式，在查找<code>cache line</code>过程中不借助物理地址，这种方式会导致<code>cache</code>别名(<code>cache alias</code>)问题。比如当两个虚拟地址对应相同物理地址，并且没有映射到同一<code>cache</code>行，那么就会产生问题。另外，当发生进程切换时，由于页表可能发生变化，所以要对<code>cache</code>进行<code>invalidate</code>等操作，效率较低。</p><h4 id="VIPT-Virtual-Index-Physical-Tag"><a href="#VIPT-Virtual-Index-Physical-Tag" class="headerlink" title="VIPT(Virtual Index Physical Tag)"></a>VIPT(Virtual Index Physical Tag)</h4><p>使用虚拟地址做索引，物理地址做<code>Tag</code>。在利用虚拟地址索引<code>cache</code>同时，同时会利用<code>TLB/MMU</code>将虚拟地址转换为物理地址。然后将转换后的物理地址，与虚拟地址索引到的<code>cache line</code>中的<code>Tag</code>作比较，如果匹配则命中。这种方式要比<code>VIVT</code>实现复杂，当进程切换时，不在需要对<code>cache</code>进行<code>invalidate</code>等操作(因为匹配过程中需要借物理地址)。但是这种方法仍然存在<code>cache</code>别名的问题(即两个不同的虚拟地址映射到同一物理地址，且位于不同的<code>cache line</code>)，但是可以通过一定手段解决。</p><h4 id="PIPT-Physical-Index-Physical-Tag"><a href="#PIPT-Physical-Index-Physical-Tag" class="headerlink" title="PIPT(Physical Index Physical Tag)"></a>PIPT(Physical Index Physical Tag)</h4><p>使用物理地址做索引，物理地址做<code>Tag</code>。现代的<code>ARM Cortex-A</code>大多采用<code>PIPT</code>方式，由于采用物理地址作为<code>Index</code>和<code>Tag</code>，所以不会产生<code>cache alias</code>问题。不过<code>PIPT</code>的方式在芯片的设计要比<code>VIPT</code>复杂得多，而且需要等待<code>TLB/MMU</code>将虚拟地址转换为物理地址后，才能进行<code>cache line</code>寻找操作。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文涉及到很多<code>CPU</code>底层实现逻辑，如果看完还是一头雾水可以选择无视。因为需要用到这些知识点的机会本就少之又少。正如 <a href="https://golang.org/ref/mem">The Go Memory Model</a> 所言：</p><blockquote><p>If you must read the rest of this document to understand the behavior of your program, you are being too clever.</p><p>Don’t be clever.</p></blockquote><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.zhihu.com/question/24612442">https://www.zhihu.com/question/24612442</a></p><p><a href="https://zhuanlan.zhihu.com/p/29108170">https://zhuanlan.zhihu.com/p/29108170</a></p><p><a href="https://www.infoq.cn/article/cache-coherency-primer">https://www.infoq.cn/article/cache-coherency-primer</a></p><p><a href="https://zh.wikipedia.org/wiki/%E5%86%85%E5%AD%98%E5%B1%8F%E9%9A%9C">https://zh.wikipedia.org/wiki/%E5%86%85%E5%AD%98%E5%B1%8F%E9%9A%9C</a></p><p><a href="https://blog.csdn.net/hanzefeng/article/details/82893317">https://blog.csdn.net/hanzefeng/article/details/82893317</a></p><p><a href="https://www.cnblogs.com/linhaostudy/p/9193162.html">https://www.cnblogs.com/linhaostudy/p/9193162.html</a></p><p><a href="https://hacpai.com/article/1459654970712">https://hacpai.com/article/1459654970712</a></p><p><a href="http://cenalulu.github.io/linux/all-about-cpu-cache/">http://cenalulu.github.io/linux/all-about-cpu-cache/</a></p><p><a href="https://mp.weixin.qq.com/s/viQp36FeMZSqUoFy3VrBNw">https://mp.weixin.qq.com/s/viQp36FeMZSqUoFy3VrBNw</a></p><p><a href="https://blog.csdn.net/hx_op/article/details/89244618">https://blog.csdn.net/hx_op/article/details/89244618</a></p><p><a href="https://wweir.cc/post/%E6%8E%A2%E7%B4%A2-golang-%E4%B8%80%E8%87%B4%E6%80%A7%E5%8E%9F%E8%AF%AD">https://wweir.cc/post/%E6%8E%A2%E7%B4%A2-golang-%E4%B8%80%E8%87%B4%E6%80%A7%E5%8E%9F%E8%AF%AD</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Golang </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Golang RWMutext 代码走读</title>
      <link href="/2020/06/06/golang-rmmutex/"/>
      <url>/2020/06/06/golang-rmmutex/</url>
      
        <content type="html"><![CDATA[<pre><code>type RWMutex struct {   w           Mutex  // held if there are pending writers   writerSem   uint32 // 写的信号量   readerSem   uint32 // 读的信号量   readerCount int32  // 等待写的个数   readerWait  int32  // 等待读的个数}// 加“读锁”// 对readerCount + 1 。// 然后看 readerCount是不是小于0// 小于0表示 正在加写锁，然后阻塞到rw.readerSem 这个信号上。func (rw *RWMutex) RLock() {   if atomic.AddInt32(&amp;rw.readerCount, 1) &lt; 0 {      // A writer is pending, wait for it.      runtime_SemacquireMutex(&amp;rw.readerSem, false, 0)   }}// 释放 “读锁”// 对readerCount - 1 。// 然后看 readerCount是不是小于0// 小于0表示 正在加写锁，然后调用rw.rUnlockSlowfunc (rw *RWMutex) RUnlock() {   if r := atomic.AddInt32(&amp;rw.readerCount, -1); r &lt; 0 {      // Outlined slow-path to allow the fast-path to be inlined      rw.rUnlockSlow(r)   }}// r+1 == -rwmutexMaxReaders 表示“读锁”已经释放，抛出异常// rw.readerWait - 1 // rw.readerWait - 1 = 0 表示所有读锁都释放了// 所有读锁都释放了可以唤醒 rw.writerSem 对应 写锁的lock方法继续执行func (rw *RWMutex) rUnlockSlow(r int32) {   if r+1 == 0 || r+1 == -rwmutexMaxReaders {      race.Enable()      throw("sync: RUnlock of unlocked RWMutex")   }   // A writer is pending.   if atomic.AddInt32(&amp;rw.readerWait, -1) == 0 {      // The last reader unblocks the writer.      runtime_Semrelease(&amp;rw.writerSem, false, 1)   }}// mutex 加锁，保证写锁和写锁之间互斥// rw.readerCount - rwmutexMaxReaders// r 表示读锁数量// rw.readerWait + 读lock的数量 // 等待 rw.writerSem 的信号 （读锁那边释放完了，会发这个信号）func (rw *RWMutex) Lock() {   // First, resolve competition with other writers.   rw.w.Lock()   // Announce to readers there is a pending writer.   r := atomic.AddInt32(&amp;rw.readerCount, -rwmutexMaxReaders) + rwmutexMaxReaders   // Wait for active readers.   if r != 0 &amp;&amp; atomic.AddInt32(&amp;rw.readerWait, r) != 0 {      runtime_SemacquireMutex(&amp;rw.writerSem, false, 0)   }}// rw.readerCount + rwmutexMaxReaders// r 表示读锁的数量，大于 rwmutexMaxReaders 就抛出异常// 发送 rw.readerSem  信号量，通知RLock 代码可以继续执行。func (rw *RWMutex) Unlock() {   // Announce to readers there is no active writer.   r := atomic.AddInt32(&amp;rw.readerCount, rwmutexMaxReaders)   if r &gt;= rwmutexMaxReaders {      race.Enable()      throw("sync: Unlock of unlocked RWMutex")   }   // Unblock blocked readers, if any.   for i := 0; i &lt; int(r); i++ {      runtime_Semrelease(&amp;rw.readerSem, false, 0)   }</code></pre>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Golang </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K8S学习笔记</title>
      <link href="/2020/05/10/k8s-study/"/>
      <url>/2020/05/10/k8s-study/</url>
      
        <content type="html"><![CDATA[<h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><h3 id="Docker-File-编写"><a href="#Docker-File-编写" class="headerlink" title="Docker File 编写"></a>Docker File 编写</h3><pre><code>#源镜像FROM golang:latest# 容器环境变量添加，会覆盖默认的变量值ENV GOPROXY=https://goproxy.cn,directENV GO111MODULE="on"ENV test="on"# 作者LABEL author="fanlv"LABEL email="fanlvlgh@gmail.com"#设置工作目录WORKDIR /go/src/gitee.com/fanlv/GolangDemo/GoTest/docker# 复制仓库源文件到容器里COPY . .# 编译可执行二进制文件(一定要写这些编译参数，指定了可执行程序的运行平台,参考：https://www.jianshu.com/p/4b345a9e768e)RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o mem_test -gcflags "-N -l" gotest.go# 构建生产镜像，使用最小的linux镜像，只有5M# 同一个文件里允许多个FROM出现的，每个FROM被称为一个阶段，多个FROM就是多个阶段，最终以最后一个FROM有效，以前的FROM被抛弃# 多个阶段的使用场景就是将编译环境和生产环境分开# 参考：https://docs.docker.com/engine/reference/builder/#fromFROM alpine:latestWORKDIR /root/# 从编译阶段复制文件# 这里使用了阶段索引值，第一个阶段从0开始，如果使用阶段别名则需要写成 COPY --from=build_stage /go/src/app/webserver /COPY --from=0 /go/src/gitee.com/fanlv/GolangDemo/GoTest/docker .#暴露端口EXPOSE 8000#最终运行docker的命令ENTRYPOINT  ["./mem_test"]</code></pre><h3 id="Docker-File-发布"><a href="#Docker-File-发布" class="headerlink" title="Docker File 发布"></a>Docker File 发布</h3><pre><code># 编译 docker file# docker build -t gomemtest .# 运行# docker run --name gomemtest -p 8000:8000 -d  gomemtest#发布docker# docker tag gomemtest fanlv/gomemtest# docker push fanlv/gomemtest# docker tag gomemtest fanlv/gomemtest:v1.0.0# docker push fanlv/gomemtest:tagname:v1.0.0</code></pre><h3 id="进入容器"><a href="#进入容器" class="headerlink" title="进入容器"></a>进入容器</h3><pre><code>docker exec -it gomemtest sh </code></pre><h3 id="安装-minikube"><a href="#安装-minikube" class="headerlink" title="安装 minikube"></a>安装 minikube</h3><p><a href="https://minikube.sigs.k8s.io/docs/start/">minikube Installation</a></p><pre><code>brew install minikubebrew install kubernetes-clikubectl get po -A  minikube kubectl -- get po -A</code></pre><h3 id="部署-Application"><a href="#部署-Application" class="headerlink" title="部署 Application"></a>部署 Application</h3><pre><code>kubectl create deployment mem --image=fanlv/gomemtestkubectl delete -n default deployment mem kubectl get services memminikube service mem  // web 访问服务端口kubectl port-forward service/mem 8000:8000</code></pre><h3 id="LoadBalancer-deployments"><a href="#LoadBalancer-deployments" class="headerlink" title="LoadBalancer deployments"></a>LoadBalancer deployments</h3><pre><code>kubectl create deployment balanced --image=k8s.gcr.io/echoserver:1.4  kubectl expose deployment balanced --type=LoadBalancer --port=8080minikube tunnelkubectl get services balanced</code></pre><h3 id="node"><a href="#node" class="headerlink" title="node"></a>node</h3><pre><code>k get nodes // 获取nodek describe node minikube // 输出显示了节点的状态、 CPU 和内存数据、系统信息、运行容器的节点等</code></pre><h3 id="Pods"><a href="#Pods" class="headerlink" title="Pods"></a>Pods</h3><pre><code>k get podsk get services</code></pre><h3 id=""><a href="#" class="headerlink" title=""></a></h3><p>k run mem –image=fanlv/gomemtest –port=8000 –generator=run/v1</p><p>k expose rc mem –type=LoadBalancer –name mem-http</p><h3 id="kubectl-explain-来发现可能的-API-对象字段"><a href="#kubectl-explain-来发现可能的-API-对象字段" class="headerlink" title="kubectl explain 来发现可能的 API 对象字段"></a>kubectl explain 来发现可能的 API 对象字段</h3><pre><code>kubectl explain podskubectl explain pod.spec</code></pre><h3 id="使用-kubectl-create-来创建-pod"><a href="#使用-kubectl-create-来创建-pod" class="headerlink" title="使用 kubectl create 来创建 pod"></a>使用 kubectl create 来创建 pod</h3><pre><code> kubectl create -f kubia-manual.yaml  </code></pre><h3 id="得到运行中-pod-的完整定义"><a href="#得到运行中-pod-的完整定义" class="headerlink" title="得到运行中 pod 的完整定义"></a>得到运行中 pod 的完整定义</h3><pre><code> kubectl get po kubia-manual -o yaml  kubectl get po kubia-manual -o json  </code></pre><h3 id="pod-列表中查看新创建的-pod"><a href="#pod-列表中查看新创建的-pod" class="headerlink" title="pod 列表中查看新创建的 pod"></a>pod 列表中查看新创建的 pod</h3><pre><code>kubectl get pods</code></pre><h3 id="查看应用程序日志"><a href="#查看应用程序日志" class="headerlink" title="查看应用程序日志"></a>查看应用程序日志</h3><pre><code>docker logs &lt;container id&gt;kubectl get &lt;pod name&gt;kubectl get &lt;pod name&gt; -c &lt;容器 名称&gt;</code></pre><h3 id="将本地网络端口转发到pod中的端口"><a href="#将本地网络端口转发到pod中的端口" class="headerlink" title="将本地网络端口转发到pod中的端口"></a>将本地网络端口转发到pod中的端口</h3><pre><code>kubectl port-forward kubia manual 8888:8080</code></pre><h3 id="使用标签组织pod"><a href="#使用标签组织pod" class="headerlink" title="使用标签组织pod"></a>使用标签组织pod</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-10bd1c785d3dc87d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><pre><code>kubectl create -f kubia-manual-with-labels.yamlkubectl get po --show-labels    k label po balanced-5b47687d6d-n2svl evn=debug --overwrite</code></pre><h3 id="使用标签选择器列出-pod"><a href="#使用标签选择器列出-pod" class="headerlink" title="使用标签选择器列出 pod"></a>使用标签选择器列出 pod</h3><pre><code>kubectl get po -l creation_method=manualkubectl get po -l env</code></pre><h3 id="查找对象的注解"><a href="#查找对象的注解" class="headerlink" title="查找对象的注解"></a>查找对象的注解</h3><pre><code>kubectl get po kubia-zxzij -o yaml// 添加和修改注解kubectl annotate pod kubia-manual mycompany . com/someannotation="foo bar ”kubectl describe pod kubia-manual</code></pre><h3 id="停止和移除pod"><a href="#停止和移除pod" class="headerlink" title="停止和移除pod"></a>停止和移除pod</h3><pre><code>kubectl delete po kubia-gpu// 使用标签选择器删除 podkubectl delete po -1 creation method=manual//kubectl delete po - 1 rel=canary// 删除命名空间中的（几乎）所有资源kubec tl delete all --all// 查看之前的日志k logs balanced-5b47687d6d-n2svl --previous</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-10bd1c785d3dc87d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><pre><code>kubectl create -f kubia-rc.yaml</code></pre><h3 id="水平缩放-pod"><a href="#水平缩放-pod" class="headerlink" title="水平缩放 pod"></a>水平缩放 pod</h3><pre><code>kubectl scale re kubia --replicas=lO</code></pre><ol><li>单体应用，方便部署。 不好水平扩展。</li><li>微服务，扩容方便，只用针对单个服务扩容。</li><li>微服务调试变得困难，依赖各个模块。 还需要做服务治理。</li><li>环境差异。 依赖库版本不同。</li></ol><p>docker 好处。 </p><ol><li>提供一致的开发环境。简单了部署流程，服务发布流程化。</li><li>比虚拟机更轻量。</li></ol><p>k8s好处</p><ol><li>简化应用部署。</li><li>更好利用硬件。</li><li>健康检查和自修复。</li><li>自动扩容</li></ol>]]></content>
      
      
      <categories>
          
          <category> System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用的排序算法</title>
      <link href="/2020/01/16/sort/"/>
      <url>/2020/01/16/sort/</url>
      
        <content type="html"><![CDATA[<h2 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h2><pre><code>func insertSort(nums []int) {    for i := 1; i &lt; len(nums); i++ {        tmp := nums[i]        for j := i; j &gt;= 0; j-- {            if j &gt; 0 &amp;&amp; tmp &lt; nums[j-1] {                nums[j] = nums[j-1]            } else {                nums[j] = tmp                break            }        }    }}</code></pre><h2 id="折半插入排序"><a href="#折半插入排序" class="headerlink" title="折半插入排序"></a>折半插入排序</h2><pre><code>func binaryInsertSort(nums []int) {    for i := 1; i &lt; len(nums); i++ {        left, right := 0, i-1        mid := 0        for left &lt; right {            mid = (left + right) &gt;&gt; 1            if nums[mid] &gt; nums[i] {                right = mid - 1            } else {                left = mid + 1            }        }        index := right        tmp := nums[i]        if nums[right] &lt; tmp {            index++        }        for j := i; j &gt;= index+1; j-- {            nums[j] = nums[j-1]        }        nums[index] = tmp    }}    </code></pre><h2 id="shell-排序"><a href="#shell-排序" class="headerlink" title="shell 排序"></a>shell 排序</h2><pre><code>func shellSort(nums []int) {    n := len(nums)    if n == 0 {        return    }    for gap := n / 2; gap &gt; 0; gap = gap / 2 {        for i := gap; i &lt; n; i++ {            for j := i - gap; j &gt;= 0 &amp;&amp; nums[j] &gt; nums[j+gap]; j -= gap {                nums[j], nums[j+gap] = nums[j+gap], nums[j]            }        }    }}</code></pre><h2 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h2><pre><code>func bubbleSort(nums []int) {    for i := 0; i &lt; len(nums); i++ {        for j := len(nums) - 1; j &gt; i; j-- {            if nums[j] &lt; nums[j-1] {                nums[j], nums[j-1] = nums[j-1], nums[j]            }        }    }}</code></pre><h2 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h2><pre><code>func selectSort(a []int) {    for i := 0; i &lt; len(a); i++ {        minIndex := i        for j := i + 1; j &lt; len(a); j++ {            if a[minIndex] &gt; a[j] {                minIndex = j            }        }        a[i], a[minIndex] = a[minIndex], a[i]    }}</code></pre><h2 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h2><pre><code>func quickSort(arr []int) {    if len(arr) &lt;= 1 {        return    }    idx := partition(arr)    quickSort(arr[:idx])    quickSort(arr[idx+1:])}func partition(a []int) int {    left := 0    right := len(a) - 1    flag := 0    for left &lt; right {        for left &lt; right &amp;&amp; a[right] &gt;= a[flag] {            right--        }        for left &lt; right &amp;&amp; a[left] &lt;= a[flag] {            left++        }        a[left], a[right] = a[right], a[left]    }    a[flag], a[right] = a[right], a[flag]    return right}</code></pre><h2 id="基数排序"><a href="#基数排序" class="headerlink" title="基数排序"></a>基数排序</h2><pre><code>func radixSort(a []int) {    n := len(a)    lists := make([][]int, 10)    max := a[0]    for i := 1; i &lt; len(a); i++ {        if max &lt; a[i] {            max = a[i]        }    }    exp := 1    for max &gt; 0 {        //将之前的元素清空        for i := 0; i &lt; 10; i++ {            lists[i] = []int{}        }        for i := 0; i &lt; n; i++ {            index := a[i] / exp % 10            array := lists[index]            array = append(array, a[i])            lists[index] = array        }        index := 0        for i := 0; i &lt; 10; i++ {            arr := lists[i]            for j := 0; j &lt; len(arr); j++ {                a[index] = arr[j]                index++            }        }        max /= 10        exp *= 10    }}</code></pre><h2 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h2><pre><code>func mergeSort(nums []int) []int {    if len(nums) &lt;= 1 {        return nums    }    left, right := 0, len(nums)    mid := (left + right) &gt;&gt; 1    return merge(mergeSort(nums[:mid]), mergeSort(nums[mid:]))}func merge(left, right []int) []int {    res := make([]int, 0, len(left)+len(right))    leftIndex := 0    rightIndex := 0    for leftIndex &lt; len(left) || rightIndex &lt; len(right) {        if leftIndex &lt; len(left) &amp;&amp; rightIndex &lt; len(right) {            if left[leftIndex] &lt; right[rightIndex] {                res = append(res, left[leftIndex])                leftIndex++            } else {                res = append(res, right[rightIndex])                rightIndex++            }        } else if leftIndex == len(left) &amp;&amp; rightIndex &lt; len(right) {            res = append(res, right[rightIndex])            rightIndex++        } else if rightIndex == len(right) &amp;&amp; leftIndex &lt; len(left) {            res = append(res, left[leftIndex])            leftIndex++        }    }    return res}</code></pre><h2 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h2><pre><code>func heapSort(a []int) {    //构建大顶堆    lenA := len(a)    for i := len(a)/2 - 1; i &gt;= 0; i-- {        adjustHead(a, i, lenA)    }    for i := len(a) - 1; i &gt; 0; i-- {        a[0], a[i] = a[i], a[0]        adjustHead(a, 0, i)    }}func adjustHead(a []int, i, length int) {    tmp := a[i]    //用k := 2*i + 1 的到i子节点的位置    for k := 2*i + 1; k &lt; length; k = 2*k + 1 {        // 让k先指向子节点中最大的节点        if k+1 &lt; length &amp;&amp; a[k] &lt; a[k+1] {            k++        }        if a[k] &gt; tmp { //子节点比根节点大            a[i], a[k] = a[k], a[i]            i = k        } else {            break        }    }}</code></pre><h2 id="KMP查找子串"><a href="#KMP查找子串" class="headerlink" title="KMP查找子串"></a>KMP查找子串</h2><pre><code>func kmp(str string, needle string) int {    next := makeNext(str)    j := 0    for i := 0; i &lt; len(str); i++ {        for j &gt; 0 &amp;&amp; str[i] != needle[j] {            j = next[j-1]        }        if str[i] == needle[j] {            j++        }        if j == len(needle) {            return i - j + 1        }    }    return -1}func makeNext(s string) []int {    next := make([]int, len(s))    next[0] = 0    k := 0    for i := 1; i &lt; len(s); i++ {        for k &gt; 0 &amp;&amp; s[k] != s[i] {            k = next[k-1]        }        if s[k] == s[i] {            k++        }        next[i] = k    }    return next}</code></pre>]]></content>
      
      
      <categories>
          
          <category> Arithmetic </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sort </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis 高可用解决方案总结</title>
      <link href="/2019/08/17/redis-ha/"/>
      <url>/2019/08/17/redis-ha/</url>
      
        <content type="html"><![CDATA[<h2 id="一、主从复制"><a href="#一、主从复制" class="headerlink" title="一、主从复制"></a>一、主从复制</h2><h3 id="什么是主从复制"><a href="#什么是主从复制" class="headerlink" title="什么是主从复制"></a>什么是主从复制</h3><p>我们正常在项目中对<code>redis</code>进行应用，一般都不会是单点的。因为，单点的宕机即不可用，不能保证可用性。另外，单点<code>redis</code>读写指令都会打到同一个服务里面，也会影响性能。在通常的应用中，对<code>redis</code>的读操作远远多于写操作，所以，我们一般会选择“一主多从”的集群策略。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d40150a311ac6baf.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_ha.jpg"></p><ul><li>主中的数据有两个副本（<code>replication</code>）即从<code>redis1</code>和从<code>redis2</code>，即使一台服务器宕机其它两台服务也可以继续提供服务。</li><li>主中的数据和从上的数据保持实时同步，当主写入数据时通过主从复制机制会复制到两个从服务上。</li><li>只有一个主<code>redis</code>，可以有多个从 <code>redis</code>。</li><li>主从复制不会阻塞<code>master</code>，在同步数据时，<code>master</code>可以继续处理<code>client</code>请求。</li></ul><p>一个可以即是主又是从，如下图：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e9183477493b1ea0.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_ha2.jpg"></p><h3 id="主从复制过程"><a href="#主从复制过程" class="headerlink" title="主从复制过程"></a>主从复制过程</h3><p>一般当<code>slave</code>第一次启动连接<code>master</code>，或者“被认为是第一次连接”，是主从采用全量复制。全量复制的执行流程如下：</p><ol start="0"><li><code>slave redis</code>启动. 会从<code>redis.conf</code>中读取<code>master ip</code>和<code>host</code>。</li><li>定时任务每秒检查是否有新的<code>mater</code>需要连接，如果发现就与<code>master</code>建立<code>socket</code>连接。</li><li><code>slave</code>发送<code>ping</code>指令到<code>mater</code>。</li><li>如果<code>mater</code>配置<code>require pass</code>，<code>slave</code>需要发送认证给<code>master</code>。</li><li><code>Salve</code>会发送<code>sync</code>命令到<code>Master</code>。</li><li><code>Master</code>启动一个后台进程，将<code>Redis</code>中的数据快照<code>rdb</code>保存到文件中。</li><li>启动后台进程的同时，<code>Master</code>会将保存数据快照期间接收到的写命令缓存起来。</li><li><code>Master</code>完成写文件操作后，将<code>rdb</code>发送给<code>Salve</code>。</li><li><code>Salve</code>将<code>rdb</code>保存到磁盘上，然后加载<code>rdb</code>到<code>redis</code>内存中。</li><li>当<code>Salve</code>完成数据快照的恢复后，<code>aster</code>将这期间收集的写命令发送给<code>Salve</code>端。</li><li>后续<code>Master</code>收集到的写命令都会通过之前建立的连接. 增量发送给<code>salve</code>端。</li></ol><p>调用流程图如下：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-015172e80a36cbf8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_ha3.png"></p><h4 id="增量复制"><a href="#增量复制" class="headerlink" title="增量复制"></a>增量复制</h4><p>当<code>slave</code>节点与<code>master</code>全量同步后，<code>master</code>节点上数据再次发生更新，就会触发增量复制。</p><p>当我们在 <code>master</code> 服务器增减数据的时候，就会触发 <code>replicationFeedSalves()</code>函数，接下来在 <code>Master</code> 服务器上调用的每一个命令都会使用<code>replicationFeedSlaves()</code> 函数来同步到<code>Slave</code>服务器。当然，在执行此函数之前<code>master</code>服务器会判断用户执行的命令是否有数据更新，如果有数据更新并且<code>slave</code>服务器不为空，才会执行此函数，函数主要的工作就是把用户执行的命令发送到所有的 <code>slave</code>服务器，让<code>slave</code>服务器执行。<br>流程如下图：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-afa5a277af081522.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_ha5.png"></p><h4 id="断点续传（continue-replication）"><a href="#断点续传（continue-replication）" class="headerlink" title="断点续传（continue replication）"></a>断点续传（continue replication）</h4><p>断点续传或者说是断点恢复复制，也就是说 slave 因为某种原因与<code>master</code>断开连接了一段时间，然后又与<code>master</code>发生重连。<code>redis2.8</code>以后对于这种场景进行了优化，开始加入了<code>PSYNC</code>同步策略。这种策略性能一定是大于全量复制的。</p><ol><li>从服务器向主服务器发送<code>PSYNC</code>命令，携带主服务器的<code>runid</code>和复制偏移量；</li><li>主服务器验证<code>runid</code>和自身<code>runid</code>是否一致，如不一致，则进行全量复制；</li><li>主服务器验证复制偏移量是否在积压缓冲区内，如不在，则进行全量复制；</li><li>如都验证通过，则主服务器将保持在积压区内的偏移量后的所有数据发送给从服务器，主从服务器再次回到一致状态。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5e4cccf724c0171d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_ha6.png"></p><h4 id="PSYNC-核心参数"><a href="#PSYNC-核心参数" class="headerlink" title="PSYNC 核心参数"></a>PSYNC 核心参数</h4><p>介绍一下，断点续传的几个核心参数，<code>offset</code>、<code>backlog</code>、<code>runid</code>。这三个参数在 PSYNC 中起到了至关重要的作用，下面我们来一一介绍一下。</p><ul><li><p><code>offet</code>复制偏移量 , <code>offset</code>是用来记录<code>master</code>和<code>lslave</code>某个时段的数据版本状态的，<code>slave</code>每秒会向<code>master</code>上报<code>offset</code>，<code>master</code>保存下来，当触发 PSYNC 时再拿来和<code>master</code>的<code>offset</code>数据作对比。说白了，它就是记录数据在某一时刻的快照，用来对比 master 和 slave 数据差异用的。</p></li><li><p><code>backlog</code>积压缓冲区</p><ol><li>这个也是一个非常核心的参数，它默认大小为<code>1mb</code>，复制积压缓冲区是由<code>Master</code>维护的一个固定长度的<code>FIFO</code>队列，它的作用是缓存已经传播出去的命令。当<code>Master</code>进行命令传播时，不仅将命令发送给所有<code>Slave</code>，还会将命令写入到复制积压缓冲区里面。</li><li>全量复制的时候，<code>master</code>的数据更新（读写操作，主动过期删除等）会临时存放在<code>backlog</code>中待全量复制完成后增量发到slave，必须为此保留足够的空间。</li><li>断点续传时，<code>backlog</code>会存下<code>slave</code>断开连接后，<code>master</code>变更的数据。当然由于它大小有限制，而且先进先出特性，所以达到缓冲大小后会弹出老数据。这样，就可以把它作为一个衡量执行<code>sync</code>还是<code>psync</code>的一个标准<code>（backlog = offset : 部分同步，backlog &lt; offset 执行全量同步）</code>。一般为了避免，大规模全量复制，我们都会给一个恰当的值，根据公式<code>second*write_size_per_second</code>来估算：其中<code>second</code>为从服务器断线后重新连接上主服务器所需的平均时间（以秒计算）；而<code>write_size_per_second</code>则是主服务器平均每秒产生的写命令数据量（协议格式的写命令的长度总和）；</li></ol></li><li><p>master run id, <code>master</code>唯一标示，<code>slave</code>连接<code>master</code>时会传<code>runid</code>，<code>master</code>每次重启<code>runid</code>都发生变化，当<code>slave</code>发现<code>master</code>的<code>runid</code>变化时都会触发全量复制流程。</p></li></ul><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p>优点：</p><ol><li>高可靠性：一方面，采用双机主备架构，能够在主库出现故障时自动进行主备切换，从库提升为主库提供服务，保证服务平稳运行；另一方面，开启数据持久化功能和配置合理的备份策略，能有效的解决数据误操作和数据异常丢失的问题；</li><li>读写分离策略：从节点可以扩展主库节点的读能力，有效应对大并发量的读操作。</li></ol><p>缺点：</p><ol><li>故障恢复复杂，如果没有<code>RedisHA</code>系统（需要开发），当主库节点出现故障时，需要手动将一个从节点晋升为主节点，同时需要通知业务方变更配置，并且需要让其它从库节点去复制新主库节点，整个过程需要人为干预，比较繁琐；</li><li>主库的写能力受到单机的限制，可以考虑分片；</li><li>主库的存储能力受到单机的限制，可以考虑<code>Pika</code>；</li><li>原生复制的弊端在早期的版本中也会比较突出，如：<code>Redis</code>复制中断后，<code>Slave</code>会发起<code>psync</code>，此时如果同步不成功，则会进行全量同步，主库执行全量备份的同时可能会造成毫秒或秒级的卡顿；又由于<code>COW</code>机制，导致极端情况下的主库内存溢出，程序异常退出或宕机；主库节点生成备份文件导致服务器磁盘<code>IO</code>和<code>CPU</code>（压缩）资源消耗；发送数<code>GB</code>大小的备份文件导致服务器出口带宽暴增，阻塞请求，建议升级到最新版本。</li></ol><h2 id="二、Redis-哨兵-Redis-Sentinel"><a href="#二、Redis-哨兵-Redis-Sentinel" class="headerlink" title="二、Redis 哨兵 (Redis Sentinel)"></a>二、Redis 哨兵 (Redis Sentinel)</h2><h3 id="什么是哨兵"><a href="#什么是哨兵" class="headerlink" title="什么是哨兵"></a>什么是哨兵</h3><p><code>Redis Sentinel</code> 是一个分布式架构，其中包含若干个 <code>Sentinel</code> 节点和 <code>Redis</code> 数据节点，每个 <code>Sentinel</code> 节点会对数据节点和其余 <code>Sentinel</code> 节点进行监控，当它发现节点不可达时，会对节点做下线标识。如果被标识的是主节点，它还会和其他 <code>Sentinel</code> 节点进行“协商”，当大多数 <code>Sentinel</code> 节点都认为主节点不可达时，它们会选举出一个 <code>Sentinel</code> 节点来完成自动故障转移的工作，同时会将这个变化实时通知给 <code>Redis</code> 应用方。整个过程完全是自动的，不需要人工来介入，所以这套方案很有效地解决了 <code>Redis</code> 的高可用问题。</p><p>Redis 2.8 版开始正式提供名为<code>Sentinel</code>的主从切换方案，<code>Sentinel</code>用于管理多个<code>Redis</code>服务器实例，主要负责三个方面的任务：</p><ol><li>监控（<code>Monitoring</code>）： <code>Sentinel</code> 会不断地检查你的主服务器和从服务器是否运作正常。</li><li>提醒（<code>Notification</code>）： 当被监控的某个 <code>Redis</code> 服务器出现问题时， <code>Sentinel</code> 可以通过 <code>API</code> 向管理员或者其他应用程序发送通知。</li><li>自动故障迁移（<code>Automatic failover</code>）： 当一个主服务器不能正常工作时， <code>Sentinel</code> 会开始一次自动故障迁移操作， 它会将失效主服务器的其中一个从服务器升级为新的主服务器， 并让失效主服务器的其他从服务器改为复制新的主服务器； 当客户端试图连接失效的主服务器时， 集群也会向客户端返回新主服务器的地址， 使得集群可以使用新主服务器代替失效服务器。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-771ecf18a316edfa.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sentinel1.jpg"></p><p>哨兵(<code>sentinel</code>)&nbsp; 是一个分布式系统,你可以在一个架构中运行多个哨兵(<code>sentinel</code>)&nbsp; 进程,这些进程使用流言协议(<code>gossip protocols</code>)来接收关于<code>Master</code>是否下线的信息,并使用投票协议(<code>agreement protocols</code>)来决定是否执行自动故障迁移,以及选择哪个<code>Slave</code>作为新的<code>Master</code>.</p><p>每个哨兵(<code>sentinel</code>)&nbsp; 会向其它哨兵(<code>sentinel</code>)、<code>master</code>、<code>slave</code>定时发送消息,以确认对方是否”活”着,如果发现对方在指定时间(可配置)内未回应,则暂时认为对方已挂(所谓的”主观认为宕机” <code>Subjective Down</code>,简称<code>sdown</code>).</p><p>若“哨兵群”中的多数<code>sentinel</code>,都报告某一<code>master</code>没响应,系统才认为该<code>master</code>“彻底死亡”(即:客观上的真正<code>down</code>机,<code>Objective Down</code>,简称<code>odown</code>),通过一定的<code>vote</code>算法,从剩下的<code>slave</code>节点中,选一台提升为<code>master</code>,然后自动修改相关配置.</p><p>虽然哨兵(<code>sentinel</code>)&nbsp; 释出为一个单独的可执行文件 &nbsp;<code>redis-sentinel</code>&nbsp;,但实际上它只是一个运行在特殊模式下的 &nbsp;<code>Redis</code>&nbsp; 服务器，你可以在启动一个普通 &nbsp;<code>Redis</code>&nbsp; 服务器时通过给定 &nbsp;<code>--sentinel</code>&nbsp; 选项来启动哨兵(<code>sentinel</code>).</p><h3 id="基本的故障转移流程"><a href="#基本的故障转移流程" class="headerlink" title="基本的故障转移流程"></a>基本的故障转移流程</h3><ol><li><p>主节点出现故障，此时两个从节点与主节点失去连接，主从复制失败。<br><img src="https://upload-images.jianshu.io/upload_images/12321605-cc3816b327b408ef.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sentinel2.jpg"></p></li><li><p>每个 <code>Sentinel</code> 节点通过定期监控发现主节点出现了故障<br><img src="https://upload-images.jianshu.io/upload_images/12321605-40e38f9e7962d445.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sentinel3.jpg"></p></li><li><p>多个 <code>Sentinel</code> 节点对主节点的故障达成一致会选举出其中一个节点作为领导者负责故障转移。<br><img src="https://upload-images.jianshu.io/upload_images/12321605-0715f5b0582e6727.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sentinel4.jpg"></p></li><li><p><code>Sentinel</code> 领导者节点执行了故障转移，整个过程基本是跟我们手动调整一致的，只不过是自动化完成的。<br><img src="https://upload-images.jianshu.io/upload_images/12321605-e3d93d05e45854fc.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sentinel5.jpg"></p></li><li><p>故障转移后整个 <code>Redis Sentinel</code> 的结构,重新选举了新的主节点。<br><img src="https://upload-images.jianshu.io/upload_images/12321605-a897e6d46c2719d3.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sentinel6.jpg"></p></li></ol><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ol><li><p><code>Sentinel</code> 节点不应该部署在一台物理“机器”上。<br>这里特意强调物理机是因为一台物理机做成了若干虚拟机或者现今比较流行的容器，它们虽然有不同的 <code>IP</code> 地址，但实际上它们都是同一台物理机，同一台物理机意味着如果这台机器有什么硬件故障，所有的虚拟机都会受到影响，为了实现 <code>Sentinel</code> 节点集合真正的高可用，请勿将 <code>Sentinel</code> 节点部署在同一台物理机器上。</p></li><li><p>部署至少三个且奇数个的 <code>Sentinel</code> 节点。</p></li><li><p>个以上是通过增加 <code>Sentinel</code> 节点的个数提高对于故障判定的准确性，因为领导者选举需要至少一半加 1 个节点，奇数个节点可以在满足该条件的基础上节省一个节点。</p></li></ol><h3 id="优缺点-1"><a href="#优缺点-1" class="headerlink" title="优缺点"></a>优缺点</h3><p>优点：</p><ol><li><code>Redis Sentinel</code>集群部署简单；</li><li>能够解决<code>Redis</code>主从模式下的高可用切换问题；</li><li>很方便实现<code>Redis</code>数据节点的线形扩展，轻松突破<code>Redis</code>自身单线程瓶颈，可极大满足<code>Redis</code>大容量或高性能的业务需求；</li><li>可以实现一套<code>Sentinel</code>监控一组<code>Redis</code>数据节点或多组数据节点。</li></ol><p>缺点：</p><ol><li>部署相对<code>Redis</code>主从模式要复杂一些，原理理解更繁琐；</li><li>资源浪费，<code>Redis</code>数据节点中<code>slave</code>节点作为备份节点不提供服务；</li><li><code>Redis Sentinel</code>主要是针对<code>Redis</code>数据节点中的主节点的高可用切换，对<code>Redis</code>的数据节点做失败判定分为主观下线和客观下线两种，对于<code>Redis</code>的从节点有对节点做主观下线操作，并不执行故障转移。</li><li>不能解决读写分离问题，实现起来相对复杂。</li></ol><h2 id="三、Redis-集群-（Redis-Cluster）"><a href="#三、Redis-集群-（Redis-Cluster）" class="headerlink" title="三、Redis 集群 （Redis Cluster）"></a>三、Redis 集群 （Redis Cluster）</h2><h3 id="什么是-Redis-集群"><a href="#什么是-Redis-集群" class="headerlink" title="什么是 Redis 集群"></a>什么是 Redis 集群</h3><p><code>Redis</code> 集群是一个分布式（<code>distributed</code>）、容错（<code>fault-tolerant</code>）的 <code>Redis</code> 实现， 集群可以使用的功能是普通单机 <code>Redis</code> 所能使用的功能的一个子集（<code>subset</code>）。</p><p><code>Redis</code> 集群中不存在中心（<code>central</code>）节点或者代理（<code>proxy</code>）节点， 集群的其中一个主要设计目标是达到线性可扩展性（<code>linear scalability</code>）。</p><p><code>Redis</code> 集群提供了一种运行 <code>Redis</code> 的方式，其中数据在多个 <code>Redis</code> 节点间自动分区。<code>Redis</code> 集群还在分区期间提供一定程度的可用性，即在实际情况下能够在某些节点发生故障或无法通信时继续运行。但是，如果发生较大故障（例如，大多数主站不可用时），集群会停止运行。</p><h3 id="集群的模型"><a href="#集群的模型" class="headerlink" title="集群的模型"></a>集群的模型</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5c73401d2a284189.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_cluster1.png"></p><ol><li>所有的节点通过服务通道直接相连，各个节点之间通过二进制协议优化传输的速度和带宽。</li><li>客户端与节点之间通过 ascii 协议进行通信</li><li>客户端与节点直连，不需要中间 Proxy 层。客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可。</li><li>尽管这些节点彼此相连，功能相同，但是仍然分为两种节点：master 和 slave。</li></ol><h3 id="各个节点之间都传递了什么信息"><a href="#各个节点之间都传递了什么信息" class="headerlink" title="各个节点之间都传递了什么信息"></a>各个节点之间都传递了什么信息</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a94db6b07f492f8c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_cluster2.png"></p><p>通过上面的图我们可以知道各个节点之间通过 PING-PONG 机制通信，下面是一段关于 PING-PONG 机制的会话”内容”。</p><pre><code>节点M：PING，嘿，朋友你好吗？我是 XYZ 哈希槽的 master ，配置信息是 FF89X1JK。节点N：PONG，我很好朋友，我也是 XYZ 哈希槽的 master ，配置信息是 FF89X1JK。节点M：我这里有一些关于我最近收到的其他节点的信息 ，A 节点回复了我的 PING 消息，我认为 A 节点是正常的。B 没有回应我的消息，我猜它现在可能出问题了，但是我需要一些 ACK(Acknowledgement) 消息来确认。节点N：我也想给你分享一些关于其它节点的信息，C 和 D 节点在指定的时间内回应了我， 我认为它们都是正常的，但是 B 也没有回应我，我觉得它现在可能已经挂掉了。</code></pre><p>每个节点会向集群中的其他节点发送节点状态信息，如果某个节点挂掉停止了服务，那么会执行投票容错机制，关于这个机制，会在下面讲到。</p><h3 id="Hash-槽-slot"><a href="#Hash-槽-slot" class="headerlink" title="Hash 槽(slot)"></a>Hash 槽(slot)</h3><p>Redis 集群不使用一致的散列，而是一种不同的分片形式，其中每个键在概念上都是我们称之为散列槽的一部分，目的是使数据均匀的存储在诸多节点中。这点类似于 HashMap 中的桶(bucket)。<br><img src="https://upload-images.jianshu.io/upload_images/12321605-95ffb2dd63839402.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_cluster3.png"></p><p>Redis 集群中有 16384 个散列槽，为了计算给定密钥的散列槽，Redis 对 key 采用 CRC16 算法，以下是负责将键映射到槽的算法：</p><pre><code>slot = crc16(key) mod NUMER_SLOTS</code></pre><p>例如，你可能有 3 个节点，其中一个集群：</p><p>节点 A 包含从 0 到 5500 的散列槽。<br>节点 B 包含从 5501 到 11000 的散列槽。<br>节点 C 包含 从 11001 到 16383 的散列槽。<br>Hash 槽可以轻松地添加和删除集群中的节点。例如，如果我想添加一个新节点 D，我需要将节点 A，B，C 中的一些散列槽移动到 D。同样，如果我想从节点 A 中删除节点 A，可以只移动由 A 服务的散列槽到 B 和 C。当节点 A 为空时，可以将它从群集中彻底删除。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2b9fe948264b0e40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_cluster4.png"></p><ol><li><p>对象保存到 Redis 之前先经过 CRC16 哈希到一个指定的 Node 上，例如 Object4 最终 Hash 到了 Node1 上。</p></li><li><p>每个 Node 被平均分配了一个 Slot 段，对应着 0-16384，Slot 不能重复也不能缺失，否则会导致对象重复存储或无法存储。</p></li><li><p>Node 之间也互相监听，一旦有 Node 退出或者加入，会按照 Slot 为单位做数据的迁移。例如 Node1 如果掉线了，0-5640 这些 Slot 将会平均分摊到 Node2 和 Node3 上,由于 Node2 和 Node3 本身维护的 Slot 还会在自己身上不会被重新分配，所以迁移过程中不会影响到 5641-16384Slot 段的使用。</p></li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0a10c4294c3c4761.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_cluster5.png"></p><p>想扩展并发读就添加 Slaver，想扩展并发写就添加 Master，想扩容也就是添加 Master，任何一个 Slaver 或者几个 Master 挂了都不会是灾难性的故障。</p><p>简单总结下哈希 Slot 的优缺点：</p><p>缺点：每个 Node 承担着互相监听、高并发数据写入、高并发数据读出，工作任务繁重</p><p>优点：将 Redis 的写操作分摊到了多个节点上，提高写的并发能力，扩容简单。</p><h3 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c41dbad3d2a4360c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_cluster6.png"></p><ul><li>集群中的节点不断的 <code>PING</code> 其他的节点，当一个节点向另一个节点发送 <code>PING</code> 命令， 但是目标节点未能在给定的时限内回复， 那么发送命令的节点会将目标节点标记为 <code>PFAIL</code>(<code>possible failure</code>，可能已失效)。</li><li>当节点接收到其他节点发来的信息时， 它会记下那些被其他节点标记为失效的节点。 这被称为失效报告（<code>failure report</code>）。</li><li>如果节点已经将某个节点标记为 <code>PFAIL</code> ， 并且根据节点所收到的失效报告显式， 集群中的大部分其他主节点也认为那个节点进入了失效状态， 那么节点会将那个失效节点的状态标记为 <code>FAIL</code> 。</li><li>一旦某个节点被标记为 <code>FAIL</code> ， 关于这个节点已失效的信息就会被广播到整个集群， 所有接收到这条信息的节点都会将失效节点标记为 <code>FAIL</code> 。</li></ul><p>简单来说， 一个节点要将另一个节点标记为失效， 必须先询问其他节点的意见， 并且得到大部分主节点的同意才行。</p><ul><li>如果被标记为 <code>FAIL</code> 的是从节点， 那么当这个节点重新上线时， <code>FAIL</code> 标记就会被移除。 一个从节点是否处于 <code>FAIL</code> 状态， 决定了这个从节点在有需要时能否被提升为主节点。</li><li>如果一个主节点被打上 <code>FAIL</code> 标记之后， 经过了节点超时时限的四倍时间， 再加上十秒钟之后， 针对这个主节点的槽的故障转移操作仍未完成， 并且这个主节点已经重新上线的话， 那么移除对这个节点的 <code>FAIL</code> 标记。在不符合上面的条件后，一旦某个主节点进入 <code>FAIL</code> 状态， 如果这个主节点有一个或多个从节点存在， 那么其中一个从节点会被升级为新的主节点， 而其他从节点则会开始对这个新的主节点进行复制。</li></ul><h3 id="优缺点-2"><a href="#优缺点-2" class="headerlink" title="优缺点"></a>优缺点</h3><p>优点：</p><ol><li>无中心架构；</li><li>数据按照<code>slot</code>存储分布在多个节点，节点间数据共享，可动态调整数据分布；</li><li>可扩展性：可线性扩展到 1000 多个节点，节点可动态添加或删除；</li><li>高可用性：部分节点不可用时，集群仍可用。通过增加<code>Slave</code>做<code>standby</code>数据副本，能够实现故障自动<code>failover</code>，节点之间通过<code>gossip</code>协议交换状态信息，用投票机制完成<code>Slave</code>到<code>Master</code>的角色提升；</li><li>降低运维成本，提高系统的扩展性和可用性。</li></ol><p>缺点：</p><ol><li><code>Client</code>实现复杂，驱动要求实现<code>Smart Client</code>，缓存<code>slots mapping</code>信息并及时更新，提高了开发难度，客户端的不成熟影响业务的稳定性。目前仅<code>JedisCluster</code>相对成熟，异常处理部分还不完善，比如常见的<code>“max redirect exception”</code>。</li><li>节点会因为某些原因发生阻塞（阻塞时间大于<code>clutser-node-timeout</code>），被判断下线，这种<code>failover</code>是没有必要的。</li><li>数据通过异步复制，不保证数据的强一致性。</li><li>多个业务使用同一套集群时，无法根据统计区分冷热数据，资源隔离性较差，容易出现相互影响的情况。</li><li><code>Slave</code>在集群中充当“冷备”，不能缓解读压力，当然可以通过<code>SDK</code>的合理设计来提高<code>Slave</code>资源的利用率。</li><li><code>Key</code>批量操作限制，如使用<code>mset</code>、<code>mget</code>目前只支持具有相同<code>slot</code>值的<code>Key</code>执行批量操作。对于映射为不同<code>slot</code>值的<code>Key</code>由于<code>Keys</code>不支持跨<code>slot</code>查询，所以执行<code>mset</code>、<code>mget</code>、<code>sunion</code>等操作支持不友好。</li><li><code>Key</code>事务操作支持有限，只支持多<code>key</code>在同一节点上的事务操作，当多个<code>Key</code>分布于不同的节点上时无法使用事务功能。</li><li><code>Key</code>作为数据分区的最小粒度，不能将一个很大的键值对象如<code>hash</code>、<code>list</code>等映射到不同的节点。</li><li>不支持多数据库空间，单机下的<code>redis</code>可以支持到 16 个数据库，集群模式下只能使用 1 个数据库空间，即 db 0。</li><li>复制结构只支持一层，从节点只能复制主节点，不支持嵌套树状复制结构。</li><li>避免产生<code>hot-key</code>，导致主库节点成为系统的短板。</li><li>避免产生<code>big-key</code>，导致网卡撑爆、慢查询等。</li><li>重试时间应该大于<code>cluster-node-time</code>时间。</li><li><code>Redis Cluster</code>不建议使用<code>pipeline</code>和<code>multi-keys</code>操作，减少<code>max redirect</code>产生的场景。</li></ol><h2 id="四、Redis-自研高可用架构"><a href="#四、Redis-自研高可用架构" class="headerlink" title="四、Redis 自研高可用架构"></a>四、Redis 自研高可用架构</h2><p>Redis 自研的高可用解决方案，主要体现在配置中心、故障探测和 failover 的处理机制上，通常需要根据企业业务的实际线上环境来定制化。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b82013e6da242531.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_custome1.png"></p><p>优点：</p><ul><li>高可靠性、高可用性；</li><li>自主可控性高；</li><li>贴切业务实际需求，可缩性好，兼容性好。</li></ul><p>缺点：</p><ul><li>实现复杂，开发成本高；</li><li>需要建立配套的周边设施，如监控，域名服务，存储元数据信息的数据库等；</li><li>维护成本高。</li></ul><h2 id="五、Redis-代理中间件"><a href="#五、Redis-代理中间件" class="headerlink" title="五、Redis 代理中间件"></a>五、Redis 代理中间件</h2><h3 id="Codis"><a href="#Codis" class="headerlink" title="Codis"></a>Codis</h3><h4 id="什么是-Codis"><a href="#什么是-Codis" class="headerlink" title="什么是 Codis"></a>什么是 Codis</h4><p><code>Codis</code> 是一个代理中间件，用的是 <code>GO</code> 语言开发的，如下图，<code>Codis</code> 在系统的位置是这样的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7742a644c16a2db6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_codis0.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a778dd9930066ecf.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="codis1.jpg"></p><p><code>Codis</code>分为四个部分，分别是<code>Codis Proxy</code> (<code>codis-proxy</code>)、<code>Codis Dashboard</code> (<code>codis-config</code>)、<code>Codis Redis</code> (<code>codis-server</code>)和<code>ZooKeeper/Etcd</code>.<br><code>Codis</code>就是起着一个中间代理的作用，能够把所有的<code>Redis</code>实例当成一个来使用，在客户端操作着<code>SDK</code>的时候和操作<code>Redis</code>的时候是一样的，没有差别。<br>因为<code>Codis</code>是一个无状态的，所以可以增加多个<code>Codis</code>来提升<code>QPS</code>,同时也可以起着容灾的作用。</p><h4 id="Codis-分片原理"><a href="#Codis-分片原理" class="headerlink" title="Codis 分片原理"></a>Codis 分片原理</h4><p>在<code>Codis</code>中，<code>Codis</code>会把所有的<code>key</code>分成 1024 个槽，这 1024 个槽对应着的就是<code>Redis</code>的集群，这个在<code>Codis</code>中是会在内存中维护着这 1024 个槽与<code>Redis</code>实例的映射关系。这个槽是可以配置，可以设置成 2048 或者是 4096 个。看你的<code>Redis</code>的节点数量有多少，偏多的话，可以设置槽多一些。<br><code>Codis</code>中<code>key</code>的分配算法，先是把<code>key</code>进行<code>CRC32</code> 后，得到一个 32 位的数字，然后再<code>hash%1024</code>后得到一个余数，这个值就是这个<code>key</code>对应着的槽，这槽后面对应着的就是<code>redis</code>的实例。(可以思考一下，为什么 Codis 很多命令行不支持，例如 KEYS 操作)</p><blockquote><p><code>CRC32</code>:<code>CRC</code>本身是“冗余校验码”的意思，<code>CRC32</code>则表示会产生一个<code>32bit</code>（8 位十六进制数）的校验值。由于<code>CRC32</code>产生校验值时源数据块的每一个<code>bit</code>（位）都参与了计算，所以数据块中即使只有一位发生了变化，也会得到不同的<code>CRC32</code>值。</p></blockquote><pre><code>Codis中Key的算法代码如下//Codis中Key的算法hash = crc32(command.key)slot_index = hash % 1024redis = slots[slot_index].redisredis.do(command)</code></pre><h4 id="Codis-之间的槽位同步"><a href="#Codis-之间的槽位同步" class="headerlink" title="Codis 之间的槽位同步"></a>Codis 之间的槽位同步</h4><blockquote><p>思考一个问题：如果这个 Codis 节点只在自己的内存里面维护着槽位与实例的关系,那么它的槽位信息怎么在多个实例间同步呢？</p></blockquote><p>Codis 把这个工作交给了 ZooKeeper 来管理，当 Codis 的 Codis Dashbord 改变槽位的信息的时候，其他的 Codis 节点会监听到 ZooKeeper 的槽位变化，会及时同步过来。如图：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c2088ea1947ad811.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="codis2.jpg"></p><h4 id="Codis-中的扩容"><a href="#Codis-中的扩容" class="headerlink" title="Codis 中的扩容"></a>Codis 中的扩容</h4><blockquote><p>思考一个问题：在 Codis 中增加了 Redis 节点后,槽位的信息怎么变化，原来的 key 怎么迁移和分配？如果在扩容的时候，这个时候有新的 key 进来，Codis 的处理策略是怎么样的？</p></blockquote><p>因为<code>Codis</code>是一个代理中间件，所以这个当需要扩容<code>Redis</code>实例的时候，可以直接增加<code>redis</code>节点。在槽位分配的时候，可以手动指定<code>Codis Dashbord</code>来为新增的节点来分配特定的槽位。</p><p>在<code>Codis</code>中实现了自定义的扫描指令<code>SLOTSSCAN</code>，可以扫描指定的<code>slot</code>下的所有的<code>key</code>，将这些<code>key</code>迁移到新的<code>Redis</code>的节点中(话外语：这个是<code>Codis</code>定制化的其中一个好处)。</p><p>首先，在迁移的时候，会在原来的<code>Redis</code>节点和新的<code>Redis</code>里都保存着迁移的槽位信息，在迁移的过程中，如果有<code>key</code>打进将要迁移或者正在迁移的旧槽位的时候，这个时候<code>Codis</code>的处理机制是，先是将这个<code>key</code>强制迁移到新的<code>Redis</code>节点中，然后再告诉<code>Codis</code>,下次如果有新的<code>key</code>的打在这个槽位中的话，那么转发到新的节点。代码策略如下：</p><pre><code>slot_index = crc32(command.key) % 1024if slot_index in migrating_slots:    do_migrate_key(command.key)  # 强制执行迁移    redis = slots[slot_index].new_rediselse:    redis = slots[slot_index].redisredis.do(command)</code></pre><h4 id="自动均衡策略"><a href="#自动均衡策略" class="headerlink" title="自动均衡策略"></a>自动均衡策略</h4><p>面对着上面讲的迁移策略，如果有成千上万个节点新增进来，都需要我们手动去迁移吗？那岂不是得累死啊。当然，<code>Codis</code>也是考虑到了这一点，所以提供了自动均衡策略。自动均衡策略是这样的，<code>Codis</code> 会在机器空闲的时候，观察<code>Redis</code>中的实例对应着的<code>slot</code>数，如果不平衡的话就会自动进行迁移。</p><h4 id="Codis-的牺牲"><a href="#Codis-的牺牲" class="headerlink" title="Codis 的牺牲"></a>Codis 的牺牲</h4><p>因为<code>Codis</code>在<code>Redis</code>的基础上的改造，所以在<code>Codis</code>上是不支持事务的，同时也会有一些命令行不支持，在官方的文档上有(<code>Codis</code>不支持的命令)<br>官方的建议是单个集合的总容量不要超过 1M,否则在迁移的时候会有卡顿感。在<code>Codis</code>中，增加了<code>proxy</code>来当中转层，所以在网络开销上，是会比单个的<code>Redis</code>节点的性能有所下降的，所以这部分会有些的性能消耗。可以增加<code>proxy</code>的数量来避免掉这块的性能损耗。</p><h4 id="MGET-的过程"><a href="#MGET-的过程" class="headerlink" title="MGET 的过程"></a>MGET 的过程</h4><blockquote><p>思考一个问题：如果熟悉 Redis 中的 MGET、MSET 和 MSETNX 命令的话，就会知道这三个命令都是原子性的命令。但是，为什么 Codis 支持 MGET 和 MSET,却不支持 MSETNX 命令呢？</p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a0862d85c6170458.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="codis3.jpg"></p><p>原因如下:</p><p>在<code>Codis</code>中的<code>MGET</code>命令的原理是这样的，先是在<code>Redis</code>中的各个实例里获取到符合的<code>key</code>，然后再汇总到<code>Codis</code>中，如果是<code>MSETNX</code>的话，因为<code>key</code>可能存在在多个<code>Redis</code>的实例中，如果某个实例的设值成功，而另一个实例的设值不成功，从本质上讲这是不成功的，但是分布在多个实例中的<code>Redis</code>是没有回滚机制的，所以会产生脏数据，所以 MSETNX 就是不能支持了。</p><h4 id="Codis-集群总结"><a href="#Codis-集群总结" class="headerlink" title="Codis 集群总结"></a>Codis 集群总结</h4><ul><li><code>Codis</code>是一个代理中间件，通过内存保存着槽位和实例节点之间的映射关系,槽位间的信息同步交给<code>ZooKeeper</code>来管理。</li><li>不支持事务和官方的某些命令，原因就是分布多个的<code>Redis</code>实例没有回滚机制和<code>WAL</code>,所以是不支持的.</li></ul><h3 id="Twemproxy-代理"><a href="#Twemproxy-代理" class="headerlink" title="Twemproxy 代理"></a>Twemproxy 代理</h3><h4 id="什么是-Twemproxy"><a href="#什么是-Twemproxy" class="headerlink" title="什么是 Twemproxy"></a>什么是 Twemproxy</h4><p>Twemproxy 也叫 nutcraker。是 Twtter 开源的一个 Redis 和 Memcache 代理服务器，主要用于管理 Redis 和 Memcached 集群，减少与 Cache 服务器直接连接的数量。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b33bfe6e772aefb5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_twemproxy1.png"></p><p>基于 twemproxy 的高可用架构图</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-da2fbbbc086ebc77.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_twemproxy2.png"></p><h3 id="Twemproxy-特性"><a href="#Twemproxy-特性" class="headerlink" title="Twemproxy 特性"></a>Twemproxy 特性</h3><ul><li>轻量级、速度快</li><li>保持长连接</li><li>减少了直接与缓存服务器连接的连接数量</li><li>使用 pipelining 处理请求和响应</li><li>支持代理到多台服务器上</li><li>同时支持多个服务器池</li><li>自动分片数据到多个服务器上</li><li>实现完整的 memcached 的 ASCII 和再分配协议</li><li>通过 yaml 文件配置服务器池</li><li>支持多个哈希模式，包括一致性哈希和分布</li><li>能够配置删除故障节点</li><li>可以通过端口监控状态</li><li>支持 linux, *bsd,os x 和 solaris</li></ul><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="http://blog.itpub.net/31545684/viewspace-2213629/">Redis 主从复制看这篇就够了</a></p><p><a href="https://blog.csdn.net/zb6340430/article/details/80914681">（一）redis 主从原理及流程——主从复制</a></p><p><a href="https://yq.aliyun.com/articles/626532">这可能是目前最全的 Redis 高可用技术解决方案总结</a></p><p><a href="https://segmentfault.com/a/1190000018278099?utm_source=tag-newest">Redis 哨兵机制</a></p><p><a href="https://blog.csdn.net/itcats_cn/article/details/82428716">redis 如何实现高可用【主从复制、哨兵机制】</a></p><p><a href="https://blog.csdn.net/codejas/article/details/79854953">理解 Redis 集群</a></p><p><a href="https://blog.csdn.net/yejingtao703/article/details/78484151">三张图秒懂 Redis 集群设计原理</a></p><p><a href="https://juejin.im/post/5c132b076fb9a04a08218eef">为什么大厂都喜欢用 Codis 来管理分布式集群？</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis 源码分析(七) ：skiplist</title>
      <link href="/2019/08/17/reids-source-code-7/"/>
      <url>/2019/08/17/reids-source-code-7/</url>
      
        <content type="html"><![CDATA[<h2 id="一、skiplist由来"><a href="#一、skiplist由来" class="headerlink" title="一、skiplist由来"></a>一、skiplist由来</h2><p><code>skiplist</code>本质上也是一种查找结构，用于解决算法中的查找问题（<code>Searching</code>），即根据给定的<code>key</code>，快速查到它所在的位置（或者对应的<code>value</code>）。</p><p>我们在《Redis内部数据结构详解》系列的第一篇中介绍dict的时候，曾经讨论过：一般查找问题的解法分为两个大类：一个是基于各种平衡树，一个是基于哈希表。但<code>skiplist</code>却比较特殊，它没法归属到这两大类里面。</p><p>这种数据结构是由<code>William Pugh</code>发明的，最早出现于他在1990年发表的论文《Skip Lists: A Probabilistic Alternative to Balanced Trees》。对细节感兴趣的同学可以下载论文原文来阅读。</p><p><code>skiplist</code>，顾名思义，首先它是一个<code>list</code>。实际上，它是在有序链表的基础上发展起来的。</p><p>我们先来看一个有序链表，如下图（最左侧的灰色节点表示一个空的头结点）：</p><p>在这样一个链表中，如果我们要查找某个数据，那么需要从头开始逐个进行比较，直到找到包含数据的那个节点，或者找到第一个比给定数据大的节点为止（没找到）。也就是说，时间复杂度为<code>O(n)</code>。同样，当我们要插入新数据的时候，也要经历同样的查找过程，从而确定插入位置。</p><p>假如我们每相邻两个节点增加一个指针，让指针指向下下个节点，如下图：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b9c9c68ae7d6e4ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="skip_list1.png"></p><p>这样所有新增加的指针连成了一个新的链表，但它包含的节点个数只有原来的一半（上图中是7, 19, 26）。现在当我们想查找数据的时候，可以先沿着这个新链表进行查找。当碰到比待查数据大的节点时，再回到原来的链表中进行查找。比如，我们想查找23，查找的路径是沿着下图中标红的指针所指向的方向进行的：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d3697e471bbe55df.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="skip_list2.png"></p><ul><li>23首先和7比较，再和19比较，比它们都大，继续向后比较。</li><li>但23和26比较的时候，比26要小，因此回到下面的链表（原链表），与22比较。</li><li>23比22要大，沿下面的指针继续向后和26比较。23比26小，说明待查数据23在原链表中不存在，而且它的插入位置应该在22和26之间。</li></ul><p>在这个查找过程中，由于新增加的指针，我们不再需要与链表中每个节点逐个进行比较了。需要比较的节点数大概只有原来的一半。</p><p>利用同样的方式，我们可以在上层新产生的链表上，继续为每相邻的两个节点增加一个指针，从而产生第三层链表。如下图：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-68e86d7421386af3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="skip_list3.png"></p><p><code>skiplist</code>正是受这种多层链表的想法的启发而设计出来的。实际上，按照上面生成链表的方式，上面每一层链表的节点个数，是下面一层的节点个数的一半，这样查找过程就非常类似于一个二分查找，使得查找的时间复杂度可以降低到<code>O(log n)</code>。</p><p><strong>但是，这种方法在插入数据的时候有很大的问题。新插入一个节点之后，就会打乱上下相邻两层链表上节点个数严格的2:1的对应关系。如果要维持这种对应关系，就必须把新插入的节点后面的所有节点（也包括新插入的节点）重新进行调整，这会让时间复杂度重新蜕化成O(n)。删除数据也有同样的问题。</strong></p><p><code>skiplist</code>为了避免这一问题，它不要求上下相邻两层链表之间的节点个数有严格的对应关系，而是为每个节点随机出一个层数<code>(level)</code>。比如，一个节点随机出的层数是3，那么就把它链入到第1层到第3层这三层链表中。为了表达清楚，下图展示了如何通过一步步的插入操作从而形成一个<code>skiplist</code>的过程：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-23e9cc403b8bb1bf.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="skip_list4.jpg"></p><p>从上面<code>skiplist</code>的创建和插入过程可以看出，每一个节点的层数<code>（level）</code>是随机出来的，而且新插入一个节点不会影响其它节点的层数。因此，插入操作只需要修改插入节点前后的指针，而不需要对很多节点都进行调整。这就降低了插入操作的复杂度。实际上，这是<code>skiplist</code>的一个很重要的特性，这让它在插入性能上明显优于平衡树的方案。这在后面我们还会提到。</p><p>根据上图中的<code>skiplist</code>结构，我们很容易理解这种数据结构的名字的由来。<code>skiplist</code>，翻译成中文，可以翻译成“跳表”或“跳跃表”，指的就是除了最下面第1层链表之外，它会产生若干层稀疏的链表，这些链表里面的指针故意跳过了一些节点（而且越高层的链表跳过的节点越多）。这就使得我们在查找数据的时候能够先在高层的链表中进行查找，然后逐层降低，最终降到第1层链表来精确地确定数据位置。在这个过程中，我们跳过了一些节点，从而也就加快了查找速度。</p><h2 id="二、skiplist性能和实现逻辑"><a href="#二、skiplist性能和实现逻辑" class="headerlink" title="二、skiplist性能和实现逻辑"></a>二、skiplist性能和实现逻辑</h2><h3 id="skiplist生成随机层数的方法"><a href="#skiplist生成随机层数的方法" class="headerlink" title="skiplist生成随机层数的方法"></a>skiplist生成随机层数的方法</h3><pre><code>// redis 5.0.2的客户端代码，redis 3.2.x版本最大Level是32#define ZSKIPLIST_MAXLEVEL 64 /* Should be enough for 2^64 elements */#define ZSKIPLIST_P 0.25      /* Skiplist P = 1/4 *//* Returns a random level for the new skiplist node we are going to create. * The return value of this function is between 1 and ZSKIPLIST_MAXLEVEL * (both inclusive), with a powerlaw-alike distribution where higher * levels are less likely to be returned. */int zslRandomLevel(void) {  // 跳跃表获取随机level值  越大的数出现的几率越小    int level = 1;    while ((random()&amp;0xFFFF) &lt; (ZSKIPLIST_P * 0xFFFF))  // 每往上提一层的概率为4分之一        level += 1;    return (level&lt;ZSKIPLIST_MAXLEVEL) ? level : ZSKIPLIST_MAXLEVEL;}</code></pre><p>由上面的代码可以看出，Redis最大的层数是 64，<code>level</code>层数最小是 1 ，level+1的概率是<code>1/4</code>（比如level=2的概率是<code>1/4</code>，level=3的概率是<code>1/16</code>依次类推）</p><h3 id="skiplist的算法性能分析"><a href="#skiplist的算法性能分析" class="headerlink" title="skiplist的算法性能分析"></a>skiplist的算法性能分析</h3><p>我们先来计算一下每个节点所包含的平均指针数目（概率期望）。节点包含的指针数目，相当于这个算法在空间上的额外开销<code>(overhead)</code>，可以用来度量空间复杂度。</p><p>根据前面<code>zslRandomLevel</code>()代码，我们很容易看出，产生越高的节点层数，概率越低。定量的分析如下：</p><ul><li>节点层数至少为1。而大于1的节点层数，满足一个概率分布。</li><li>节点层数恰好等于1的概率为1-p。</li><li>节点层数大于等于2的概率为p，而节点层数恰好等于2的概率为p(1-p)。</li><li>节点层数大于等于3的概率为p 2 ，而节点层数恰好等于3的概率为p 2 (1-p)。</li><li>节点层数大于等于4的概率为p 3 ，而节点层数恰好等于4的概率为p 3 (1-p)。</li><li>依次类推</li></ul><p>因此，一个节点的平均层数（也即包含的平均指针数目），计算如下：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1470ca1d5a138c3c.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="skip_list5.jpg"></p><p>现在很容易计算出：</p><ul><li>当p=1/4时，每个节点所包含的平均指针数目为1.33（平衡树每个节点包含指针数是2）。这也是Redis里的skiplist实现在空间上的开销。</li></ul><h3 id="skiplist算法时间复杂度"><a href="#skiplist算法时间复杂度" class="headerlink" title="skiplist算法时间复杂度"></a>skiplist算法时间复杂度</h3><p>为了分析时间复杂度，我们计算一下<code>skiplist</code>的平均查找长度。查找长度指的是查找路径上跨越的跳数，而查找过程中的比较次数就等于查找长度加1。</p><p>为了计算查找长度，这里我们需要利用一点小技巧。我们注意到，每个节点插入的时候，它的层数是由随机函数<code>zslRandomLevel()</code>计算出来的，而且随机的计算不依赖于其它节点，每次插入过程都是完全独立的。所以从统计上来说，一个<code>skiplist</code>结构的形成与节点的插入顺序无关。</p><p>这样的话，为了计算查找长度，我们可以将查找过程倒过来看，从右下方第1层上最后到达的那个节点开始，沿着查找路径向左向上回溯，类似于爬楼梯的过程。我们假设当回溯到某个节点的时候，它才被插入，这虽然相当于改变了节点的插入顺序，但从统计上不影响整个<code>skiplist</code>的形成结构。</p><p>现在假设我们从一个层数为<code>i</code>的节点<code>x</code>出发，需要向左向上攀爬<code>k</code>层。这时我们有两种可能：</p><p>如果节点<code>x</code>有第<code>(i+1)</code>层指针，那么我们需要向上走。这种情况概率为<code>p</code>。</p><p>如果节点<code>x</code>没有第<code>(i+1)</code>层指针，那么我们需要向左走。这种情况概率为<code>(1-p)</code>。</p><p>这两种情形如下图所示：<br>[图片上传失败…(image-4eddd1-1565973809739)]</p><p>用<code>C(k)</code>表示向上攀爬<code>k</code>个层级所需要走过的平均查找路径长度（概率期望），那么：</p><pre><code>C(0)=0C(k)=(1-p)×(上图中情况b的查找长度) + p×(上图中情况c的查找长度)</code></pre><p>代入，得到一个差分方程并化简：</p><pre><code>C(k)=(1-p)(C(k)+1) + p(C(k-1)+1)C(k)=1/p+C(k-1)C(k)=k/p</code></pre><p>这个结果的意思是，我们每爬升1个层级，需要在查找路径上走<code>1/p</code>步。而我们总共需要攀爬的层级数等于整个<code>skiplist</code>的总层数-1。</p><p>那么接下来我们需要分析一下当<code>skiplist</code>中有<code>n</code>个节点的时候，它的总层数的概率均值是多少。这个问题直观上比较好理解。根据节点的层数随机算法，容易得出：</p><ul><li>第1层链表固定有<code>n</code>个节点；</li><li>第2层链表平均有<code>n*p</code>个节点；</li><li>第3层链表平均有<code>n*p 2</code>个节点；</li><li>…</li></ul><p>所以，从第1层到最高层，各层链表的平均节点数是一个指数递减的等比数列。容易推算出，总层数的均值为<code>log 1/p n</code>，而最高层的平均节点数为<code>1/p</code>。</p><p>综上，粗略来计算的话，平均查找长度约等于：</p><pre><code>C(log 1/p n-1)=(log 1/p n-1)/p</code></pre><p>即，平均时间复杂度为O(log n)。</p><p>当然，这里的时间复杂度分析还是比较粗略的。比如，沿着查找路径向左向上回溯的时候，可能先到达左侧头结点，然后沿头结点一路向上；还可能先到达最高层的节点，然后沿着最高层链表一路向左。但这些细节不影响平均时间复杂度的最后结果。另外，这里给出的时间复杂度只是一个概率平均值，但实际上计算一个精细的概率分布也是有可能的。详情还请参见William Pugh的论文《Skip Lists: A Probabilistic Alternative to Balanced Trees》。</p><h2 id="三、redis的实现"><a href="#三、redis的实现" class="headerlink" title="三、redis的实现"></a>三、redis的实现</h2><h3 id="结构体定义"><a href="#结构体定义" class="headerlink" title="结构体定义"></a>结构体定义</h3><pre><code>typedef struct zskiplistNode {  // 跳跃表节点    robj *obj;  // redis对象    double score;   // 分值    struct zskiplistNode *backward; // 后退指针    struct zskiplistLevel {        struct zskiplistNode *forward;  // 前进指针        unsigned int span;  // 跨度    } level[];} zskiplistNode;typedef struct zskiplist {    struct zskiplistNode *header, *tail;    unsigned long length;   // 跳跃表长度    int level;  // 目前跳跃表的最大层数节点} zskiplist;</code></pre><p>redis 的跳跃表是一个双向的链表，并且在<code>zskiplist</code>结构体中保存了跳跃表的长度和头尾节点，方便从头查找或从尾部遍历。</p><p><code>zskiplistNode</code>定义了<code>skiplist</code>的节点结构。</p><ul><li><code>obj</code>字段存放的是节点数据，它的类型是一个<code>string robj</code>。本来一个<code>string robj</code>可能存放的不是<code>sds</code>，而是<code>long</code>型，但<code>zadd</code>命令在将数据插入到<code>skiplist</code>里面之前先进行了解码，所以这里的<code>obj</code>字段里存储的一定是一个<code>sds</code>。这样做的目的应该是为了方便在查找的时候对数据进行字典序的比较，而且，<code>skiplist</code>里的数据部分是数字的可能性也比较小。</li><li><code>score</code>字段是数据对应的分数。</li><li><code>backward</code>字段是指向链表前一个节点的指针（前向指针）。节点只有1个前向指针，所以只有第1层链表是一个双向链表。</li><li><code>level[]</code>存放指向各层链表后一个节点的指针（后向指针）。每层对应1个后向指针，用<code>forward</code>字段表示。另外，每个后向指针还对应了一个<code>span</code>值，它表示当前的指针跨越了多少个节点。<code>span</code>用于计算元素排名<code>(rank)</code>，这正是前面我们提到的Redis对于<code>skiplist</code>所做的一个扩展。需要注意的是，<code>level[]</code>是一个柔性数组<code>（flexible array member）</code>，因此它占用的内存不在<code>zskiplistNode</code>结构里面，而需要插入节点的时候单独为它分配。也正因为如此，<code>skiplist</code>的每个节点所包含的指针数目才是不固定的，我们前面分析过的结论——<code>skiplist</code>每个节点包含的指针数目平均为<code>1/(1-p)</code>——才能有意义。</li></ul><p><code>zskiplist</code>定义了真正的<code>skiplist</code>结构，它包含：</p><ul><li>头指针<code>header</code>和尾指针<code>tail</code>。</li><li>链表长度<code>length</code>，即链表包含的节点总数。注意，新创建的<code>skiplist</code>包含一个空的头指针，这个头指针不包含在<code>length</code>计数中。</li><li><code>level</code>表示<code>skiplist</code>的总层数，即所有节点层数的最大值。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-30c99c16bbca5335.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="skip_list8.jpg"></p><p>注意：图中前向指针上面括号中的数字，表示对应的<code>span</code>的值。即当前指针跨越了多少个节点，这个计数不包括指针的起点节点，但包括指针的终点节点。</p><p>假设我们在这个<code>skiplist</code>中查找<code>score=89.0</code>的元素（即Bob的成绩数据），在查找路径中，我们会跨域图中标红的指针，这些指针上面的<code>span</code>值累加起来，就得到了Bob的排名<code>(2+2+1)-1=4</code>（减1是因为rank值以0起始）。需要注意这里算的是从小到大的排名，而如果要算从大到小的排名，只需要用<code>skiplist</code>长度减去查找路径上的<code>span</code>累加值，即<code>6-(2+2+1)=1</code>。</p><p>可见，在查找<code>skiplist</code>的过程中，通过累加<code>span</code>值的方式，我们就能很容易算出排名。相反，如果指定排名来查找数据（类似<code>zrange</code>和<code>zrevrange</code>那样），也可以不断累加<code>span</code>并时刻保持累加值不超过指定的排名，通过这种方式就能得到一条<code>O(log n)</code>的查找路径。</p><h3 id="跳跃表创建及插入"><a href="#跳跃表创建及插入" class="headerlink" title="跳跃表创建及插入"></a>跳跃表创建及插入</h3><p>跳跃表的创建就是一些基本的初始化操作，需要注意的是 redis 的跳跃表最大层数为 64，是为了能够足够支撑优化<code>2^64</code>个元素的查找。假设每个元素出现在上一层索引的概率为0.5，每个元素出现在第n层的概率为<code>1/2^n</code>，所以当有<code>2^n</code>个元素时，需要n层索引保证查询时间复杂度为<code>O(logN)</code>。</p><pre><code>zskiplistNode *zslCreateNode(int level, double score, robj *obj) {  // 跳跃表节点创建    zskiplistNode *zn = zmalloc(sizeof(*zn)+level*sizeof(struct zskiplistLevel));    zn-&gt;score = score;    zn-&gt;obj = obj;    return zn;}zskiplist *zslCreate(void) {    // 跳跃表创建    int j;    zskiplist *zsl;    zsl = zmalloc(sizeof(*zsl));    zsl-&gt;level = 1;    zsl-&gt;length = 0;    zsl-&gt;header = zslCreateNode(ZSKIPLIST_MAXLEVEL,0,NULL); // 创建头结点    for (j = 0; j &lt; ZSKIPLIST_MAXLEVEL; j++) {  // 初始化头结点        zsl-&gt;header-&gt;level[j].forward = NULL;        zsl-&gt;header-&gt;level[j].span = 0;    }    zsl-&gt;header-&gt;backward = NULL;    zsl-&gt;tail = NULL;    return zsl;}</code></pre><p>redis 的跳跃表出现在上层索引节点的概率为0.25，在这样的概率下跳跃表的查询效率会略大于O(logN)，但是索引的存储内存却能节省一半。</p><pre><code>zskiplistNode *zslInsert(zskiplist *zsl, double score, robj *obj) { // 跳跃表zset节点插入    zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x;    unsigned int rank[ZSKIPLIST_MAXLEVEL];    int i, level;    serverAssert(!isnan(score));    x = zsl-&gt;header;    for (i = zsl-&gt;level-1; i &gt;= 0; i--) {   // 获取带插入节点的位置        /* store rank that is crossed to reach the insert position */        rank[i] = i == (zsl-&gt;level-1) ? 0 : rank[i+1];        while (x-&gt;level[i].forward &amp;&amp;            (x-&gt;level[i].forward-&gt;score &lt; score ||                (x-&gt;level[i].forward-&gt;score == score &amp;&amp;                compareStringObjects(x-&gt;level[i].forward-&gt;obj,obj) &lt; 0))) { // 如果当前节点分支小于带插入节点            rank[i] += x-&gt;level[i].span;    // 记录各层x前一个节点的索引跨度            x = x-&gt;level[i].forward;    // 查找一下个节点        }        update[i] = x;  // 记录各层x的前置节点    }    level = zslRandomLevel();   // 获取当前节点的level    if (level &gt; zsl-&gt;level) {   // 如果level大于当前skiplist的level 将大于部分的header初始化        for (i = zsl-&gt;level; i &lt; level; i++) {            rank[i] = 0;            update[i] = zsl-&gt;header;            update[i]-&gt;level[i].span = zsl-&gt;length;        }        zsl-&gt;level = level;    }    x = zslCreateNode(level,score,obj); // 创建新节点    for (i = 0; i &lt; level; i++) {        x-&gt;level[i].forward = update[i]-&gt;level[i].forward;  // 建立x节点索引        update[i]-&gt;level[i].forward = x;    // 将各层x的前置节点的后置节点置为x        /* update span covered by update[i] as x is inserted here */        x-&gt;level[i].span = update[i]-&gt;level[i].span - (rank[0] - rank[i]);  // 计算x节点各层索引跨度        update[i]-&gt;level[i].span = (rank[0] - rank[i]) + 1; // 计算x前置节点的索引跨度    }    /* increment span for untouched levels */    for (i = level; i &lt; zsl-&gt;level; i++) {  // 如果level小于zsl的level        update[i]-&gt;level[i].span++; // 将x前置节点的索引跨度加一    }    x-&gt;backward = (update[0] == zsl-&gt;header) ? NULL : update[0];    // 设置x前置节点    if (x-&gt;level[0].forward)        x-&gt;level[0].forward-&gt;backward = x;  // 设置x后面节点的前置节点    else        zsl-&gt;tail = x;    zsl-&gt;length++;  // length+1    return x;}</code></pre><h3 id="Redis中skiplist实现的特殊性"><a href="#Redis中skiplist实现的特殊性" class="headerlink" title="Redis中skiplist实现的特殊性"></a>Redis中skiplist实现的特殊性</h3><p>在Redis中，<code>skiplist</code>被用于实现暴露给外部的一个数据结构：<code>sorted set</code>。准确地说，<code>sorted set</code>底层不仅仅使用了<code>skiplist</code>，还使用了<code>ziplist</code>和<code>dict</code>。</p><p>我们简单分析一下<code>sorted set</code>的几个查询命令：</p><ul><li><code>zrevrank</code>由数据查询它对应的排名，这在前面介绍的<code>skiplist</code>中并不支持。</li><li><code>zscore</code>由数据查询它对应的分数，这也不是<code>skiplist</code>所支持的。</li><li><code>zrevrange</code>根据一个排名范围，查询排名在这个范围内的数据。这在前面介绍的<code>skiplist</code>中也不支持。</li><li><code>zrevrangebyscore</code>根据分数区间查询数据集合，是一个<code>skiplist</code>所支持的典型的范围查找（<code>score</code>相当于<code>key</code>）。</li></ul><p>实际上，Redis中<code>sorted set</code>的实现是这样的：</p><ul><li>当数据较少时，<code>sorted set</code>是由一个<code>ziplist</code>来实现的。</li><li>当数据多的时候，<code>sorted set</code>是由一个<code>dict</code> + 一个<code>skiplist</code>来实现的。简单来讲，<code>dict</code>用来查询数据到分数的对应关系，而<code>skiplist</code>用来根据分数查询数据（可能是范围查找）。</li></ul><p>现在我们集中精力来看一下<code>sorted set</code>与<code>skiplist</code>的关系：</p><ul><li><code>zscore</code>的查询，不是由<code>skiplist</code>来提供的，而是由那个<code>dict</code>来提供的。</li><li>为了支持排名<code>(rank)</code>，Redis里对<code>skiplist</code>做了扩展，使得根据排名能够快速查到数据，或者根据分数查到数据之后，也同时很容易获得排名。而且，根据排名的查找，时间复杂度也为<code>O(log n)</code>。</li><li><code>zrevrange</code>的查询，是根据排名查数据，由扩展后的<code>skiplist</code>来提供。</li><li><code>zrevrank</code>是先在<code>dict</code>中由数据查到分数，再拿分数到<code>skiplist</code>中去查找，查到后也同时获得了排名。</li></ul><p>前述的查询过程，也暗示了各个操作的时间复杂度：</p><ul><li><code>zscore</code>只用查询一个<code>dict</code>，所以时间复杂度为<code>O(1)</code></li><li><code>zrevrank</code>, <code>zrevrange</code>, <code>zrevrangebyscore</code>由于要查询<code>skiplist</code>，所以<code>zrevrank</code>的时间复杂度为<code>O(log n)</code>，而<code>zrevrange</code>, <code>zrevrangebyscore</code>的时间复杂度为<code>O(log(n)+M)</code>，其中M是当前查询返回的元素个数。</li></ul><p>总结起来，Redis中的<code>skiplist</code>跟前面介绍的经典的<code>skiplist</code>相比，有如下不同：</p><ul><li>分数<code>(score)</code>允许重复，即<code>skiplist</code>的key允许重复。这在最开始介绍的经典<code>skiplist</code>中是不允许的。</li><li>在比较时，不仅比较分数（相当于<code>skiplist</code>的<code>key</code>），还比较数据本身。在Redis的<code>skiplist</code>实现中，数据本身的内容唯一标识这份数据，而不是由key来唯一标识。另外，当多个元素分数相同的时候，还需要根据数据内容来进字典排序。</li><li>第1层链表不是一个单向链表，而是一个双向链表。这是为了方便以倒序方式获取一个范围内的元素。</li><li>在skiplist中可以很方便地计算出每个元素的排名(rank)。</li></ul><h3 id="Redis中的sorted-set"><a href="#Redis中的sorted-set" class="headerlink" title="Redis中的sorted set"></a>Redis中的sorted set</h3><p>我们前面提到过，Redis中的<code>sorted set</code>，是在<code>skiplist</code>, <code>dict</code>和<code>ziplist</code>基础上构建起来的:</p><ul><li>当数据较少时，<code>sorted set</code>是由一个<code>ziplist</code>来实现的。</li><li>当数据多的时候，<code>sorted set</code>是由一个叫zset的数据结构来实现的，这个<code>zset</code>包含一个<code>dict</code> + 一个<code>skiplist</code>。dict用来查询数据到分数<code>(score)</code>的对应关系，而<code>skiplist</code>用来根据分数查询数据（可能是范围查找）。</li></ul><p>在这里我们先来讨论一下前一种情况——基于<code>ziplist</code>实现的<code>sorted set</code>。在本系列前面关于<code>ziplist</code>的文章里，我们介绍过，<code>ziplist</code>就是由很多数据项组成的一大块连续内存。由于<code>sorted set</code>的每一项元素都由数据和<code>score</code>组成，因此，当使用<code>zadd</code>命令插入一个(数据, <code>score</code>)对的时候，底层在相应的<code>ziplist</code>上就插入两个数据项：数据在前，<code>score</code>在后。</p><p><code>ziplist</code>的主要优点是节省内存，但它上面的查找操作只能按顺序查找（可以正序也可以倒序）。因此，<code>sorted set</code>的各个查询操作，就是在<code>ziplist</code>上从前向后（或从后向前）一步步查找，每一步前进两个数据项，跨域一个(数据, <code>score</code>)对。</p><p>随着数据的插入，<code>sorted set</code>底层的这个<code>ziplist</code>就可能会转成<code>zset</code>的实现（转换过程详见<code>t_zset.c</code>的<code>zsetConvert</code>）。</p><pre><code>zset-max-ziplist-entries 128zset-max-ziplist-value 64</code></pre><p>这个配置的意思是说，在如下两个条件之一满足的时候，<code>ziplist</code>会转成<code>zset</code>（具体的触发条件参见<code>t_zset.c</code>中的<code>zaddGenericCommand</code>相关代码）：</p><ul><li>当<code>sorted set</code>中的元素个数，即(数据, score)对的数目超过128的时候，也就是ziplist数据项超过256的时候。</li><li>当<code>sorted set</code>中插入的任意一个数据的长度超过了64的时候。</li></ul><p>最后，<code>zset</code>结构的代码定义如下：</p><pre><code>typedef struct zset { dict *dict; zskiplist *zsl; } zset; </code></pre><h2 id="四、skiplist与平衡树、哈希表的比较"><a href="#四、skiplist与平衡树、哈希表的比较" class="headerlink" title="四、skiplist与平衡树、哈希表的比较"></a>四、skiplist与平衡树、哈希表的比较</h2><ul><li><code>skiplist</code>和各种平衡树（如<code>AVL</code>、红黑树等）的元素是有序排列的，而哈希表不是有序的。因此，在哈希表上只能做单个<code>key</code>的查找，不适宜做范围查找。所谓范围查找，指的是查找那些大小在指定的两个值之间的所有节点。</li><li>在做范围查找的时候，平衡树比<code>skiplist</code>操作要复杂。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在<code>skiplist</code>上进行范围查找就非常简单，只需要在找到小值之后，对第1层链表进行若干步的遍历就可以实现。</li><li>平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而<code>skiplist</code>的插入和删除只需要修改相邻节点的指针，操作简单又快速。</li><li>从内存占用上来说，<code>skiplist</code>比平衡树更灵活一些。一般来说，平衡树每个节点包含2个指针（分别指向左右子树），而skiplist每个节点包含的指针数目平均为<code>1/(1-p)</code>，具体取决于参数<code>p</code>的大小。如果像Redis里的实现一样，取<code>p=1/4</code>，那么平均每个节点包含1.33个指针，比平衡树更有优势。</li><li>查找单个<code>key</code>，<code>skiplist</code>和平衡树的时间复杂度都为O(log n)，大体相当；而哈希表在保持较低的哈希值冲突概率的前提下，查找时间复杂度接近O(1)，性能更高一些。所以我们平常使用的各种<code>Map</code>或<code>dictionary</code>结构，大都是基于哈希表实现的。</li><li>从算法实现难度上来比较，<code>skiplist</code>比平衡树要简单得多。</li></ul><h2 id="五、Redis为什么用skiplist而不用平衡树？"><a href="#五、Redis为什么用skiplist而不用平衡树？" class="headerlink" title="五、Redis为什么用skiplist而不用平衡树？"></a>五、Redis为什么用skiplist而不用平衡树？</h2><p>在前面我们对于<code>skiplist</code>和平衡树、哈希表的比较中，其实已经不难看出Redis里使用<code>skiplist</code>而不用平衡树的原因了。现在我们看看，对于这个问题，Redis的作者 <code>@antirez</code> 是怎么说的：</p><p>There are a few reasons:</p><ol><li><p>They are not very memory intensive. It’s up to you basically. Changing parameters about the probability of a node to have a given number of levels will make then less memory intensive than btrees.</p></li><li><p>A sorted set is often target of many ZRANGE or ZREVRANGE operations, that is, traversing the skip list as a linked list. With this operation the cache locality of skip lists is at least as good as with other kind of balanced trees.</p></li><li><p>They are simpler to implement, debug, and so forth. For instance thanks to the skip list simplicity I received a patch (already in Redis master) with augmented skip lists implementing ZRANK in O(log(N)). It required little changes to the code.</p></li></ol><p>这里从内存占用、对范围查找的支持和实现难易程度这三方面总结的原因，我们在前面其实也都涉及到了。</p><h2 id="参数资料"><a href="#参数资料" class="headerlink" title="参数资料"></a>参数资料</h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzA4NTg1MjM0Mg==&amp;mid=2657261425&amp;idx=1&amp;sn=d840079ea35875a8c8e02d9b3e44cf95&amp;scene=0#wechat_redirect">Redis为什么用跳表而不用平衡树？</a></p><p><a href="http://czrzchao.com/redisSourceSkiplist#skiplist">redis源码解读(七):基础数据结构之skiplist</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis 源码分析(六) ：quciklist</title>
      <link href="/2019/08/12/reids-source-code-6/"/>
      <url>/2019/08/12/reids-source-code-6/</url>
      
        <content type="html"><![CDATA[<h2 id="一、什么是quicklist"><a href="#一、什么是quicklist" class="headerlink" title="一、什么是quicklist"></a>一、什么是quicklist</h2><p>由于考虑到链表<code>adlist</code>的附加空间相对太高，<code>prev</code>和<code>next</code>指针就要占去 16 个字节 (64bit系统的指针是8个字节)，另外每个节点的内存都是单独分配，会加剧内存的碎片化，影响内存管理效率。</p><p><code>quicklist</code>是一个3.2版本之后新增的基础数据结构，是redis自定义的一种复杂数据结构，将<code>ziplist</code>和<code>adlist</code>结合到了一个数据结构中。主要是作为<code>list</code>的基础数据结构。<br>在3.2之前，<code>list</code>是根据元素数量的多少采用<code>ziplist</code>或者<code>adlist</code>作为基础数据结构，3.2之后统一改用<code>quicklist</code>，从数据结构的角度来说<code>quicklist</code>结合了两种数据结构的优缺点，复杂但是实用：</p><ul><li>链表在插入，删除节点的时间复杂度很低；但是内存利用率低，且由于内存不连续容易产生内存碎片</li><li>压缩表内存连续，存储效率高；但是插入和删除的成本太高，需要频繁的进行数据搬移、释放或申请内存</li></ul><p>而<code>quicklist</code>通过将每个压缩表用双向链表的方式连接起来，来寻求一种收益最大化。</p><h3 id="redis-list数据结构特点"><a href="#redis-list数据结构特点" class="headerlink" title="redis list数据结构特点"></a>redis list数据结构特点</h3><ul><li>表<code>list</code>是一个能维持数据项先后顺序的双向链表</li><li>在表<code>list</code>的两端追加和删除数据极为方便，时间复杂度为O(1)</li><li>表<code>list</code>也支持在任意中间位置的存取操作，时间复杂度为O(N)</li><li>表<code>list</code>经常被用作队列使用</li></ul><h2 id="二、数据结构"><a href="#二、数据结构" class="headerlink" title="二、数据结构"></a>二、数据结构</h2><pre><code>typedef struct quicklistNode {    struct quicklistNode *prev; // 前一个节点    struct quicklistNode *next; // 后一个节点    unsigned char *zl;  // ziplist    unsigned int sz;             // ziplist的内存大小    unsigned int count : 16;     // zpilist中数据项的个数    unsigned int encoding : 2;   // 1为ziplist 2是LZF压缩存储方式    unsigned int container : 2;      unsigned int recompress : 1;   // 压缩标志, 为1 是压缩    unsigned int attempted_compress : 1; // 节点是否能够被压缩,只用在测试    unsigned int extra : 10; /* more bits to steal for future usage */} quicklistNode;</code></pre><p><code>quicklistNode</code>实际上就是对<code>ziplist</code>的进一步封装，其中包括：</p><ul><li>指向前后压缩表节点的两个指针</li><li><code>zl</code>：<code>ziplist</code>指针</li><li><code>sz</code>：<code>ziplist</code>的内存占用大小</li><li><code>count</code>：<code>ziplist</code>内部数据的个数</li><li><code>encoding</code>：<code>ziplist</code>编码方式，1为默认方式，2为LZF数据压缩方式</li><li><code>recompress</code>：是否压缩，1表示压缩</li></ul><p>这里从变量<code>count</code>开始，都采用了位域的方式进行数据的内存声明，使得6个<code>unsigned int</code>变量只用到了一个<code>unsigned int</code>的内存大小。</p><p>C语言支持位域的方式对结构体中的数据进行声明，也就是可以指定一个类型占用几位：</p><ol><li>如果相邻位域字段的类型相同，且其位宽之和小于类型的<code>sizeof</code>大小，则后面的字段将紧邻前一个字段存储，直到不能容纳为止；</li><li>如果相邻位域字段的类型相同，但其位宽之和大于类型的<code>sizeof</code>大小，则后面的字段将从新的存储单元开始，其偏移量为其类型大小的整数倍；</li><li>如果相邻的位域字段的类型不同，则各编译器的具体实现有差异，VC6采取不压缩方式，Dev-C++采取压缩方式；</li><li>如果位域字段之间穿插着非位域字段，则不进行压缩；</li><li>整个结构体的总大小为最宽基本类型成员大小的整数倍。</li></ol><p><code>sizeof(quicklistNode); // output:32</code>，通过位域的声明方式，<code>quicklistNode</code>可以节省24个字节。</p><p>通过<code>quicklist</code>将<code>quicklistNode</code>连接起来就是一个完整的数据结构了</p><pre><code>typedef struct quicklist {    quicklistNode *head;    // 头结点    quicklistNode *tail;    // 尾节点    unsigned long count;    // 所有数据的数量    unsigned int len;       // quicklist节点数量    int fill : 16;          // 单个ziplist的大小限制，由list-max-ziplist-size给定    unsigned int compress : 16;   // 压缩深度,由list-compress-depth给定} quicklist;</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-bf8538664e279aaf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_quick_list.png"></p><p>由于<code>quicklist</code>结构包含了压缩表和链表，那么每个<code>quicklistNode</code>的大小就是一个需要仔细考量的点。如果单个<code>quicklistNode</code>存储的数据太多，就会影响插入效率；但是如果单个<code>quicklistNode</code>太小，就会变得跟链表一样造成空间浪费。<br><code>quicklist</code>通过<code>fill</code>对单个<code>quicklistNode</code>的大小进行限制：<code>fill</code>可以被赋值为正整数或负整数，full的大小由<code>list-max-ziplist-size</code>给定。</p><h3 id="list-max-ziplist-size"><a href="#list-max-ziplist-size" class="headerlink" title="list-max-ziplist-size"></a>list-max-ziplist-size</h3><p>1、<code>list-max-ziplist-size</code>取值，可以取正值，也可以取负值。</p><p>当取正值的时候，表示按照数据项个数来限定每个<code>quicklist</code>节点上的<code>ziplist</code>长度。比如，当这个参数配置成5的时候，表示每个<code>quicklist</code>节点的<code>ziplist</code>最多包含5个数据项，最大为32768个。</p><pre><code>#define FILL_MAX (1 &lt;&lt; 15)  // 32768void quicklistSetFill(quicklist *quicklist, int fill) { // set ziplist的单个节点最大存储数据量    if (fill &gt; FILL_MAX) {  // 个数        fill = FILL_MAX;    } else if (fill &lt; -5) { // 内存大小        fill = -5;    }    quicklist-&gt;fill = fill;}</code></pre><p>当取负值的时候，表示按照占用字节数来限定每个<code>quicklist</code>节点上的<code>ziplist</code>长度。这时，它只能取-1到-5这五个值，每个值含义如下：</p><ul><li>-5: 每个<code>quicklist</code>节点上的<code>ziplist</code>大小不能超过64 Kb。（注：1kb =&gt; 1024 bytes）</li><li>-4: 每个<code>quicklist</code>节点上的<code>ziplist</code>大小不能超过32 Kb。</li><li>-3: 每个<code>quicklist</code>节点上的<code>ziplist</code>大小不能超过16 Kb。</li></ul><p>-2: 每个<code>quicklist</code>节点上的<code>ziplist</code>大小不能超过8 Kb。（**-2是Redis给出的默认值**）</p><ul><li>-1: 每个<code>quicklist</code>节点上的<code>ziplist</code>大小不能超过4 Kb。</li></ul><p>2、<code>list-max-ziplist-size</code>配置产生的原因？</p><p>每个<code>quicklist</code>节点上的<code>ziplist</code>越短，则内存碎片越多。内存碎片多了，有可能在内存中产生很多无法被利用的小碎片，从而降低存储效率。这种情况的极端是每个<code>quicklist</code>节点上的<code>ziplist</code>只包含一个数据项，这就蜕化成一个普通的双向链表了。</p><p>每个<code>quicklist</code>节点上的<code>ziplist</code>越长，则为<code>ziplist</code>分配大块连续内存空间的难度就越大。有可能出现内存里有很多小块的空闲空间（它们加起来很多），但却找不到一块足够大的空闲空间分配给<code>ziplist</code>的情况。这同样会降低存储效率。这种情况的极端是整个<code>quicklist</code>只有一个节点，所有的数据项都分配在这仅有的一个节点的<code>ziplist</code>里面。这其实蜕化成一个<code>ziplist</code>了。</p><p>可见，一个<code>quicklist</code>节点上的<code>ziplist</code>要保持一个合理的长度。那到底多长合理呢？Redis提供了一个配置参数<code>list-max-ziplist-size</code>，就是为了让使用者可以来根据实际应用场景进行调整优化。</p><h3 id="list-compress-depth"><a href="#list-compress-depth" class="headerlink" title="list-compress-depth"></a>list-compress-depth</h3><p>其表示一个<code>quicklist</code>两端不被压缩的节点个数。注：这里的节点个数是指<code>quicklist</code>双向链表的节点个数，而不是指<code>ziplist</code>里面的数据项个数。实际上，一个<code>quicklist</code>节点上的<code>ziplist</code>，如果被压缩，就是整体被压缩的。</p><p>1、<code>list-compress-depth</code>的取值：</p><ul><li>0: 是个特殊值，表示都不压缩。这是Redis的默认值。</li><li>1: 表示<code>quicklist</code>两端各有1个节点不压缩，中间的节点压缩。</li><li>2: 表示<code>quicklist</code>两端各有2个节点不压缩，中间的节点压缩。</li><li>3: 表示<code>quicklist</code>两端各有3个节点不压缩，中间的节点压缩。</li></ul><p>2、<code>list-compress-depth</code>配置产生原因？</p><p>当表<code>list</code>存储大量数据的时候，最容易被访问的很可能是两端的数据，中间的数据被访问的频率比较低（访问起来性能也很低）。如果应用场景符合这个特点，那么<code>list</code>还提供了一个选项，能够把中间的数据节点进行压缩，从而进一步节省内存空间。Redis的配置参数<code>list-compress-depth</code>就是用来完成这个设置的。</p><h2 id="三、quicklist典型基本操作函数"><a href="#三、quicklist典型基本操作函数" class="headerlink" title="三、quicklist典型基本操作函数"></a>三、quicklist典型基本操作函数</h2><p>当我们使用<code>lpush</code>或<code>rpush</code>等命令第一次向一个不存在的<code>list</code>里面插入数据的时候，Redis会首先调用<code>quicklistCreate</code>接口创建一个空的<code>quicklist</code>。</p><h3 id="Create"><a href="#Create" class="headerlink" title="Create"></a>Create</h3><pre><code>/* Create a new quicklist. * Free with quicklistRelease(). */quicklist *quicklistCreate(void) {    struct quicklist *quicklist;    quicklist = zmalloc(sizeof(*quicklist));    quicklist-&gt;head = quicklist-&gt;tail = NULL;    quicklist-&gt;len = 0;    quicklist-&gt;count = 0;    quicklist-&gt;compress = 0;    quicklist-&gt;fill = -2;    return quicklist;}</code></pre><p>从上述代码中，我们看到<code>quicklist</code>是一个不包含空余头节点的双向链表（<code>head</code>和<code>tail</code>都初始化为<code>NULL</code>）。</p><h3 id="Push"><a href="#Push" class="headerlink" title="Push"></a>Push</h3><p><code>quicklist</code>只能在头尾插入节点，以在头部插入节点为例：</p><pre><code>int quicklistPushHead(quicklist *quicklist, void *value, size_t sz) {   // 在头部插入数据    quicklistNode *orig_head = quicklist-&gt;head;    if (likely(_quicklistNodeAllowInsert(quicklist-&gt;head, quicklist-&gt;fill, sz))) {  // 判断是否能够被插入到头节点中        quicklist-&gt;head-&gt;zl = ziplistPush(quicklist-&gt;head-&gt;zl, value, sz, ZIPLIST_HEAD);  // 调用ziplist的api在头部插入数据        quicklistNodeUpdateSz(quicklist-&gt;head); // 更新节点的sz    } else {    // 需要新增节点        quicklistNode *node = quicklistCreateNode();    // 新建节点        node-&gt;zl = ziplistPush(ziplistNew(), value, sz, ZIPLIST_HEAD);  // 新建一个ziplist并插入一个节点        quicklistNodeUpdateSz(node);    // 更新节点的sz        _quicklistInsertNodeBefore(quicklist, quicklist-&gt;head, node);   // 将新节点插入到头节点之前    }    quicklist-&gt;count++; // count自增    quicklist-&gt;head-&gt;count++;    return (orig_head != quicklist-&gt;head);  // 返回0为用已有节点 返回1为新建节点}</code></pre><p><code>quicklist</code>的主要操作基本都是复用<code>ziplist</code>的<code>api</code>，其中<code>likely</code>是针对条件语句的优化，告知编译器这种情况很可能出现，让编译器针对这种条件进行优化；与之对应的还有<code>unlikely</code>。由于绝大部分时候都不需要新增节点，因此用<code>likely</code>做了优化<br>在<code>_quicklistNodeAllowInsert</code>函数中，针对单个节点的内存大小做了校验</p><pre><code>REDIS_STATIC int _quicklistNodeAllowInsert(const quicklistNode *node,                                           const int fill, const size_t sz) {   // 判断当前node是否还能插入数据    if (unlikely(!node))        return 0;    int ziplist_overhead;    /* size of previous offset */    if (sz &lt; 254)   // 小于254时后一个节点的pre只有1字节,否则为5字节        ziplist_overhead = 1;    else        ziplist_overhead = 5;    /* size of forward offset */    if (sz &lt; 64)    // 小于64字节当前节点的encoding为1        ziplist_overhead += 1;    else if (likely(sz &lt; 16384))    // 小于16384 encoding为2字节        ziplist_overhead += 2;    else    // encoding为5字节        ziplist_overhead += 5;    /* new_sz overestimates if 'sz' encodes to an integer type */    unsigned int new_sz = node-&gt;sz + sz + ziplist_overhead; // 忽略了连锁更新的情况    if (likely(_quicklistNodeSizeMeetsOptimizationRequirement(new_sz, fill)))   // // 校验fill为负数是否超过单存储限制        return 1;    else if (!sizeMeetsSafetyLimit(new_sz)) // 校验单个节点是否超过8kb，主要防止fill为正数时单个节点内存过大        return 0;    else if ((int)node-&gt;count &lt; fill)   // fill为正数是否超过存储限制        return 1;    else        return 0;}</code></pre><p>同样，因为默认的<code>fill</code>为-2，所以针对为负数并且不会超过单个节点存储限制的条件做了<code>likely</code>优化；除此之外在计算的时候还忽略了<code>ziplist</code>可能发生的连锁更新；以及<code>fill</code>为正数时单个节点不能超过8kb</p><h3 id="Pop"><a href="#Pop" class="headerlink" title="Pop"></a>Pop</h3><pre><code>/* Default pop function * * Returns malloc'd value from quicklist */int quicklistPop(quicklist *quicklist, int where, unsigned char **data,                 unsigned int *sz, long long *slong) {    unsigned char *vstr;    unsigned int vlen;    long long vlong;    if (quicklist-&gt;count == 0)        return 0;    int ret = quicklistPopCustom(quicklist, where, &amp;vstr, &amp;vlen, &amp;vlong,                                 _quicklistSaver);    if (data)        *data = vstr;    if (slong)        *slong = vlong;    if (sz)        *sz = vlen;    return ret;}</code></pre><p><code>quicklist</code>的<code>pop</code>操作是调用<code>quicklistPopCustom</code>来实现的。</p><p><code>quicklistPopCustom</code>的实现过程基本上跟<code>quicklistPush</code>相反：</p><ol><li>从头部或尾部节点的<code>ziplist</code>中把对应的数据项删除；</li><li>如果在删除后<code>ziplist</code>为空了，那么对应的头部或尾部节点也要删除；</li><li>删除后还可能涉及到里面节点的解压缩问题。</li></ol><h3 id="节点压缩"><a href="#节点压缩" class="headerlink" title="节点压缩"></a>节点压缩</h3><p>由于<code>list</code>这个结构大部分时候只会用到头尾的数据，因此redis利用<code>lzf</code>算法对节点中间的元素进行压缩，以达到节省内存空间的效果。压缩节点的结构体和具体函数如下：</p><pre><code>typedef struct quicklistLZF {  // lzf结构体    unsigned int sz; /* LZF size in bytes*/    char compressed[];} quicklistLZF;REDIS_STATIC int __quicklistCompressNode(quicklistNode *node) { // 压缩节点#ifdef REDIS_TEST    node-&gt;attempted_compress = 1;#endif    /* Don't bother compressing small values */    if (node-&gt;sz &lt; MIN_COMPRESS_BYTES)  // 小于48字节不进行压缩        return 0;    quicklistLZF *lzf = zmalloc(sizeof(*lzf) + node-&gt;sz);    /* Cancel if compression fails or doesn't compress small enough */    if (((lzf-&gt;sz = lzf_compress(node-&gt;zl, node-&gt;sz, lzf-&gt;compressed,                                 node-&gt;sz)) == 0) ||        lzf-&gt;sz + MIN_COMPRESS_IMPROVE &gt;= node-&gt;sz) {   // 如果压缩失败或压缩后节省的空间不到8字节放弃压缩        /* lzf_compress aborts/rejects compression if value not compressable. */        zfree(lzf);        return 0;    }    lzf = zrealloc(lzf, sizeof(*lzf) + lzf-&gt;sz);    // 重新分配内存    zfree(node-&gt;zl);    // 释放原有节点    node-&gt;zl = (unsigned char *)lzf;    // 将压缩节点赋值给node    node-&gt;encoding = QUICKLIST_NODE_ENCODING_LZF;   // 记录编码    node-&gt;recompress = 0;    return 1;}</code></pre><h3 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h3><p><code>quicklist</code>不仅实现了从头部或尾部插入，也实现了从任意指定的位置插入。<code>quicklistInsertAfter</code>和<code>quicklistInsertBefore</code>就是分别在指定位置后面和前面插入数据项。这种在任意指定位置插入数据的操作，情况比较复杂。</p><ul><li>当插入位置所在的<code>ziplist</code>大小没有超过限制时，直接插入到<code>ziplist</code>中就好了</li><li>当插入位置所在的<code>ziplist</code>大小超过了限制，但插入的位置位于<code>ziplist</code>两端，并且相邻的<code>quicklist</code>链表节点的<code>ziplist</code>大小没有超过限制，那么就转而插入到相邻的那个<code>quicklist</code>链表节点的<code>ziplist</code>中</li><li>当插入位置所在的<code>ziplist</code>大小超过了限制，但插入的位置位于<code>ziplist</code>两端，并且相邻的<code>quicklist</code>链表节点的<code>ziplist</code>大小也超过限制，这时需要新创建一个<code>quicklist</code>链表节点插入</li><li>对于插入位置所在的<code>ziplist</code>大小超过了限制的其它情况（主要对应于在<code>ziplist</code>中间插入数据的情况），则需要把当前<code>ziplist</code>分裂为两个节点，然后再其中一个节点上插入数据</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><code>quicklist</code>除了常用的增删改查外还提供了<code>merge</code>、将<code>ziplist</code>转换为<code>quicklist</code>等<code>api</code>，这里就不详解了，可以具体查看<code>quicklist.h</code>和<code>quicklist.c</code>文件。</p><ul><li><code>quicklist</code>是redis在<code>ziplist</code>和<code>adlist</code>两种数据结构的基础上融合而成的一个实用的复杂数据结构</li><li><code>quicklist</code>在3.2之后取代<code>adlist</code>和<code>ziplist</code>作为<code>list</code>的基础数据类型</li><li><code>quicklist</code>的大部分<code>api</code>都是直接复用<code>ziplist</code></li><li><code>quicklist</code>的单个节点最大存储默认为8kb</li><li><code>quicklist</code>提供了基于<code>lzf</code>算法的压缩<code>api</code>，通过将不常用的中间节点数据压缩达到节省内存的目的</li><li><code>quicklist</code>将双向链表和<code>ziplist</code>两者的优点结合起来，在时间和空间上做了一个均衡，能较大程度上提高Redis的效率。<code>push</code>和<code>pop</code>等操作操作的时间复杂度也都达到了最优。</li></ul><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://www.cnblogs.com/virgosnail/p/9542470.html">Redis—quickList(快速列表)</a></p><p><a href="http://czrzchao.com/redisSourceQuicklist#quicklist">redis源码解读(六):基础数据结构之quicklist</a></p><p><a href="https://www.cnblogs.com/exceptioneye/p/7044341.html">Redis数据结构之quicklist</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis 源码分析(五) ：ziplist</title>
      <link href="/2019/08/10/reids-source-code-5/"/>
      <url>/2019/08/10/reids-source-code-5/</url>
      
        <content type="html"><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p><code>ziplist</code>是redis节省内存的典型例子之一，这个数据结构通过特殊的编码方式将数据存储在连续的内存中。在3.2之前是list的基础数据结构之一，在3.2之后被<code>quicklist</code>替代。但是仍然是<code>zset</code>底层实现之一。</p><h2 id="二、存储结构"><a href="#二、存储结构" class="headerlink" title="二、存储结构"></a>二、存储结构</h2><p><strong>压缩表没有数据结构代码定义</strong>，完全是通过内存的特殊编码方式实现的一种紧凑存储数据结构。我们可以通过<code>ziplist</code>的初始化函数和操作<code>api</code>来倒推其内存分布。</p><pre><code>#define ZIP_END 255#define ZIPLIST_BYTES(zl)       (*((uint32_t*)(zl)))    // 获取ziplist的bytes指针#define ZIPLIST_TAIL_OFFSET(zl) (*((uint32_t*)((zl)+sizeof(uint32_t)))) // 获取ziplist的tail指针#define ZIPLIST_LENGTH(zl)      (*((uint16_t*)((zl)+sizeof(uint32_t)*2)))   // 获取ziplist的len指针#define ZIPLIST_HEADER_SIZE     (sizeof(uint32_t)*2+sizeof(uint16_t))   // ziplist头大小#define ZIPLIST_END_SIZE        (sizeof(uint8_t))   // ziplist结束标志位大小#define ZIPLIST_ENTRY_HEAD(zl)  ((zl)+ZIPLIST_HEADER_SIZE)  // 获取第一个元素的指针#define ZIPLIST_ENTRY_TAIL(zl)  ((zl)+intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl)))    // 获取最后一个元素的指针#define ZIPLIST_ENTRY_END(zl)   ((zl)+intrev32ifbe(ZIPLIST_BYTES(zl))-1)    // 获取结束标志位指针unsigned char *ziplistNew(void) {   // 创建一个压缩表    unsigned int bytes = ZIPLIST_HEADER_SIZE+1; // zip头加结束标识位数    unsigned char *zl = zmalloc(bytes);    ZIPLIST_BYTES(zl) = intrev32ifbe(bytes);    // 大小端转换    ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(ZIPLIST_HEADER_SIZE);    ZIPLIST_LENGTH(zl) = 0; // len赋值为0    zl[bytes-1] = ZIP_END;  // 结束标志位赋值    return zl;}</code></pre><p>通过上面的源码，我们不难看出<code>ziplist</code>的头是由两个<code>unint32_t</code>和一个<code>unint16_t</code>组成。这3个数字分别保存是<code>ziplist</code>的内存占用、元素数量和最后一个元素的偏移量。除此之外，<code>ziplist</code>还包含一个结束标识，用常量255表示。整个<code>ziplist</code>描述内容占用了11个字节。初始化后的内存图如下：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-63de3bf3d019f760.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_zip_list.jpg"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-711c43158d719f10.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_zip_list2.png"></p><h3 id="zlentry的内存布局"><a href="#zlentry的内存布局" class="headerlink" title="zlentry的内存布局"></a>zlentry的内存布局</h3><p><code>zlentry</code>每个节点由三部分组成：<code>prevlength</code>、<code>encoding</code>、<code>data</code></p><ul><li><code>prevlengh</code>: 记录上一个节点的长度，为了方便反向遍历ziplist</li><li><code>encoding</code>: 当前节点的编码规则.</li><li><code>data</code>: 当前节点的值，可以是数字或字符串</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a36b1b5bd4f4ee33.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_zip_list_entity.jpg"></p><ul><li><code>entry</code>的前8位小于254，则这8位就表示上一个节点的长度</li><li><code>entry</code>的前8位等于254，则意味着上一个节点的长度无法用8位表示，后面32位才是真实的prevlength。用254 不用255(11111111)作为分界是因为255是zlend的值，它用于判断ziplist是否到达尾部。</li></ul><h3 id="zlentry数据结构"><a href="#zlentry数据结构" class="headerlink" title="zlentry数据结构"></a>zlentry数据结构</h3><pre><code>typedef struct zlentry {    // 压缩列表节点    unsigned int prevrawlensize, prevrawlen;    // prevrawlen是前一个节点的长度，prevrawlensize是指prevrawlen的大小，有1字节和5字节两种    unsigned int lensize, len;  // len为当前节点长度 lensize为编码len所需的字节大小    unsigned int headersize;    // 当前节点的header大小    unsigned char encoding; // 节点的编码方式    unsigned char *p;   // 指向节点的指针} zlentry;void zipEntry(unsigned char *p, zlentry *e) {   // 根据节点指针返回一个enrty    ZIP_DECODE_PREVLEN(p, e-&gt;prevrawlensize, e-&gt;prevrawlen);    // 获取prevlen的值和长度    ZIP_DECODE_LENGTH(p + e-&gt;prevrawlensize, e-&gt;encoding, e-&gt;lensize, e-&gt;len);  // 获取当前节点的编码方式、长度等    e-&gt;headersize = e-&gt;prevrawlensize + e-&gt;lensize; // 头大小    e-&gt;p = p;}</code></pre><h2 id="三、编码方式"><a href="#三、编码方式" class="headerlink" title="三、编码方式"></a>三、编码方式</h2><p><code>zlentry</code>中<code>len</code>字段配合<code>encoding</code>字段进行了编码, 尽量压缩字段长度, 减少内存使用. 如果实体内容被编码成整数, 则长度默认为1, 如果实体内容被编码为字符串, 则会根据不同长度进行不同编码.编码原则是第一个字节前两个bit位标识占用空间长度, 分别有以下几种, 后面紧跟着存储实际值.</p><h3 id="zlentry之prevrawlen编码"><a href="#zlentry之prevrawlen编码" class="headerlink" title="zlentry之prevrawlen编码"></a>zlentry之prevrawlen编码</h3><p><code>zlentry</code>中<code>prevrawlen</code>进行了压缩编码, 如果字段小于254, 则直接用一个字节保存, 如果大于254字节, 则使用5个字节进行保存, 第一个字节固定值254, 后四个字节保存实际字段值. <code>zipPrevEncodeLength</code>函数是对改字段编码的函数, 我们可以通过此函数看下编码格式.</p><pre><code>/*prevrawlen字段进行编码函数*/static unsigned int zipPrevEncodeLength(unsigned char *p, unsigned int len) {     /*     *ZIP_BIGLEN值为254, 返回值表示len所占用的空间大小, 要么1要么5     */    if (p == NULL) {        return (len &lt; ZIP_BIGLEN) ? 1 : sizeof(len)+1;    } else {          /*len小于254直接用一个字节保存*/        if (len &lt; ZIP_BIGLEN) {            p[0] = len;            return 1;        } else {               /*大于254,第一个字节赋值为254, 后四个字节保存值*/            p[0] = ZIP_BIGLEN;            memcpy(p+1,&amp;len,sizeof(len));            memrev32ifbe(p+1);            return 1+sizeof(len);        }    }}</code></pre><h3 id="字符串编码"><a href="#字符串编码" class="headerlink" title="字符串编码"></a>字符串编码</h3><pre><code>/*字符串编码标识使用了最高2bit位 */#define ZIP_STR_06B (0 &lt;&lt; 6)  //6bit#define ZIP_STR_14B (1 &lt;&lt; 6)  //14bit#define ZIP_STR_32B (2 &lt;&lt; 6)  //32bit/*zlentry中len字段进行编码过程*/static unsigned int zipEncodeLength(unsigned char *p, unsigned char encoding, unsigned int rawlen) {    unsigned char len = 1, buf[5];    if (ZIP_IS_STR(encoding)) {        /*          *6bit可以存储, 占用空间为1个字节, 值存储在字节后6bit中.          */        if (rawlen &lt;= 0x3f) {            if (!p) return len;            buf[0] = ZIP_STR_06B | rawlen;        } else if (rawlen &lt;= 0x3fff) {            len += 1;            if (!p) return len;               /*14bit可以存储, 置前两个bit位为ZIP_STR_14B标志 */            buf[0] = ZIP_STR_14B | ((rawlen &gt;&gt; 8) &amp; 0x3f);            buf[1] = rawlen &amp; 0xff;        } else {            len += 4;            if (!p) return len;            buf[0] = ZIP_STR_32B;            buf[1] = (rawlen &gt;&gt; 24) &amp; 0xff;            buf[2] = (rawlen &gt;&gt; 16) &amp; 0xff;            buf[3] = (rawlen &gt;&gt; 8) &amp; 0xff;            buf[4] = rawlen &amp; 0xff;        }    } else {        /* 内容编码为整型, 长度默认为1*/        if (!p) return len;        buf[0] = encoding;    }    /* Store this length at p */    memcpy(p,buf,len);    return len;}</code></pre><p>由上面代码可以看字符串节点分为3类：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-cf731e9e006ee635.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_zip_list_string_encode.jpg"></p><ul><li>当<code>data</code>小于63字节时(2^6)，节点存为上图的第一种类型，高2位为00，低6位表示data的长度。</li><li>当<code>data</code>小于16383字节时(2^14)，节点存为上图的第二种类型，高2位为01，后续14位表示data的长度。</li><li>当<code>data</code>小于4294967296字节时(2^32)，节点存为上图的第二种类型，高2位为10，下一字节起连续32位表示data的长度。</li></ul><h3 id="整数编码"><a href="#整数编码" class="headerlink" title="整数编码"></a>整数编码</h3><pre><code>`zlentry`中`encoding`和`p`表示元素编码和内容, 下面分析下具体编码规则, 可以看到这里对内存节省真是到了魔性的地步. `encoding`是保存在`len`字段第一个字节中, 第一个字节最高2bit标识字符串编码, 5和6bit位标识是整数编码, 解码时直接从第一个字节中获取编码信息./* 整数编码标识使用了5和6bit位 */#define ZIP_INT_16B (0xc0 | 0&lt;&lt;4)  //16bit整数#define ZIP_INT_32B (0xc0 | 1&lt;&lt;4)  //32bit整数#define ZIP_INT_64B (0xc0 | 2&lt;&lt;4)  //64bit整数#define ZIP_INT_24B (0xc0 | 3&lt;&lt;4)  //24bit整数#define ZIP_INT_8B 0xfe            //8bit整数#define ZIP_INT_IMM_MASK 0x0f#define ZIP_INT_IMM_MIN 0xf1    /* 11110001 */#define ZIP_INT_IMM_MAX 0xfd    /* 11111101 */static int zipTryEncoding(unsigned char *entry, unsigned int entrylen, long long *v, unsigned char *encoding) {    long long value;    if (entrylen &gt;= 32 || entrylen == 0) return 0;       if (string2ll((char*)entry,entrylen,&amp;value)) {        /* 0-12之间的值, 直接在保存在了encoding字段中, 其他根据值大小, 直接设置为相应的编码*/        if (value &gt;= 0 &amp;&amp; value &lt;= 12) {            *encoding = ZIP_INT_IMM_MIN+value;        } else if (value &gt;= INT8_MIN &amp;&amp; value &lt;= INT8_MAX) {            *encoding = ZIP_INT_8B;        } else if (value &gt;= INT16_MIN &amp;&amp; value &lt;= INT16_MAX) {            *encoding = ZIP_INT_16B;        } else if (value &gt;= INT24_MIN &amp;&amp; value &lt;= INT24_MAX) {            *encoding = ZIP_INT_24B;        } else if (value &gt;= INT32_MIN &amp;&amp; value &lt;= INT32_MAX) {            *encoding = ZIP_INT_32B;        } else {            *encoding = ZIP_INT_64B;        }        *v = value;        return 1;    }    return 0;}</code></pre><p>由上面代码可以看出整数节点分为6类：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b771a4360ca5b3af.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_zip_list_int_encode.jpg"></p><p>整数节点的<code>encoding</code>的长度为8位，其中高2位用来区分整数节点和字符串节点（<strong>高2位为11时是整数节点</strong>），低6位用来区分整数节点的类型。</p><p>值得注意的是 最后一种<code>encoding</code>是存储整数<code>0~12</code>的节点的<code>encoding</code>，它没有额外的<code>data</code>部分，<code>encoding</code>的高4位表示这个类型，低4位就是它的<code>data</code>。这种类型的节点的<code>encoding</code>大小介于<code>ZIP_INT_24B</code>与<code>ZIP_INT_8B</code>之间（<code>1~13</code>），但是为了表示整数0，取出低四位xxxx之后会将其-1作为实际的data值（<code>0~12</code>）。</p><h3 id="编码总结"><a href="#编码总结" class="headerlink" title="编码总结"></a>编码总结</h3><p>不同于整数节点<code>encoding</code>永远是8位，字符串节点的encoding可以有8位、16位、40位三种长度</p><p>相同<code>encoding</code>类型的整数节点 <code>data</code>长度是固定的，但是相同<code>encoding</code>类型的字符串节点，<code>data</code>长度取决于<code>encoding</code>后半部分的值。</p><h2 id="四、添加元素"><a href="#四、添加元素" class="headerlink" title="四、添加元素"></a>四、添加元素</h2><p>有了一个初始化后的<code>ziplist</code>，就可以往里添加数据了，以<code>push</code>函数为例对<code>ziplist</code>的插入过程做一个解析，顺便把<code>ziplist</code>的完整数据结构做一个整理：</p><pre><code>unsigned char *ziplistPush(unsigned char *zl, unsigned char *s, unsigned int slen, int where) { // push    unsigned char *p;    p = (where == ZIPLIST_HEAD) ? ZIPLIST_ENTRY_HEAD(zl) : ZIPLIST_ENTRY_END(zl);    return __ziplistInsert(zl,p,s,slen);}</code></pre><p><code>push</code>的方式分为头尾两种，主体还是要看<code>__ziplistInsert</code>函数：</p><pre><code>unsigned char *__ziplistInsert(unsigned char *zl, unsigned char *p, unsigned char *s, unsigned int slen) {  // 插入    size_t curlen = intrev32ifbe(ZIPLIST_BYTES(zl)), reqlen;    unsigned int prevlensize, prevlen = 0;    size_t offset;    int nextdiff = 0;    unsigned char encoding = 0;    long long value = 123456789; /* initialized to avoid warning. Using a value                                    that is easy to see if for some reason                                    we use it uninitialized. */    zlentry tail;    /* Find out prevlen for the entry that is inserted. */    if (p[0] != ZIP_END) {  // 如果不是在尾部插入        ZIP_DECODE_PREVLEN(p, prevlensize, prevlen);    // 获取prevlen    } else {    // 在尾部插入        unsigned char *ptail = ZIPLIST_ENTRY_TAIL(zl);  // 获取最后一个entry        if (ptail[0] != ZIP_END) {  // 如果ziplist不为空            prevlen = zipRawEntryLength(ptail); // prevlen就是最后一个enrty的长度        }    }    /* See if the entry can be encoded */    if (zipTryEncoding(s,slen,&amp;value,&amp;encoding)) {  // 尝试对value进行整数编码        /* 'encoding' is set to the appropriate integer encoding */        reqlen = zipIntSize(encoding);  // 数据长度    } else {        /* 'encoding' is untouched, however zipEncodeLength will use the         * string length to figure out how to encode it. */        reqlen = slen;  // 字符数组长度    }    /* We need space for both the length of the previous entry and     * the length of the payload. */    reqlen += zipPrevEncodeLength(NULL,prevlen);    // 获取pre编码长度    reqlen += zipEncodeLength(NULL,encoding,slen);  // 获取编码长度    /* When the insert position is not equal to the tail, we need to     * make sure that the next entry can hold this entry's length in     * its prevlen field. */    int forcelarge = 0;    nextdiff = (p[0] != ZIP_END) ? zipPrevLenByteDiff(p,reqlen) : 0;    // 如果不在尾部插入，需要判断当前prelen大小是否够用    if (nextdiff == -4 &amp;&amp; reqlen &lt; 4) { // 如果当前节点prelen为5个字节或1个字节已经够用        nextdiff = 0;        forcelarge = 1;    }    /* Store offset because a realloc may change the address of zl. */    offset = p-zl;  // 记录偏移量，因为realloc可能会改变ziplist的地址    zl = ziplistResize(zl,curlen+reqlen+nextdiff);  //  重新申请内存    p = zl+offset;  // 拿到p指针    /* Apply memory move when necessary and update tail offset. */    if (p[0] != ZIP_END) {  // 不是在尾部插入        /* Subtract one because of the ZIP_END bytes */        memmove(p+reqlen,p-nextdiff,curlen-offset-1+nextdiff);  // 通过内存拷贝将原有数据后移，因为移动前后内存地址有重叠需要用memmove        /* Encode this entry's raw length in the next entry. */        if (forcelarge)            zipPrevEncodeLengthForceLarge(p+reqlen,reqlen); // 当下一个节点的prelen空间已经够用时，不需要压缩，防止连锁更新        else            zipPrevEncodeLength(p+reqlen,reqlen);   // 将reqlen保存到后一个节点中        /* Update offset for tail */        ZIPLIST_TAIL_OFFSET(zl) =            intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))+reqlen); // 更新tail值        zipEntry(p+reqlen, &amp;tail);        if (p[reqlen+tail.headersize+tail.len] != ZIP_END) {    // 如果下一个节点的prelen扩展了需要加上nextdiff            ZIPLIST_TAIL_OFFSET(zl) =                intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))+nextdiff);        }    } else {    // 如果是在尾部插入直接更新tail_offset        /* This element will be the new tail. */        ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(p-zl);    }    if (nextdiff != 0) {    // 连锁更新        offset = p-zl;  // 记录offset预防地址变更        zl = __ziplistCascadeUpdate(zl,p+reqlen);        p = zl+offset;    }    /* Write the entry */    p += zipPrevEncodeLength(p,prevlen);    // 记录prelen    p += zipEncodeLength(p,encoding,slen);  // 记录encoding和len    if (ZIP_IS_STR(encoding)) { // 保存字符串        memcpy(p,s,slen);    } else {    // 保存数字        zipSaveInteger(p,value,encoding);    }    ZIPLIST_INCR_LENGTH(zl,1);  // ziplist的len加1    return zl;}</code></pre><p>一个完整的插入流程大致是这样的：</p><ol><li>获取p指针的<code>prelen</code></li><li>根据<code>prelen</code>值计算当前带插入节点的<code>reqlen</code></li><li>校验p指针对应的节点的<code>prelen</code>是否够<code>reqlen</code>使用，不够需要扩展，够不进行压缩</li><li>重新申请内存，如果不是在尾部插入需要将对应数据后移</li><li>更新<code>ziplist</code>的<code>tailoffset</code>值</li><li>尝试进行连锁更新</li><li>保存当前节点，分表保存<code>prevlen</code>、<code>encoding</code>、对应内容</li><li><code>ziplist</code>的<code>len</code>加1</li></ol><p>通过对push的梳理，<code>ziplist</code>的内存分布就很清晰了：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6c1c42e416aefd0b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_zip_list_memory.png"></p><p>通过连续的内存和上述编码方式，<code>ziplist</code>可以很方便的拿到头尾节点；由于每个节点都保存了前一个节点的长度，因此可以通过尾节点很方便的利用内存偏移进行遍历；相比链表或hash表大大压缩了内存；最主要这个数据结构的大部分场景都是<code>pop</code>或<code>push</code>，因此在查找和中间插入场景下的时间复杂度提升也是可以接受的。</p><h2 id="五、已知节点的位置，求data的值"><a href="#五、已知节点的位置，求data的值" class="headerlink" title="五、已知节点的位置，求data的值"></a>五、已知节点的位置，求data的值</h2><p>根据<code>entry</code>布局 可以看出，若要算出<code>data</code>的偏移量，得先计算出<code>prevlength</code>所占内存大小（1字节和5字节）：</p><pre><code>//根据ptr指向的entry，返回这个entry的prevlensize#define ZIP_DECODE_PREVLENSIZE(ptr, prevlensize) do {                          \if ((ptr)[0] &lt; ZIP_BIGLEN) {                                               \    (prevlensize) = 1;                                                     \} else {                                                                   \    (prevlensize) = 5;                                                     \}                                                                          \} while(0);</code></pre><p>接着再用<code>ZIP_DECODE_LENGTH(ptr + prevlensize, encoding, lensize, len)</code>算出<code>encoding</code>所占的字节，返回给<code>lensize</code>；<code>data</code>所占的字节返回给<code>len</code></p><pre><code>//根据ptr指向的entry求出该entry的len（encoding里存的 data所占字节）和lensize（encoding所占的字节）#define ZIP_DECODE_LENGTH(ptr, encoding, lensize, len) do {                    \    ZIP_ENTRY_ENCODING((ptr), (encoding));                                     \    if ((encoding) &lt; ZIP_STR_MASK) {                                           \        if ((encoding) == ZIP_STR_06B) {                                       \            (lensize) = 1;                                                     \            (len) = (ptr)[0] &amp; 0x3f;                                           \        } else if ((encoding) == ZIP_STR_14B) {                                \            (lensize) = 2;                                                     \            (len) = (((ptr)[0] &amp; 0x3f) &lt;&lt; 8) | (ptr)[1];                       \        } else if (encoding == ZIP_STR_32B) {                                  \            (lensize) = 5;                                                     \            (len) = ((ptr)[1] &lt;&lt; 24) |                                         \                    ((ptr)[2] &lt;&lt; 16) |                                         \                    ((ptr)[3] &lt;&lt;  8) |                                         \                    ((ptr)[4]);                                                \        } else {                                                               \            assert(NULL);                                                      \        }                                                                      \    } else {                                                                   \        (lensize) = 1;                                                         \        (len) = zipIntSize(encoding);                                          \    }                                                                          \} while(0);//将ptr的encoding解析成1个字节：00000000、01000000、10000000(字符串类型)和11??????(整数类型)//如果是整数类型，encoding直接照抄ptr的;如果是字符串类型，encoding被截断成一个字节并清零后6位#define ZIP_ENTRY_ENCODING(ptr, encoding) do {  \    (encoding) = (ptr[0]); \    if ((encoding) &lt; ZIP_STR_MASK) (encoding) &amp;= ZIP_STR_MASK; \} while(0)//根据encoding返回数据(整数)所占字节数unsigned int zipIntSize(unsigned char encoding) {    switch(encoding) {    case ZIP_INT_8B:  return 1;    case ZIP_INT_16B: return 2;    case ZIP_INT_24B: return 3;    case ZIP_INT_32B: return 4;    case ZIP_INT_64B: return 8;    default: return 0; /* 4 bit immediate */    }    assert(NULL);    return 0;}</code></pre><h2 id="六、查找元素"><a href="#六、查找元素" class="headerlink" title="六、查找元素"></a>六、查找元素</h2><p>查找元素直接从指定位置开始,一个一个查找, 直到找到或者到达尾部.</p><pre><code>/* 从位置p开始查找元素, skip表示每查找一次跳过的元素个数*/unsigned char *ziplistFind(unsigned char *p, unsigned char *vstr, unsigned int vlen, unsigned int skip) {    int skipcnt = 0;    unsigned char vencoding = 0;    long long vll = 0;    while (p[0] != ZIP_END) {        unsigned int prevlensize, encoding, lensize, len;        unsigned char *q;                  /*取出元素中元素内容放入q中*/        ZIP_DECODE_PREVLENSIZE(p, prevlensize);        ZIP_DECODE_LENGTH(p + prevlensize, encoding, lensize, len);        q = p + prevlensize + lensize;        if (skipcnt == 0) {            /* 如果元素是字符串编码, */            if (ZIP_IS_STR(encoding)) {                if (len == vlen &amp;&amp; memcmp(q, vstr, vlen) == 0) {                    return p;                }            } else {                /*元素是整数编码, 按照整型进行比较*/                if (vencoding == 0) {                    if (!zipTryEncoding(vstr, vlen, &amp;vll, &amp;vencoding)) {                        /* 如果无法进行整数编码, 则直接赋值为UCHAR_MAX以后不会在进行整数类型比较*/                        vencoding = UCHAR_MAX;                    }                    assert(vencoding);                }                /*如果待查元素是整型编码, 直接进行比较*/                if (vencoding != UCHAR_MAX) {                    long long ll = zipLoadInteger(q, encoding);                    if (ll == vll) {                        return p;                    }                }            }            /* 重置跳过元素值 */            skipcnt = skip;        } else {            /* Skip entry */            skipcnt--;        }        /* 移动到下个元素位置 */        p = q + len;    }    return NULL;}</code></pre><h2 id="七、删除元素"><a href="#七、删除元素" class="headerlink" title="七、删除元素"></a>七、删除元素</h2><p>删除元素主要通过<code>ziplistDelete</code>和<code>ziplistDeleteRange</code>来进行</p><pre><code>/* 删除一个元素*/unsigned char *ziplistDelete(unsigned char *zl, unsigned char **p) {    size_t offset = *p-zl;    zl = __ziplistDelete(zl,*p,1);    *p = zl+offset;    return zl;}/* 删除一段数据 */unsigned char *ziplistDeleteRange(unsigned char *zl, unsigned int index, unsigned int num) {     /*根据索引查找出元素位置，下面介绍该函数*/    unsigned char *p = ziplistIndex(zl,index);    return (p == NULL) ? zl : __ziplistDelete(zl,p,num);}unsigned char *ziplistIndex(unsigned char *zl, int index) {    unsigned char *p;    unsigned int prevlensize, prevlen = 0;     /*传入索引与零比较，比零大则从头部开始查找，比零小则从尾部开始查找*/    if (index &lt; 0) {        index = (-index)-1;        p = ZIPLIST_ENTRY_TAIL(zl);        if (p[0] != ZIP_END) {               /*不断取出prevlen值，从后向前开始查找*/            ZIP_DECODE_PREVLEN(p, prevlensize, prevlen);            while (prevlen &gt; 0 &amp;&amp; index--) {                p -= prevlen;                ZIP_DECODE_PREVLEN(p, prevlensize, prevlen);            }        }    } else {        p = ZIPLIST_ENTRY_HEAD(zl);        while (p[0] != ZIP_END &amp;&amp; index--) {            p += zipRawEntryLength(p);        }    }    return (p[0] == ZIP_END || index &gt; 0) ? NULL : p;}/* 真正执行删除操作函数*/static unsigned char *__ziplistDelete(unsigned char *zl, unsigned char *p, unsigned int num) {    unsigned int i, totlen, deleted = 0;    size_t offset;    int nextdiff = 0;    zlentry first, tail;    first = zipEntry(p);    for (i = 0; p[0] != ZIP_END &amp;&amp; i &lt; num; i++) {        p += zipRawEntryLength(p);        deleted++;    }    totlen = p-first.p;    if (totlen &gt; 0) {        if (p[0] != ZIP_END) {            /* 如果删除元素没有到尾部，则需要重新计算删除元素后面元素中prevlen字段占用空间，类似插入时进行的操作 */            nextdiff = zipPrevLenByteDiff(p,first.prevrawlen);            p -= nextdiff;            zipPrevEncodeLength(p,first.prevrawlen);            /* 重置尾部偏移量 */            ZIPLIST_TAIL_OFFSET(zl) =                intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))-totlen);            /* 如果删除元素没有到尾部，尾部偏移量需要加上nextdiff偏移量 */            tail = zipEntry(p);            if (p[tail.headersize+tail.len] != ZIP_END) {                ZIPLIST_TAIL_OFFSET(zl) =                   intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))+nextdiff);            }            /* 移动元素至删除元素位置*/            memmove(first.p,p,                intrev32ifbe(ZIPLIST_BYTES(zl))-(p-zl)-1);        } else {            /* 如果删除的元素到达尾部，则不需要移动*/            ZIPLIST_TAIL_OFFSET(zl) =                intrev32ifbe((first.p-zl)-first.prevrawlen);        }        /* 重置ziplist空间 */        offset = first.p-zl;        zl = ziplistResize(zl, intrev32ifbe(ZIPLIST_BYTES(zl))-totlen+nextdiff);        ZIPLIST_INCR_LENGTH(zl,-deleted);        p = zl+offset;        /* 同样和插入时一样，需要遍历检测删除元素后面的元素prevlen空间是否足够，不足时进行扩展*/        if (nextdiff != 0)            zl = __ziplistCascadeUpdate(zl,p);    }    return zl;}</code></pre><h2 id="八、连锁更新"><a href="#八、连锁更新" class="headerlink" title="八、连锁更新"></a>八、连锁更新</h2><p>由于每个节点都保存着前一个节点的长度，并且redis出于节省内存的考量，针对254这个分界点上下将<code>prelen</code>的长度分别设为1和5字节。因此当我们插入一个节点时，后一个节点的<code>prelen</code>可能就需要进行扩展；那么如果后一个节点原本的长度为253呢？由于<code>prelen</code>的扩展，导致再后一个节点也需要进行扩展。在最极端情况下会将整个<code>ziplist</code>都进行更新。</p><p>在push的代码中可以看到如果当前节点的<code>prelen</code>字段进行了扩展，会调用<code>__ziplistCascadeUpdate</code>进行连锁更新：</p><pre><code>unsigned char *__ziplistCascadeUpdate(unsigned char *zl, unsigned char *p) {    // 连锁更新    size_t curlen = intrev32ifbe(ZIPLIST_BYTES(zl)), rawlen, rawlensize;    size_t offset, noffset, extra;    unsigned char *np;    zlentry cur, next;    while (p[0] != ZIP_END) {   // 遍历所有节点        zipEntry(p, &amp;cur);  // 获取当前节点        rawlen = cur.headersize + cur.len;  // 当前节点长度        rawlensize = zipPrevEncodeLength(NULL,rawlen);  // 当前节点所需要的prelen大小        /* Abort if there is no next entry. */        if (p[rawlen] == ZIP_END) break;    // 没有下一个节点        zipEntry(p+rawlen, &amp;next);  // 获取上一个节点        /* Abort when "prevlen" has not changed. */        if (next.prevrawlen == rawlen) break;   // prelen没变直接break        if (next.prevrawlensize &lt; rawlensize) { // 只有当需要扩展的时候才会触发连锁更新            /* The "prevlen" field of "next" needs more bytes to hold             * the raw length of "cur". */            offset = p-zl;  // 记录偏移量，预防内存地址变更            extra = rawlensize-next.prevrawlensize;            zl = ziplistResize(zl,curlen+extra);    // 重新申请内存            p = zl+offset;            /* Current pointer and offset for next element. */            np = p+rawlen;            noffset = np-zl;            /* Update tail offset when next element is not the tail element. */            if ((zl+intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))) != np) { // 更新tailoffset                ZIPLIST_TAIL_OFFSET(zl) =                    intrev32ifbe(intrev32ifbe(ZIPLIST_TAIL_OFFSET(zl))+extra);            }            /* Move the tail to the back. */            memmove(np+rawlensize,                np+next.prevrawlensize,                curlen-noffset-next.prevrawlensize-1);  // 内存拷贝            zipPrevEncodeLength(np,rawlen); // 记录新的prelen            /* Advance the cursor */            p += rawlen;    // 检查下一个节点            curlen += extra;    // 更新curlen        } else {    // 小于之前的size或者相等都并不会引起连锁更新            if (next.prevrawlensize &gt; rawlensize) {                zipPrevEncodeLengthForceLarge(p+rawlen,rawlen); // 当原有的prelensize大于当前所需时，不进行收缩直接赋值减少后续连锁更新的可能性            } else {                zipPrevEncodeLength(p+rawlen,rawlen);            }            /* Stop here, as the raw length of "next" has not changed. */            break;  // 直接结束连锁更新        }    }    return zl;}</code></pre><p>可以看到<code>ziplist</code>的连锁更新是一个一个节点进行校验，直到遍历完整个<code>ziplist</code>或遇到不需要更新的节点为止。</p><p>尽管连锁更新的复杂度较高，但它真正造成性能问题的几率是很低的。 </p><ol><li>首先，压缩列表里要恰好有多个连续的、长度介于250 字节至253 宇节之间的节点，连锁更新才有可能被引发，在实际中，这种情况并不多见。</li><li>其次，即使出现连锁更新，但只要被更新的节点数量不多，就不会对性能造成任何影响：比如说，对三五个节点进行连锁更新是绝对不会影响性能的。</li></ol><p>因为以上原因，<code>ziplistPush</code>等命令的平均复杂度仅为0（在实际中，我们可以放心地使用这些函数，而不必担心连锁更新会影响压缩列表的性能。</p><h2 id="九、总结"><a href="#九、总结" class="headerlink" title="九、总结"></a>九、总结</h2><ol><li><code>ziplist</code>是 redis 为了节省内存，提升存储效率自定义的一种紧凑的数据结构</li><li><code>ziplist</code>保存着尾节点的偏移量，可以方便的拿到头尾节点</li><li>每一个<code>entry</code>都保存着前一个<code>entry</code>的长度，可以很方便的从尾遍历</li><li>每个<code>entry</code>中都可以保存一个字节数组或整数，不同类型和大小的数据有不同的编码方式</li><li>添加和删除节点可能会引发连锁更新，极端情况下会更新整个<code>ziplist</code>，但是概率很小</li></ol><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://www.jianshu.com/p/afaf78aaf615">Redis源码分析-压缩列表ziplist</a></p><p><a href="http://czrzchao.com/redisSourceZiplist">redis源码解读(五):基础数据结构之ziplist</a></p><p><a href="https://www.cnblogs.com/ourroad/p/4896387.html">Redis之ziplist数据结构</a></p><p><a href="https://blog.csdn.net/qiangzhenyi1207/article/details/80353104">redis源码之压缩列表ziplist</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis 源码分析(四) ：intset</title>
      <link href="/2019/08/10/reids-source-code-4/"/>
      <url>/2019/08/10/reids-source-code-4/</url>
      
        <content type="html"><![CDATA[<h2 id="一、什么是intset"><a href="#一、什么是intset" class="headerlink" title="一、什么是intset"></a>一、什么是intset</h2><p><code>intset</code>是Redis内存数据结构之一，用来实现Redis的Set结构（<strong>当集合元素不大于设定值并且元素都是整数时，就会用<code>intset</code>作为<code>set</code>的底层数据结构</strong>），它的特点有：</p><ul><li>元素类型只能为数字。</li><li>元素有三种类型：<code>int16_t</code>、<code>int32_t</code>、<code>int64_t</code>。</li><li>元素有序，不可重复。</li><li><code>intset</code>和<code>sds</code>一样，内存连续，就像数组一样。</li></ul><h2 id="二、数据结构定义"><a href="#二、数据结构定义" class="headerlink" title="二、数据结构定义"></a>二、数据结构定义</h2><pre><code>typedef struct intset {    uint32_t encoding;  // 编码类型 int16_t、int32_t、int64_t    uint32_t length;    // 长度 最大长度:2^32    int8_t contents[];  // 柔性数组} intset;</code></pre><ul><li><p><code>encoding</code>为<code>inset</code>的编码方式，有3种编码方式，分别对应不同范围的整型：</p><pre><code>  #define INTSET_ENC_INT16 (sizeof(int16_t))  // -32768~32767  #define INTSET_ENC_INT32 (sizeof(int32_t))  // -2147483648~2147483647  #define INTSET_ENC_INT64 (sizeof(int64_t))  // -2^63~2^63-1</code></pre></li><li><p><code>intset</code>的编码是由最大的一个数决定的，如果有一个数是<code>int64</code>，那么整个<code>inset</code>的编码都是<code>int64</code>。</p></li><li><p><code>length</code>是<code>inset</code>的整数个数</p></li><li><p><code>contents</code>整数数组</p></li></ul><p><code>intset</code>的内存是连续的，所有的数据增删改查操作都是在内存地址偏移的基础上进行的，并且整数的保存也是有序的，一个保存了5个<code>int16</code>的<code>intset</code>的内存示意图如下：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8f5883befccdfdc2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_intset.png"></p><p>由于<code>intset</code>是在内存上直接操作赋值，并且所存储的值都超过了一个字节，所以需要考虑大小端的问题：</p><ul><li>大端模式，是指数据的高字节保存在内存的低地址中，而数据的低字节保存在内存的高地址中，这样的存储模式有点儿类似于把数据当作字符串顺序处理：地址由小向大增加，而数据从高位往低位放；这和我们的阅读习惯一致。</li><li>小端模式，是指数据的高字节保存在内存的高地址中，而数据的低字节保存在内存的低地址中，这种存储模式将地址的高低和数据位权有效地结合起来，高地址部分权值高，低地址部分权值低。</li></ul><p>redis 的所有存储方式都是小端存储，在<code>endianconv.h</code>中有一段大小端的宏定义，如果当前cpu的字节序为大端就进行相应的转换：</p><pre><code>#if (BYTE_ORDER == LITTLE_ENDIAN)    #define memrev16ifbe(p)    #define memrev32ifbe(p)    #define memrev64ifbe(p)    #define intrev16ifbe(v) (v)    #define intrev32ifbe(v) (v)    #define intrev64ifbe(v) (v)#else    #define memrev16ifbe(p) memrev16(p)    #define memrev32ifbe(p) memrev32(p)    #define memrev64ifbe(p) memrev64(p)    #define intrev16ifbe(v) intrev16(v)    #define intrev32ifbe(v) intrev32(v)    #define intrev64ifbe(v) intrev64(v)#endif</code></pre><h2 id="创建集合"><a href="#创建集合" class="headerlink" title="创建集合"></a>创建集合</h2><pre><code>/* Create an empty intset. */intset *intsetNew(void) {    intset *is = zmalloc(sizeof(intset));  // 分配空间    is-&gt;encoding = intrev32ifbe(INTSET_ENC_INT16);  // 初试创建默认元素大小为 2 字节    is-&gt;length = 0;    return is;}</code></pre><h2 id="新增元素"><a href="#新增元素" class="headerlink" title="新增元素"></a>新增元素</h2><p><code>intsetAdd </code>的过程涉及到了<code>intset</code>的升级、查找和插入。</p><pre><code>intset *intsetAdd(intset *is, int64_t value, uint8_t *success) {     /*为了节省空间, 判断添加的元素需要编码为何种数据类型, 比如int16, int32, int64*/    uint8_t valenc = _intsetValueEncoding(value);    uint32_t pos;    if (success) *success = 1;    /*如果intset编码位数无法容纳新元素，则需要重新更新整个intset编码*/    if (valenc &gt; intrev32ifbe(is-&gt;encoding)) {        /* 更新编码并添加新元素 */        return intsetUpgradeAndAdd(is,value);    } else {        /*搜索新添加元素是否已经存在，存在则返回失败，此函数在查找一节会详细讲解*/        if (intsetSearch(is,value,&amp;pos)) {            if (success) *success = 0;            return is;        }                /*扩展内存空间*/        is = intsetResize(is,intrev32ifbe(is-&gt;length)+1);                if (pos &lt; intrev32ifbe(is-&gt;length))             /*如果添加元素位置不是一整块内存尾部，则需将其后面元素后移一个元素位置*/            intsetMoveTail(is,pos,pos+1);    }        /*pos位置处赋值*/    _intsetSet(is,pos,value);    is-&gt;length = intrev32ifbe(intrev32ifbe(is-&gt;length)+1);    return is;}/*根据元素大小决定元素存储长度*/static uint8_t _intsetValueEncoding(int64_t v) {    if (v &lt; INT32_MIN || v &gt; INT32_MAX)        return INTSET_ENC_INT64;    else if (v &lt; INT16_MIN || v &gt; INT16_MAX)        return INTSET_ENC_INT32;    else        return INTSET_ENC_INT16;}/*重置intset空间大小，每次zrealloc扩展内存大小*/static intset *intsetResize(intset *is, uint32_t len) {    uint32_t size = len*intrev32ifbe(is-&gt;encoding);    is = zrealloc(is,sizeof(intset)+size);    return is;}/*向后移动元素*/static void intsetMoveTail(intset *is, uint32_t from, uint32_t to) {    void *src, *dst;    uint32_t bytes = intrev32ifbe(is-&gt;length)-from;    uint32_t encoding = intrev32ifbe(is-&gt;encoding);    if (encoding == INTSET_ENC_INT64) {        src = (int64_t*)is-&gt;contents+from;        dst = (int64_t*)is-&gt;contents+to;        bytes *= sizeof(int64_t);    } else if (encoding == INTSET_ENC_INT32) {        src = (int32_t*)is-&gt;contents+from;        dst = (int32_t*)is-&gt;contents+to;        bytes *= sizeof(int32_t);    } else {        src = (int16_t*)is-&gt;contents+from;        dst = (int16_t*)is-&gt;contents+to;        bytes *= sizeof(int16_t);    }    memmove(dst,src,bytes); // 由于移动前后地址会有重叠，因此要利用memmove进行内存拷贝 memcpy无法保障结果正确性}/* 更新集合编码并添加新元素 */static intset *intsetUpgradeAndAdd(intset *is, int64_t value) {    uint8_t curenc = intrev32ifbe(is-&gt;encoding);    uint8_t newenc = _intsetValueEncoding(value);    int length = intrev32ifbe(is-&gt;length);    int prepend = value &lt; 0 ? 1 : 0;    /* 设置新编码，并扩展足够内存空间*/    is-&gt;encoding = intrev32ifbe(newenc);    is = intsetResize(is,intrev32ifbe(is-&gt;length)+1);    /* 取出原来空间中元素，从后开始往前依次放入新的位置 */    while(length--)        _intsetSet(is,length+prepend,_intsetGetEncoded(is,length,curenc));    /* 放置value值，要么在数组头，要么在数组尾部 */    if (prepend)        _intsetSet(is,0,value);    else        _intsetSet(is,intrev32ifbe(is-&gt;length),value);    is-&gt;length = intrev32ifbe(intrev32ifbe(is-&gt;length)+1);    return is;}</code></pre><h2 id="查找元素"><a href="#查找元素" class="headerlink" title="查找元素"></a>查找元素</h2><p>为了确保<code>intset</code>元素的唯一性，再插入之前会进行一次查找，<code>intsetSearch</code>函数定义如下：</p><pre><code>uint8_t intsetFind(intset *is, int64_t value) {    /*判断待查元素编码是否符合条件，不符合直接返回false，否则进入intsetSearch进行实际查找*/    uint8_t valenc = _intsetValueEncoding(value);    return valenc &lt;= intrev32ifbe(is-&gt;encoding) &amp;&amp; intsetSearch(is,value,NULL);}static uint8_t intsetSearch(intset *is, int64_t value, uint32_t *pos) {    int min = 0, max = intrev32ifbe(is-&gt;length)-1, mid = -1;    int64_t cur = -1;    /* 集合为空，直接返回第一个位置 */    if (intrev32ifbe(is-&gt;length) == 0) {        if (pos) *pos = 0;        return 0;    } else {        /* _intsetGet函数仅仅获取set集合中pos位置的值， 如果待查元素大于集合尾部元素，则直接返回待查元素位置为集合长度*/        if (value &gt; _intsetGet(is,intrev32ifbe(is-&gt;length)-1)) {            if (pos) *pos = intrev32ifbe(is-&gt;length);            return 0;        /*如果待查元素小于集合头部元素，则直接返回待查元素位置为0*/        } else if (value &lt; _intsetGet(is,0)) {            if (pos) *pos = 0;            return 0;        }    }    /*二分查找*/    while(max &gt;= min) {        mid = ((unsigned int)min + (unsigned int)max) &gt;&gt; 1;        cur = _intsetGet(is,mid);        if (value &gt; cur) {            min = mid+1;        } else if (value &lt; cur) {            max = mid-1;        } else {            break;        }    }        /*找到元素返回1，否则返回0，pos为元素应该位置*/    if (value == cur) {        if (pos) *pos = mid;        return 1;    } else {        if (pos) *pos = min;        return 0;    }}</code></pre><p>上述函数的作用就是利用<code>intset</code>有序的特性，通过二分法对目标<code>value</code>进行查找，如果找到返回1，反之返回0，<code>pos</code>作为引用传入函数中，会被赋值为<code>value</code>在<code>intset</code>中对应的位置。<br><code>intsetSearch</code>中多次调用的<code>_intsetGet</code>是用来获取对应<code>pos</code>的<code>value</code>值的函数：</p><pre><code>static int64_t _intsetGet(intset *is, int pos) {    // 获取值    return _intsetGetEncoded(is,pos,intrev32ifbe(is-&gt;encoding));}static int64_t _intsetGetEncoded(intset *is, int pos, uint8_t enc) {    // 根据encode获取对应的值    int64_t v64;    int32_t v32;    int16_t v16;    if (enc == INTSET_ENC_INT64) {        memcpy(&amp;v64,((int64_t*)is-&gt;contents)+pos,sizeof(v64));        memrev64ifbe(&amp;v64); // 大小端转换        return v64;    } else if (enc == INTSET_ENC_INT32) {        memcpy(&amp;v32,((int32_t*)is-&gt;contents)+pos,sizeof(v32));        memrev32ifbe(&amp;v32);        return v32;    } else {        memcpy(&amp;v16,((int16_t*)is-&gt;contents)+pos,sizeof(v16));        memrev16ifbe(&amp;v16);        return v16;    }}</code></pre><p>可以看到<code>intset</code>在获取值的时候都是通过地址偏移、内存拷贝，然后进行大小端转换处理完成的。</p><h2 id="删除元素"><a href="#删除元素" class="headerlink" title="删除元素"></a>删除元素</h2><pre><code>intset *intsetRemove(intset *is, int64_t value, int *success) {    uint8_t valenc = _intsetValueEncoding(value);    uint32_t pos;    if (success) *success = 0;        /*查找元素是否存在*/    if (valenc &lt;= intrev32ifbe(is-&gt;encoding) &amp;&amp; intsetSearch(is,value,&amp;pos)) {        uint32_t len = intrev32ifbe(is-&gt;length);        if (success) *success = 1;        /*删除元素，并移动其他元素覆盖原来位置，这里没有缓存空间，而是直接重置原来空间，可能是为了节省内存*/        if (pos &lt; (len-1)) intsetMoveTail(is,pos+1,pos);        is = intsetResize(is,len-1);        is-&gt;length = intrev32ifbe(len-1);    }    return is;}</code></pre><h2 id="升级"><a href="#升级" class="headerlink" title="升级"></a>升级</h2><p>当插入的<code>value</code>大于当前<code>intset</code>的<code>encode</code>时就需要对<code>intset</code>进行升级，以适应更大的值：</p><pre><code>static intset *intsetUpgradeAndAdd(intset *is, int64_t value) { // 升级并且添加新元素    uint8_t curenc = intrev32ifbe(is-&gt;encoding);    uint8_t newenc = _intsetValueEncoding(value);    int length = intrev32ifbe(is-&gt;length);    int prepend = value &lt; 0 ? 1 : 0;    /* First set new encoding and resize */    is-&gt;encoding = intrev32ifbe(newenc);    is = intsetResize(is,intrev32ifbe(is-&gt;length)+1);    /* Upgrade back-to-front so we don't overwrite values.     * Note that the "prepend" variable is used to make sure we have an empty     * space at either the beginning or the end of the intset. */    while(length--) // 从尾部开始，将原有数据进行迁移        _intsetSet(is,length+prepend,_intsetGetEncoded(is,length,curenc));    /* Set the value at the beginning or the end. */    if (prepend)    // 小于0在集合头部        _intsetSet(is,0,value);    else    // 在集合尾部        _intsetSet(is,intrev32ifbe(is-&gt;length),value);    is-&gt;length = intrev32ifbe(intrev32ifbe(is-&gt;length)+1);    return is;}</code></pre><p>首先当需要对原有<code>intset</code>进行升级时，插入的元素一定是大于当前<code>intset</code>的最大值或者小于当前<code>intset</code>的最小值的，因此带插入的<code>value</code>一定是在首尾，只需判断其正负即可。</p><p>升级的操作主要是将原本数据的内存地址大小进行一个统一的变更，从原<code>intset</code>的<code>length+prepend</code>开始，一个一个扩展迁移。</p><p>进行完扩展迁移之后把带插入的元素插入到头或尾即可。</p><p>一个<code>INTSET_ENC_INT16</code>-&gt;<code>INTSET_ENC_INT32</code>的升级示例如下图：<br><img src="https://upload-images.jianshu.io/upload_images/12321605-8684dff92f350f6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_intset_2.png"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>intset实质就是一个有序数组，内存连续，无重复</li><li>可以看到添加删除元素都比较耗时，查找元素是<code>O(logN)</code>时间复杂度，不适合大规模的数据</li><li>有三种编码方式，通过升级的方式进行编码切换</li><li>不支持降级</li><li>数据使用小端存储</li></ul><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://blog.csdn.net/yangbodong22011/article/details/78671625">Redis源码分析（intset）</a></p><p><a href="http://czrzchao.com/redisSourceIntset#intset">redis源码解读(四):基础数据结构之intset</a></p><p><a href="https://www.cnblogs.com/ourroad/p/4892945.html">Redis之intset数据结构):基础数据结构之intset</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis 源码分析(三) ：dict</title>
      <link href="/2019/08/09/reids-source-code-3/"/>
      <url>/2019/08/09/reids-source-code-3/</url>
      
        <content type="html"><![CDATA[<h1 id="一、什么是dict"><a href="#一、什么是dict" class="headerlink" title="一、什么是dict"></a>一、什么是dict</h1><p><code>dict</code> (dictionary 字典)，通常的存储结构是Key-Value形式的，通过Hash函数对<code>key</code>求Hash值来确定<code>Value</code>的位置，因此也叫Hash表，是一种用来解决算法中查找问题的数据结构，默认的算法复杂度接近O(1)，Redis本身也叫Remote Dictionary Server(远程字典服务器)，其实也就是一个大字典，它的<code>key</code>通常来说是String类型的，但是<code>Value</code>可以是<br><code>String</code>、<code>Set</code>、<code>ZSet</code>、<code>Hash</code>、<code>List</code>等不同的类型，下面我们看下dict的数据结构定义。</p><h1 id="二、Redis-Dict数据结构"><a href="#二、Redis-Dict数据结构" class="headerlink" title="二、Redis Dict数据结构"></a>二、Redis Dict数据结构</h1><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f7405f677814ab2b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_dict.png"></p><p>从上图可以看出与dict相关的关键数据结构有三个，分别是：</p><ul><li><code>dict</code>是Redis中的字典结构，包含两个dictht。</li><li><code>dictht</code>表示一个Hash表。</li><li><code>dictEntry</code> 是Redis中的字典结构，包含两个dictht。</li></ul><p><code>dictEntry</code>代码如下</p><pre><code>// redis 5.0.2typedef struct dictEntry {    void *key; //key void*表示任意类型指针    union {//联合体中对于数字类型提供了专门的类型优化        void *val;        uint64_t u64;        int64_t s64;        double d;    } v;    struct dictEntry *next; //next指针，用拉链法解决哈希冲突} dictEntry;</code></pre><p><code>dictht</code>代码如下</p><pre><code>// redis 5.0.2/* This is our hash table structure. Every dictionary has two of this as we * implement incremental rehashing, for the old to the new table. */typedef struct dictht {    dictEntry **table; //数组指针，每个元素都是一个指向dictEntry的指针    unsigned long size; //表示这个dictht已经分配空间的大小，大小总是2^n    unsigned long sizemask;//sizemask = size - 1; 是用来求hash值的掩码，为2^n-1    unsigned long used; //目前已有的元素数量} dictht;</code></pre><p><code>dict</code>代码如下</p><pre><code>typedef struct dict {    dictType *type; //type中定义了对于Hash表的操作函数，比如Hash函数，key比较函数等    void *privdata; //privdata是可以传递给dict的私有数据         dictht ht[2]; //每一个dict都包含两个dictht，一个用于rehash    long rehashidx; /* rehashing not in progress if rehashidx == -1 */    unsigned long iterators; /* number of iterators currently running */} dict;typedef struct dictType {    uint64_t (*hashFunction)(const void *key);// 计算hash值的函数    void *(*keyDup)(void *privdata, const void *key);// 键复制    void *(*valDup)(void *privdata, const void *obj);// 值复制    int (*keyCompare)(void *privdata, const void *key1, const void *key2);// 键比较    void (*keyDestructor)(void *privdata, void *key);// 键销毁    void (*valDestructor)(void *privdata, void *obj);// 值销毁} dictType;</code></pre><p>其实通过上面的三个数据结构，已经可以大概看出dict的组成，数据（Key-Value）存储在每一个dictEntry节点；然后一条Hash表就是一个dictht结构，里面标明了Hash表的size,used等信息；最后每一个Redis的dict结构都会默认包含两个dictht，如果有一个Hash表满足特定条件需要扩容，则会申请另一个Hash表，然后把元素ReHash过来，ReHash的意思就是重新计算每个Key的Hash值，然后把它存放在第二个Hash表合适的位置，但是这个操作在Redis中并不是集中式一次完成的，而是在后续的增删改查过程中逐步完成的，这个叫渐进式ReHash，我们后文会专门讨论。</p><h2 id="hash算法"><a href="#hash算法" class="headerlink" title="hash算法"></a>hash算法</h2><p>redis内置2种hash算法</p><ul><li><p>dictGenHashFunction，对字符串进行hash</p></li><li><p>dictGenCaseHashFunction，对字符串进行hash，不区分大小写</p><pre><code>  /* The default hashing function uses SipHash implementation   * in siphash.c. */    uint64_t siphash(const uint8_t *in, const size_t inlen, const uint8_t *k);  uint64_t siphash_nocase(const uint8_t *in, const size_t inlen, const uint8_t *k);    uint64_t dictGenHashFunction(const void *key, int len) {      return siphash(key,len,dict_hash_function_seed);  }    uint64_t dictGenCaseHashFunction(const unsigned char *buf, int len) {      return siphash_nocase(buf,len,dict_hash_function_seed);  }</code></pre></li></ul><h1 id="三、Dict的基本操作"><a href="#三、Dict的基本操作" class="headerlink" title="三、Dict的基本操作"></a>三、Dict的基本操作</h1><h2 id="创建Dict"><a href="#创建Dict" class="headerlink" title="创建Dict"></a>创建Dict</h2><pre><code>/* Reset a hash table already initialized with ht_init(). * NOTE: This function should only be called by ht_destroy(). */static void _dictReset(dictht *ht){    ht-&gt;table = NULL;    ht-&gt;size = 0;    ht-&gt;sizemask = 0;    ht-&gt;used = 0;}/* Create a new hash table */dict *dictCreate(dictType *type,        void *privDataPtr){    dict *d = zmalloc(sizeof(*d));    _dictInit(d,type,privDataPtr);    return d;}/* Initialize the hash table */int _dictInit(dict *d, dictType *type,        void *privDataPtr){    _dictReset(&amp;d-&gt;ht[0]);    _dictReset(&amp;d-&gt;ht[1]);    d-&gt;type = type;    d-&gt;privdata = privDataPtr;    d-&gt;rehashidx = -1;    d-&gt;iterators = 0;    return DICT_OK;}</code></pre><p>需要注意的是创建初始化一个dict时并没有为buckets分配空间，table是赋值为null的。只有在往dict里添加dictEntry节点时才会为buckets分配空间，真正意义上创建一张hash表。</p><p>执行dictCreate后会得到如下布局：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c0beca1119c3a95a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_dict_create.png"></p><h2 id="新增-dictAdd"><a href="#新增-dictAdd" class="headerlink" title="新增 - dictAdd"></a>新增 - dictAdd</h2><pre><code>#define dictSetVal(d, entry, _val_) do { \    if ((d)-&gt;type-&gt;valDup) \        (entry)-&gt;v.val = (d)-&gt;type-&gt;valDup((d)-&gt;privdata, _val_); \    else \        (entry)-&gt;v.val = (_val_); \} while(0)/* Add an element to the target hash table */int dictAdd(dict *d, void *key, void *val){    dictEntry *entry = dictAddRaw(d,key,NULL);//只在buckets的某个索引里新建一个dictEntry并调整链表的位置,只设置key，不设置不设置val    if (!entry) return DICT_ERR;    dictSetVal(d, entry, val);    return DICT_OK;}/* Low level add or find: * This function adds the entry but instead of setting a value returns the * dictEntry structure to the user, that will make sure to fill the value * field as he wishes. * * This function is also directly exposed to the user API to be called * mainly in order to store non-pointers inside the hash value, example: * * entry = dictAddRaw(dict,mykey,NULL); * if (entry != NULL) dictSetSignedIntegerVal(entry,1000); * * Return values: * * If key already exists NULL is returned, and "*existing" is populated * with the existing entry if existing is not NULL. * * If key was added, the hash entry is returned to be manipulated by the caller. */dictEntry *dictAddRaw(dict *d, void *key, dictEntry **existing){    long index;    dictEntry *entry;    dictht *ht;    if (dictIsRehashing(d)) _dictRehashStep(d);//判断是否是在rehash，如果是rehash会渐进式reash    /* Get the index of the new element, or -1 if     * the element already exists. */    if ((index = _dictKeyIndex(d, key, dictHashKey(d,key), existing)) == -1)        return NULL;    /* Allocate the memory and store the new entry.     * Insert the element in top, with the assumption that in a database     * system it is more likely that recently added entries are accessed     * more frequently. */    ht = dictIsRehashing(d) ? &amp;d-&gt;ht[1] : &amp;d-&gt;ht[0];//如果正在rehash的话存第二个hashtable里面    entry = zmalloc(sizeof(*entry));    entry-&gt;next = ht-&gt;table[index];    ht-&gt;table[index] = entry;    ht-&gt;used++;    /* Set the hash entry fields. */    dictSetKey(d, entry, key);    return entry;}</code></pre><p>主要分为以下几个步骤:</p><ol><li>根据key的hash值找到应该存放的位置(buckets索引)。</li><li>若dict是刚创建的还没有为bucekts分配内存，则会在找位置(_dictKeyIndex)时调用_dictExpandIfNeeded，为dictht[0]expand一个大小为4的buckets；若dict正好到了expand的时机，则会expand它的dictht[1]，并将rehashidx置为0打开rehash开关，_dictKeyIndex返回的会是dictht[1]的索引。</li><li>申请一个dictEntry大小的内存插入到buckets对应索引下的链表头部，并给dictEntry设置next指针和key。</li><li>为dictEntry设置value</li></ol><h2 id="删除-dictDelete"><a href="#删除-dictDelete" class="headerlink" title="删除 - dictDelete"></a>删除 - dictDelete</h2><pre><code>#define dictCompareKeys(d, key1, key2) \(((d)-&gt;type-&gt;keyCompare) ? \    (d)-&gt;type-&gt;keyCompare((d)-&gt;privdata, key1, key2) : \    (key1) == (key2))    /* Remove an element, returning DICT_OK on success or DICT_ERR if the * element was not found. */int dictDelete(dict *ht, const void *key) {    return dictGenericDelete(ht,key,0) ? DICT_OK : DICT_ERR;}/* Search and remove an element. This is an helper function for * dictDelete() and dictUnlink(), please check the top comment * of those functions. */static dictEntry *dictGenericDelete(dict *d, const void *key, int nofree) {    uint64_t h, idx;    dictEntry *he, *prevHe;    int table;    if (d-&gt;ht[0].used == 0 &amp;&amp; d-&gt;ht[1].used == 0) return NULL;    if (dictIsRehashing(d)) _dictRehashStep(d);    h = dictHashKey(d, key);    for (table = 0; table &lt;= 1; table++) {        idx = h &amp; d-&gt;ht[table].sizemask;        he = d-&gt;ht[table].table[idx];//找到key对应的bucket索引        prevHe = NULL;        while(he) {            if (key==he-&gt;key || dictCompareKeys(d, key, he-&gt;key)) {                /* Unlink the element from the list */                if (prevHe)                    prevHe-&gt;next = he-&gt;next;                else                    d-&gt;ht[table].table[idx] = he-&gt;next;                if (!nofree) {                    dictFreeKey(d, he);                    dictFreeVal(d, he);                    zfree(he);                }                d-&gt;ht[table].used--;                return he;            }            prevHe = he;            he = he-&gt;next;        }        if (!dictIsRehashing(d)) break;    }    return NULL; /* not found */}/* Clear &amp; Release the hash table */void dictRelease(dict *d){    _dictClear(d,&amp;d-&gt;ht[0],NULL);    _dictClear(d,&amp;d-&gt;ht[1],NULL);    zfree(d);}</code></pre><h2 id="修改-dictReplace"><a href="#修改-dictReplace" class="headerlink" title="修改 - dictReplace"></a>修改 - dictReplace</h2><pre><code>/* Add or Overwrite: * Add an element, discarding the old value if the key already exists. * Return 1 if the key was added from scratch, 0 if there was already an * element with such key and dictReplace() just performed a value update * operation. */int dictReplace(dict *d, void *key, void *val){    dictEntry *entry, *existing, auxentry;    /* Try to add the element. If the key     * does not exists dictAdd will succeed. */    entry = dictAddRaw(d,key,&amp;existing);    if (entry) {        dictSetVal(d, entry, val);        return 1;    }    /* Set the new value and free the old one. Note that it is important     * to do that in this order, as the value may just be exactly the same     * as the previous one. In this context, think to reference counting,     * you want to increment (set), and then decrement (free), and not the     * reverse. */    auxentry = *existing;    dictSetVal(d, existing, val);    dictFreeVal(d, &amp;auxentry);    return 0;}</code></pre><h2 id="查询-dictFind"><a href="#查询-dictFind" class="headerlink" title="查询 - dictFind"></a>查询 - dictFind</h2><pre><code>dictEntry *dictFind(dict *d, const void *key){    dictEntry *he;    uint64_t h, idx, table;    if (d-&gt;ht[0].used + d-&gt;ht[1].used == 0) return NULL; /* dict is empty */    if (dictIsRehashing(d)) _dictRehashStep(d);    h = dictHashKey(d, key);    for (table = 0; table &lt;= 1; table++) {        idx = h &amp; d-&gt;ht[table].sizemask;        he = d-&gt;ht[table].table[idx];        while(he) {            if (key==he-&gt;key || dictCompareKeys(d, key, he-&gt;key))                return he;            he = he-&gt;next;        }        if (!dictIsRehashing(d)) return NULL;    }    return NULL;}</code></pre><h2 id="Rehash"><a href="#Rehash" class="headerlink" title="Rehash"></a>Rehash</h2><h3 id="什么是Rehash"><a href="#什么是Rehash" class="headerlink" title="什么是Rehash"></a>什么是Rehash</h3><p>随着操作的不断执行，hash表保存的键值对会逐渐的增多或者减少，这时就会暴露一些问题。如果hash表很大，但是键值对太少，也就是hash表的负载(dictht-&gt;used/dictht-&gt;size)太小，就会有大量的内存浪费；如果hash表的负载太大，就会影响字典的查找效率。这时候就需要进行rehash将hash表的负载控制在一个合理的范围。</p><h3 id="什么时候会触发Rehash"><a href="#什么时候会触发Rehash" class="headerlink" title="什么时候会触发Rehash"></a>什么时候会触发Rehash</h3><p>当调用<code>dictAdd</code>为dict添加一个dictEntry节点时候，会<code>_dictKeyIndex</code>找到应该放置在buckets的哪个索引里，在这里会调用<code>_dictExpandIfNeeded</code>检查当前哈希表的空间是需要扩充（Rehash），若满足条件：dictht[0]的dictEntry节点数/buckets的索引数&gt;=1则调用dictExpand，若dictEntry节点数/buckets的索引数&gt;=dict_force_resize_ratio(默认是5)，则强制执行dictExpand扩充dictht[1]。</p><pre><code>/* Returns the index of a free slot that can be populated with * a hash entry for the given 'key'. * If the key already exists, -1 is returned * and the optional output parameter may be filled. * * Note that if we are in the process of rehashing the hash table, the * index is always returned in the context of the second (new) hash table. */static long _dictKeyIndex(dict *d, const void *key, uint64_t hash, dictEntry **existing){    unsigned long idx, table;    dictEntry *he;    if (existing) *existing = NULL;    /* Expand the hash table if needed */    if (_dictExpandIfNeeded(d) == DICT_ERR)        return -1;    for (table = 0; table &lt;= 1; table++) {        idx = hash &amp; d-&gt;ht[table].sizemask;        /* Search if this slot does not already contain the given key */        he = d-&gt;ht[table].table[idx];        while(he) {            if (key==he-&gt;key || dictCompareKeys(d, key, he-&gt;key)) {                if (existing) *existing = he;                return -1;            }            he = he-&gt;next;        }        if (!dictIsRehashing(d)) break;    }    return idx;}/* Expand the hash table if needed *///判断dictht[1]是否需要扩充(并将dict调整为正在rehash状态)；若dict刚创建，则扩充dictht[0]  static int _dictExpandIfNeeded(dict *d){    /* Incremental rehashing already in progress. Return. */    if (dictIsRehashing(d)) return DICT_OK; //如果正在ReHash，那直接返回OK，其实也表明申请了空间不久。    /* If the hash table is empty expand it to the initial size. */    if (d-&gt;ht[0].size == 0) return dictExpand(d, DICT_HT_INITIAL_SIZE);//如果 0 号哈希表的大小为0，表示还未创建，按照默认大小`DICT_HT_INITIAL_SIZE=4`去创建    /* If we reached the 1:1 ratio, and we are allowed to resize the hash     * table (global setting) or we should avoid it but the ratio between     * elements/buckets is over the "safe" threshold, we resize doubling     * the number of buckets. */     //如果满足 0 号哈希表used&gt;size &amp;&amp;（dict_can_resize为1 或者 used/size &gt; 5） 那就默认扩两倍大小    if (d-&gt;ht[0].used &gt;= d-&gt;ht[0].size &amp;&amp;        (dict_can_resize ||         d-&gt;ht[0].used/d-&gt;ht[0].size &gt; dict_force_resize_ratio))    {        return dictExpand(d, d-&gt;ht[0].used*2);    }    return DICT_OK;}/* Expand or create the hash table *///三个功能://1.为刚初始化的dict的dictht[0]分配table(buckets)//2.为已经达到rehash要求的dict的dictht[1]分配一个更大(下一个2^n)的table(buckets),并将rehashidx置为0//3.为需要缩小bucket的dict分配一个更小的buckets，并将rehashidx置为0(打开rehash开关)int dictExpand(dict *d, unsigned long size){    /* the size is invalid if it is smaller than the number of     * elements already inside the hash table */    if (dictIsRehashing(d) || d-&gt;ht[0].used &gt; size)        return DICT_ERR;    dictht n; /* the new hash table */    unsigned long realsize = _dictNextPower(size);////从4开始找大于等于size的最小2^n作为新的slot数量    /* Rehashing to the same table size is not useful. */    if (realsize == d-&gt;ht[0].size) return DICT_ERR;    /* Allocate the new hash table and initialize all pointers to NULL */    n.size = realsize;    n.sizemask = realsize-1;    n.table = zcalloc(realsize*sizeof(dictEntry*));    n.used = 0;    /* Is this the first initialization? If so it's not really a rehashing     * we just set the first hash table so that it can accept keys. */    if (d-&gt;ht[0].table == NULL) {//刚创建的dict        d-&gt;ht[0] = n;//为d-&gt;ht[0]赋值        return DICT_OK;    }    /* Prepare a second hash table for incremental rehashing */    d-&gt;ht[1] = n;    d-&gt;rehashidx = 0;//设置为0表示开始从0号bucket Rehash    return DICT_OK;}</code></pre><h3 id="Rehash的过程"><a href="#Rehash的过程" class="headerlink" title="Rehash的过程"></a>Rehash的过程</h3><p>假设一个dict已经有4个dictEntry节点(value分别为”a”,”b”,”c”,”d”)，根据key的不同，存放在buckets的不同索引下。<br><img src="https://upload-images.jianshu.io/upload_images/12321605-348aa8977982f891.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_rehash_1.png"></p><p>现在如果我们想添加一个dictEntry，由于d-&gt;ht[0].used &gt;= d-&gt;ht[0].size (4&gt;=4)，满足了扩充dictht[1]的条件，会执行dictExpand。根据扩充规则，dictht[1]的buckets会扩充到8个槽位。<br><img src="https://upload-images.jianshu.io/upload_images/12321605-ef5ae05e3b9b7c17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redis_rehash_2.png"></p><p>之后再将要添加的dictEntry加入到dictht[1]的buckets中的某个索引下，不过这个操作不属于dictExpand，不展开了。<br>扩充之后的dict的成员变量rehashidx被赋值为0，此后每次CRUD都会执行一次被动rehash把dictht[0]的buckets中的一个链表迁移到dictht[1]中，直到迁移完毕。</p><h3 id="Rehash的方式"><a href="#Rehash的方式" class="headerlink" title="Rehash的方式"></a>Rehash的方式</h3><ol><li><p>主动Rehash，一毫秒执行一次</p><pre><code> /* Rehash for an amount of time between ms milliseconds and ms+1 milliseconds */ int dictRehashMilliseconds(dict *d, int ms) {     long long start = timeInMilliseconds();     int rehashes = 0;      while(dictRehash(d,100)) {//每次最多执行buckets的100个链表rehash         rehashes += 100;         if (timeInMilliseconds()-start &gt; ms) break;     }     return rehashes; }</code></pre></li><li><p>被动Rehash，字典的增删改查(CRUD)调用dictAdd，dicFind，dictDelete，dictGetRandomKey等函数时，会调用_dictRehashStep，迁移buckets中的一个非空bucket</p></li><li><pre><code> if (dictIsRehashing(d)) _dictRehashStep(d);</code></pre></li></ol><p>rehash函数</p><pre><code>/* Performs N steps of incremental rehashing. Returns 1 if there are still * keys to move from the old to the new hash table, otherwise 0 is returned. * * Note that a rehashing step consists in moving a bucket (that may have more * than one key as we use chaining) from the old to the new hash table, however * since part of the hash table may be composed of empty spaces, it is not * guaranteed that this function will rehash even a single bucket, since it * will visit at max N*10 empty buckets in total, otherwise the amount of * work it does would be unbound and the function may block for a long time. */int dictRehash(dict *d, int n) {    //int empty_visits = n*10; empty_visits表示每次最多跳过10倍步长的空桶    //（一个桶就是ht-&gt;table数组的一个位置），然后当我们找到一个非空的桶时，    // 就将这个桶中所有的key全都ReHash到 1 号Hash表。最后每次都会判断是否将所有的key全部ReHash了，    // 如果已经全部完成，就释放掉ht[0],然后将ht[1]变成ht[0]。    int empty_visits = n*10; /* Max number of empty buckets to visit. */    if (!dictIsRehashing(d)) return 0;    while(n-- &amp;&amp; d-&gt;ht[0].used != 0) {//遍历n个bucket,ht[0]中还有dictEntry        dictEntry *de, *nextde;        /* Note that rehashidx can't overflow as we are sure there are more         * elements because ht[0].used != 0 */        assert(d-&gt;ht[0].size &gt; (unsigned long)d-&gt;rehashidx);        while(d-&gt;ht[0].table[d-&gt;rehashidx] == NULL) {            //当前bucket为空时跳到下一个bucket并且            d-&gt;rehashidx++;            if (--empty_visits == 0) return 1;        }        //直到当前bucket不为空bucket时        de = d-&gt;ht[0].table[d-&gt;rehashidx];        /* Move all the keys in this bucket from the old to the new hash HT */        while(de) {//把当前bucket的所有ditcEntry节点都移到ht[1]            uint64_t h;            nextde = de-&gt;next;            /* Get the index in the new hash table */            //hash函数算出的值&amp; 新hashtable(buckets)的sizemask,保证h会小于新buckets的size            h = dictHashKey(d, de-&gt;key) &amp; d-&gt;ht[1].sizemask;            de-&gt;next = d-&gt;ht[1].table[h];//插入到链表的最前面！省时间            d-&gt;ht[1].table[h] = de;            d-&gt;ht[0].used--;            d-&gt;ht[1].used++;            de = nextde;        }        d-&gt;ht[0].table[d-&gt;rehashidx] = NULL;//当前bucket已经完全移走        d-&gt;rehashidx++;    }    /* Check if we already rehashed the whole table... */    if (d-&gt;ht[0].used == 0) {        zfree(d-&gt;ht[0].table);//释放掉ht[0].table的内存(buckets)        d-&gt;ht[0] = d-&gt;ht[1];//浅复制，table只是一个地址，直接给ht[0]就好        _dictReset(&amp;d-&gt;ht[1]);//ht[1]的table置空        d-&gt;rehashidx = -1;        return 0;    }    /* More to rehash... */    return 1;}</code></pre><h3 id="安全-非安全迭代器"><a href="#安全-非安全迭代器" class="headerlink" title="安全/非安全迭代器"></a>安全/非安全迭代器</h3><p>safe迭代器：用户在迭代过程中可以对元素进行CRUD<br>undsafe迭代器：用户在迭代过程中禁止对元素进行CRUD</p><p>redis在<code>dict</code>结构里增加一个<code>iterator</code>成员，用来表示绑定在当前<code>dict</code>上的safe迭代器数量，dict每次CRUD执行<code>_dictRehashStep</code>时判断一下是否有绑定safe迭代器，如果有则不进行rehash以免扰乱迭代器的迭代，这样safe迭代时字典就可以正常进行CRUD操作了。</p><pre><code>static void _dictRehashStep(dict *d) {    if (d-&gt;iterators == 0) dictRehash(d,1);}</code></pre><p>unsafe迭代器在执行迭代过程中不允许对dict进行其他操作，如何保证这一点呢？</p><p>redis在第一次执行迭代时会用<code>dictht[0]</code>、<code>dictht[1]</code>的<code>used</code>、<code>size</code>、<code>buckets</code>地址计算一个<code>fingerprint</code>(指纹)，在迭代结束后释放迭代器时再计算一遍<code>fingerprint</code>看看是否与第一次计算的一致，若不一致则用断言终止进程，生成指纹的函数如下:</p><pre><code>//unsafe迭代器在第一次dictNext时用dict的两个dictht的table、size、used进行hash算出一个结果//最后释放iterator时再调用这个函数生成指纹，看看结果是否一致，不一致就报错.//safe迭代器不会用到这个long long dictFingerprint(dict *d) {    long long integers[6], hash = 0;    int j;    integers[0] = (long) d-&gt;ht[0].table;//把指针类型转换成long    integers[1] = d-&gt;ht[0].size;    integers[2] = d-&gt;ht[0].used;    integers[3] = (long) d-&gt;ht[1].table;    integers[4] = d-&gt;ht[1].size;    integers[5] = d-&gt;ht[1].used;    /* We hash N integers by summing every successive integer with the integer     * hashing of the previous sum. Basically:     *     * Result = hash(hash(hash(int1)+int2)+int3) ...     *     * This way the same set of integers in a different order will (likely) hash     * to a different number. */    for (j = 0; j &lt; 6; j++) {        hash += integers[j];        /* For the hashing step we use Tomas Wang's 64 bit integer hash. */        hash = (~hash) + (hash &lt;&lt; 21); // hash = (hash &lt;&lt; 21) - hash - 1;        hash = hash ^ (hash &gt;&gt; 24);        hash = (hash + (hash &lt;&lt; 3)) + (hash &lt;&lt; 8); // hash * 265        hash = hash ^ (hash &gt;&gt; 14);        hash = (hash + (hash &lt;&lt; 2)) + (hash &lt;&lt; 4); // hash * 21        hash = hash ^ (hash &gt;&gt; 28);        hash = hash + (hash &lt;&lt; 31);    }    return hash;}</code></pre><h3 id="dictIterator定义"><a href="#dictIterator定义" class="headerlink" title="dictIterator定义"></a>dictIterator定义</h3><pre><code>typedef struct dictIterator {    dict *d;    long index;//当前buckets索引，buckets索引类型是unsinged long，而这个初始化会是-1,所以long    int table, safe;//table是ht的索引只有0和1，safe是安全迭代器和不安全迭代器    //安全迭代器就等于加了一个锁在dict，使dict在CRUD时ditcEntry不能被动rehash    dictEntry *entry, *nextEntry;//当前hash节点以及下一个hash节点    /* unsafe iterator fingerprint for misuse detection. */    long long fingerprint;//dict.c里的dictFingerprint(),不安全迭代器相关} dictIterator;</code></pre><h3 id="dictGetIterator-创建一个迭代器"><a href="#dictGetIterator-创建一个迭代器" class="headerlink" title="dictGetIterator:创建一个迭代器"></a>dictGetIterator:创建一个迭代器</h3><pre><code>//默认是new一个unsafe迭代器dictIterator *dictGetIterator(dict *d)//获取一个iterator就是为这个dict new一个迭代器{    //不设置成员变量fingerprint，在dictNext的时候才设置。    dictIterator *iter = zmalloc(sizeof(*iter));    iter-&gt;d = d;    iter-&gt;table = 0;    iter-&gt;index = -1;    iter-&gt;safe = 0;    iter-&gt;entry = NULL;    iter-&gt;nextEntry = NULL;    return iter;}dictIterator *dictGetSafeIterator(dict *d) {    dictIterator *i = dictGetIterator(d);    i-&gt;safe = 1;    return i;}</code></pre><h3 id="dictNext-迭代一个dictEntry节点"><a href="#dictNext-迭代一个dictEntry节点" class="headerlink" title="dictNext:迭代一个dictEntry节点"></a>dictNext:迭代一个dictEntry节点</h3><p>虽然safe迭代器会禁止rehash，但在迭代时有可能已经rehash了一部分，所以迭代器也会遍历在dictht[1]中的所有dictEntry。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://blog.csdn.net/yangbodong22011/article/details/78467583">Redis源码分析（dict）</a></p><p><a href="http://czrzchao.com/redisSourceDict">redis源码解读(三):基础数据结构之dict</a></p><p><a href="https://blog.csdn.net/yangbodong22011/article/details/78467583">Redis源码分析（dict）</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《MySQL实战45讲》</title>
      <link href="/2019/08/01/note/mysql-45-lesson/"/>
      <url>/2019/08/01/note/mysql-45-lesson/</url>
      
        <content type="html"><![CDATA[<h2 id="binlog-amp-amp-redo-log"><a href="#binlog-amp-amp-redo-log" class="headerlink" title="binlog &amp;&amp; redo log"></a>binlog &amp;&amp; redo log</h2><h3 id="什么是-binlog"><a href="#什么是-binlog" class="headerlink" title="什么是 binlog"></a>什么是 binlog</h3><ul><li>binlog 是逻辑日志，记录的是这个语句的原始逻辑/变化，比如“<code>给 ID=2 这一行的 c 字段加 1 </code>”。 </li><li>binlog 是追加写，不会覆盖之前的数据，可以提供完整的数据归档的能力。</li></ul><h3 id="什么是-redo-log"><a href="#什么是-redo-log" class="headerlink" title="什么是 redo log"></a>什么是 redo log</h3><ul><li>redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；</li><li>redo log 提供 crash-safe 能力。</li><li>一般只有4G ，4个文件，循环复写。</li></ul><h3 id="binlog-和-redo-log-不同点"><a href="#binlog-和-redo-log-不同点" class="headerlink" title="binlog 和 redo log 不同点"></a>binlog 和 redo log 不同点</h3><p>因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。</p><ol><li>redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。</li><li>redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。</li><li>redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。</li></ol><h3 id="binlog-的写入机制"><a href="#binlog-的写入机制" class="headerlink" title="binlog 的写入机制"></a>binlog 的写入机制</h3><p>其实，binlog 的写入逻辑比较简单：事务执行过程中，先把日志写到 binlog cache，事务提交的时候，再把 binlog cache 写到 binlog 文件中。</p><p>一个事务的 binlog 是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。这就涉及到了 binlog cache 的保存问题。</p><p>系统给 binlog cache 分配了一片内存，每个线程一个，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。</p><p>事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 中，并清空 binlog cache。状态如图 1 所示。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d9e2d6cb67016131.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>可以看到，每个线程有自己 binlog cache，但是共用同一份 binlog 文件。</p><ul><li>图中的 write，指的就是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快。</li><li>图中的 fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为 fsync 才占磁盘的 IOPS。</li></ul><p>write 和 fsync 的时机，是由参数 sync_binlog 控制的：</p><ul><li>sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync；</li><li>sync_binlog=1 的时候，表示每次提交事务都会执行 fsync；</li><li>sync_binlog=N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。</li></ul><p>因此，在出现 IO 瓶颈的场景里，将 sync_binlog 设置成一个比较大的值，可以提升性能。在实际的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成 0，比较常见的是将其设置为 100~1000 中的某个数值。</p><p>但是，将 sync_binlog 设置为 N，对应的风险是：如果主机发生异常重启，会丢失最近 N 个事务的 binlog 日志。</p><h3 id="redo-log-的写入机制"><a href="#redo-log-的写入机制" class="headerlink" title="redo log 的写入机制"></a>redo log 的写入机制</h3><p>事务在执行过程中，生成的 redo log 是要先写到 redo log buffer 的。</p><p>redo log buffer 里面的内容，是不是每次生成后都要直接持久化到磁盘呢？答案是，不需要。</p><p>如果事务执行期间 MySQL 发生异常重启，那这部分日志就丢了。由于事务并没有提交，所以这时日志丢了也不会有损失。</p><p>那么，另外一个问题是，事务还没提交的时候，redo log buffer 中的部分日志有没有可能被持久化到磁盘呢？答案是，确实会有。</p><p>这个问题，要从 redo log 可能存在的三种状态说起。这三种状态，对应的就是图 2 中的三个颜色块。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-15e11ef8355650a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="redo log 三种状态"></p><ol><li>存在 redo log buffer 中，物理上是在 MySQL 进程内存中，就是图中的红色部分；</li><li>写到磁盘 (write)，但是没有持久化（fsync)，物理上是在文件系统的 page cache 里面，也就是图中的黄色部分；</li><li>持久化到磁盘，对应的是 hard disk，也就是图中的绿色部分。</li></ol><p>日志写到 redo log buffer 是很快的，wirte 到 page cache 也差不多，但是持久化到磁盘的速度就慢多了。</p><p>为了控制 redo log 的写入策略，InnoDB 提供了 innodb_flush_log_at_trx_commit 参数，它有三种可能取值：</p><ul><li>设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ;</li><li>设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘；</li><li>设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache。</li></ul><p>InnoDB 有一个后台线程，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘。</p><p>注意，事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些 redo log 也会被后台线程一起持久化到磁盘。也就是说，一个没有提交的事务的 redo log，也是可能已经持久化到磁盘的。</p><p>实际上，除了后台线程每秒一次的轮询操作外，还有两种场景会让一个没有提交的事务的 redo log 写入到磁盘中。</p><ol><li>一种是，redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘。注意，由于这个事务并没有提交，所以这个写盘动作只是 write，而没有调用 fsync，也就是只留在了文件系统的 page cache。</li><li>另一种是，并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。假设一个事务 A 执行到一半，已经写了一些 redo log 到 buffer 中，这时候有另外一个线程的事务 B 提交，如果 innodb_flush_log_at_trx_commit 设置的是 1，那么按照这个参数的逻辑，事务 B 要把 redo log buffer 里的日志全部持久化到磁盘。这时候，就会带上事务 A 在 redo log buffer 里的日志一起持久化到磁盘。</li></ol><p>这里需要说明的是，我们介绍两阶段提交的时候说过，时序上 redo log 先 prepare， 再写 binlog，最后再把 redo log commit。</p><p>如果把 innodb_flush_log_at_trx_commit 设置成 1，那么 redo log 在 prepare 阶段就要持久化一次，因为有一个崩溃恢复逻辑是要依赖于 prepare 的 redo log，再加上 binlog 来恢复的。（如果你印象有点儿模糊了，可以再回顾下第 15 篇文章中的相关内容）。</p><p>每秒一次后台轮询刷盘，再加上崩溃恢复这个逻辑，InnoDB 就认为 redo log 在 commit 的时候就不需要 fsync 了，只会 write 到文件系统的 page cache 中就够了。</p><p>通常我们说 MySQL 的“双 1”配置，指的就是 sync_binlog 和 innodb_flush_log_at_trx_commit 都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是 redo log（prepare 阶段），一次是 binlog。</p><h3 id="redo-log-存储方式"><a href="#redo-log-存储方式" class="headerlink" title="redo log 存储方式"></a>redo log 存储方式</h3><p>当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log 里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3dded44ffd4d9c82.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。</p><p>write pos 和 checkpoint <strong>之间的是还空着的部分，可以用来记录新的操作</strong>。如果 write pos 追上 checkpoint，<strong>表示“粉板”满了，这时候不能再执行新的更新</strong>，得停下来先擦掉一些记录，把 checkpoint 推进一下。</p><p>有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 <strong>crash-safe</strong>。</p><p>redo log 用于保证 crash-safe 能力。<code>innodb_flush_log_at_trx_commit</code> 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。</p><h3 id="组提交（group-commit）机制"><a href="#组提交（group-commit）机制" class="headerlink" title="组提交（group commit）机制"></a>组提交（group commit）机制</h3><p>日志逻辑序列号（log sequence number，LSN）。LSN 是单调递增的，用来对应 redo log 的一个个写入点。每次写入长度为 length 的 redo log， LSN 的值就会加上 length。</p><p>LSN 也会写到 InnoDB 的数据页中，来确保数据页不会被多次执行重复的 redo log。关于 LSN 和 redo log、checkpoint 的关系，我会在后面的文章中详细展开。</p><p>如图 3 所示，是三个并发事务 (trx1, trx2, trx3) 在 prepare 阶段，都写完 redo log buffer，持久化到磁盘的过程，对应的 LSN 分别是 50、120 和 160。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-126180d07c62c833.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>MySQL 为了让组提交的效果更好，把 redo log 做 fsync 的时间拖到了步骤 1 之后。也就是说，上面的图变成了这样：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-bc7bede59859cd59.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这么一来，binlog 也可以组提交了。在执行图 5 中第 4 步把 binlog fsync 到磁盘时，如果有多个事务的 binlog 已经写完了，也是一起持久化的，这样也可以减少 IOPS 的消耗。</p><p>不过通常情况下第 3 步执行得会很快，所以 binlog 的 write 和 fsync 间的间隔时间短，导致能集合到一起持久化的 binlog 比较少，因此 binlog 的组提交的效果通常不如 redo log 的效果那么好。</p><p>如果你想提升 binlog 组提交的效果，可以通过设置 <code>binlog_group_commit_sync_delay</code> 和 <code>binlog_group_commit_sync_no_delay_count</code> 来实现。</p><ol><li><code>binlog_group_commit_sync_delay</code> 参数，表示延迟多少微秒后才调用 fsync;</li><li><code>binlog_group_commit_sync_no_delay_count</code> 参数，表示累积多少次以后才调用 fsync。</li></ol><h4 id="binlog-的三种格式"><a href="#binlog-的三种格式" class="headerlink" title="binlog 的三种格式"></a>binlog 的三种格式</h4><p>binlog 的三种格式 ：statement、row、mixed</p><pre><code>mysql&gt; CREATE TABLE `t` (  `id` int(11) NOT NULL,  `a` int(11) DEFAULT NULL,  `t_modified` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,  PRIMARY KEY (`id`),  KEY `a` (`a`),  KEY `t_modified`(`t_modified`)) ENGINE=InnoDB;insert into t values(1,1,'2018-11-13');insert into t values(2,2,'2018-11-12');insert into t values(3,3,'2018-11-11');insert into t values(4,4,'2018-11-10');insert into t values(5,5,'2018-11-09');</code></pre><p>注意，下面这个语句包含注释，如果你用 MySQL 客户端来做这个实验的话，要记得加 -c 参数，否则客户端会自动去掉注释。</p><pre><code>mysql&gt; delete from t /*comment*/  where a&gt;=4 and t_modified&lt;='2018-11-10' limit 1;</code></pre><p>当 binlog_format=statement 时，binlog 里面记录的就是 SQL 语句的原文。你可以用</p><pre><code>mysql&gt; show binlog events in 'master.000001';</code></pre><p>命令看 binlog 中的内容。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-209fd6c3cdf883d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图 4 delete 执行 warnings"></p><ul><li>第二行是一个 BEGIN，跟第四行的 commit 对应，表示中间是一个事务；</li><li>第三行就是真实执行的语句了。可以看到，在真实执行的 delete 命令之前，还有一个“use ‘test’”命令。这条命令不是我们主动执行的，而是 MySQL 根据当前要操作的表所在的数据库，自行添加的。这样做可以保证日志传到备库去执行的时候，不论当前的工作线程在哪个库里，都能够正确地更新到 test 库的表 t。</li><li>use ‘test’命令之后的 delete 语句，就是我们输入的 SQL 原文了。可以看到，binlog“忠实”地记录了 SQL 命令，甚至连注释也一并记录了。</li><li>最后一行是一个 COMMIT。你可以看到里面写着 xid=61。你还记得这个 XID 是做什么用的吗？如果记忆模糊了，可以再回顾一下第 15 篇文章中的相关内容。</li></ul><p>为了说明 statement 和 row 格式的区别，我们来看一下这条 delete 命令的执行效果图：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0b2d0b2aaa4c9ba9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图 4 delete 执行 warnings"></p><p>可以看到，运行这条 delete 命令产生了一个 warning，原因是当前 binlog 设置的是 statement 格式，并且语句中有 limit，所以这个命令可能是 unsafe 的。</p><p>为什么这么说呢？这是因为 delete 带 limit，很可能会出现主备数据不一致的情况。比如上面这个例子：</p><ol><li>如果 delete 语句使用的是索引 a，那么会根据索引 a 找到第一个满足条件的行，也就是说删除的是 a=4 这一行；</li><li>但如果使用的是索引 <code>t_modified</code>，那么删除的就是 <code>t_modified='2018-11-09’</code>也就是 a=5 这一行。</li></ol><p>由于 statement 格式下，记录到 binlog 里的是语句原文，因此可能会出现这样一种情况：在主库执行这条 SQL 语句的时候，用的是索引 a；而在备库执行这条 SQL 语句的时候，却使用了索引 t_modified。因此，MySQL 认为这样写是有风险的。</p><p>那么，如果我把 binlog 的格式改为 binlog_format=‘row’， 是不是就没有这个问题了呢？我们先来看看这时候 binog 中的内容吧。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9339bcfa75858829.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图 5 row 格式 binlog 示例"></p><p>可以看到，与 statement 格式的 binlog 相比，前后的 BEGIN 和 COMMIT 是一样的。但是，row 格式的 binlog 里没有了 SQL 语句的原文，而是替换成了两个 event：Table_map 和 Delete_rows。</p><ul><li><code>Table_map</code> event，用于说明接下来要操作的表是 test 库的表 t;</li><li><code>Delete_rows</code> event，用于定义删除的行为。</li></ul><p>其实，我们通过图 5 是看不到详细信息的，还需要借助 mysqlbinlog 工具，用下面这个命令解析和查看 binlog 中的内容。因为图 5 中的信息显示，这个事务的 binlog 是从 8900 这个位置开始的，所以可以用 start-position 参数来指定从这个位置的日志开始解析。</p><pre><code>mysqlbinlog  -vv data/master.000001 --start-position=8900;</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-945744ba90208f3c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图 6 row 格式 binlog 示例的详细信息"></p><p>从这个图中，我们可以看到以下几个信息：</p><ul><li>server id 1，表示这个事务是在 server_id=1 的这个库上执行的。</li><li>每个 event 都有 CRC32 的值，这是因为我把参数 binlog_checksum 设置成了 CRC32。</li><li>Table_map event 跟在图 5 中看到的相同，显示了接下来要打开的表，map 到数字 226。现在我们这条 SQL 语句只操作了一张表，如果要操作多张表呢？每个表都有一个对应的 Table_map event、都会 map 到一个单独的数字，用于区分对不同表的操作。</li><li>我们在 mysqlbinlog 的命令中，使用了 -vv 参数是为了把内容都解析出来，所以从结果里面可以看到各个字段的值（比如，@1=4、 @2=4 这些值）。</li><li>binlog_row_image 的默认配置是 FULL，因此 Delete_event 里面，包含了删掉的行的所有字段的值。如果把 binlog_row_image 设置为 MINIMAL，则只会记录必要的信息，在这个例子里，就是只会记录 id=4 这个信息。</li><li>最后的 Xid event，用于表示事务被正确地提交了。</li></ul><h4 id="为什么会有-mixed-格式的-binlog？"><a href="#为什么会有-mixed-格式的-binlog？" class="headerlink" title="为什么会有 mixed 格式的 binlog？"></a>为什么会有 mixed 格式的 binlog？</h4><ul><li>因为有些 statement 格式的 binlog 可能会导致主备不一致，所以要使用 row 格式。</li><li>但 row 格式的缺点是，很占空间。比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间。但如果用 row 格式的 binlog，就要把这 10 万条记录都写到 binlog 中。这样做，不仅会占用更大的空间，同时写 binlog 也要耗费 IO 资源，影响执行速度。</li><li>所以，MySQL 就取了个折中方案，也就是有了 mixed 格式的 binlog。mixed 格式的意思是，MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。</li></ul><p>也就是说，mixed 格式可以利用 statment 格式的优点，同时又避免了数据不一致的风险。</p><p>因此，如果你的线上 MySQL 设置的 binlog 格式是 statement 的话，那基本上就可以认为这是一个不合理的设置。你至少应该把 binlog 的格式设置为 mixed。</p><h4 id="为什么不用mix格式日志？"><a href="#为什么不用mix格式日志？" class="headerlink" title="为什么不用mix格式日志？"></a>为什么不用mix格式日志？</h4><p>现在越来越多的场景要求把 MySQL 的 binlog 格式设置成 row。这么做的理由有很多，一个可以直接看出来的好处：<strong>恢复数据</strong>。</p><p>接下来，我们就分别从 delete、insert 和 update 这三种 SQL 语句的角度，来看看数据恢复的问题。</p><ul><li>即使我执行的是 delete 语句，row 格式的 binlog 也会把被删掉的行的整行信息保存起来。所以，如果你在执行完一条 delete 语句以后，发现删错数据了，可以直接把 binlog 中记录的 delete 语句转成 insert，把被错删的数据插入回去就可以恢复了。</li><li>如果你是执行错了 insert 语句呢？那就更直接了。row 格式下，insert 语句的 binlog 里会记录所有的字段信息，这些信息可以用来精确定位刚刚被插入的那一行。这时，你直接把 insert 语句转成 delete 语句，删除掉这被误插入的一行数据就可以了。</li><li>如果执行的是 update 语句的话，binlog 里面会记录修改前整行的数据和修改后的整行数据。所以，如果你误执行了 update 语句的话，只需要把这个 event 前后的两行信息对调一下，再去数据库里面执行，就能恢复这个更新操作了。</li></ul><p>其实，由 delete、insert 或者 update 语句导致的数据操作错误，需要恢复到操作之前状态的情况，也时有发生。MariaDB 的Flashback工具就是基于上面介绍的原理来回滚数据的。</p><h3 id="Xid"><a href="#Xid" class="headerlink" title="Xid"></a>Xid</h3><p>redo log 和 binlog有一个共同的字段叫作 Xid。它在 MySQL 中是用来对应事务的。</p><p>MySQL 内部维护了一个全局变量 global_query_id，每次执行语句的时候将它赋值给 Query_id，然后给这个变量加 1。如果当前语句是这个事务执行的第一条语句，那么 MySQL 还会同时把 Query_id 赋值给这个事务的 Xid。</p><p>而 global_query_id 是一个纯内存变量，重启之后就清零了。所以你就知道了，在同一个数据库实例中，不同事务的 Xid 也是有可能相同的。</p><p>但是 MySQL 重启之后会重新生成新的 binlog 文件，这就保证了，同一个 binlog 文件里，Xid 一定是惟一的。</p><p>虽然 MySQL 重启不会导致同一个 binlog 里面出现两个相同的 Xid，但是如果 global_query_id 达到上限后，就会继续从 0 开始计数。从理论上讲，还是就会出现同一个 binlog 里面出现相同 Xid 的场景。</p><p>因为 global_query_id 定义的长度是 8 个字节，这个自增值的上限是 264-1。要出现这种情况，必须是下面这样的过程：</p><ol><li>执行一个事务，假设 Xid 是 A；</li><li>接下来执行2的64次方查询语句，让 global_query_id 回到 A；</li><li>再启动一个事务，这个事务的 Xid 也是 A。</li></ol><p>不过，2的64次方这个值太大了，大到你可以认为这个可能性只会存在于理论上。</p><h3 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h3><h4 id="MySQL-怎么知道-binlog-是完整的"><a href="#MySQL-怎么知道-binlog-是完整的" class="headerlink" title="MySQL 怎么知道 binlog 是完整的?"></a>MySQL 怎么知道 binlog 是完整的?</h4><ul><li>statement 格式的 binlog，最后会有 COMMIT；</li><li>row 格式的 binlog，最后会有一个 XID event。</li></ul><h4 id="redo-log-和-binlog-是怎么关联起来的"><a href="#redo-log-和-binlog-是怎么关联起来的" class="headerlink" title="redo log 和 binlog 是怎么关联起来的?"></a>redo log 和 binlog 是怎么关联起来的?</h4><p>它们有一个共同的数据字段，叫 XID。崩溃恢复的时候，会按顺序扫描 redo log：</p><ul><li>如果碰到既有 prepare、又有 commit 的 redo log，就直接提交；</li><li>如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务。</li></ul><h4 id="处于-prepare-阶段的-redo-log-加上完整-binlog，重启就能恢复，MySQL-为什么要这么设计"><a href="#处于-prepare-阶段的-redo-log-加上完整-binlog，重启就能恢复，MySQL-为什么要这么设计" class="headerlink" title="处于 prepare 阶段的 redo log 加上完整 binlog，重启就能恢复，MySQL 为什么要这么设计?"></a>处于 prepare 阶段的 redo log 加上完整 binlog，重启就能恢复，MySQL 为什么要这么设计?</h4><p>这个问题还是跟我们在反证法中说到的数据与备份的一致性有关。在时刻 B，也就是 binlog 写完以后 MySQL 发生崩溃，这时候 binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。</p><p><strong>那能不能只用 redo log，不要 binlog？</strong></p><p>如果只从崩溃恢复的角度来讲是可以的。你可以把 binlog 关掉，这样就没有两阶段提交了，但系统依然是 crash-safe 的。</p><h4 id="正常运行中的实例，数据写入后的最终落盘，是从-redo-log-更新过来的还是从-buffer-pool-更新过来的呢？"><a href="#正常运行中的实例，数据写入后的最终落盘，是从-redo-log-更新过来的还是从-buffer-pool-更新过来的呢？" class="headerlink" title="正常运行中的实例，数据写入后的最终落盘，是从 redo log 更新过来的还是从 buffer pool 更新过来的呢？"></a>正常运行中的实例，数据写入后的最终落盘，是从 redo log 更新过来的还是从 buffer pool 更新过来的呢？</h4><ul><li>如果是正常运行的实例的话，数据页被修改以后，跟磁盘的数据页不一致，称为脏页。最终数据落盘，就是把内存中的数据页写盘。这个过程，甚至与 redo log 毫无关系。</li><li>在崩溃恢复场景中，InnoDB 如果判断到一个数据页可能在崩溃恢复的时候丢失了更新，就会将它读到内存，然后让 redo log 更新内存内容。更新完成后，内存页变成脏页，就回到了第一种情况的状态。</li></ul><h4 id="redo-log-buffer-是什么？是先修改内存，还是先写-redo-log-文件？"><a href="#redo-log-buffer-是什么？是先修改内存，还是先写-redo-log-文件？" class="headerlink" title="redo log buffer 是什么？是先修改内存，还是先写 redo log 文件？"></a>redo log buffer 是什么？是先修改内存，还是先写 redo log 文件？</h4><p>这个事务要往两个表中插入记录，插入数据的过程中，生成的日志都得先保存起来，但又不能在还没 commit 的时候就直接写到 redo log 文件里。</p><p>所以，redo log buffer 就是一块内存，用来先存 redo 日志的。也就是说，在执行第一个 insert 的时候，数据的内存被修改了，redo log buffer 也写入了日志。</p><p>但是，真正把日志写到 redo log 文件（文件名是 ib_logfile+ 数字），是在执行 commit 语句的时候做的。</p><h4 id="WAL-机制是减少磁盘写，可是每次提交事务都要写-redo-log-和-binlog，这磁盘读写次数也没变少呀？"><a href="#WAL-机制是减少磁盘写，可是每次提交事务都要写-redo-log-和-binlog，这磁盘读写次数也没变少呀？" class="headerlink" title="WAL 机制是减少磁盘写，可是每次提交事务都要写 redo log 和 binlog，这磁盘读写次数也没变少呀？"></a>WAL 机制是减少磁盘写，可是每次提交事务都要写 redo log 和 binlog，这磁盘读写次数也没变少呀？</h4><p>现在你就能理解了，WAL 机制主要得益于两个方面：</p><ol><li>redo log 和 binlog 都是顺序写，磁盘的顺序写比随机写速度要快；</li><li>组提交机制，可以大幅度降低磁盘的 IOPS 消耗。</li></ol><h4 id="如果你的-MySQL-现在出现了性能瓶颈，而且瓶颈在-IO-上，可以通过哪些方法来提升性能呢？"><a href="#如果你的-MySQL-现在出现了性能瓶颈，而且瓶颈在-IO-上，可以通过哪些方法来提升性能呢？" class="headerlink" title="如果你的 MySQL 现在出现了性能瓶颈，而且瓶颈在 IO 上，可以通过哪些方法来提升性能呢？"></a>如果你的 MySQL 现在出现了性能瓶颈，而且瓶颈在 IO 上，可以通过哪些方法来提升性能呢？</h4><ol><li>设置 <code>binlog_group_commit_sync_delay</code> 和 <code>binlog_group_commit_sync_no_delay_count</code> 参数，减少 binlog 的写盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但没有丢失数据的风险。</li><li>将 <code>sync_binlog</code> 设置为大于 1 的值（比较常见是 100~1000）。这样做的风险是，主机掉电时会丢 <code>binlog</code> 日志。</li><li>将 <code>innodb_flush_log_at_trx_commit</code> 设置为 2。这样做的风险是，主机掉电的时候会丢数据。</li></ol><p>我不建议你把 <code>innodb_flush_log_at_trx_commit</code> 设置成 0。因为把这个参数设置成 0，表示 redo log 只保存在内存中，这样的话 MySQL 本身异常重启也会丢数据，风险太大。而 redo log 写到文件系统的 page cache 的速度也是很快的，所以将这个参数设置成 2 跟设置成 0 其实性能差不多，但这样做 MySQL 异常重启时就不会丢数据了，相比之下风险会更小。</p><h4 id="执行一个-update-语句以后，我再去执行-hexdump-命令直接查看-ibd-文件内容，为什么没有看到数据有改变呢？"><a href="#执行一个-update-语句以后，我再去执行-hexdump-命令直接查看-ibd-文件内容，为什么没有看到数据有改变呢？" class="headerlink" title="执行一个 update 语句以后，我再去执行 hexdump 命令直接查看 ibd 文件内容，为什么没有看到数据有改变呢？"></a>执行一个 update 语句以后，我再去执行 hexdump 命令直接查看 ibd 文件内容，为什么没有看到数据有改变呢？</h4><p>这可能是因为 WAL 机制的原因。update 语句执行完成后，InnoDB 只保证写完了 redo log、内存，可能还没来得及将数据写到磁盘。</p><h4 id="为什么-binlog-cache-是每个线程自己维护的，而-redo-log-buffer-是全局共用的？"><a href="#为什么-binlog-cache-是每个线程自己维护的，而-redo-log-buffer-是全局共用的？" class="headerlink" title="为什么 binlog cache 是每个线程自己维护的，而 redo log buffer 是全局共用的？"></a>为什么 binlog cache 是每个线程自己维护的，而 redo log buffer 是全局共用的？</h4><p>MySQL 这么设计的主要原因是，binlog 是不能“被打断的”。一个事务的 binlog 必须连续写，因此要整个事务完成后，再一起写到文件里。<br>而 redo log 并没有这个要求，中间有生成的日志可以写到 redo log buffer 中。redo log buffer 中的内容还能“搭便车”，其他事务提交的时候可以被一起写到磁盘中。</p><p>这个问题，感觉还有一点，binlog存储是以statement或者row格式存储的，而redo log是以page页格式存储的。page格式，天生就是共有的，而row格式，只跟当前事务相关</p><h4 id="事务执行期间，还没到提交阶段，如果发生-crash-的话，redo-log-肯定丢了，这会不会导致主备不一致呢？"><a href="#事务执行期间，还没到提交阶段，如果发生-crash-的话，redo-log-肯定丢了，这会不会导致主备不一致呢？" class="headerlink" title="事务执行期间，还没到提交阶段，如果发生 crash 的话，redo log 肯定丢了，这会不会导致主备不一致呢？"></a>事务执行期间，还没到提交阶段，如果发生 crash 的话，redo log 肯定丢了，这会不会导致主备不一致呢？</h4><p>不会。因为这时候 binlog 也还在 binlog cache 里，没发给备库。crash 以后 redo log 和 binlog 都没有了，从业务角度看这个事务也没有提交，所以数据是一致的。</p><h4 id="如果-binlog-写完盘以后发生-crash，这时候还没给客户端答复就重启了。等客户端再重连进来，发现事务已经提交成功了，这是不是-bug？"><a href="#如果-binlog-写完盘以后发生-crash，这时候还没给客户端答复就重启了。等客户端再重连进来，发现事务已经提交成功了，这是不是-bug？" class="headerlink" title="如果 binlog 写完盘以后发生 crash，这时候还没给客户端答复就重启了。等客户端再重连进来，发现事务已经提交成功了，这是不是 bug？"></a>如果 binlog 写完盘以后发生 crash，这时候还没给客户端答复就重启了。等客户端再重连进来，发现事务已经提交成功了，这是不是 bug？</h4><p>不是。你可以设想一下更极端的情况，整个事务都提交成功了，redo log commit 完成了，备库也收到 binlog 并执行了。但是主库和客户端网络断开了，导致事务成功的包返回不回去，这时候客户端也会收到“网络断开”的异常。这种也只能算是事务成功的，不能认为是 bug。</p><p>实际上数据库的 crash-safe 保证的是：</p><ol><li>如果客户端收到事务成功的消息，事务就一定持久化了；</li><li>如果客户端收到事务失败（比如主键冲突、回滚等）的消息，事务就一定失败了；</li><li>如果客户端收到“执行异常”的消息，应用需要重连后通过查询当前状态来继续后续的逻辑。此时数据库只需要保证内部（数据和日志之间，主库和备库之间）一致就可以了。</li></ol><h4 id="为什么binlog-是不能“被打断的”的呢？主要出于什么考虑？"><a href="#为什么binlog-是不能“被打断的”的呢？主要出于什么考虑？" class="headerlink" title="为什么binlog 是不能“被打断的”的呢？主要出于什么考虑？"></a>为什么binlog 是不能“被打断的”的呢？主要出于什么考虑？</h4><p>我觉得一个比较重要的原因是，<strong>一个线程只能同时有一个事务在执行</strong>。</p><p>由于这个设定，所以每当执行一个begin/start transaction的时候，就会默认提交上一个事务；<br><strong>这样如果一个事务的binlog被拆开的时候，在备库执行就会被当做多个事务分段自行，这样破坏了原子性，是有问题的</strong>。</p><h4 id="主从循环复制问题"><a href="#主从循环复制问题" class="headerlink" title="主从循环复制问题"></a>主从循环复制问题</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e4f57b051c5323e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ol><li>规定两个库的 server id 必须不同，如果相同，则它们之间不能设定为主备关系；</li><li>一个备库接到 binlog 并在重放的过程中，生成与原 binlog 的 server id 相同的新的 binlog；</li><li>每个库在收到从自己的主库发过来的日志后，先判断 server id，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志。</li></ol><p>按照这个逻辑，如果我们设置了双 M 结构，日志的执行流就会变成这样：</p><ol><li>从节点 A 更新的事务，binlog 里面记的都是 A 的 server id；</li><li>传到节点 B 执行一次以后，节点 B 生成的 binlog 的 server id 也是 A 的 server id；</li><li>再传回给节点 A，A 判断到这个 server id 与自己的相同，就不会再处理这个日志。所以，死循环在这里就断掉了。</li></ol><h4 id="WAL-write-ahead-log-日志与回滚（rollback）日志的区别"><a href="#WAL-write-ahead-log-日志与回滚（rollback）日志的区别" class="headerlink" title="WAL(write-ahead-log)日志与回滚（rollback）日志的区别"></a>WAL(write-ahead-log)日志与回滚（rollback）日志的区别</h4><p><strong>回滚日志：</strong></p><ul><li>复制原始数据库内容并将其保存在单独的文件（即回滚日志）中，然后将新值写入数据库。</li><li>事务提交后，则删除回滚日志。</li><li>如果事务中止，则将回滚日志中的内容复制回数据库。</li></ul><p><strong>预写日志：</strong></p><ul><li>更改将附加到预写日志文件中。</li><li>提交时，会在WAL上设置“提交”标志（原始数据库此时可能不会更改）。</li><li>在WAL的检查点执行之前，可能会有多个已经提交的事务，但并未写入数据库物理文件。</li></ul><h2 id="SQL执行过程"><a href="#SQL执行过程" class="headerlink" title="SQL执行过程"></a>SQL执行过程</h2><h3 id="一条SQL如何执行？"><a href="#一条SQL如何执行？" class="headerlink" title="一条SQL如何执行？"></a>一条SQL如何执行？</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-dafc1ef4bc3a467b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><ul><li>连接器，连接器负责跟客户端建立连接、获取权限、维持和管理连接。<code>show processlist</code> 可以查看链接状态。客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。</li><li>查询缓存，MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。<ul><li><strong>但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。</strong></li><li>查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。</li><li>MySQL 8.0 版本直接将查询缓存的整块功能删掉了。</li></ul></li><li>分析器，主要对SQl做词法分析和语法分析，检查语法错误。</li><li>优化器，优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。</li><li>执行器，执行相关操作。</li></ul><h3 id="一个SQL-更新过程"><a href="#一个SQL-更新过程" class="headerlink" title="一个SQL 更新过程"></a>一个SQL 更新过程</h3><pre><code>mysql&gt; update T set c=c+1 where ID=2;</code></pre><p>这里我给出这个 update 语句的执行流程图，图中浅色框表示是在 InnoDB 内部执行的，深色框表示是在执行器中执行的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8e2cc83183584ada.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="change-buffer"><a href="#change-buffer" class="headerlink" title="change buffer"></a>change buffer</h4><p>当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。</p><p>需要说明的是，虽然名字叫作 change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上。</p><p>将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。</p><p>显然，如果能够将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。</p><p>那么，<strong>什么条件下可以使用 change buffer 呢？</strong></p><p>对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入 (4,400) 这个记录，就要先判断现在表中是否已经存在 k=4 的记录，<strong>而这必须要将数据页读入内存才能判断</strong>。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了。</p><p>change buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 <code>innodb_change_buffer_max_size</code> 来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。</p><p>现在，你已经理解了 change buffer 的机制，那么我们再一起来看看如果要在这张表中插入一个新记录 (4,400) 的话，InnoDB 的处理流程是怎样的。</p><p>第一种情况是，这个记录要更新的目标页在内存中。这时，InnoDB 的处理流程如下：</p><ul><li>对于唯一索引来说，找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束；</li><li>对于普通索引来说，找到 3 和 5 之间的位置，插入这个值，语句执行结束。</li></ul><p>这样看来，普通索引和唯一索引对更新语句性能影响的差别，只是一个判断，只会耗费微小的 CPU 时间。</p><p>第二种情况是，这个记录要更新的目标页不在内存中。</p><p>这时，InnoDB 的处理流程如下：</p><ul><li>对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；</li><li>对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。</li></ul><p>将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。</p><p>之前我就碰到过一件事儿，有个 DBA 的同学跟我反馈说，他负责的某个业务的库内存命中率突然从 99% 降低到了 75%，整个系统处于阻塞状态，更新语句全部堵住。而探究其原因后，我发现这个业务有大量插入数据的操作，而他在前一天把其中的某个普通索引改成了唯一索引。</p><h4 id="change-buffer-的使用场景"><a href="#change-buffer-的使用场景" class="headerlink" title="change buffer 的使用场景"></a>change buffer 的使用场景</h4><p>通过上面的分析，你已经清楚了使用 change buffer 对更新过程的加速作用，也清楚了 change buffer 只限于用在普通索引的场景下，而不适用于唯一索引。那么，现在有一个问题就是：普通索引的所有场景，使用 change buffer 都可以起到加速作用吗？</p><p>因为 merge 的时候是真正进行数据更新的时刻，而 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。</p><p>因此，对于<strong>写多读少的业务来说</strong>，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。</p><p>反过来，假设一个业务的<strong>更新模式是写入之后马上会做查询</strong>，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。所以，对于这种业务模式来说，<strong>change buffer 反而起到了副作用</strong>。</p><h3 id="一个-SQL-查询过程"><a href="#一个-SQL-查询过程" class="headerlink" title="一个 SQL 查询过程"></a>一个 SQL 查询过程</h3><p>假设，执行查询的语句是 select id from T where k=5。这个查询语句在索引树上查找的过程，先是通过 B+ 树从树根开始，按层搜索到叶子节点，也就是图中右下角的这个数据页，然后可以认为数据页内部通过二分法来定位记录。</p><ul><li>对于普通索引来说，查找到满足条件的第一个记录 (5,500) 后，需要查找下一个记录，直到碰到第一个不满足 k=5 条件的记录。</li><li>对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5880ac0b4e638c14.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>那么，这个不同带来的性能差距会有多少呢？答案是，微乎其微。</p><p>你知道的，InnoDB 的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在 InnoDB 中，每个数据页的大小默认是 16KB。</p><p>因为引擎是按页读写的，所以说，当找到 k=5 的记录的时候，它所在的数据页就都在内存里了。那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。</p><p>当然，如果 k=5 这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下一个数据页，这个操作会稍微复杂一些。</p><p>但是，我们之前计算过，对于整型字段，一个数据页可以放近千个 key，因此出现这种情况的概率会很低。所以，我们计算平均性能差异时，仍可以认为这个操作成本对于现在的 CPU 来说可以忽略不计。</p><h4 id="缓冲池-buffer-pool"><a href="#缓冲池-buffer-pool" class="headerlink" title="缓冲池(buffer pool)"></a>缓冲池(buffer pool)</h4><p>内存的数据页是在 Buffer Pool (BP) 中管理的，在 WAL 里 Buffer Pool 起到了加速更新的作用。而实际上，Buffer Pool 还有一个更重要的作用，就是加速查询。</p><p>InnoDB 内存管理用的是最近最少使用 (Least Recently Used, LRU) 算法，这个算法的核心就是淘汰最久未使用的数据。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-cf5917f5d50a3516.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ol><li>将LRU分为两个部分： 新生代(new sublist) 老生代(old sublist) </li><li>新老生代收尾相连，即：新生代的尾(tail)连接着老生代的头(head)； </li><li>新页（例如被预读的页）加入缓冲池时，只加入到老生代头部： 如果数据真正被读取（预读成功），才会加入到新生代的头部 如果数据没有被读取，则会比新生代里的“热数据页”更早被淘汰出缓冲池</li></ol><p>线上库 buffer pool 64G</p><pre><code>show variables like '%join_buffer_size%';  //8Mshow variables like '%sort_buffer_size%'; //8Mshow variables like '%innodb_buffer_pool_size%'; // 64G</code></pre><h4 id="索引下推"><a href="#索引下推" class="headerlink" title="索引下推"></a>索引下推</h4><pre><code>mysql&gt; select * from tuser where name like '张%' and age=10 and ismale=1;</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-32c099559ba3e868.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="无索引下推执行流程"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-333bf3f31548787f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="索引下推执行流程"></p><h3 id="Mysql优化器"><a href="#Mysql优化器" class="headerlink" title="Mysql优化器"></a>Mysql优化器</h3><p>而优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的 CPU 资源越少。</p><p>当然，扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。</p><p>我们这个简单的查询语句并没有涉及到临时表和排序，所以 MySQL 选错索引肯定是在判断扫描行数的时候出问题了。</p><p>那么，问题就是：<strong>扫描行数是怎么判断的？</strong></p><p>这个统计信息就是索引的“<strong>区分度</strong>”。显然，一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好。</p><p>我们可以使用 show index 方法，看到一个索引的基数。如图所示，就是表 t 的 show index 的结果 。虽然这个表的每一行的三个字段值都是一样的，但是在统计信息中，这三个索引的基数值并不同，而且其实都不准确。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-66b22725a0460116.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>那么，MySQL 是怎样得到索引的基数的呢？这里，我给你简单介绍一下 MySQL 采样统计的方法。</p><p>为什么要采样统计呢？因为把整张表取出来一行行统计，虽然可以得到精确的结果，但是代价太高了，所以只能选择“采样统计”。</p><p>采样统计的时候，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。</p><p>而数据表是会持续更新的，索引统计信息也不会固定不变。所以，当变更的数据行数超过 1/M 的时候，会自动触发重新做一次索引统计。</p><p>在 MySQL 中，有两种存储索引统计的方式，可以通过设置参数 <code>innodb_stats_persistent</code> 的值来选择：</p><ul><li>设置为 on 的时候，表示统计信息会持久化存储。这时，默认的 N 是 20，M 是 10。</li><li>设置为 off 的时候，表示统计信息只存储在内存中。这时，默认的 N 是 8，M 是 16。</li></ul><p><strong>既然是统计信息不对，那就修正。analyze table t 命令，可以用来重新统计索引信息。我们来看一下执行效果。</strong></p><h4 id="索引选择异常和处理"><a href="#索引选择异常和处理" class="headerlink" title="索引选择异常和处理"></a>索引选择异常和处理</h4><ol><li>一种方法是，像我们第一个例子一样，采用 force index 强行选择一个索引。MySQL 会根据词法解析的结果分析出可能可以使用的索引作为候选项，然后在候选列表中依次判断每个索引需要扫描多少行。如果 force index 指定的索引在候选索引列表中，就直接选择这个索引，不再评估其他索引的执行代价。</li><li>既然优化器放弃了使用索引 a，说明 a 还不够合适，所以第二种方法就是，我们可以考虑修改语句，引导 MySQL 使用我们期望的索引。比如，在这个例子里，显然把“order by b limit 1” 改成 “order by b,a limit 1” ，语义的逻辑是相同的。</li><li>第三种方法是，在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。第三种方法是，在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。</li></ol><h4 id="字符串索引存储"><a href="#字符串索引存储" class="headerlink" title="字符串索引存储"></a>字符串索引存储</h4><p>但是，索引选取的越长，占用的磁盘空间就越大，相同的数据页能放下的索引值就越少，搜索的效率也就会越低。</p><p>第一种方式是使用倒序存储。如果你存储身份证号的时候把它倒过来存，每次查询的时候，你可以这么写：</p><pre><code>mysql&gt; select field_list from t where id_card = reverse('input_id_card_string');</code></pre><p>第二种方式是使用 hash 字段。你可以在表上再创建一个整数字段，来保存身份证的校验码，同时在这个字段上创建索引。</p><pre><code>mysql&gt; alter table t add id_card_crc int unsigned, add index(id_card_crc);</code></pre><p>它们的区别，主要体现在以下三个方面：</p><ol><li>从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而 hash 字段方法需要增加一个字段。当然，倒序存储方式使用 4 个字节的前缀长度应该是不够的，如果再长一点，这个消耗跟额外这个 hash 字段也差不多抵消了。</li><li>在 CPU 消耗方面，倒序方式每次写和读的时候，都需要额外调用一次 reverse 函数，而 hash 字段的方式需要额外调用一次 crc32() 函数。如果只从这两个函数的计算复杂度来看的话，reverse 函数额外消耗的 CPU 资源会更小些。</li><li>从查询效率上看，使用 hash 字段方式的查询性能相对更稳定一些。因为 crc32 算出来的值虽然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近 1。而倒序存储方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。</li></ol><h4 id="最左前缀原则"><a href="#最左前缀原则" class="headerlink" title="最左前缀原则"></a>最左前缀原则</h4><p>这里，我先和你说结论吧。B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-967b2dbcda2bbc62.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="脏页"><a href="#脏页" class="headerlink" title="脏页"></a>脏页</h3><p>当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。<strong>部分刷账页可能导致Mysql抖动</strong>。</p><h4 id="强制刷脏页的场景"><a href="#强制刷脏页的场景" class="headerlink" title="强制刷脏页的场景"></a>强制刷脏页的场景</h4><ol><li>InnoDB 的 redo log 写满了。这时候系统会停止所有更新操作，把 checkpoint 往前推进，redo log 留出空间可以继续写。</li><li>系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。你一定会说，这时候难道不能直接把内存淘汰掉，下次需要请求的时候，从磁盘读入数据页，然后拿 redo log 出来应用不就行了？这里其实是从性能考虑的。如果刷脏页一定会写盘，就保证了每个数据页有两种状态：<ul><li>一种是内存里存在，内存里就肯定是正确的结果，直接返回；</li><li>另一种是内存里没有数据，就可以肯定数据文件上是正确的结果，读入内存后返回。这样的效率最高。</li></ul></li><li>MySQL 认为系统“空闲”的时候。当然，MySQL“这家酒店”的生意好起来可是会很快就能把粉板记满的，所以“掌柜”要合理地安排时间，即使是“生意好”的时候，也要见缝插针地找时间，只要有机会就刷一点“脏页”。</li><li>MySQL 正常关闭的情况。这时候，MySQL 会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快。</li></ol><p>第一种是“redo log 写满了，要 flush 脏页”，这种情况是 InnoDB 要尽量避免的。因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果你从监控上看，这时候更新数会跌为 0。</p><p>第二种是“内存不够用了，要先将脏页写到磁盘”，这种情况其实是常态。InnoDB 用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态：</p><ul><li>第一种是，还没有使用的；</li><li>第二种是，使用了并且是干净页；</li><li>第三种是，使用了并且是脏页。</li></ul><p>而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；但如果是脏页呢，就必须将脏页先刷到磁盘，变成干净页后才能复用。</p><p>所以，刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的：</p><ul><li>一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；</li><li>日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的。</li></ul><h4 id="刷脏页速度"><a href="#刷脏页速度" class="headerlink" title="刷脏页速度"></a>刷脏页速度</h4><p>这就要用到 innodb_io_capacity 这个参数了，它会告诉 InnoDB 你的磁盘能力。这个值我建议你设置成磁盘的 IOPS。磁盘的 IOPS 可以通过 fio 这个工具来测试，下面的语句是我用来测试磁盘随机读写的命令：</p><p>然后，根据上述算得的 F1(M) 和 F2(N) 两个值，取其中较大的值记为 R，之后引擎就可以按照 innodb_io_capacity 定义的能力乘以 R% 来控制刷脏页的速度。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-21b882b7aa3fe317.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="其他问题-1"><a href="#其他问题-1" class="headerlink" title="其他问题"></a>其他问题</h3><h4 id="覆盖索引"><a href="#覆盖索引" class="headerlink" title="覆盖索引"></a>覆盖索引</h4><p>覆盖索引是指，索引上的信息足够满足查询请求，不需要再回到主键索引上去取数据。</p><h4 id="当-MySQL-去更新一行，但是要修改的值跟原来的值是相同的，这时候-MySQL-会真的去执行一次修改吗？"><a href="#当-MySQL-去更新一行，但是要修改的值跟原来的值是相同的，这时候-MySQL-会真的去执行一次修改吗？" class="headerlink" title="当 MySQL 去更新一行，但是要修改的值跟原来的值是相同的，这时候 MySQL 会真的去执行一次修改吗？"></a>当 MySQL 去更新一行，但是要修改的值跟原来的值是相同的，这时候 MySQL 会真的去执行一次修改吗？</h4><p><a href="https://time.geekbang.org/column/article/73479">https://time.geekbang.org/column/article/73479</a></p><p>InnoDB 认真执行了“把这个值修改成 (1,2)”这个操作，该加锁的加锁，该更新的更新。</p><h4 id="我查这么多数据，会不会把数据库内存打爆"><a href="#我查这么多数据，会不会把数据库内存打爆" class="headerlink" title="我查这么多数据，会不会把数据库内存打爆"></a>我查这么多数据，会不会把数据库内存打爆</h4><p>我经常会被问到这样一个问题：我的主机内存只有 100G，现在要对一个 200G 的大表做全表扫描，会不会把数据库主机的内存用光了？</p><p>实际上，服务端并不需要保存一个完整的结果集。取数据和发数据的流程是这样的：</p><ol><li>获取一行，写到 <code>net_buffer</code> 中。这块内存的大小是由参数 <code>net_buffer_length</code> 定义的，默认是 16k。</li><li>重复获取行，直到 <code>net_buffer</code> 写满，调用网络接口发出去。</li><li>如果发送成功，就清空 <code>net_buffer</code>，然后继续取下一行，并写入 <code>net_buffer</code>。</li><li>如果发送函数返回 <code>EAGAIN</code> 或 <code>WSAEWOULDBLOCK</code>，就表示本地网络栈（socket send buffer）写满了，进入等待。直到网络栈重新可写，再继续发送。</li></ol><p>也就是说，MySQL 是“<strong>边读边发的</strong>”，这个概念很重要。这就意味着，如果客户端接收得慢，会导致 MySQL 服务端由于结果发不出去，这个事务的执行时间变长。</p><h4 id="读写分离-过期读问题"><a href="#读写分离-过期读问题" class="headerlink" title="读写分离 - 过期读问题"></a>读写分离 - 过期读问题</h4><p>这种“在从库上会读到系统的一个过期状态”的现象，在这篇文章里，我们暂且称之为“过期读”。<br>不论哪种结构，客户端都希望查询从库的数据结果，跟查主库的数据结果是一样的。</p><ol><li>强制走主库方案；</li><li>sleep 方案；</li><li>判断主备无延迟方案； <code>show slave status</code> ，判断 seconds_behind_master 是否已经等于 0。如果还不等于 0 ，那就必须等到这个参数变为 0 才能执行查询请求。</li><li>配合 semi-sync 方案，要解决这个问题，就要引入半同步复制，也就是 semi-sync replication，</li></ol><ul><li>事务提交的时候，主库把 binlog 发给从库；</li><li>从库收到 binlog 以后，发回给主库一个 ack，表示收到了；</li><li>主库收到这个 ack 以后，才能给客户端返回“事务完成”的确认。</li><li>也就是说，如果启用了 semi-sync，就表示所有给客户端发送过确认的事务，都确保了备库已经收到了这个日志。</li></ul><ol start="5"><li>等主库位点方案；<ul><li>Master_Log_File 和 Read_Master_Log_Pos，表示的是读到的主库的最新位点；</li><li>Relay_Master_Log_File 和 Exec_Master_Log_Pos，表示的是备库执行的最新位点。</li><li>如果 Master_Log_File 和 Relay_Master_Log_File、Read_Master_Log_Pos 和Exec_Master_Log_Pos 这两组值完全相同，就表示接收到的日志已经同步完成。</li></ul></li><li>等 GTID 方案，对比 GTID 集合确保主备无延迟。<ul><li>Auto_Position=1 ，表示这对主备关系使用了 GTID 协议。</li><li>Retrieved_Gtid_Set，是备库收到的所有日志的 GTID 集合；</li><li>Executed_Gtid_Set，是备库所有已经执行完成的 GTID 集合。</li></ul></li></ol><p>但是，semi-sync+ 位点判断的方案，只对一主一备的场景是成立的。在一主多从场景中，主库只要等到一个从库的 ack，就开始给客户端返回确认。这时，在从库上执行查询请求，就有两种情况：</p><ol><li>如果查询是落在这个响应了 ack 的从库上，是能够确保读到最新数据；</li><li>但如果是查询落到其他从库上，它们可能还没有收到最新的日志，就会产生过期读的问题。</li></ol><h3 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h3><h3 id="当前读-，快照读"><a href="#当前读-，快照读" class="headerlink" title="当前读 ，快照读"></a>当前读 ，快照读</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-24255ed96a43bc71.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4d43ad08b8f23326.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="事务隔离"><a href="#事务隔离" class="headerlink" title="事务隔离"></a>事务隔离</h4><p>这样，对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能：</p><ol><li>如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；</li><li>如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；</li><li>如果落在黄色部分，那就包括两种情况<ul><li>a.  若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；</li><li>b.  若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。</li></ul></li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4951deb7d23236a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：</p><ol><li>版本未提交，不可见；</li><li>版本已提交，但是是在视图创建后提交的，不可见；</li><li>版本已提交，而且是在视图创建前提交的，可见。</li></ol><h2 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h2><h3 id="加锁原则"><a href="#加锁原则" class="headerlink" title="加锁原则"></a>加锁原则</h3><p>我总结的加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。 </p><ul><li>原则 1：加锁的基本单位是 next-key lock。希望你还记得，next-key lock 是前开后闭区间。</li><li>原则 2：查找过程中访问到的对象才会加锁。</li><li>优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。</li><li>优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。</li><li>一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。</li></ul><p><a href="https://time.geekbang.org/column/article/75659">https://time.geekbang.org/column/article/75659</a></p><h3 id="锁粒度"><a href="#锁粒度" class="headerlink" title="锁粒度"></a>锁粒度</h3><h4 id="全局锁"><a href="#全局锁" class="headerlink" title="全局锁"></a>全局锁</h4><p>全局锁顾名思义，全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。</p><p>全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都 select 出来存成文本。</p><h5 id="备份"><a href="#备份" class="headerlink" title="备份"></a>备份</h5><p>官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。</p><p>所以，<strong>single-transaction 方法只适用于所有的表使用事务引擎的库</strong>。如果有的表使用了不支持事务的引擎，那么备份就只能通过 FTWRL 方法。这往往是 DBA 要求业务开发人员使用 InnoDB 替代 MyISAM 的原因之一。</p><p>你也许会问，既然要全库只读，为什么不使用 set global readonly=true 的方式呢？确实 readonly 方式也可以让全库进入只读状态，但我还是会建议你用 FTWRL 方式，主要有两个原因：</p><ul><li>一是，在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。</li><li>二是，在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。</li></ul><h4 id="表级别锁"><a href="#表级别锁" class="headerlink" title="表级别锁"></a>表级别锁</h4><p>MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。</p><p>表锁的语法是 lock tables … read/write。与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。</p><p>举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。</p><p>另一类表级的锁是 MDL（metadata lock)。MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。</p><ul><li>因此，在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。</li><li>读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。</li></ul><p>MDL 会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和更新。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1ccbea6c67cff98e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们可以看到 session A 先启动，这时候会对表 t 加一个 MDL 读锁。由于 session B 需要的也是 MDL 读锁，因此可以正常执行。</p><p>之后 session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。</p><p>如果只有 session C 自己被阻塞还没什么关系，但是之后所有要在表 t 上新申请 MDL 读锁的请求也会被 session C 阻塞。前面我们说了，所有对表的增删改查操作都需要先申请 MDL 读锁，就都被锁住，等于这个表现在完全不可读写了。</p><h3 id="两阶段锁"><a href="#两阶段锁" class="headerlink" title="两阶段锁"></a>两阶段锁</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e92660ddd0e713a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>也就是说，在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。</p><h3 id="死锁和死锁检测"><a href="#死锁和死锁检测" class="headerlink" title="死锁和死锁检测"></a>死锁和死锁检测</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-64e9193bd1a3063e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这时候，事务 A 在等待事务 B 释放 id=2 的行锁，而事务 B 在等待事务 A 释放 id=1 的行锁。 事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略：</p><ul><li>一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。</li><li>另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。</li></ul><h4 id="select-和-insert死锁场景"><a href="#select-和-insert死锁场景" class="headerlink" title="select 和 insert死锁场景"></a>select 和 insert死锁场景</h4><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0a3fb6f1da08a56b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>你看到了，其实都不需要用到后面的 update 语句，就已经形成死锁了。我们按语句执行顺序来分析一下：</p><ul><li>session A 执行 select … for update 语句，由于 id=9 这一行并不存在，因此会加上间隙锁 (5,10);</li><li>session B 执行 select … for update 语句，同样会加上间隙锁 (5,10)，间隙锁之间不会冲突，因此这个语句可以执行成功；</li><li>session B 试图插入一行 (9,9,9)，被 session A 的间隙锁挡住了，只好进入等待；</li><li>session A 试图插入一行 (9,9,9)，被 session B 的间隙锁挡住了。</li></ul><h3 id="热点行问题"><a href="#热点行问题" class="headerlink" title="热点行问题"></a>热点行问题</h3><p>那如果是我们上面说到的所有事务都要更新同一行的场景呢？<br>每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。因此，你就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。</p><p>根据上面的分析，我们来讨论一下，<strong>怎么解决由这种热点行更新导致的性能问题呢</strong>？问题的症结在于，死锁检测要耗费大量的 CPU 资源。</p><ul><li>一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。</li><li>另一个思路是控制并发度。根据上面的分析，你会发现如果并发能够控制住，比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法就是，在客户端做并发控制。但是，你会很快发现这个方法不太可行，因为客户端很多。我见过一个应用，有 600 个客户端，这样即使每个客户端控制到只有 5 个并发线程，汇总到数据库服务端以后，峰值并发数也可能要达到 3000。</li></ul><p>因此，这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现；如果你的团队有能修改 MySQL 源码的人，也可以做在 MySQL 里面。基本思路就是，对于相同行的更新，在进入引擎之前排队。这样在 InnoDB 内部就不会有大量的死锁检测工作了。</p><p>要访问的行上有锁，他才要死锁检测。</p><h2 id="Order-by-实现原理"><a href="#Order-by-实现原理" class="headerlink" title="Order by 实现原理"></a>Order by 实现原理</h2><h3 id="全字段排序"><a href="#全字段排序" class="headerlink" title="全字段排序"></a>全字段排序</h3><pre><code>select city,name,age CREATE TABLE `t` (  `id` int(11) NOT NULL,  `city` varchar(16) NOT NULL,  `name` varchar(16) NOT NULL,  `age` int(11) NOT NULL,  `addr` varchar(128) DEFAULT NULL,  PRIMARY KEY (`id`),  KEY `city` (`city`)) ENGINE=InnoDB;select from t where city='杭州' order by name limit 1000  ;</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-add9a6a6719df461.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ol><li>初始化 sort_buffer，确定放入 name、city、age 这三个字段；</li><li>从索引 city 找到第一个满足 city=’杭州’条件的主键 id，也就是图中的 ID_X；</li><li>到主键 id 索引取出整行，取 name、city、age 三个字段的值，存入 sort_buffer 中；</li><li>从索引 city 取下一个记录的主键 id；</li><li>重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的 ID_Y；</li><li>对 sort_buffer 中的数据按照字段 name 做快速排序；按照排序结果取前 1000 行返回给客户端。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-82eda72c93d03354.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><code>sort_buffer_size</code>，就是 MySQL 为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于 <code>sort_buffer_size</code>，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。</p><p>你可以用下面介绍的方法，来确定一个排序语句是否使用了临时文件</p><pre><code>/* 打开optimizer_trace，只对本线程有效 */SET optimizer_trace='enabled=on'; /* @a保存Innodb_rows_read的初始值 */select VARIABLE_VALUE into @a from  performance_schema.session_status where variable_name = 'Innodb_rows_read';/* 执行语句 */select city, name,age from t where city='杭州' order by name limit 1000; /* 查看 OPTIMIZER_TRACE 输出 */SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G/* @b保存Innodb_rows_read的当前值 */select VARIABLE_VALUE into @b from performance_schema.session_status where variable_name = 'Innodb_rows_read';/* 计算Innodb_rows_read差值 */select @b-@a;</code></pre><p>这个方法是通过查看 OPTIMIZER_TRACE 的结果来确认的，你可以从 <code>number_of_tmp_files</code> 中看到是否使用了临时文件。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b99aa8b9dfcfbebb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><code>number_of_tmp_files</code> 表示的是，排序过程中使用的临时文件数。你一定奇怪，为什么需要 12 个文件？内存放不下时，就需要使用外部排序，<strong>外部排序一般使用归并排序算法</strong>。可以这么简单理解，MySQL 将需要排序的数据分成 12 份，每一份单独排序后存在这些临时文件中。然后把这 12 个有序文件再合并成一个有序的大文件。</p><p>我们的示例表中有 4000 条满足 city=’杭州’的记录，所以你可以看到 examined_rows=4000，表示参与排序的行数是 4000 行。</p><p>sort_mode 里面的 packed_additional_fields 的意思是，排序过程对字符串做了“紧凑”处理。即使 name 字段的定义是 varchar(16)，在排序过程中还是要按照实际长度来分配空间的。</p><p>同时，最后一个查询语句 select @b-@a 的返回结果是 4000，表示整个执行过程只扫描了 4000 行。</p><h3 id="rowid-排序"><a href="#rowid-排序" class="headerlink" title="rowid 排序"></a>rowid 排序</h3><p>如果 MySQL 认为排序的单行长度太大会怎么做呢？</p><p><code>max_length_for_sort_data</code>，是 MySQL 中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL 就认为单行太大，要换一个算法。</p><p>新的算法放入 sort_buffer 的字段，只有要排序的列（即 name 字段）和主键 id。</p><ol><li>始化 sort_buffer，确定放入两个字段，即 name 和 id；</li><li>从索引 city 找到第一个满足 city=’杭州’条件的主键 id，也就是图中的 ID_X；</li><li>到主键 id 索引取出整行，取 name、id 这两个字段，存入 sort_buffer 中；</li><li>从索引 city 取下一个记录的主键 id；</li><li>重复步骤 3、4 直到不满足 city=’杭州’条件为止，也就是图中的 ID_Y；</li><li>对 sort_buffer 中的数据按照字段 name 进行排序；</li><li>遍历排序结果，取前 1000 行，并按照 id 的值回到原表中取出 city、name 和 age 三个字段返回给客户端。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-378fffb77a7ad391.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-830abf5d761db5dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>现在，我们就来看看结果有什么不同。</p><ul><li>首先，图中的 examined_rows 的值还是 4000，表示用于排序的数据是 4000 行。但是 select @b-@a 这个语句的值变成 5000 了。</li><li>因为这时候除了排序过程外，在排序完成后，还要根据 id 去原表取值。由于语句是 limit 1000，因此会多读 1000 行。</li></ul><p>从 OPTIMIZER_TRACE 的结果中，你还能看到另外两个信息也变了。</p><ul><li>sort_mode 变成了 ，表示参与排序的只有 name 和 id 这两个字段。</li><li>number_of_tmp_files 变成 10 了，是因为这时候参与排序的行数虽然仍然是 4000 行，但是每一行都变小了，因此需要排序的总数据量就变小了，需要的临时文件也相应地变少了。</li></ul><h2 id="Join-实现原理"><a href="#Join-实现原理" class="headerlink" title="Join 实现原理"></a>Join 实现原理</h2><pre><code>CREATE TABLE `t2` (  `id` int(11) NOT NULL,  `a` int(11) DEFAULT NULL,  `b` int(11) DEFAULT NULL,  PRIMARY KEY (`id`),  KEY `a` (`a`)) ENGINE=InnoDB;drop procedure idata;delimiter ;;create procedure idata()begin  declare i int;  set i=1;  while(i&lt;=1000)do    insert into t2 values(i, i, i);    set i=i+1;  end while;end;;delimiter ;call idata();create table t1 like t2;insert into t1 (select * from t2 where id&lt;=100)</code></pre><h3 id="Index-Nested-Loop-Join-NLJ-（无join-buffer）"><a href="#Index-Nested-Loop-Join-NLJ-（无join-buffer）" class="headerlink" title="Index Nested-Loop Join - NLJ （无join_buffer）"></a>Index Nested-Loop Join - NLJ （无join_buffer）</h3><pre><code>select * from t1 straight_join t2 on (t1.a=t2.a);</code></pre><p>如果直接使用 join 语句，MySQL 优化器可能会选择表 t1 或 t2 作为驱动表，这样会影响我们分析 SQL 语句的执行过程。所以，为了便于分析执行过程中的性能问题，我改用 straight_join 让 MySQL 使用固定的连接方式执行查询，这样优化器只会按照我们指定的方式去 join。在这个语句里，t1 是驱动表，t2 是被驱动表。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c1b57f4a9e0419e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>可以看到，在这条语句里，被驱动表 t2 的字段 a 上有索引，join 过程用上了这个索引，因此这个语句的执行流程是这样的：</p><ol><li>从表 t1 中读入一行数据 R；</li><li>从数据行 R 中，取出 a 字段到表 t2 里去查找；</li><li>取出表 t2 中满足条件的行，跟 R 组成一行，作为结果集的一部分；</li><li>重复执行步骤 1 到 3，直到表 t1 的末尾循环结束。</li></ol><p>这个过程是先遍历表 t1，然后根据从表 t1 中取出的每行数据中的 a 值，去表 t2 中查找满足条件的记录。在形式上，这个过程就跟我们写程序时的嵌套查询类似，并且可以用上被驱动表的索引，所以我们称之为“Index Nested-Loop Join”，简称 NLJ。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9acb602e169b749f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在这个流程里：</p><ol><li>对驱动表 t1 做了全表扫描，这个过程需要扫描 100 行；</li><li>而对于每一行 R，根据 a 字段去表 t2 查找，走的是树搜索过程。由于我们构造的数据都是一一对应的，因此每次的搜索过程都只扫描一行，也是总共扫描 100 行；</li><li>所以，整个执行流程，总扫描行数是 200。</li></ol><p>到这里小结一下，通过上面的分析我们得到了两个结论：</p><ol><li>使用 join 语句，性能比强行拆成多个单表执行 SQL 语句的性能要好；</li><li>如果使用 join 语句的话，需要让小表做驱动表。</li></ol><h3 id="Simple-Nested-Loop-Join-（无join-buffer-mysql-没用）"><a href="#Simple-Nested-Loop-Join-（无join-buffer-mysql-没用）" class="headerlink" title="Simple Nested-Loop Join （无join_buffer - mysql 没用）"></a>Simple Nested-Loop Join （无join_buffer - mysql 没用）</h3><pre><code>select * from t1 straight_join t2 on (t1.a=t2.b);</code></pre><p>由于表 t2 的字段 b 上没有索引，因此再用图 2 的执行流程时，每次到 t2 去匹配的时候，就要做一次全表扫描。</p><p>你可以先设想一下这个问题，继续使用图 2 的算法，是不是可以得到正确的结果呢？如果只看结果的话，这个算法是正确的，而且这个算法也有一个名字，叫做“Simple Nested-Loop Join”。</p><p>但是，这样算来，这个 SQL 请求就要扫描表 t2 多达 100 次，总共扫描 100*1000=10 万行。</p><p>这还只是两个小表，如果 t1 和 t2 都是 10 万行的表（当然了，这也还是属于小表的范围），就要扫描 100 亿行，这个算法看上去太“笨重”了。</p><p>当然<strong>，MySQL 也没有使用这个 Simple Nested-Loop Join 算法</strong>，而是使用了另一个叫作“Block Nested-Loop Join”的算法，简称 BNL。</p><h3 id="Block-Nested-Loop-Join-BNL-（有-join-buffer）"><a href="#Block-Nested-Loop-Join-BNL-（有-join-buffer）" class="headerlink" title="Block Nested-Loop Join - BNL （有 join_buffer）"></a>Block Nested-Loop Join - BNL （有 join_buffer）</h3><p>这时候，被驱动表上没有可用的索引，算法的流程是这样的：</p><ol><li>把表 t1 的数据读入线程内存 join_buffer 中，由于我们这个语句中写的是 select *，因此是把整个表 t1 放入了内存；</li><li>扫描表 t2，把表 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-95f132a8113eebab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-89b38f61b4f9f067.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>可以看到，在这个过程中，对表 t1 和 t2 都做了一次全表扫描，因此总的扫描行数是 1100。由于 join_buffer 是以无序数组的方式组织的，因此对表 t2 中的每一行，都要做 100 次判断，总共需要在内存中做的判断次数是：100*1000=10 万次。</p><p><code>join_buffer</code> 的大小是由参数 <code>join_buffer_size</code> 设定的，默认值是 256k。如果放不下表 t1 的所有数据话，策略很简单，就是分段放。我把 <code>join_buffer_size</code> 改成 1200，再执行：</p><pre><code>select * from t1 straight_join t2 on (t1.a=t2.b);</code></pre><p>执行过程就变成了：</p><ol><li>扫描表 t1，顺序读取数据行放入 join_buffer 中，放完第 88 行 join_buffer 满了，继续第 2 步；</li><li>扫描表 t2，把 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回；</li><li>清空 join_buffer；</li><li>继续扫描表 t1，顺序读取最后的 12 行数据放入 join_buffer 中，继续执行第 2 步。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-97a082ef3ba4bb5c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>图中的步骤 4 和 5，表示清空 join_buffer 再复用。</p><p>这个流程才体现出了这个算法名字中“Block”的由来，表示“分块去 join”。</p><p>可以看到，这时候由于表 t1 被分成了两次放入 join_buffer 中，导致表 t2 会被扫描两次。虽然分成两次放入 join_buffer，但是判断等值条件的次数还是不变的，依然是 (88+12)*1000=10 万次。</p><p>第一个问题：能不能使用 join 语句？</p><ol><li>如果可以使用 Index Nested-Loop Join 算法，也就是说可以用上被驱动表上的索引，其实是没问题的；</li><li>如果使用 Block Nested-Loop Join 算法，扫描行数就会过多。尤其是在大表上的 join 操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种 join 尽量不要用。</li></ol><p>所以你在判断要不要使用 join 语句时，就是看 explain 结果里面，Extra 字段里面有没有出现“Block Nested Loop”字样。</p><p>第二个问题是：如果要使用 join，应该选择大表做驱动表还是选择小表做驱动表？</p><ol><li>如果是 Index Nested-Loop Join 算法，应该选择小表做驱动表；</li><li>如果是 Block Nested-Loop Join 算法：<ul><li>在 join_buffer_size 足够大的时候，是一样的；</li><li>在 join_buffer_size 不够大的时候（这种情况更常见），应该选择小表做驱动表。</li></ul></li></ol><p> 所以，更准确地说，在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与 join 的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。</p><h4 id="BNL-算法的性能问题"><a href="#BNL-算法的性能问题" class="headerlink" title="BNL 算法的性能问题"></a>BNL 算法的性能问题</h4><p>我们说到 InnoDB 的 LRU 算法的时候提到，由于 InnoDB 对 Bufffer Pool 的 LRU 算法做了优化，即：第一次从磁盘读入内存的数据页，会先放在 old 区域。如果 1 秒之后这个数据页不再被访问了，就不会被移动到 LRU 链表头部，这样对 Buffer Pool 的命中率影响就不大。</p><p>但是，如果一个使用 BNL 算法的 join 语句，多次扫描一个冷表，而且这个语句执行时间超过 1 秒，就会在再次扫描冷表的时候，把冷表的数据页移到 LRU 链表头部。</p><p>这种情况对应的，是冷表的数据量小于整个 Buffer Pool 的 3/8，能够完全放入 old 区域的情况。</p><p>如果这个冷表很大，就会出现另外一种情况：业务正常访问的数据页，没有机会进入 young 区域。</p><p>由于优化机制的存在，一个正常访问的数据页，要进入 young 区域，需要隔 1 秒后再次被访问到。但是，由于我们的 join 语句在循环读磁盘和淘汰内存页，进入 old 区域的数据页，很可能在 1 秒之内就被淘汰了。这样，就会导致这个 MySQL 实例的 Buffer Pool 在这段时间内，young 区域的数据页没有被合理地淘汰。</p><p>也就是说，这两种情况都会影响 Buffer Pool 的正常运作。</p><p>大表 join 操作虽然对 IO 有影响，但是在语句执行结束后，对 IO 的影响也就结束了。但是，对 Buffer Pool 的影响就是持续性的，需要依靠后续的查询请求慢慢恢复内存命中率。</p><p>也就是说，BNL 算法对系统的影响主要包括三个方面：</p><ol><li>可能会多次扫描被驱动表，占用磁盘 IO 资源；</li><li>判断 join 条件需要执行 M*N 次对比（M、N 分别是两张表的行数），如果是大表就会占用非常多的 CPU 资源；</li><li>可能会导致 Buffer Pool 的热数据被淘汰，影响内存命中率。</li></ol><h3 id="Multi-Range-Read-优化"><a href="#Multi-Range-Read-优化" class="headerlink" title="Multi-Range Read 优化"></a>Multi-Range Read 优化</h3><p>在介绍 join 语句的优化方案之前，我需要先和你介绍一个知识点，即：Multi-Range Read 优化 (MRR)。这个优化的主要目的是尽量使用顺序读盘。</p><p>主键索引是一棵 B+ 树，在这棵树上，每次只能根据一个主键 id 查到一行数据。因此，回表肯定是一行行搜索主键索引的，基本流程如图 1 所示。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-183dd8d210bc98e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果随着 a 的值递增顺序查询的话，<strong>id 的值就变成随机的</strong>，那么就会出现随机访问，性能相对较差。虽然“按行查”这个机制不能改，但是调整查询的顺序，还是能够加速的。</p><p>因为大多数的数据都是按照主键递增顺序插入得到的，所以我们可以认为，如果按照主键的递增顺序查询的话，对磁盘的读比较接近顺序读，能够提升读性能。</p><p>这就是 MRR 优化的设计思路。此时，语句的执行流程变成了这样：</p><ol><li>根据索引 a，定位到满足条件的记录，将 id 值放入 read_rnd_buffer 中 ;</li><li>将 read_rnd_buffer 中的 id 进行递增排序；</li><li>排序后的 id 数组，依次到主键 id 索引中查记录，并作为结果返回。</li></ol><p>这里，read_rnd_buffer 的大小是由 read_rnd_buffer_size 参数控制的。如果步骤 1 中，read_rnd_buffer 放满了，就会先执行完步骤 2 和 3，然后清空 read_rnd_buffer。之后继续找索引 a 的下个记录，并继续循环。</p><p>另外需要说明的是，如果你想要稳定地使用 MRR 优化的话，需要设置set optimizer_switch=”mrr_cost_based=off”。（官方文档的说法，是现在的优化器策略，判断消耗的时候，会更倾向于不使用 MRR，把 mrr_cost_based 设置为 off，就是固定使用 MRR 了。）</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-83dda4dd3e250e4f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-814460f0b60c64c7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>MRR 能够提升性能的核心在于，这条查询语句在索引 a 上做的是一个范围查询（也就是说，这是一个多值查询），可以得到足够多的主键 id。这样通过排序以后，再去主键索引查数据，才能体现出“顺序性”的优势。</p><h3 id="Batched-Key-Access-BKA-（NLJ-join-buffer）"><a href="#Batched-Key-Access-BKA-（NLJ-join-buffer）" class="headerlink" title="Batched Key Access - BKA （NLJ - join_buffer）"></a>Batched Key Access - BKA （NLJ - join_buffer）</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-af4e048ecf38627d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>NLJ 算法执行的逻辑是：<strong>从驱动表 t1，一行行地取出 a 的值</strong>，再到被驱动表 t2 去做 join。也就是说，对于表 t2 来说，每次都是匹配一个值。这时，MRR 的优势就用不上了。</p><p>那怎么才能一次性地多传些值给表 t2 呢？方法就是，从表 t1 里一次性地多拿些行出来，一起传给表 t2。</p><p>既然如此，我们就把表 t1 的数据取出来一部分，先放到一个临时内存。这个临时内存不是别人，就是 join_buffer。</p><p>通过上一篇文章，我们知道 <code>join_buffer</code> 在 BNL 算法里的作用，是暂存驱动表的数据。但是在 NLJ 算法里并没有用。那么，我们刚好就可以复用 <code>join_buffer</code> 到 BKA 算法中。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-750b48f0ee00b608.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="临时表去join"><a href="#临时表去join" class="headerlink" title="临时表去join"></a>临时表去join</h4><pre><code>select * from t1 join t2 on (t1.b=t2.b) where t2.b&gt;=1 and t2.b&lt;=2000;</code></pre><p>我们在文章开始的时候，在表 t2 中插入了 100 万行数据，但是经过 where 条件过滤后，需要参与 join 的只有 2000 行数据。如果这条语句同时是一个低频的 SQL 语句，那么再为这个语句在表 t2 的字段 b 上创建一个索引就很浪费了。</p><p>但是，如果使用 BNL 算法来 join 的话，这个语句的执行流程是这样的：</p><ol><li>把表 t1 的所有字段取出来，存入 join_buffer 中。这个表只有 1000 行，join_buffer_size 默认值是 256k，可以完全存入。</li><li>扫描表 t2，取出每一行数据跟 join_buffer 中的数据进行对比，<ul><li>如果不满足 t1.b=t2.b，则跳过；</li><li>如果满足 t1.b=t2.b, 再判断其他条件，也就是是否满足 t2.b 处于[1,2000]的条件，如果是，就作为结果集的一部分返回，否则跳过。</li></ul></li></ol><p> 我在上一篇文章中说过，对于表 t2 的每一行，判断 join 是否满足的时候，都需要遍历 join_buffer 中的所有行。因此判断等值条件的次数是 1000*100 万 =10 亿次，这个判断的工作量很大。</p><p> <img src="https://upload-images.jianshu.io/upload_images/12321605-c93c567cd51dd9c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/12321605-cd13a7493613b65b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>可以看到，explain 结果里 Extra 字段显示使用了 BNL 算法。在我的测试环境里，这条语句需要执行 1 分 11 秒。</p><p>这时候，我们可以考虑使用临时表。使用临时表的大致思路是：</p><ol><li>把表 t2 中满足条件的数据放在临时表 tmp_t 中；</li><li>为了让 join 使用 BKA 算法，给临时表 tmp_t 的字段 b 加上索引；</li><li>让表 t1 和 tmp_t 做 join 操作。</li></ol><p>此时，对应的 SQL 语句的写法如下：</p><pre><code>create temporary table temp_t(id int primary key, a int, b int, index(b))engine=innodb;insert into temp_t select * from t2 where b&gt;=1 and b&lt;=2000;select * from t1 join temp_t on (t1.b=temp_t.b);</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c7ab0399b824b793.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ol><li>执行 insert 语句构造 temp_t 表并插入数据的过程中，对表 t2 做了全表扫描，这里扫描行数是 100 万。</li><li>之后的 join 语句，扫描表 t1，这里的扫描行数是 1000；join 比较过程中，做了 1000 次带索引的查询。相比于优化前的 join 语句需要做 10 亿次条件判断来说，这个优化效果还是很明显的。</li></ol><h4 id="扩展hash-join"><a href="#扩展hash-join" class="headerlink" title="扩展hash join"></a>扩展hash join</h4><p>看到这里你可能发现了，其实上面计算 10 亿次那个操作，看上去有点儿傻。如果 join_buffer 里面维护的不是一个无序数组，而是一个哈希表的话，那么就不是 10 亿次判断，而是 100 万次 hash 查找。这样的话，整条语句的执行速度就快多了吧？</p><p>这，也正是 MySQL 的优化器和执行器一直被诟病的一个原因：不支持哈希 join。并且，MySQL 官方的 roadmap，也是迟迟没有把这个优化排上议程。</p><p>实际上，这个优化思路，我们可以自己实现在业务端。实现流程大致如下：</p><ol><li>select * from t1;取得表 t1 的全部 1000 行数据，在业务端存入一个 hash 结构，比如 C++ 里的 set、PHP 的数组这样的数据结构。</li><li>select * from t2 where b&gt;=1 and b&lt;=2000; 获取表 t2 中满足条件的 2000 行数据。</li><li>把这 2000 行数据，一行一行地取到业务端，到 hash 结构的数据表中寻找匹配的数据。满足匹配的条件的这行数据，就作为结果集的一行。</li></ol><h2 id="临时表"><a href="#临时表" class="headerlink" title="临时表"></a>临时表</h2><h3 id="临时表的特点"><a href="#临时表的特点" class="headerlink" title="临时表的特点"></a>临时表的特点</h3><ol><li>建表语法是 create temporary table …。</li><li>一个临时表只能被创建它的 session 访问，对其他线程不可见。所以，图中 session A 创建的临时表 t，对于 session B 就是不可见的。</li><li>临时表可以与普通表同名。</li><li>session A 内有同名的临时表和普通表的时候，show create 语句，以及增删改查语句访问的是临时表。session A 内有同名的临时表和普通表的时候，show create 语句，以及增删改查语句访问的是临时表。</li><li>show tables 命令不显示临时表。</li></ol><p>由于临时表只能被创建它的 session 访问，所以在这个 session 结束的时候，会自动删除临时表。也正是由于这个特性，临时表就特别适合我们文章开头的 join 优化这种场景。为什么呢？</p><p>原因主要包括以下两个方面：</p><ol><li>不同 session 的临时表是可以重名的，如果有多个 session 同时执行 join 优化，不需要担心表名重复导致建表失败的问题。</li><li>不需要担心数据删除问题。如果使用普通表，在流程执行过程中客户端发生了异常断开，或者数据库发生异常重启，还需要专门来清理中间过程中生成的数据表。而临时表由于会自动回收，所以不需要这个额外的操作。</li></ol><h3 id="内存临时表"><a href="#内存临时表" class="headerlink" title="内存临时表"></a>内存临时表</h3><pre><code>mysql&gt; CREATE TABLE `words` (  `id` int(11) NOT NULL AUTO_INCREMENT,  `word` varchar(64) DEFAULT NULL,  PRIMARY KEY (`id`)) ENGINE=InnoDB;delimiter ;;create procedure idata()begin  declare i int;  set i=0;  while i&lt;10000 do    insert into words(word) values(concat(char(97+(i div 1000)), char(97+(i % 1000 div 100)), char(97+(i % 100 div 10)), char(97+(i % 10))));    set i=i+1;  end while;end;;delimiter ;call idata();</code></pre><p>这个语句的意思很直白，随机排序取前 3 个。虽然这个 SQL 语句写法很简单，但执行流程却有点复杂的。</p><pre><code>mysql&gt; select word from words order by rand() limit 3;</code></pre><p>我们先用 explain 命令来看看这个语句的执行情况。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-19f40953fafe8881.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ol><li>创建一个临时表。这个临时表使用的是 memory 引擎，表里有两个字段，第一个字段是 double 类型，为了后面描述方便，记为字段 R，第二个字段是 varchar(64) 类型，记为字段 W。并且，这个表没有建索引。</li><li>从 words 表中，按主键顺序取出所有的 word 值。对于每一个 word 值，调用 rand() 函数生成一个大于 0 小于 1 的随机小数，并把这个随机小数和 word 分别存入临时表的 R 和 W 字段中，到此，扫描行数是 10000。</li><li>现在临时表有 10000 行数据了，接下来你要在这个没有索引的内存临时表上，按照字段 R 排序。</li><li>初始化 sort_buffer。sort_buffer 中有两个字段，一个是 double 类型，另一个是整型。</li><li>从内存临时表中一行一行地取出 R 值和位置信息（我后面会和你解释这里为什么是“位置信息”），分别存入 sort_buffer 中的两个字段里。这个过程要对内存临时表做全表扫描，此时扫描行数增加 10000，变成了 20000。</li><li>在 sort_buffer 中根据 R 的值进行排序。注意，这个过程没有涉及到表操作，所以不会增加扫描行数。</li><li>排序完成后，取出前三个结果的位置信息，依次到内存临时表中取出 word 值，返回给客户端。这个过程中，访问了表的三行数据，总扫描行数变成了 20003。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-184b04aae91a1b8f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="磁盘临时表"><a href="#磁盘临时表" class="headerlink" title="磁盘临时表"></a>磁盘临时表</h3><p>其实不是的。tmp_table_size 这个配置限制了内存临时表的大小，默认值是 16M。如果临时表大小超过了 tmp_table_size，那么内存临时表就会转成磁盘临时表。</p><p>磁盘临时表使用的引擎默认是 InnoDB，是由参数 internal_tmp_disk_storage_engine 控制的。</p><pre><code>set tmp_table_size=1024;set sort_buffer_size=32768;set max_length_for_sort_data=16;/* 打开 optimizer_trace，只对本线程有效 */SET optimizer_trace='enabled=on'; /* 执行语句 */select word from words order by rand() limit 3;/* 查看 OPTIMIZER_TRACE 输出 */SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1b6eb162e47ef602.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因为将 max_length_for_sort_data 设置成 16，小于 word 字段的长度定义，所以我们看到 sort_mode 里面显示的是 rowid 排序，这个是符合预期的，参与排序的是随机值 R 字段和 rowid 字段组成的行。</p><p>这个 SQL 语句的排序确实没有用到临时文件，采用是 MySQL 5.6 版本引入的一个新的排序算法，即：优先队列排序算法。接下来，我们就看看为什么没有使用临时文件的算法，也就是归并排序算法，而是采用了优先队列排序算法。</p><p>这个 SQL 语句的排序确实没有用到临时文件，采用是 MySQL 5.6 版本引入的一个新的排序算法，即：优先队列排序算法。接下来，我们就看看为什么没有使用临时文件的算法，也就是归并排序算法，而是采用了优先队列排序算法。</p><p>其实，我们现在的 SQL 语句，只需要取 R 值最小的 3 个 rowid。但是，如果使用归并排序算法的话，虽然最终也能得到前 3 个值，但是这个算法结束后，已经将 10000 行数据都排好序了。</p><p>图 5 的 OPTIMIZER_TRACE 结果中，filesort_priority_queue_optimization 这个部分的 chosen=true，就表示使用了优先队列排序算法，这个过程不需要临时文件，因此对应的 number_of_tmp_files 是 0。</p><p>你可能会问，这里也用到了 limit，为什么没用优先队列排序算法呢？原因是，这条 SQL 语句是 limit 1000，如果使用优先队列算法的话，需要维护的堆的大小就是 1000 行的 (name,rowid)，超过了我设置的 sort_buffer_size 大小，所以只能使用归并排序算法。</p><h3 id="什么时候会使用临时表"><a href="#什么时候会使用临时表" class="headerlink" title="什么时候会使用临时表"></a>什么时候会使用临时表</h3><h4 id="union-执行流程"><a href="#union-执行流程" class="headerlink" title="union 执行流程"></a>union 执行流程</h4><pre><code>create table t1(id int primary key, a int, b int, index(a));delimiter ;;create procedure idata()begin  declare i int;  set i=1;  while(i&lt;=1000)do    insert into t1 values(i, i, i);    set i=i+1;  end while;end;;delimiter ;call idata();</code></pre><p>然后，我们执行下面这条语句：</p><pre><code>(select 1000 as f) union (select id from t1 order by id desc limit 2);</code></pre><p>这条语句用到了 union，它的语义是，取这两个子查询结果的并集。并集的意思就是这两个集合加起来，重复的行只保留一行。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4001801f06638ca4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li>第二行的 key=PRIMARY，说明第二个子句用到了索引 id。</li><li>第三行的 Extra 字段，表示在对子查询的结果集做 union 的时候，使用了临时表 (Using temporary)。</li></ul><p>这个语句的执行流程是这样的：</p><ol><li>创建一个内存临时表，这个临时表只有一个整型字段 f，并且 f 是主键字段。</li><li>执行第一个子查询，得到 1000 这个值，并存入临时表中。</li><li>执行第二个子查询：<ul><li>拿到第一行 id=1000，试图插入临时表中。但由于 1000 这个值已经存在于临时表了，违反了唯一性约束，所以插入失败，然后继续执行；</li><li>取到第二行 id=999，插入临时表成功。</li></ul></li><li>从临时表中按行取出数据，返回结果，并删除临时表，结果中包含两行数据分别是 1000 和 999。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-27a0ebaf024b74ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="group-by-执行流程"><a href="#group-by-执行流程" class="headerlink" title="group by 执行流程"></a>group by 执行流程</h4><pre><code>select id%10 as m, count(*) as c from t1 group by m;</code></pre><p>这个语句的逻辑是把表 t1 里的数据，按照 id%10 进行分组统计，并按照 m 的结果排序后输出。它的 explain 结果如下：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-35b19794e6c2f1db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在 Extra 字段里面，我们可以看到三个信息：</p><ul><li>Using index，表示这个语句使用了覆盖索引，选择了索引 a，不需要回表；</li><li>Using temporary，表示使用了临时表；</li><li>Using filesort，表示需要排序。</li></ul><p>这个语句的执行流程是这样的：</p><ol><li>创建内存临时表，表里有两个字段 m 和 c，主键是 m；</li><li>扫描表 t1 的索引 a，依次取出叶子节点上的 id 值，计算 id%10 的结果，记为 x；<ul><li>如果临时表中没有主键为 x 的行，就插入一个记录 (x,1);</li><li>如果表中有主键为 x 的行，就将 x 这一行的 c 值加 1；</li></ul></li><li>遍历完成后，再根据字段 m 做排序，得到结果集返回给客户端。</li></ol><p>这个流程的执行图如下：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-638c0dea792883ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>图中最后一步，对内存临时表的排序，在第 17 篇文章中已经有过介绍，我把图贴过来，方便你回顾。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-edd24def2b9c5a44.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果你的需求并不需要对结果进行排序，那你可以在 SQL 语句末尾增加 order by null，也就是改成：如果你的需求并不需要对结果进行排序，那你可以在 SQL 语句末尾增加 order by null，也就是改成：</p><pre><code>select id%10 as m, count(*) as c from t1 group by m order by null;</code></pre><p>把内存临时表的大小限制为最大 1024 字节，并把语句改成 id % 100，这样返回结果里有 100 行数据。但是，这时的内存临时表大小不够存下这 100 行数据，也就是说，执行过程中会发现内存临时表大小到达了上限（1024 字节）。</p><p>那么，这时候就会把内存临时表转成磁盘临时表，磁盘临时表默认使用的引擎是 InnoDB。 这时，返回的结果如图 9 所示。</p><p>如果这个表 t1 的数据量很大，很可能这个查询需要的磁盘临时表就会占用大量的磁盘空间。</p><h4 id="group-by-优化方法-–-索引"><a href="#group-by-优化方法-–-索引" class="headerlink" title="group by 优化方法 – 索引"></a>group by 优化方法 – 索引</h4><p>在 MySQL 5.7 版本支持了 generated column 机制，用来实现列数据的关联更新。你可以用下面的方法创建一个列 z，然后在 z 列上创建一个索引（如果是 MySQL 5.6 及之前的版本，你也可以创建普通列和索引，来解决这个问题）。</p><pre><code>alter table t1 add column z int generated always as(id % 100), add index(z);</code></pre><p>这样，索引 z 上的数据就是类似图 10 这样有序的了。上面的 group by 语句就可以改成：</p><pre><code>select z, count(*) as c from t1 group by z;</code></pre><h4 id="group-by-优化方法-–-直接排序"><a href="#group-by-优化方法-–-直接排序" class="headerlink" title="group by 优化方法 – 直接排序"></a>group by 优化方法 – 直接排序</h4><p>所以，如果可以通过加索引来完成 group by 逻辑就再好不过了。但是，如果碰上不适合创建索引的场景，我们还是要老老实实做排序的。那么，这时候的 group by 要怎么优化呢？</p><p>如果我们明明知道，一个 group by 语句中需要放到临时表上的数据量特别大，却还是要按照“先放到内存临时表，插入一部分数据后，发现内存临时表不够用了再转成磁盘临时表”，看上去就有点儿傻。</p><p>那么，我们就会想了，MySQL 有没有让我们直接走磁盘临时表的方法呢？</p><p>在 group by 语句中加入 SQL_BIG_RESULT 这个提示（hint），就可以告诉优化器：这个语句涉及的数据量很大，请直接用磁盘临时表。</p><p>MySQL 的优化器一看，磁盘临时表是 B+ 树存储，存储效率不如数组来得高。所以，既然你告诉我数据量很大，那从磁盘空间考虑，还是直接用数组来存吧。</p><pre><code>select SQL_BIG_RESULT id%100 as m, count(*) as c from t1 group by m;</code></pre><p>的执行流程就是这样的：</p><ol><li>初始化 sort_buffer，确定放入一个整型字段，记为 m；</li><li>扫描表 t1 的索引 a，依次取出里面的 id 值, 将 id%100 的值存入 sort_buffer 中；</li><li>扫描完成后，对 sort_buffer 的字段 m 做排序（如果 sort_buffer 内存不够用，就会利用磁盘临时文件辅助排序）；</li><li>排序完成后，就得到了一个有序数组。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ae7d42d638c79aa1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-394d69bd71231348.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>从 Extra 字段可以看到，这个语句的执行没有再使用临时表，而是直接用了排序算法。</p><p>基于上面的 union、union all 和 group by 语句的执行过程的分析，我们来回答文章开头的问题：MySQL 什么时候会使用内部临时表？</p><ol><li>如果语句执行过程可以一边读数据，一边直接得到结果，是不需要额外内存的，否则就需要额外的内存，来保存中间结果；</li><li>join_buffer 是无序数组，sort_buffer 是有序数组，临时表是二维表结构；</li><li>如果执行逻辑需要用到二维表特性，就会优先考虑使用临时表。比如我们的例子中，union 需要用到唯一索引约束， group by 还需要用到另外一个字段来存累积计数。</li></ol><h4 id="distinct-和-group-by-的性能"><a href="#distinct-和-group-by-的性能" class="headerlink" title="distinct 和 group by 的性能"></a>distinct 和 group by 的性能</h4><pre><code>select a from t group by a order by null;select distinct a from t;</code></pre><p>首先需要说明的是，这种 group by 的写法，并不是 SQL 标准的写法。标准的 group by 语句，是需要在 select 部分加一个聚合函数，比如：</p><pre><code>select a,count(*) from t group by a order by null;</code></pre><p>这条语句的逻辑是：按照字段 a 分组，计算每组的 a 出现的次数。在这个结果里，由于做的是聚合计算，相同的 a 只出现一次。</p><p>没有了 count(*) 以后，也就是不再需要执行“计算总数”的逻辑时，第一条语句的逻辑就变成是：按照字段 a 做分组，相同的 a 的值只返回一行。而这就是 distinct 的语义，所以不需要执行聚合函数时，distinct 和 group by 这两条语句的语义和执行流程是相同的，因此执行性能也相同。</p><p>这两条语句的执行流程是下面这样的。</p><ol><li>创建一个临时表，临时表有一个字段 a，并且在这个字段 a 上创建一个唯一索引；</li><li>遍历表 t，依次取数据插入临时表中：如果发现唯一键冲突，就跳过；否则插入成功；</li><li>遍历完成后，将临时表作为结果集返回给客户端。</li></ol><h4 id="group-by-指导原则"><a href="#group-by-指导原则" class="headerlink" title="group by 指导原则"></a>group by 指导原则</h4><ol><li>如果对 group by 语句的结果没有排序要求，要在语句后面加 order by null；</li><li>尽量让 group by 过程用上表的索引，确认方法是 explain 结果里没有 Using temporary 和 Using filesort；</li><li>如果 group by 需要统计的数据量不大，尽量只使用内存临时表；也可以通过适当调大 tmp_table_size 参数，来避免用到磁盘临时表；</li><li>如果数据量实在太大，使用 SQL_BIG_RESULT 这个提示，来告诉优化器直接使用排序算法得到 group by 的结果。</li></ol><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="重建表"><a href="#重建表" class="headerlink" title="重建表"></a>重建表</h3><p>试想一下，如果你现在有一个表 A，需要做空间收缩，为了把表中存在的空洞去掉，你可以怎么做呢？</p><p>你可以新建一个与表 A 结构相同的表 B，然后按照主键 ID 递增的顺序，把数据一行一行地从表 A 里读出来再插入到表 B 中。</p><p>这里，你可以使用 alter table A engine=InnoDB 命令来重建表。在 MySQL 5.5 版本之前，这个命令的执行流程跟我们前面描述的差不多，区别只是这个临时表 B 不需要你自己创建，MySQL 会自动完成转存数据、交换表名、删除旧表的操作。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2a6bac61608d54ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="Online-DDL"><a href="#Online-DDL" class="headerlink" title="Online DDL"></a>Online DDL</h3><ol><li>建立一个临时文件，扫描表 A 主键的所有数据页；</li><li>用数据页中表 A 的记录生成 B+ 树，存储到临时文件中；</li><li>生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中，对应的是图中 state2 的状态；</li><li>临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件，对应的就是图中 state3 的状态；</li><li>用临时文件替换表 A 的数据文件。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-79ff4f42f79adfce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>确实，图 4 的流程中，alter 语句在启动的时候需要获取 MDL 写锁，但是这个写锁在真正拷贝数据之前就退化成读锁了。</p><p>为什么要退化呢？为了实现 Online，MDL 读锁不会阻塞增删改操作。</p><p>那为什么不干脆直接解锁呢？为了保护自己，禁止其他线程对这个表同时做 DDL。</p><p>而对于一个大表来说，Online DDL 最耗时的过程就是拷贝数据到临时表的过程，这个步骤的执行期间可以接受增删改操作。所以，相对于整个 DDL 过程来说，锁的时间非常短。对业务来说，就可以认为是 Online 的。</p><p>另一种典型的大事务场景，就是大表 DDL。这个场景,处理方案就是，计划内的 DDL，建议使用 gh-ost 方案。</p><h4 id="Online-和-inplace"><a href="#Online-和-inplace" class="headerlink" title="Online 和 inplace"></a>Online 和 inplace</h4><p>如果说这两个逻辑之间的关系是什么的话，可以概括为：</p><ol><li>DDL 过程如果是 Online 的，就一定是 inplace 的；</li><li>反过来未必，也就是说 inplace 的 DDL，有可能不是 Online 的。截止到 MySQL 8.0，添加全文索引（FULLTEXT index）和空间索引 (SPATIAL index) 就属于这种情况。</li></ol><p>有同学问到使用 optimize table、analyze table 和 alter table 这三种方式重建表的区别。<br>这里，我顺便再简单和你解释一下。</p><ul><li>从 MySQL 5.6 版本开始，alter table t engine = InnoDB（也就是 recreate）默认的就是上面图 4 的流程了；</li><li>analyze table t 其实不是重建表，只是对表的索引信息做重新统计，没有修改数据，这个过程中加了 MDL 读锁；</li><li>optimize table t 等于 recreate+analyze。</li></ul><h3 id="count-、count-主键-id-和-count-1"><a href="#count-、count-主键-id-和-count-1" class="headerlink" title="count(*)、count(主键 id) 和 count(1)"></a>count(*)、count(主键 id) 和 count(1)</h3><p>对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。</p><p>对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。</p><p>对于 count(字段) 来说： </p><ol><li>如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加；</li><li>如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。</li></ol><p>但是 <code>count(*)</code> 是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加。</p><p>所以结论是：按照效率排序的话，count(字段)&lt; count(主键 id) &lt; count(1) ≈count(<em>)，所以我建议你，尽量使用 count(</em>)。</p><h3 id="内存表"><a href="#内存表" class="headerlink" title="内存表"></a>内存表</h3><p>可见，InnoDB 和 Memory 引擎的数据组织方式是不同的：</p><ul><li>InnoDB 引擎把数据放在主键索引上，其他索引上保存的是主键 id。这种方式，我们称之为索引组织表（Index Organizied Table）。</li><li>而 Memory 引擎采用的是把数据单独存放，索引上保存数据位置的数据组织形式，我们称之为堆组织表（Heap Organizied Table）。</li></ul><p>从中我们可以看出，这两个引擎的一些典型不同：</p><ol><li>InnoDB 表的数据总是有序存放的，而内存表的数据就是按照写入顺序存放的；</li><li>当数据文件有空洞的时候，InnoDB 表在插入新数据的时候，为了保证数据有序性，只能在固定的位置写入新值，而内存表找到空位就可以插入新值；</li><li>数据位置发生变化的时候，InnoDB 表只需要修改主键索引，而内存表需要修改所有索引；</li><li>InnoDB 表用主键索引查询时需要走一次索引查找，用普通索引查询的时候，需要走两次索引查找。而内存表没有这个区别，所有索引的“地位”都是相同的。</li><li>InnoDB 支持变长数据类型，不同记录的长度可能不同；内存表不支持 Blob 和 Text 字段，并且即使定义了 varchar(N)，实际也当作 char(N)，也就是固定长度字符串来存储，因此内存表的每行数据长度相同。</li><li>内存表不支持行锁，只支持表锁。因此，一张表只要有更新，就会堵住其他所有在这个表上的读写操作。</li><li>数据放在内存中，是内存表的优势，但也是一个劣势。因为，数据库重启的时候，所有的内存表都会被清空。</li></ol><p>由于内存表的这些特性，每个数据行被删除以后，空出的这个位置都可以被接下来要插入的数据复用。比如，如果要在表 t1 中执行：</p><p>基于内存表的特性，我们还分析了它的一个适用场景，就是内存临时表。内存表支持 hash 索引，这个特性利用起来，对复杂查询的加速效果还是很不错的。</p><h3 id="自增主键为什么不是连续的"><a href="#自增主键为什么不是连续的" class="headerlink" title="自增主键为什么不是连续的"></a>自增主键为什么不是连续的</h3><p>MySQL 5.1.22 版本引入了一个新策略，新增参数 innodb_autoinc_lock_mode，默认值是 1。</p><ul><li>这个参数的值被设置为 0 时，表示采用之前 MySQL 5.0 版本的策略，即语句执行结束后才释放锁；</li><li>这个参数的值被设置为 1 时：<ul><li>普通 insert 语句，自增锁在申请之后就马上释放；</li><li>类似 insert … select 这样的批量插入数据的语句，自增锁还是要等语句结束后才被释放.</li></ul></li><li>这个参数的值被设置为 2 时，所有的申请自增主键的动作都是申请后就释放锁。</li></ul><p>在 MySQL 5.7 及之前的版本，自增值保存在内存里，并没有持久化。每次重启后，第一次打开表的时候，都会去找自增值的最大值 max(id)，然后将 max(id)+1 作为这个表当前的自增值。﻿</p><p>在 MyISAM 引擎里面，自增值是被写在数据文件上的。而在 InnoDB 中，自增值是被记录在内存的。MySQL 直到 8.0 版本，才给 InnoDB 表的自增值加上了持久化的能力，确保重启前后一个表的自增值不变。</p><p>MySQL 5.1.22 版本开始引入的参数 innodb_autoinc_lock_mode，控制了自增值申请时的锁范围。从并发性能的角度考虑，我建议你将其设置为 2，同时将 binlog_format 设置为 row。我在前面的文章中其实多次提到，binlog_format 设置为 row，是很有必要的。今天的例子给这个结论多了一个理由。</p><p>在什么场景下自增主键可能不连续</p><p>1：唯一键冲突<br>2：事务回滚<br>3：自增主键的批量申请</p><p>深层次原因是，不判断自增主键是否已存在可减少加锁的时间范围和粒度-&gt;为了更高的性能-&gt;自增主键不能回退-&gt;自增主键不连续</p><h3 id="慢查询性能问题"><a href="#慢查询性能问题" class="headerlink" title="慢查询性能问题"></a>慢查询性能问题</h3><ol><li>索引没有设计好；</li><li>SQL 语句没写好；</li><li>MySQL 选错了索引。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> Note </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis 源码分析(二) ：ADList</title>
      <link href="/2019/06/14/reids-source-code-2/"/>
      <url>/2019/06/14/reids-source-code-2/</url>
      
        <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>ADList(A generic doubly linked list)是 redis 自定义的一种双向链表，广泛运用于 redisClients 、 redisServer 、发布订阅、慢查询、监视器等。（<strong>注：3.0及以前还会被运用于<code>list</code>结构中，在3.2以后被<code>quicklist</code>取代</strong>）。</p><ul><li>链表提供了高效的节点重排能力，以及顺序性的节点访问方式，并且可以通过增删节点来灵活地调整链表的长度。</li><li>链表在Redis 中的应用非常广泛，比如列表键的底层实现之一就是链表。当一个列表键包含了数量较多的元素，又或者列表中包含的元素都是比较长的字符串时，Redis 就会使用链表作为列表键的底层实现。</li><li>链表结构是 Redis 中一个常用的结构，它可以存储多个字符串</li><li>它是有序的</li><li>能够存储2的32次方减一个节点（超过 40 亿个节点）</li><li>Redis 链表是双向的，因此即可以从左到右，也可以从右到左遍历它存储的节点</li><li>链表结构查找性能不佳，但 插入和删除速度很快</li></ul><p>由于是双向链表，所以只能够从左到右，或者从右到左地访问和操作链表里面的数据节点。 但是使用链表结构就意味着读性能的丧失，所以要在大量数据中找到一个节点的操作性能是不佳的，因为链表只能从一个方向中去遍历所要节点，比如从查找节点 10000 开始查询，它需要按照节点1 、节点 2、节点 3……直至节点 10000，这样的顺序查找，然后把一个个节点和你给出的值比对，才能确定节点所在。如果这个链表很大，如有上百万个节点，可能需要遍历几十万次才能找到所需要的节点，显然查找性能是不佳的。</p><p>链表结构的优势在于插入和删除的便利 ，因为链表的数据节点是分配在不同的内存区域的，并不连续，只是根据上一个节点保存下一个节点的顺序来索引而己，无需移动元素。</p><p>因为是双向链表结构，所以 Redis 链表命令分为左操作和右操作两种命令，左操作就意味着是从左到右，右操作就意味着是从右到左。</p><h2 id="链表的数据结构"><a href="#链表的数据结构" class="headerlink" title="链表的数据结构"></a>链表的数据结构</h2><pre><code>typedef struct listNode{      struct listNode *prev;      struct listNode * next;      void * value;  }</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-87f65509ab658cbc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="list_node.png">    </p><pre><code>typedef struct list{    //表头节点    listNode  * head;    //表尾节点    listNode  * tail;    //链表长度    unsigned long len;    //节点值复制函数    void *(*dup) (void *ptr);    //节点值释放函数    void (*free) (void *ptr);    //节点值对比函数    int (*match)(void *ptr, void *key);}</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c896f9d20e45e942.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="list.png"></p><ul><li>dup为节点复制函数</li><li>free为节点释放函数</li><li>match为节点比较函数<br>通过这样的定义，adlist有了以下优点：</li><li>双向：可以灵活的访问前置或者后置节点</li><li>list头指针和尾指针：可以方便的获取头尾节点或者从头尾遍历查找</li><li>len：使获取list由O(N)变为O(1)</li><li>通过void实现多态：不同的实例化链表对象可以持有不同的值，其对应的3个操作函数也可以自定义，是不是有点interface的感觉！</li></ul><h2 id="链表迭代器"><a href="#链表迭代器" class="headerlink" title="链表迭代器"></a>链表迭代器</h2><pre><code>typedef struct listIter {   // 列表迭代器    listNode *next;    int direction;  // 迭代器遍历方向} listIter;</code></pre><p>其中direction用于标识迭代器的遍历方向：</p><pre><code>#define AL_START_HEAD 0     // 从头遍历#define AL_START_TAIL 1     // 从尾遍历</code></pre><p>通过定义listIter，redis 在需要遍历list时，不需要再复制各种tmp值，只需要调用listIter的遍历函数。 以listSearchKey为例：</p><pre><code>listNode *listSearchKey(list *list, void *key)  // list查找key{    listIter iter;    listNode *node;    listRewind(list, &amp;iter);    // 初始化迭代器    while((node = listNext(&amp;iter)) != NULL) {   // 迭代器遍历        if (list-&gt;match) {  // 如果定义了match函数            if (list-&gt;match(node-&gt;value, key)) {                return node;            }        } else {    // 直接进行值比较            if (key == node-&gt;value) {                return node;            }        }    }    return NULL;}</code></pre><p>所有和遍历有关的行为都收敛到了listIter中，list就专注负责存储。</p><h2 id="链表的特性"><a href="#链表的特性" class="headerlink" title="链表的特性"></a>链表的特性</h2><ul><li>双端：链表节点带有prev 和next 指针，获取某个节点的前置节点和后置节点的时间复杂度都是O（N）</li><li>无环：表头节点的 prev 指针和表尾节点的next 都指向NULL，对立案表的访问时以NULL为截止</li><li>表头和表尾：因为链表带有head指针和tail 指针，程序获取链表头结点和尾节点的时间复杂度为O(1)</li><li>长度计数器：链表中存有记录链表长度的属性 len</li><li>多态：链表节点使用 void* 指针来保存节点值，并且可以通过list 结构的dup 、 free、 match三个属性为节点值设置类型特定函数。</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/jaycekon/p/6227442.html">深入浅出Redis-redis底层数据结构（上）</a></p><p><a href="https://blog.csdn.net/yangshangwei/article/details/82792672">Redis-05Redis数据结构–链表( linked-list)</a></p><p><a href="http://czrzchao.com/redisSourceAdlist#adlist">redis源码解读(二):基础数据结构之ADLIST</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis 源码分析(一) ：sds</title>
      <link href="/2019/06/14/reids-source-code-1/"/>
      <url>/2019/06/14/reids-source-code-1/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是sds"><a href="#什么是sds" class="headerlink" title="什么是sds"></a>什么是sds</h2><p>字符串是Redis中最为常见的数据存储类型，其底层实现是简单动态字符串sds(simple dynamic string)，是可以修改的字符串。</p><p>它类似于Java中的ArrayList，它采用预分配冗余空间的方式来减少内存的频繁分配。</p><h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><pre><code>// 3.0及以前struct sdshdr {    // 记录buf数组中已使用字节数量    unsigned int len;    // 记录buf数组中未使用的字节数量    unsigned int free;    // 字节数组，存储字符串    char buf[];};// &gt;=3.2struct __attribute__ ((__packed__)) sdshdr5 {    unsigned char flags; /* 3 lsb of type, and 5 msb of string length */    char buf[];};struct __attribute__ ((__packed__)) sdshdr8 {    uint8_t len; /* used */    uint8_t alloc; /* excluding the header and null terminator */    unsigned char flags; /* 3 lsb of type, 5 unused bits */    char buf[];};struct __attribute__ ((__packed__)) sdshdr16 {    uint16_t len; /* used */    uint16_t alloc; /* excluding the header and null terminator */    unsigned char flags; /* 3 lsb of type, 5 unused bits */    char buf[];};struct __attribute__ ((__packed__)) sdshdr32 {    uint32_t len; /* used */    uint32_t alloc; /* excluding the header and null terminator */    unsigned char flags; /* 3 lsb of type, 5 unused bits */    char buf[];};struct __attribute__ ((__packed__)) sdshdr64 {    uint64_t len; /* used */    uint64_t alloc; /* excluding the header and null terminator */    unsigned char flags; /* 3 lsb of type, 5 unused bits */    char buf[];};//在3.2以后的版本，redis 的SDS分为了5种数据结构，分别应对不同长度的字符串需求，具体的类型选择如下。static inline char sdsReqType(size_t string_size) { // 获取类型    if (string_size &lt; 1&lt;&lt;5)     // 32        return SDS_TYPE_5;    if (string_size &lt; 1&lt;&lt;8)     // 256        return SDS_TYPE_8;    if (string_size &lt; 1&lt;&lt;16)    // 65536 64k        return SDS_TYPE_16;    if (string_size &lt; 1ll&lt;&lt;32)  // 4294967296 4GB        return SDS_TYPE_32;    return SDS_TYPE_64;}</code></pre><p><code>__attribute__ ((__packed__))</code>这个声明就是用来告诉编译器取消内存对齐优化，按照实际的占用字节数进行对齐</p><pre><code>printf("%ld\n", sizeof(struct sdshdr8));  // 3printf("%ld\n", sizeof(struct sdshdr16)); // 5printf("%ld\n", sizeof(struct sdshdr32)); // 9printf("%ld\n", sizeof(struct sdshdr64)); // 17</code></pre><p>通过加上<code>__attribute__ ((__packed__))</code>声明，sdshdr16节省了1个字节，sdshdr32节省了3个字节，sdshdr64节省了7个字节。 </p><p><strong>但是内存不对齐怎么办呢，不能为了一点内存大大拖慢cpu的寻址效率啊？redis 通过自己在malloc等c语言内存分配函数上封装了一层zmalloc，将内存分配收敛，并解决了内存对齐的问题</strong>。在内存分配前有这么一段代码：</p><pre><code>if (_n&amp;(sizeof(long)-1)) _n += sizeof(long)-(_n&amp;(sizeof(long)-1)); \    // 确保内存对齐！</code></pre><p>这段代码写的比较抽象，简而言之就是先判断当前要分配的_n个内存是否是long类型的整数倍，如果不是就在_n的基础上加上内存大小差值，从而达到了内存对齐的保证。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-16c503713df2b8e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sds_example.png"></p><ul><li>len记录当前字节数组的长度（不包括\0），使得获取字符串长度的时间复杂度由O(N)变为了O(1)</li><li>alloc记录了当前字节数组总共分配的内存大小（不包括\0）</li><li>flags记录了当前字节数组的属性、用来标识到底是sdshdr8还是sdshdr16等</li><li>buf保存了字符串真正的值以及末尾的一个\0</li></ul><p>整个SDS的内存是连续的，统一开辟的。为何要统一开辟呢？因为在大多数操作中，buf内的字符串实体才是操作对象。如果统一开辟内存就能通过buf头指针进行寻址，拿到整个struct的指针，<strong>而且通过buf的头指针减一直接就能获取flags的值，骚操作代码如下</strong>！</p><pre><code>// flags值的定义#define SDS_TYPE_5  0#define SDS_TYPE_8  1#define SDS_TYPE_16 2#define SDS_TYPE_32 3#define SDS_TYPE_64 4// 通过buf获取头指针#define SDS_HDR_VAR(T,s) struct sdshdr##T *sh = (void*)((s)-(sizeof(struct sdshdr##T)));#define SDS_HDR(T,s) ((struct sdshdr##T *)((s)-(sizeof(struct sdshdr##T))))// 通过buf的-1下标拿到flags值unsigned char flags = s[-1];</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-32ae9a7cfb5be130.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="SDS.png"></p><h2 id="SDS的两种存储形式"><a href="#SDS的两种存储形式" class="headerlink" title="SDS的两种存储形式"></a>SDS的两种存储形式</h2><pre><code>&gt; set codehole abcdefghijklmnopqrstuvwxyz012345678912345678OK&gt; debug object codeholeValue at:0x7fec2de00370 refcount:1 encoding:embstr serializedlength:45 lru:5958906 lru_seconds_idle:1&gt; set codehole abcdefghijklmnopqrstuvwxyz0123456789123456789OK&gt; debug object codeholeValue at:0x7fec2dd0b750 refcount:1 encoding:raw serializedlength:46 lru:5958911 lru_seconds_idle:1...</code></pre><p>一个字符的差别，存储形式 encoding 就发生了变化。一个是 embstr，一个是 row。</p><p>在了解存储格式的区别之前，首先了解下<strong>RedisObject</strong>结构体。</p><p>所有的 Redis 对象都有一个 Redis 对象头结构体</p><pre><code>struct RedisObject { // 一共占用16字节    int4 type; // 4bits  类型    int4 encoding; // 4bits 存储格式    int24 lru; // 24bits 记录LRU信息    int32 refcount; // 4bytes     void *ptr; // 8bytes，64-bit system } robj;</code></pre><ul><li><p>不同的对象具有不同的类型 type(4bit)，同一个类型的 type 会有不同的存储形式 encoding(4bit)。</p></li><li><p>为了记录对象的 LRU 信息，使用了 24 个 bit 的 lru 来记录 LRU 信息。</p></li><li><p>每个对象都有个引用计数 refcount，当引用计数为零时，对象就会被销毁，内存被回收。ptr 指针将指向对象内容 (body) 的具体存储位置。</p></li><li><p> 一个 RedisObject 对象头共需要占据 16 字节的存储空间。</p></li></ul><p>embstr 存储形式是这样一种存储形式，它将 RedisObject 对象头和 SDS 对象连续存在一起，使用 malloc 方法一次分配。而 raw 存储形式不一样，它需要两次 malloc，两个对象头在内存地址上一般是不连续的。</p><p>在字符串比较小时，SDS 对象头的大小是capacity+3——SDS结构体的内存大小至少是 3。意味着分配一个字符串的最小空间占用为 19 字节 (16+3)。</p><p>如果总体超出了 64 字节，Redis 认为它是一个大字符串，不再使用 emdstr 形式存储，而该用 raw 形式。而64-19-结尾的\0，所以empstr只能容纳44字节。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-38f23873b2076cb5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="embstr_raw.png"></p><p>再看一下RedisObject的10种存储格式——encoding</p><pre><code>//这两个宏定义申明是在server.h文件中#define OBJ_ENCODING_RAW 0     /* Raw representation */#define OBJ_ENCODING_INT 1     /* Encoded as integer */#define OBJ_ENCODING_HT 2      /* Encoded as hash table */#define OBJ_ENCODING_ZIPMAP 3  /* Encoded as zipmap */#define OBJ_ENCODING_LINKEDLIST 4 /* No longer used: old list encoding. */#define OBJ_ENCODING_ZIPLIST 5 /* Encoded as ziplist */#define OBJ_ENCODING_INTSET 6  /* Encoded as intset */#define OBJ_ENCODING_SKIPLIST 7  /* Encoded as skiplist */#define OBJ_ENCODING_EMBSTR 8  /* Embedded sds string encoding */#define OBJ_ENCODING_QUICKLIST 9 /* Encoded as linked list of ziplists */</code></pre><h2 id="创建SDS"><a href="#创建SDS" class="headerlink" title="创建SDS"></a>创建SDS</h2><p>由于sdshdr5的只用来存储长度为32字节以下的字符数组，因此flags的5个bit就能满足长度记录，加上type所需的3bit，刚好为8bit一个字节，因此sdshdr5不需要单独的len记录长度，并且只有32个字节的存储空间，动态的变更内存余地较小，所以 redis 直接不存储alloc，当sdshdr5需要扩展时会直接变更成更大的SDS数据结构。<br>除此之外，SDS都会多分配1个字节用来保存’\0’。</p><pre><code>sds sdsnewlen(const void *init, size_t initlen) {   // 创建sds    void *sh;    sds s;  // 指向字符串头指针    char type = sdsReqType(initlen);    if (type == SDS_TYPE_5 &amp;&amp; initlen == 0) type = SDS_TYPE_8;  // 如果是空字符串直接使用SDS_TYPE_8，方便后续拼接    int hdrlen = sdsHdrSize(type);    unsigned char *fp; /* flags pointer. */    sh = s_malloc(hdrlen+initlen+1);    // 分配空间大小为 sdshdr大小+字符串长度+1    if (!init)        memset(sh, 0, hdrlen+initlen+1);    // 初始化内存空间    if (sh == NULL) return NULL;    s = (char*)sh+hdrlen;    fp = ((unsigned char*)s)-1; // 获取flags指针    switch(type) {        case SDS_TYPE_5: {            *fp = type | (initlen &lt;&lt; SDS_TYPE_BITS);    // sdshdr5的前5位保存长度，后3位保存type            break;        }        case SDS_TYPE_8: {            SDS_HDR_VAR(8,s);       // 获取sdshdr指针            sh-&gt;len = initlen;      // 设置len            sh-&gt;alloc = initlen;    // 设置alloc            *fp = type; // 设置type            break;        }        case SDS_TYPE_16: {            SDS_HDR_VAR(16,s);            sh-&gt;len = initlen;            sh-&gt;alloc = initlen;            *fp = type;            break;        }        case SDS_TYPE_32: {            SDS_HDR_VAR(32,s);            sh-&gt;len = initlen;            sh-&gt;alloc = initlen;            *fp = type;            break;        }        case SDS_TYPE_64: {            SDS_HDR_VAR(64,s);            sh-&gt;len = initlen;            sh-&gt;alloc = initlen;            *fp = type;            break;        }    }    if (initlen &amp;&amp; init)        memcpy(s, init, initlen);   // 内存拷贝字字符数组赋值    s[initlen] = '\0';  // 字符数组最后一位设为\0    return s;}</code></pre><h2 id="SDS拼接"><a href="#SDS拼接" class="headerlink" title="SDS拼接"></a>SDS拼接</h2><pre><code>static inline size_t sdslen(const sds s) {    unsigned char flags = s[-1];    switch(flags&amp;SDS_TYPE_MASK) {        case SDS_TYPE_5:            return SDS_TYPE_5_LEN(flags);        case SDS_TYPE_8:            return SDS_HDR(8,s)-&gt;len;        case SDS_TYPE_16:            return SDS_HDR(16,s)-&gt;len;        case SDS_TYPE_32:            return SDS_HDR(32,s)-&gt;len;        case SDS_TYPE_64:            return SDS_HDR(64,s)-&gt;len;    }    return 0;}sds sdscatlen(sds s, const void *t, size_t len) {    size_t curlen = sdslen(s);  // 获取当前字符串长度    s = sdsMakeRoomFor(s,len);  // 重点!    if (s == NULL) return NULL;    memcpy(s+curlen, t, len);   // 内存拷贝    sdssetlen(s, curlen+len);   // 设置sds-&gt;len    s[curlen+len] = '\0';       // 在buf的末尾追加一个\0    return s;}sds sdsMakeRoomFor(sds s, size_t addlen) {  // 确保sds字符串在拼接时有足够的空间    void *sh, *newsh;    size_t avail = sdsavail(s); // 获取可用长度    size_t len, newlen;    char type, oldtype = s[-1] &amp; SDS_TYPE_MASK;    int hdrlen;    /* Return ASAP if there is enough space left. */    if (avail &gt;= addlen) return s;    len = sdslen(s);    // 获取字符串长度 O(1)    // 用sds（指向结构体尾部，字符串首部）减去结构体长度得到结构体首部指针     // 结构体类型是不确定的，所以是void *sh    sh = (char*)s-sdsHdrSize(oldtype);  // 获取当前sds指针    newlen = (len+addlen);  // 分配策略 SDS_MAX_PREALLOC=1024*1024=1M    if (newlen &lt; SDS_MAX_PREALLOC)        newlen *= 2;    // 如果拼接后的字符串小于1M，就分配两倍的内存    else        newlen += SDS_MAX_PREALLOC; // 如果拼接后的字符串大于1M，就分配多分配1M的内存    type = sdsReqType(newlen);  // 获取新字符串的sds类型    // 由于SDS_TYPE_5没有记录剩余空间（用多少分配多少），所以是不合适用来进行追加的    // 为了防止下次追加出现这种情况，所以直接分配SDS_TYPE_8类型    if (type == SDS_TYPE_5) type = SDS_TYPE_8;  // 如果type为SDS_TYPE_5直接优化成SDS_TYPE_8    hdrlen = sdsHdrSize(type);    if (oldtype==type) {        newsh = s_realloc(sh, hdrlen+newlen+1); // 类型没变在原有基础上realloc        if (newsh == NULL) return NULL;        s = (char*)newsh+hdrlen;    } else {        /* Since the header size changes, need to move the string forward,         * and can't use realloc */        newsh = s_malloc(hdrlen+newlen+1);  // 类型发生变化需要重新malloc        if (newsh == NULL) return NULL;        memcpy((char*)newsh+hdrlen, s, len+1);  // 将老字符串拷贝到新的内存快中        s_free(sh); // 释放老的sds内存        s = (char*)newsh+hdrlen;        s[-1] = type;   // 设置sds-&gt;flags        sdssetlen(s, len);  // 设置sds-&gt;len    }    sdssetalloc(s, newlen); // 设置sds-&gt;alloc    return s;}</code></pre><p>SDS字符串分配策略：</p><ol><li>拼接后的字符串长度不超过1M，分配两倍的内存</li><li>拼接够的字符串长度超过1M，多分配1M的内存 (字符串最大长度为 512M)</li></ol><p>通过这两种策略，在字符串拼接时会预分配一部分内存，下次拼接的时候就可能不再需要进行内存分配了，将原本N次字符串拼接需要N次内存重新分配的次数优化到最多需要N次，是典型的空间换时间的做法。<br>当然，如果新的字符串长度超过了原有字符串类型的限定那么还会涉及到一个重新生成sdshdr的过程。</p><p>还有一个细节需要注意，由于sdshrd5并不存储alloc值，因此无法获取sdshrd5的可用大小，如果继续采用sdshrd5进行存储，在之后的拼接过程中每次都还是要进行内存重分配。因此在发生拼接行为时，sdshrd5会被直接优化成sdshrd8。</p><h2 id="惰性空间释放策略"><a href="#惰性空间释放策略" class="headerlink" title="惰性空间释放策略"></a>惰性空间释放策略</h2><pre><code>void sdsclear(sds s) {  //重置sds的buf空间，懒惰释放    struct sdshdr *sh = (void*) (s-(sizeof(struct sdshdr)));    sh-&gt;free += sh-&gt;len;    //表头free成员+已使用空间的长度len = 新的free    sh-&gt;len = 0;            //已使用空间变为0    sh-&gt;buf[0] = '\0';         //字符串置空}sds sdstrim(sds s, const char *cset) {  // sds trim操作    char *start, *end, *sp, *ep;    size_t len;    sp = start = s;    ep = end = s+sdslen(s)-1;    while(sp &lt;= end &amp;&amp; strchr(cset, *sp)) sp++; // 从头遍历    while(ep &gt; sp &amp;&amp; strchr(cset, *ep)) ep--;   // 从尾部遍历    len = (sp &gt; ep) ? 0 : ((ep-sp)+1);    if (s != sp) memmove(s, sp, len);   // 内存拷贝    s[len] = '\0';    sdssetlen(s,len);   // 重新设置sds-&gt;len    return s;}</code></pre><p>将A. :<code>HelloWroldA.:</code>进行<code>sdstrim(s,"Aa. :");</code>后，如下图所示： </p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a8e3722dd42649b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sdstrim.png"></p><p>可以看到内存空间并没有被释放，甚至空闲的空间都没有被置空。由于SDS是通过len值标识字符串长度，因此SDS完全不需要受限于c语言字符串的那一套\0结尾的字符串形式。在后续需要拼接扩展时，这部分空间也能够再次被利用起来，降低了内存重新分配的概率。</p><p>当然，SDS也提供了真正的释放空间的方法，以供真正需要释放空闲内存时使用。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>redis 3.2之后，针对不同长度的字符串引入了不同的SDS数据结构，并且强制内存对齐1，将内存对齐交给统一的内存分配函数，从而达到节省内存的目的</li><li>SDS的字符串长度通过sds-&gt;len来控制，不受限于C语言字符串\0，可以存储二进制数据，并且将获取字符串长度的时间复杂度降到了O(1)</li><li>SDS的头和buf字节数组的内存是连续的，可以通过寻址方式获取SDS的指针以及flags值</li><li>SDS的拼接扩展有一个内存预分配策略，用空间减少每次拼接的内存重分配可能性</li><li>SDS的缩短并不会真正释放掉对应空闲空间</li><li>SDS分配内存都会多分配1个字节用来在buf的末尾追加一个\0，在部分场景下可以和C语言字符串保证同样的行为甚至复用部分string.h的函数</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/czrzchao/article/details/78990345">redis源码解读(一):基础数据结构之SDS</a></p><p><a href="https://blog.csdn.net/qq193423571/article/details/81637075">Redis深入浅出——字符串和SDS</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL 索引那些事</title>
      <link href="/2019/04/04/mysql-index/"/>
      <url>/2019/04/04/mysql-index/</url>
      
        <content type="html"><![CDATA[<h2 id="1-MySQL-常见几种索引类型"><a href="#1-MySQL-常见几种索引类型" class="headerlink" title="1. MySQL 常见几种索引类型"></a>1. MySQL 常见几种索引类型</h2><p>1.1 普通索引，是最基本的索引，它没有任何限制。它有以下几种创建方式：</p><pre><code>（1）直接创建索引    CREATE INDEX index_name ON table(column(length))（2）修改表结构的方式添加索引    ALTER TABLE table_name ADD INDEX index_name ON (column(length))（3）创建表的时候同时创建索引    CREATE TABLE `table` (        `id` int(11) NOT NULL AUTO_INCREMENT ,        `title` char(255) CHARACTER NOT NULL ,        `content` text CHARACTER NULL ,        `time` int(10) NULL DEFAULT NULL ,        PRIMARY KEY (`id`),        INDEX index_name (title(length))    )（4）删除索引    DROP INDEX index_name ON table</code></pre><p>1.2.  唯一索引，与前面的普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。它有以下几种创建方式：</p><pre><code>（1）创建唯一索引    CREATE UNIQUE INDEX indexName ON table(column(length))（2）修改表结构    ALTER TABLE table_name ADD UNIQUE indexName ON (column(length))（3）创建表的时候直接指定    CREATE TABLE `table` (        `id` int(11) NOT NULL AUTO_INCREMENT ,        `title` char(255) CHARACTER NOT NULL ,        `content` text CHARACTER NULL ,        `time` int(10) NULL DEFAULT NULL ,        UNIQUE indexName (title(length))    );</code></pre><p>1.3 主键索引，是一种特殊的唯一索引，一个表只能有一个主键，不允许有空值。</p><p>   一般是在建表的时候同时创建主键索引：</p><pre><code>   CREATE TABLE `table` (   `id` int(11) NOT NULL AUTO_INCREMENT ,   `title` char(255) NOT NULL ,   PRIMARY KEY (`id`)   );</code></pre><p>1.4  组合索引，指多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用组合索引时遵循最左前缀集合</p><pre><code>ALTER TABLE `table` ADD INDEX name_city_age (name,city,age);</code></pre><p>1.5  全文索引，主要用来查找文本中的关键字，而不是直接与索引中的值相比较。fulltext 索引跟其它索引大不相同，它更像是一个搜索引擎，而不是简单的 where 语句的参数匹配。fulltext 索引配合 match against 操作使用，而不是一般的 where 语句加 like。它可以在 create table，alter table ，create index 使用，不过目前只有 char、varchar，text 列上可以创建全文索引。值得一提的是，在数据量较大时候，现将数据放入一个没有全局索引的表中，然后再用 CREATE index 创建 fulltext 索引，要比先为一张表建立 fulltext 然后再将数据写入的速度快很多。</p><pre><code>（1）创建表的适合添加全文索引    CREATE TABLE `table` (        `id` int(11) NOT NULL AUTO_INCREMENT ,        `title` char(255) CHARACTER NOT NULL ,        `content` text CHARACTER NULL ,        `time` int(10) NULL DEFAULT NULL ,        PRIMARY KEY (`id`),        FULLTEXT (content)    );（2）修改表结构添加全文索引    ALTER TABLE article ADD FULLTEXT index_content(content)（3）直接创建索引    CREATE FULLTEXT INDEX index_content ON article(content)</code></pre><h2 id="2-MyISAM-和-InnoDB-索引实现"><a href="#2-MyISAM-和-InnoDB-索引实现" class="headerlink" title="2. MyISAM 和 InnoDB 索引实现"></a>2. MyISAM 和 InnoDB 索引实现</h2><h3 id="2-1-MyISAM-索引实现"><a href="#2-1-MyISAM-索引实现" class="headerlink" title="2.1. MyISAM 索引实现"></a>2.1. MyISAM 索引实现</h3><p>MyISAM 引擎使用 B+Tree 作为索引结构，叶节点的 data 域存放的是数据记录的地址。下图是 MyISAM 索引的原理图：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d1af7a310e711e1f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="MyISAM1.png"></p><p>这里设表一共有三列，假设我们以 Col1 为主键，则图 8 是一个 MyISAM 表的主索引（Primary key）示意。可以看出 MyISAM 的索引文件仅仅保存数据记录的地址。在 MyISAM 中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求 key 是唯一的，而辅助索引的 key 可以重复。如果我们在 Col2 上建立一个辅助索引，则此索引的结构如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7f0bb0cbdb8c317e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="MyISAM2.png"></p><p>同样也是一颗 B+Tree，data 域保存数据记录的地址。因此，MyISAM 中索引检索的算法为首先按照 B+Tree 搜索算法搜索索引，如果指定的 Key 存在，则取出其 data 域的值，然后以 data 域的值为地址，读取相应数据记录。</p><p>MyISAM 的索引方式也叫做<strong>“非聚集”</strong>的，之所以这么称呼是为了与 InnoDB 的聚集索引区分。</p><h3 id="2-2-InnoDB-索引实现"><a href="#2-2-InnoDB-索引实现" class="headerlink" title="2.2. InnoDB 索引实现"></a>2.2. InnoDB 索引实现</h3><p>虽然 InnoDB 也使用 B+Tree 作为索引结构，但具体实现方式却与 MyISAM 截然不同。</p><p>第一个重大区别是 InnoDB 的数据文件本身就是索引文件。从上文知道，MyISAM 索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在 InnoDB 中，表数据文件本身就是按 B+Tree 组织的一个索引结构，这棵树的叶节点 data 域保存了完整的数据记录。这个索引的 key 是数据表的主键，因此 InnoDB 表数据文件本身就是主索引。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-788ffd17399abb7b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="InnoDB1.png"></p><p>图 10 是 InnoDB 主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为 InnoDB 的数据文件本身要按主键聚集，所以 InnoDB 要求表必须有主键（MyISAM 可以没有），如果没有显式指定，则 MySQL 系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则 MySQL 自动为 InnoDB 表生成一个隐含字段作为主键，这个字段长度为 6 个字节，类型为长整形。</p><p>第二个与 MyISAM 索引的不同是 InnoDB 的辅助索引 data 域存储相应记录主键的值而不是地址。换句话说，InnoDB 的所有辅助索引都引用主键作为 data 域。例如，图 11 为定义在 Col3 上的一个辅助索引：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4a33c65b8bee4a05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="InnoDB2.png"></p><h4 id="2-2-1-Innodb-的聚集索引"><a href="#2-2-1-Innodb-的聚集索引" class="headerlink" title="2.2.1  Innodb 的聚集索引"></a>2.2.1  Innodb 的聚集索引</h4><p>Innodb 的存储索引是基于 B+tree，理所当然，聚集索引也是基于 B+tree。与非聚集索引的区别则是，聚集索引既存储了索引，也存储了行值。当一个表有一个聚集索引，它的数据是存储在索引的叶子页（leaf pages）。因此 innodb 也能理解为基于索引的表。</p><h4 id="2-2-2-Innodb-如何选择一个聚集索引"><a href="#2-2-2-Innodb-如何选择一个聚集索引" class="headerlink" title="2.2.2 Innodb 如何选择一个聚集索引"></a>2.2.2 Innodb 如何选择一个聚集索引</h4><p>对于 Innodb，主键毫无疑问是一个聚集索引。但是当一个表没有主键，或者没有一个索引，Innodb 会如何处理呢。请看如下规则</p><p>如果一个主键被定义了，那么这个主键就是作为聚集索引</p><p>如果没有主键被定义，那么该表的第一个唯一非空索引被作为聚集索引</p><p>如果没有主键也没有合适的唯一索引，那么 innodb 内部会生成一个隐藏的主键作为聚集索引，这个隐藏的主键是一个 6 个字节的列，改列的值会随着数据的插入自增。</p><p>还有一个需要注意的是：</p><p>次级索引的叶子节点并不存储行数据的物理地址。而是存储的该行的主键值。</p><p>所以：一次级索引包含了两次查找。一次是查找次级索引自身。然后查找主键（聚集索引）</p><h4 id="2-2-3-建立自增主键的原因是："><a href="#2-2-3-建立自增主键的原因是：" class="headerlink" title="2.2.3  建立自增主键的原因是："></a>2.2.3  建立自增主键的原因是：</h4><p>Innodb 中的每张表都会有一个聚集索引，而聚集索引又是以物理磁盘顺序来存储的，自增主键会把数据自动向后插入，避免了插入过程中的聚集索引排序问题。聚集索引的排序，必然会带来大范围的数据的物理移动，这里面带来的磁盘 IO 性能损耗是非常大的。</p><p>而如果聚集索引上的值可以改动的话，那么也会触发物理磁盘上的移动，于是就可能出现 page 分裂，表碎片横生。</p><p>解读中的第二点相信看了上面关于聚集索引的解释后就很清楚了。</p><h3 id="2-3-聚集索引和非聚集索引解释"><a href="#2-3-聚集索引和非聚集索引解释" class="headerlink" title="2.3. 聚集索引和非聚集索引解释"></a>2.3. 聚集索引和非聚集索引解释</h3><p>聚集（clustered）索引，也叫聚簇索引。</p><blockquote><p>定义：数据行的物理顺序与列值（一般是主键的那一列）的逻辑顺序相同，一个表中只能拥有一个聚集索引。</p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/12321605-36a596bc5c6b7811.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="mysql-index-clustered.jpg"></p><p>非聚集（unclustered）索引。</p><blockquote><p>定义：该索引中索引的逻辑顺序与磁盘上行的物理存储顺序不同，一个表中可以拥有多个非聚集索引。</p></blockquote><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f8f6eecbb36726cb.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="mysql-index-unclustered.jpg"></p><p>非聚集索引查询过程：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-cfee18215d932101.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="mysql-index-unclustered2.jpg"></p><h2 id="3-索引的缺点"><a href="#3-索引的缺点" class="headerlink" title="3. 索引的缺点"></a>3. 索引的缺点</h2><ol><li>虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行 insert、update 和 delete。因为更新表时，不仅要保存数据，还要保存一下索引文件。</li><li>建立索引会占用磁盘空间的索引文件。一般情况这个问题不太严重，但如果你在一个大表上创建了多种组合索引，索引文件的会增长很快。</li><li>索引只是提高效率的一个因素，如果有大数据量的表，就需要花时间研究建立最优秀的索引，或优化查询语句。</li></ol><h2 id="4-注意事项"><a href="#4-注意事项" class="headerlink" title="4. 注意事项"></a>4. 注意事项</h2><p>使用索引时，有以下一些技巧和注意事项：</p><ol><li><p>索引不会包含有 null 值的列</p><p> 只要列中包含有 null 值都将不会被包含在索引中，复合索引中只要有一列含有 null 值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为 null。</p></li><li><p>使用短索引</p><p> 对串列进行索引，如果可能应该指定一个前缀长度。例如，如果有一个 char(255)的列，如果在前 10 个或 20 个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和 I/O 操作。</p></li><li><p>索引列排序</p><p> 查询只使用一个索引，因此如果 where 子句中已经使用了索引的话，那么 order by 中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引。</p></li><li><p>like 语句操作</p><p> 一般情况下不推荐使用 like 操作，如果非使用不可，如何使用也是一个问题。like “%aaa%” 不会使用索引而 like “aaa%”可以使用索引。</p></li><li><p>不要在列上进行运算</p><p> 这将导致索引失效而进行全表扫描，例如</p><pre><code> SELECT * FROM table_name WHERE YEAR(column_name)&lt;2017;</code></pre></li><li><p> 不使用 not in 和&lt;&gt;操作</p></li></ol><h3 id="4-1-SQL-索引优化案例分析"><a href="#4-1-SQL-索引优化案例分析" class="headerlink" title="4.1  SQL 索引优化案例分析"></a>4.1  SQL 索引优化案例分析</h3><p>假设访问 mysql 各种访问方式的耗时如下</p><ol><li><p>随机访问耗时：需要寻道、寻扇区、数据传输，平均耗时大约在 10ms 量级</p></li><li><p>顺序访问耗时：顺序访问需要数据传输，平均耗时大约在 0.01ms 量级（根据磁盘的数据传输速率计算）</p></li><li><p>FETCH 耗时：获取表记录的耗时，平均耗时按在 0.1ms 量级算</p></li></ol><h4 id="4-1-1-创建表"><a href="#4-1-1-创建表" class="headerlink" title="4.1.1 创建表"></a>4.1.1 创建表</h4><pre><code>CREATE TABLE `charge_table`(   `uid` int(10) UNSIGNED NOT NULL DEFAULT '0' COMMENT 'user id',    `client_type` TINYINT(3) UNSIGNED NOT NULL DEFAULT '0' COMMENT 'user id',    `recharge_time` INT(10) UNSIGNED NOT NULL DEFAULT '0' COMMENT 'charge time',    `recharge_gold` INT(10) UNSIGNED NOT NULL DEFAULT '0' COMMENT 'charge gold',    PRIMARY KEY (`uid`),    KEY `rtime` (`recharge_time`))ENGINE=INNODB DEFAULT CHARSET=utf8</code></pre><p>索引分析：</p><p>有根据充值时间段查询充值记录的需求，因此在 recharge_time 上建了索引</p><p>但是在 uid 上建立了主键即 uid 也是聚集索引，因此数据表按照 uid 的顺序组织</p><p>表按 uid 聚集，因此 recharge_time 相邻的数据在表中并不相邻，而是分散在不同地方</p><p>执行下面的语句</p><pre><code>select * from charge_tablewhere recharge_time &lt;= unix_timestamp()and recharge_time &gt;= unix_timestamp() - 60 * 60order by recharge_time desclimit 30;</code></pre><p>根据 SQL 我们可以分析出这条 SQL 执行的过程如下：</p><ol><li><p>索引访问：</p><p>1 次随机访问找到索引上第一条符合条件的索引行</p><p>29 次顺序访问找到满足条件的剩余 29 个索引行</p></li><li><p>表访问：</p><p>30 次随机访问找到表上符合条件的表行</p></li><li><p>FETCH：</p><p>30 次 FETCH 获取 100 条记录</p></li></ol><p>本地响应时间 =</p><p>随机访问次数 _ 随机访问耗时 + 顺序访问次数 _ 顺序访问耗时 + FETCH 次数 * FETCH 耗时</p><p>= 31 _ 10ms + 29 _ 0.01 + 30 * 0.1 = 313.29ms</p><p><strong>优化方法：增加自增主键 id</strong></p><p><strong>优化原理：将对表的随机访问转为顺序访问</strong></p><p>索引访问：1 次随机访问 + 29 次顺序访问</p><p>表访问：1 次随机访问 + 29 次顺序访问</p><p>FETCH：30 次</p><p>本地响应时间 =</p><p>随机访问次数 _ 随机访问耗时 + 顺序访问次数 _ 顺序访问耗时 + FETCH 次数 * FETCH 耗时</p><p>= 2 _ 10ms + 58 _ 0.01 + 30 * 0.1 = 23.58ms</p><h2 id="5-Explain"><a href="#5-Explain" class="headerlink" title="5 Explain"></a>5 Explain</h2><p>在日常工作中，我们会有时会开慢查询去记录一些执行时间比较久的 SQL 语句，找出这些 SQL 语句并不意味着完事了，些时我们常常用到 explain 这个命令来查看一个这些 SQL 语句的执行计划，查看该 SQL 语句有没有使用上了索引，有没有做全表扫描，这都可以通过 explain 命令来查看。所以我们深入了解 MySQL 的基于开销的优化器，还可以获得很多可能被优化器考虑到的访问策略的细节，以及当运行 SQL 语句时哪种策略预计会被优化器采用。（QEP：sql 生成一个执行计划 query Execution plan）</p><pre><code>mysql&gt; explain select * from servers;</code></pre><table><thead><tr><th>id</th><th>select_type</th><th>table</th><th>type</th><th>possible_keys</th><th>key</th><th>key_len</th><th>ref</th><th>rows</th><th>Extra</th></tr></thead><tbody><tr><td>1</td><td>SIMPLE</td><td>servers</td><td>ALL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>1</td><td>NULL</td></tr></tbody></table><h3 id="5-1-Explain-参数解析"><a href="#5-1-Explain-参数解析" class="headerlink" title="5.1 Explain 参数解析"></a>5.1 Explain 参数解析</h3><h4 id="id"><a href="#id" class="headerlink" title="id"></a>id</h4><p>我的理解是 SQL 执行的顺序的标识,SQL 从大到小的执行</p><ol><li><p>id 相同时，执行顺序由上至下</p></li><li><p>如果是子查询，id 的序号会递增，id 值越大优先级越高，越先被执行</p></li><li><p>id 如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id 值越大，优先级越高，越先执行</p></li></ol><h4 id="select-type"><a href="#select-type" class="headerlink" title="select_type"></a>select_type</h4><p>示查询中每个 select 子句的类型</p><ol><li><p>SIMPLE(简单 SELECT,不使用 UNION 或子查询等)</p></li><li><p>PRIMARY(查询中若包含任何复杂的子部分,最外层的 select 被标记为 PRIMARY)</p></li><li><p>UNION(UNION 中的第二个或后面的 SELECT 语句)</p></li><li><p>DEPENDENT UNION(UNION 中的第二个或后面的 SELECT 语句，取决于外面的查询)</p></li><li><p>UNION RESULT(UNION 的结果)</p></li><li><p>SUBQUERY(子查询中的第一个 SELECT)</p></li><li><p>DEPENDENT SUBQUERY(子查询中的第一个 SELECT，取决于外面的查询)</p></li><li><p>DERIVED(派生表的 SELECT, FROM 子句的子查询)</p></li><li><p>UNCACHEABLE SUBQUERY(一个子查询的结果不能被缓存，必须重新评估外链接的第一行)</p></li></ol><h4 id="table"><a href="#table" class="headerlink" title="table"></a>table</h4><p>显示这一行的数据是关于哪张表的，有时不是真实的表名字,看到的是 derivedx(x 是个数字,我的理解是第几步执行的结果)<br>mysql&gt; explain select _ from (select _ from ( select * from t1 where id=2602) a) b;</p><table><thead><tr><th>id</th><th>select_type</th><th>table</th><th>type</th><th>possible_keys</th><th>key</th><th>key_len</th><th>ref</th><th>rows</th><th>Extra</th></tr></thead><tbody><tr><td>1</td><td>PRIMARY</td><td><derived2></derived2></td><td>system</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>1</td><td></td></tr><tr><td>2</td><td>DERIVED</td><td><derived3></derived3></td><td>system</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>1</td><td></td></tr><tr><td>3</td><td>DERIVED</td><td>t1</td><td>const</td><td>PRIMARY,idx_t1_id</td><td>PRIMARY</td><td>4</td><td></td><td>1</td><td></td></tr></tbody></table><h4 id="type"><a href="#type" class="headerlink" title="type"></a>type</h4><p>表示 MySQL 在表中找到所需行的方式，又称“访问类型”。</p><p>常用的类型有： ALL, index, range, ref, eq_ref, const, system, NULL（从左到右，性能从差到好）</p><p>ALL：Full Table Scan， MySQL 将遍历全表以找到匹配的行</p><p>index: Full Index Scan，index 与 ALL 区别为 index 类型只遍历索引树</p><p>range:只检索给定范围的行，使用一个索引来选择行</p><p>ref: 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值</p><p>eq_ref: 类似 ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用 primary key 或者 unique key 作为关联条件</p><p>const、system: 当 MySQL 对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。如将主键置于 where 列表中，MySQL 就能将该查询转换为一个常量,system 是 const 类型的特例，当查询的表只有一行的情况下，使用 system</p><p>NULL: MySQL 在优化过程中分解语句，执行时甚至不用访问表或索引，例如从一个索引列里选取最小值可以通过单独索引查找完成。</p><h4 id="possible-keys"><a href="#possible-keys" class="headerlink" title="possible_keys"></a>possible_keys</h4><p>指出 MySQL 能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用</p><p>该列完全独立于 EXPLAIN 输出所示的表的次序。这意味着在 possible_keys 中的某些键实际上不能按生成的表次序使用。<br>如果该列是 NULL，则没有相关的索引。在这种情况下，可以通过检查 WHERE 子句看是否它引用某些列或适合索引的列来提高你的查询性能。如果是这样，创造一个适当的索引并且再次用 EXPLAIN 检查查询</p><h4 id="Key"><a href="#Key" class="headerlink" title="Key"></a>Key</h4><p>key 列显示 MySQL 实际决定使用的键（索引）</p><p>如果没有选择索引，键是 NULL。要想强制 MySQL 使用或忽视 possible_keys 列中的索引，在查询中使用 FORCE INDEX、USE INDEX 或者 IGNORE INDEX。</p><h4 id="key-len"><a href="#key-len" class="headerlink" title="key_len"></a>key_len</h4><p>表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度（key_len 显示的值为索引字段的最大可能长度，并非实际使用长度，即 key_len 是根据表定义计算而得，不是通过表内检索出的）</p><p>不损失精确性的情况下，长度越短越好</p><h4 id="ref"><a href="#ref" class="headerlink" title="ref"></a>ref</h4><p>表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值</p><h4 id="rows"><a href="#rows" class="headerlink" title="rows"></a>rows</h4><p>表示 MySQL 根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数</p><h4 id="Extra"><a href="#Extra" class="headerlink" title="Extra"></a>Extra</h4><p>该列包含 MySQL 解决查询的详细信息,有以下几种情况：</p><p>Using where:列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表的全部的请求列都是同一个索引的部分的时候，表示 mysql 服务器将在存储引擎检索行后再进行过滤</p><p>Using temporary：表示 MySQL 需要使用临时表来存储结果集，常见于排序和分组查询</p><p>Using filesort：MySQL 中无法利用索引完成的排序操作称为“文件排序”</p><p>Using join buffer：改值强调了在获取连接条件时没有使用索引，并且需要连接缓冲区来存储中间结果。如果出现了这个值，那应该注意，根据查询的具体情况可能需要添加索引来改进能。</p><p>Impossible where：这个值强调了 where 语句会导致没有符合条件的行。</p><p>Select tables optimized away：这个值意味着仅通过使用索引，优化器可能仅从聚合函数结果中返回一行</p><p>总结：</p><ul><li>EXPLAIN 不会告诉你关于触发器、存储过程的信息或用户自定义函数对查询的影响情况</li><li>EXPLAIN 不考虑各种 Cache</li><li>EXPLAIN 不能显示 MySQL 在执行查询时所作的优化工作</li><li>部分统计信息是估算的，并非精确值</li><li>EXPALIN 只能解释 SELECT 操作，其他操作要重写为 SELECT 后查看执行计划。</li></ul><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://blog.csdn.net/alexshi5/article/details/81814772">https://blog.csdn.net/alexshi5/article/details/81814772</a></p><p><a href="http://blog.codinglabs.org/articles/theory-of-mysql-index.html">http://blog.codinglabs.org/articles/theory-of-mysql-index.html</a></p><p><a href="https://www.cnblogs.com/luyucheng/p/6289714.html">https://www.cnblogs.com/luyucheng/p/6289714.html</a></p><p><a href="https://blog.csdn.net/itguangit/article/details/82145322">https://blog.csdn.net/itguangit/article/details/82145322</a></p><p><a href="https://www.cnblogs.com/xuanzhi201111/p/4175635.html">https://www.cnblogs.com/xuanzhi201111/p/4175635.html</a></p>]]></content>
      
      
      <categories>
          
          <category> MySQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> MySQL </tag>
            
            <tag> Backend </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>内存管理、寻址方式那些事</title>
      <link href="/2018/11/08/memory-page-and-addressing/"/>
      <url>/2018/11/08/memory-page-and-addressing/</url>
      
        <content type="html"><![CDATA[<h2 id="一、内存"><a href="#一、内存" class="headerlink" title="一、内存"></a>一、内存</h2><h3 id="1-1-什么是内存"><a href="#1-1-什么是内存" class="headerlink" title="1.1 什么是内存"></a>1.1 什么是内存</h3><p>　　简单地说，内存就是一个数据货架。内存有一个最小的存储单位，大多数都是一个字节。内存用内存地址（memory address）来为每个字节的数据顺序编号。因此，内存地址说明了数据在内存中的位置。内存地址从0开始，每次增加1。这种线性增加的存储器地址称为线性地址（linear address）。</p><p>　　内存地址的编号有上限。地址空间的范围和地址总线（address bus）的位数直接相关。CPU通过地址总线来向内存说明想要存取数据的地址。以英特尔32位的80386型CPU为例，这款CPU有32个针脚可以传输地址信息。每个针脚对应了一位。如果针脚上是高电压，那么这一位是1。如果是低电压，那么这一位是0。32位的电压高低信息通过地址总线传到内存的32个针脚，内存就能把电压高低信息转换成32位的二进制数，从而知道CPU想要的是哪个位置的数据。用十六进制表示，32位地址空间就是从0x00000000 到0xFFFFFFFF，<strong>所以32位操作系统单个进程最大的内存使用空间一般不大于4G</strong>。</p><h3 id="1-2-什么虚拟内存地址"><a href="#1-2-什么虚拟内存地址" class="headerlink" title="1.2 什么虚拟内存地址"></a>1.2 什么虚拟内存地址</h3><p>　　内存的一项主要任务，就是存储进程的相关数据。我们之前已经看到过进程空间的程序段、全局数据、栈和堆，以及这些这些存储结构在进程运行中所起到的关键作用。有趣的是，尽管进程和内存的关系如此紧密，但进程并不能直接访问内存。在Linux下，进程不能直接读写内存中地址为0x1位置的数据。进程中能访问的地址，只能是<strong>虚拟内存地址（virtual memory address）</strong>。操作系统会把虚拟内存地址翻译成真实的内存地址。这种内存管理方式，称为虚拟内存（virtual memory）。</p><h2 id="二、内存分页"><a href="#二、内存分页" class="headerlink" title="二、内存分页"></a>二、内存分页</h2><h3 id="2-1-为什么要使用内存分页"><a href="#2-1-为什么要使用内存分页" class="headerlink" title="2.1 为什么要使用内存分页"></a>2.1 为什么要使用内存分页</h3><blockquote><p>既然386的CPU的地址总线是32位的，就可以寻址2^32一共4G的内存了，内存分页也只能寻址4G的空间，不能多于4G，为什么还要使用这个机制？</p></blockquote><blockquote><p>假设内存是连续分配的（也就是程序在物理内存上是连续的）</p></blockquote><blockquote><ol><li>进程A进来，向os申请了200的内存空间，于是os把<code>0~199</code>分配给A</li><li>进程B进来，向os申请了5的内存空间，os把<code>200~204</code>分配给它</li><li>进程C进来，向os申请了100的内存空间，os把<code>205~304</code>分配给它</li><li>这个时候进程B运行完了，把<code>200~204</code>还给os</li></ol><p>但是很长时间以后，只要系统中的出现的进程的大小&gt;5的话，<code>200~204</code>这段空间都不会被分配出去（只要A和C不退出）。过了一段更长的时间，内存中就会出现许许多多<code>200~204</code>这样不能被利用的碎片</p><p>而分页机制让程序可以在逻辑上连续、物理上离散。也就是说在一段连续的物理内存上，可能<code>0~4</code>（这个值取决于页面的大小）属于A，而<code>5~9</code>属于B，<code>10~14</code>属于C，从而保证任何一个“内存片段”都可以被分配出去。</p></blockquote><p>　　为了解决交换系统存在的缺陷，分页系统横空出世。分页系统的核心在于：将虚拟内存空间和物理内存空间皆划分为大小相同的页面，如4KB、8KB或16KB等，并以页面作为内存空间的最小分配单位，一个程序的一个页面可以存放在任意一个物理页面里。</p><p>　　（1）解决空间浪费碎片化问题</p><p>　　由于将虚拟内存空间和物理内存空间按照某种规定的大小进行分配，这里我们称之为页（Page），然后按照页进行内存分配，也就克服了外部碎片的问题。</p><p>　　（2）解决程序大小受限问题</p><p>　　程序增长有限是因为一个程序需要全部加载到内存才能运行，因此解决的办法就是使得一个程序无须全部加载就可以运行。使用分页也可以解决这个问题，只需将当前需要的页面放在内存里，其他暂时不用的页面放在磁盘上，这样一个程序同时占用内存和磁盘，其增长空间就大大增加了。而且，分页之后，如果一个程序需要更多的空间，给其分配一个新页即可（而无需将程序倒出倒进从而提高空间增长效率）。</p><h3 id="2-2-CPU-读取内存数据的过程"><a href="#2-2-CPU-读取内存数据的过程" class="headerlink" title="2.2 CPU 读取内存数据的过程"></a>2.2 CPU 读取内存数据的过程</h3><p>　　这个过程由内存管理单元（MMU）完成，MMU接收CPU发出的虚拟地址，将其翻译为物理地址后发送给内存。内存管理单元按照该物理地址进行相应访问后读出或写入相关数据，如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-5c9b10a191d4b044.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="readmemory.jpg"></p><h3 id="2-3-MMU如何把虚拟地址翻译成物理地址的"><a href="#2-3-MMU如何把虚拟地址翻译成物理地址的" class="headerlink" title="2.3 MMU如何把虚拟地址翻译成物理地址的"></a>2.3 MMU如何把虚拟地址翻译成物理地址的</h3><p>　　答案是查页表，对于每个程序，内存管理单元MMU都为其保存一个页表，该页表中存放的是虚拟页面到物理页面的映射。每当为一个虚拟页面寻找到一个物理页面之后，就在页表里增加一条记录来保留该映射关系。当然，随着虚拟页面进出物理内存，页表的内容也会不断更新变化。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4e5de5c292f2eeaf.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="yebiao.jpg"></p><p>　　页表的根本功能是提供从虚拟页面到物理页面的映射。因此，页表的记录条数与虚拟页面数相同。此外，内存管理单元依赖于页表来进行一切与页面有关的管理活动，这些活动包括判断某一页面号是否在内存里，页面是否受到保护，页面是否非法空间等等。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d09e397709228894.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="yebiaodes.jpg"></p><h3 id="2-4-为什么要多级页表"><a href="#2-4-为什么要多级页表" class="headerlink" title="2.4 为什么要多级页表"></a>2.4 为什么要多级页表</h3><p>　　在32的系统中，系统分配给每个进程的虚拟地址为4G，对于每个虚拟地址页建立一个记录，这样也需要4G/4k个，假设每条记录大小为4B，这样对于每个进程需要4M的页表，对于一个helloworld程序而言，不足4K的程序需要4M的页表，未免有些浪费。</p><p>　　这种单一的连续分页表，需要给每一个虚拟页预留一条记录的位置。但对于任何一个应用进程，其进程空间真正用到的地址都相当有限。我们还记得，进程空间会有栈和堆。进程空间为栈和堆的增长预留了地址，但栈和堆很少会占满进程空间。这意味着，如果使用连续分页表，很多条目都没有真正用到。因此，Linux中的分页表，采用了多层的数据结构。多层的分页表能够减少所需的空间。</p><p>　　多层分页表还有另一个优势。单层分页表必须存在于连续的内存空间。而多层分页表的二级表，可以散步于内存的不同位置。这样的话，操作系统就可以利用零碎空间来存储分页表。还需要注意的是，这里简化了多层分页表的很多细节。最新Linux系统中的分页表多达3层，管理的内存地址也比本章介绍的长很多。不过，多层分页表的基本原理都是相同。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8d7da10594609b8f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="duojiyebiao.png"></p><p>　　使用多级页表机制，对于第一级的页表如图所示只需4K的空间，用于索引二级页表的地址。还是以helloworld代码为例，可能只需要一个物理页，因此只需要一条记录，故对于第二级的页表也只需要一个页表，对于一级页表中的其他记录可以对应为空。这样只需要8k的空间就可以完成页面的映射。大大节省了页表所占的空间。</p><h3 id="2-5-分页系统的优缺点"><a href="#2-5-分页系统的优缺点" class="headerlink" title="2.5 分页系统的优缺点"></a>2.5 分页系统的优缺点</h3><p>　　优点：</p><p>　　（1）分页系统不会产生外部碎片，一个进程占用的内存空间可以不是连续的，并且一个进程的虚拟页面在不需要的时候可以放在磁盘中。</p><p>　　（2）分页系统可以共享小的地址，即页面共享。只需要在对应给定页面的页表项里做一个相关的记录即可。</p><p>　　缺点：页表很大，占用了大量的内存空间。</p><h3 id="2-6-缺页中断处理"><a href="#2-6-缺页中断处理" class="headerlink" title="2.6 缺页中断处理"></a>2.6 缺页中断处理</h3><p>　　在分页系统中，一个虚拟页面既有可能在物理内存，也有可能保存在磁盘上。如果CPU发出的虚拟地址对应的页面不在物理内存，就将产生一个缺页中断，而缺页中断服务程序负责将需要的虚拟页面找到并加载到内存。缺页中断的处理步骤如下，省略了中间很多的步骤，只保留最核心的几个步骤：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1461d6324e34273d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="queyezhongduan.jpg"></p><h3 id="2-7-页面置换算法"><a href="#2-7-页面置换算法" class="headerlink" title="2.7 页面置换算法"></a>2.7 页面置换算法</h3><p>　　常用的算法有：页面置换的目标、随机更换算法、先进先出算法、第二次机会算法、时钟算法、最优更换算法、NRU（最近未被使用）算法、 LRU（最近最少使用）算法、工作集算法、工作集时钟算法</p><h2 id="三、内存分页寻址模式"><a href="#三、内存分页寻址模式" class="headerlink" title="三、内存分页寻址模式"></a>三、内存分页寻址模式</h2><h3 id="3-1-四种寻址方式"><a href="#3-1-四种寻址方式" class="headerlink" title="3.1 四种寻址方式"></a>3.1 四种寻址方式</h3><ol><li><p>不分页模式(没有魔法，只支持4GB)</p></li><li><p>32位分页模式(有魔法，能超过4GB)</p></li><li><p>PAE分页模式(32位地址+64位的页表项，这个应该属于过渡阶段)</p></li><li><p>IA-32e分页模式(也就是目前的”64位模式”)</p></li></ol><p>后三种模式都必须启动分页管理，关闭分页就会回到第一种模式。</p><h3 id="3-2-开启分页管理的好处"><a href="#3-2-开启分页管理的好处" class="headerlink" title="3.2 开启分页管理的好处"></a>3.2 开启分页管理的好处</h3><ol><li>交换内存(虚拟内存)，x86 默认是 4KB 分页，操作系统可以将一些长久没有使用的内存页交换到外存上(一页只有4KB，写盘是非常快的)，然后这些页就可以分配到别的地方使用，这样可以使得”内存容量”大增，而调度得好的话在性能上影响不大。</li><li>延迟装载，现在一个游戏安装程序动辄上G，如果一次性装载完再执行，那等个几分钟才出安装界面也是正常的，实际上各种操作系统都用CPU的分页机制实现了延迟装载：装载程序时会在进程中建立内存页到可执行文件的映射，并将相应的页表项初始化为不可用状态，等执行到这个页的指令或访问这个页的数据时CPU就会触发缺页异常，操作系统到这个时候才根据映射把相应的指令或数据读到某页内存中并修改原来不可用的页表项指向它，然后重新执行导致异常的那条指令。所以即使是上G的程序也是瞬间装载的。</li><li>共享内存，我们可以复制页表来实现进程间内存的共享。比如fork子进程就很快，因为只是复制了页表，所以子进程一开始是直接使用父进程内存的。再比如动态链接库，它们的指令所占内存页是在各个进程中共享的，所以公共动态链接库的繁荣可以减少内存占用。</li><li>copy on write，除了上面提到的缺页异常可实现延迟装载，还有个页保护异常可实现 copy on write，每个页表项有记录该页是否可写的标志位，当某进程fork一个子进程的时候会将所有可写的内存页设置为不可写，当子进程修改数据时会触发页保护异常，操作系统会将该页复制一份让子进程修改，父进程的数据完全不受影响。</li></ol><h3 id="3-3-32位分页模式"><a href="#3-3-32位分页模式" class="headerlink" title="3.3  32位分页模式"></a>3.3  32位分页模式</h3><p>32位分页模式相较于不分页的情况，多了个线性地址-&gt;物理地址的转换，一般情况下这个转换分两级：</p><ol><li>CR3 寄存器高20位(31:12 bit)存着第一级页目录表(4KB大小)的物理地址的高20位(低12位全部为0，这样的物理地址是4KB对齐的)，<strong>页目录表可看成是个长度为1024的数组，每个元素的大小是4字节</strong>，它的低12位(11:0 bit)是一些标志位，比如第0位标志着这个页目录项是否可用，如果可用则高20位记录着下一级页表的物理地址，如果不可用那么线性地址如果走这个页目录项转换就会发生缺页异常。<br>第一级转换取线性地址的高10位(31:22 bit)作为数组下标定位页目录表中的项。</li><li>第二级页表也是4KB大小，也分为1024项，第0位也是可用与否的标志位，如果可用则高20位记录着一个内存页的物理地址，这里用线性地址的21:12 bit 来定位页表项，经过这两级映射最终得出了一个4KB内存页的起始地址，再加上线性地址的低12位页内偏移就得出了最终的物理地址。</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1bf63995886f1ef0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="4kb-32bit.png"></p><hr><p>如果完全按上面这种方式映射的话，那么转换后还是32位的地址，那么最多也只能用4GB内存了。但是第一级页目录项还可以直接映射一个4MB的内存页——当页目录项中第7位为1的时候(为0就还有一级页表要转换，也就是上面的情况)。</p><h3 id="3-4-32为操作系统如何突破4G内存限制"><a href="#3-4-32为操作系统如何突破4G内存限制" class="headerlink" title="3.4  32为操作系统如何突破4G内存限制"></a>3.4  32为操作系统如何突破4G内存限制</h3><p>这时页目录项的高10位(31:22 bit)作为物理地址的31:22 bit，低22位取线性地址的低22位作为页内偏移，然后相比于上面的映射，21:12 bit这里还有10个位空出来了，然后CPU就发扬了不用白不用的抠门精神，在其中8位(20:13 bit)多存储了物理地址的高8位(39:32 bit，实际上不同型号的CPU支持的位数可能不一样)，这样利用映射4MB页的机制我们能使用最大达到40bit的物理地址，也就是高达1TB的内存。下图演示了这种映射：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d08140cfdd3c3934.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="32-4g.png"></p><p>但是这种映射存在一些缺点：</p><ol><li>线性地址还是32位的，也就是一个进程最多使用4GB内存(32位系统一般设计为一个进程一个页目录表，进程切换时自动修改CR3寄存器)</li><li>只有4MB的内存页能使用超过4GB的内存，而4MB对于磁盘交换来说未免太大了点，所以一般只能把那种不需要做磁盘交换的内存做4MB映射，比如操作系统内核；当然也可以把这样的内存页分配给需要大块内存的程序。<br>而PAE分页模式解决了第2个缺点(因为页表项变成了8字节，最后一级页表项也能表示一个宽地址，最高可达52位，跟IA-32e一样)，但线性地址还是32位的，也就是单个进程还是最多使用4GB内存，但所有进程加起来理论上可以使用4PB内存(不过要1M个进程，太不现实)。</li></ol><p>由此可见32位系统用超过4GB内存也是完全可能的，ReadyFor4GB有可能是通过4MB内存页映射实现的(如果系统原来是32位分页模式的话)，也可能是原来就是PAE分页模式只是系统限制了只使用4GB以下内存，也可能是从32位分页模式大改造成PAE分页模式(但这个的难度要高很多，4字节页表项改成8字节页表项，能兼容原来的内核代码？)。</p><h3 id="3-5-IA-32e（64位模式）"><a href="#3-5-IA-32e（64位模式）" class="headerlink" title="3.5  IA-32e（64位模式）"></a>3.5  IA-32e（64位模式）</h3><p>32位线性地址限制了进程的地址空间，即使有超过4GB的内存，在一个进程中也用不了。那么就扩大地址宽度吧，之前我们从16位扩到20位(8088到8086)，然后从20位扩到32位(80386)，那就扩到48位吧(想想当年把1MB内存当成宝贝，现在一个内碎片都可能超过1MB了)。现在可以放这张全家福了：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-dd2fdb296e4c6840.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="mem.png"></p><p>IA-32e模式中线性地址是48位(256TB)，物理地址是52位(4PB，最高值，各CPU实际支持的最大宽度普遍低于这个值，后面部分有检测方法)。</p><p>48位是6字节，但实际上64位程序中的地址(指针)的大小是8字节，文档(v1:3.3.7.1 Canonical Addressing)规定64位线性地址必须遵循一个规则：如果第47位是0，那么63:48bit全部是0；否则全部是1。其实就是扩展了符号位的意思，如果分页转换前的线性地址不符合这个规则就会触发CPU异常，这个应该是督促系统程序员兼容以后的真64位CPU。于是只有以下地址才是合法的地址：</p><pre><code>0x00000000 00000000 ～ 0x00007FFF FFFFFFFF0xFFFF8000 00000000 ～ 0xFFFFFFFF FFFFFFFF</code></pre><p>而CPU做线性地址到物理地址转换的时候只取其低48位。这个地址转换有4级，每级还是一个4KB的页表，其中每项8字节，那么就只有512个项，每级用线性地址中的9位做下标索引，4*9=36，刚好剩下12位填充4KB内存页的页内偏移：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4a4804f4dbe9c796.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="64bit.png"></p><p>让我们来算算各级页表项管理的地址空间的大小：</p><p>PTE 4KB</p><p>PDE 4KB * 512 = 2MB</p><p>PDPTE 2MB * 512 = 1GB</p><p>PML4E 1GB * 512 = 512GB</p><p>所以我们看出来了，不直接上64位貌似还挺有道理的，48位就已经很浪费了！第一级页表中一项竟然就映射了512GB的地址空间，我看了下Alienware Area-51才32GB的内存，所以以目前的内存大小来看，砍掉一级变成3级映射也是完全够用的。</p><p>在 IA-32e 模式中为了提高地址转换速度，同时为了减少页表数量(确实很有必要，内存越大，页表越多)，我们可以在 PDE 中直接映射2MB的内存页(普遍支持)，甚至可以在 PDPTE 中直接映射 1GB 的内存页(部分CPU支持)。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.douban.com/note/658219783/">https://www.douban.com/note/658219783/</a></p><p><a href="https://www.cnblogs.com/edisonchou/p/5094066.html">https://www.cnblogs.com/edisonchou/p/5094066.html</a></p><p><a href="https://blog.csdn.net/gaoxiang__/article/details/41578339">https://blog.csdn.net/gaoxiang/article/details/41578339</a></p><p><a href="https://www.cnblogs.com/vamei/p/9329278.html">https://www.cnblogs.com/vamei/p/9329278.html</a></p>]]></content>
      
      
      <categories>
          
          <category> System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Memory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>B-Tree、B+Tree、B*Tree</title>
      <link href="/2018/11/06/b-tree/"/>
      <url>/2018/11/06/b-tree/</url>
      
        <content type="html"><![CDATA[<h2 id="一、B-Tree"><a href="#一、B-Tree" class="headerlink" title="一、B-Tree"></a>一、B-Tree</h2><h3 id="1-1-什么是B-Tree"><a href="#1-1-什么是B-Tree" class="headerlink" title="1.1 什么是B-Tree"></a>1.1 什么是B-Tree</h3><p> 1970年，R.Bayer和E.mccreight提出了一种适用于外查找的树，它是一种平衡的多叉树，称为B树，其定义如下</p><ul><li><p>根结点至少有两个子女。</p></li><li><p>每个中间节点都包含k-1个元素和k个孩子，其中 m/2 &lt;= k &lt;= m</p></li><li><p>每一个叶子节点都包含k-1个元素，其中 m/2 &lt;= k &lt;= m</p></li><li><p>所有的叶子结点都位于同一层。</p></li><li><p>每个节点中的元素从小到大排列，节点当中k-1个元素正好是k个孩子包含的元素的值域分划。</p></li></ul><p><strong>M = 3</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-cc7d4648619bcd33.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="b-tree.jpg"></p><h3 id="1-2-B-Tree-查找"><a href="#1-2-B-Tree-查找" class="headerlink" title="1.2 B-Tree 查找"></a>1.2 B-Tree 查找</h3><p>假设我们要查找的数据是 5</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-35e358d588334f46.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="b-tree-search.png"></p><h2 id="二、B-Tree"><a href="#二、B-Tree" class="headerlink" title="二、B+Tree"></a>二、B+Tree</h2><h3 id="2-1-什么是B-Tree"><a href="#2-1-什么是B-Tree" class="headerlink" title="2.1 什么是B+Tree"></a>2.1 什么是B+Tree</h3><p>B+ 树是一种树数据结构，是一个n叉树，每个节点通常有多个孩子，一棵B+树包含根节点、内部节点和叶子节点。根节点可能是一个叶子节点，也可能是一个包含两个或两个以上孩子节点的节点。</p><p>一个m阶的B+树具有如下几个特征：</p><ul><li><p>有k个子树的中间节点包含有k个元素（B树中是k-1个元素），每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。</p></li><li><p>所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。</p></li><li><p>所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素。</p></li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fa7fb59a9bc03a42.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="b+tree-data.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-87539ddc4fd0e56d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="5.jpg"></p><h3 id="2-2-B-Tree特点"><a href="#2-2-B-Tree特点" class="headerlink" title="2.2 B+Tree特点"></a>2.2 B+Tree特点</h3><p>B+的特性：</p><ul><li><p>所有关键字都出现在叶子结点的链表中（稠密索引），且链表中的关键字是有序的；</p></li><li><p>不可能在非叶子结点命中；</p></li><li><p><strong>非叶子结点相当于是叶子结点的索引（稀疏索引），叶子结点相当于是存储（关键字）数据的数据层</strong>；</p></li><li><p>更适合文件索引系统；</p></li></ul><p>B-树中的卫星数据（Satellite Information）：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-63ad6daab0d8db1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="b-tree-data.png"></p><p>B+树中的卫星数据（Satellite Information）：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fa7fb59a9bc03a42.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="b+tree-data.png"></p><p>数据量相同的情况下，B+树的结构比B-树更加“矮胖”，因此查询时候IO次数也更少。</p><h3 id="2-3-B-Tree的优势"><a href="#2-3-B-Tree的优势" class="headerlink" title="2.3 B+Tree的优势"></a>2.3 B+Tree的优势</h3><ul><li><p>单一节点存储更多的元素，使得查询的IO次数更少，由于B+树在内部节点上不包含数据信息，因此在内存页中能够存放更多的key。 数据存放的更加紧密，具有更好的空间局部性。因此访问叶子节点上关联的数据也具有更好的缓存命中率。</p></li><li><p>所有查询都要查找到叶子节点，查询性能稳定。</p></li><li><p>所有叶子节点形成有序链表，便于范围查询。B+树的叶子结点都是相链的，因此对整棵树的便利只需要一次线性遍历叶子结点即可。而且由于数据顺序排列并且相连，所以便于区间查找和搜索。而B树则需要进行每一层的递归遍历。相邻的元素可能在内存中不相邻，所以缓存命中性没有B+树好。</p></li></ul><h2 id="三、B-Tree"><a href="#三、B-Tree" class="headerlink" title="三、B*Tree"></a>三、B*Tree</h2><h3 id="3-1-什么是B-Tree"><a href="#3-1-什么是B-Tree" class="headerlink" title="3.1 什么是B*Tree"></a>3.1 什么是B*Tree</h3><p>B*Tree是B+Tree的变体，在B+Tree的非根和非叶子结点再增加指向兄弟的指针；</p><p>B*Tree定义了非叶子结点关键字个数至少为<code>(2/3) * M</code>，即块的最低使用率为2/3</p><h3 id="3-2-B-Tree和B-Tree区别"><a href="#3-2-B-Tree和B-Tree区别" class="headerlink" title="3.2 B+Tree和B*Tree区别"></a>3.2 B+Tree和B*Tree区别</h3><ul><li>B+树的分裂：当一个结点满时，分配一个新的结点，并将原结点中1/2的数据复制到新结点，最后在父结点中增加新结点的指针；B+树的分裂只影响原结点和父结点，而不会影响兄弟结点，所以它不需要指向兄弟的指针；</li><li>B*树的分裂：当一个结点满时，如果它的下一个兄弟结点未满，那么将一部分数据移到兄弟结点中，再在原结点插入关键字，最后修改父结点中兄弟结点的关键字（因为兄弟结点的关键字范围改变了）；如果兄弟也满了，则在原结点与兄弟结点之间增加新结点，并各复制1/3的数据到新结点，最后在父结点增加新结点的指针；</li></ul><p>所以，B*树分配新结点的概率比B+树要低，空间使用率更高；</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-cef5a84039a675a6.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="6.jpg"></p><h2 id="四、小结"><a href="#四、小结" class="headerlink" title="四、小结"></a>四、小结</h2><ul><li>B-树：多路搜索树，每个结点存储M/2到M个关键字，非叶子结点存储指向关键字范围的子结点，所有关键字在整颗树中出现，且只出现一次，非叶子结点可以命中；</li><li>B+树：在B-树基础上，为叶子结点增加链表指针，所有关键字都在叶子结点中出现，非叶子结点作为叶子结点的索引；B+树总是到叶子结点才命中；</li><li>B*树：在B+树基础上，为非叶子结点也增加链表指针，将结点的最低利用率从1/2提高到2/3；</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.sohu.com/a/156886901_479559">https://www.sohu.com/a/156886901_479559</a></p><p><a href="https://www.cnblogs.com/vincently/p/4526560.html">https://www.cnblogs.com/vincently/p/4526560.html</a></p><p><a href="https://blog.csdn.net/andyzhaojianhui/article/details/76988560">https://blog.csdn.net/andyzhaojianhui/article/details/76988560</a></p>]]></content>
      
      
      <categories>
          
          <category> Arithmetic </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataStructure </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MyISAM和InnoDB区别和应用场景</title>
      <link href="/2018/10/26/mysql-engine/"/>
      <url>/2018/10/26/mysql-engine/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是MyISAM-和InnoDB"><a href="#什么是MyISAM-和InnoDB" class="headerlink" title="什么是MyISAM 和InnoDB"></a>什么是MyISAM 和InnoDB</h3><blockquote><p>MyISAM是MySQL的默认数据库引擎（5.5版之前），由早期的ISAM所改良。虽然性能极佳，但却有一个缺点：不支持事务处理（transaction）。</p><p>InnoDB，是MySQL的数据库引擎之一，为MySQL AB发行binary的标准之一。InnoDB由Innobase Oy公司所开发，2006年五月时由甲骨文公司并购。与传统的ISAM与MyISAM相比，InnoDB的最大特色就是支持了ACID兼容的事务（Transaction）功能，类似于PostgreSQL。</p></blockquote><p>MyISAM：它是基于传统的ISAM类型，ISAM是Indexed Sequential Access Method (有索引的顺序访问方法) 的缩写，它是存储记录和文件的标准方法。不是事务安全的，而且不支持外键，如果执行大量的select，insert MyISAM比较适合。</p><p>InnoDB：支持事务安全的引擎，支持外键、行锁、事务是他的最大特点。如果有大量的update和insert，建议使用InnoDB，特别是针对多个并发和QPS较高的情况。</p><h3 id="MyISAM与InnoDB的主要区别"><a href="#MyISAM与InnoDB的主要区别" class="headerlink" title="MyISAM与InnoDB的主要区别:"></a>MyISAM与InnoDB的主要区别:</h3><ol><li><p>存储结构</p><p> MyISAM：每个MyISAM在磁盘上存储成三个文件。1）.frm 用于存储表的定义。2）.MYD 用于存放数据。 3）.MYI 用于存放表索引</p><p> InnoDB：所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。</p></li><li><p>存储空间</p><p> MyISAM：可被压缩，存储空间较小。支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能有空格，会被去掉)、动态表、压缩表。</p><p> InnoDB：需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。</p></li><li><p>可移植性、备份及恢复</p><p> MyISAM：数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作。</p><p> InnoDB：免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了。</p></li><li><p>事务支持</p><p> MyISAM：强调的是性能，每次查询具有原子性,其执行数度比InnoDB类型更快，但是不提供事务支持。<br> InnoDB：提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。</p></li><li><p>AUTO_INCREMENT</p><p> MyISAM：可以和其他字段一起建立联合索引。引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，他可以根据前面几列进行排序后递增。<br> InnoDB：InnoDB中必须包含只有该字段的索引。引擎的自动增长列必须是索引，如果是组合索引也必须是组合索引的第一列。</p></li><li><p>表锁差异</p><p> MyISAM：只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。</p><p> InnoDB：</p><ol><li>支持事务和行级锁，是innodb的最大特色。行锁大幅度提高了多用户并发操作的新能。但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。</li><li>事务的ACID属性：atomicity,consistent,isolation,durable。</li><li>并发事务带来的几个问题：更新丢失，脏读，不可重复读，幻读。</li><li>事务隔离级别：未提交读(Read uncommitted)，已提交读(Read committed)，可重复读(Repeatable read)，可序列化(Serializable)</li></ol><table><thead><tr><th align="left">读数据一致性及并发副作用</th><th align="center">读数据一致性</th><th align="right">脏读</th><th align="right">不可重复读</th><th align="right">幻读</th></tr></thead><tbody><tr><td align="left">为提交读(read uncommitted)</td><td align="center">最低级别，不读物理上顺坏的数据</td><td align="right">是</td><td align="right">是</td><td align="right">是</td></tr><tr><td align="left">已提交读(read committed)</td><td align="center">语句级</td><td align="right">否</td><td align="right">是</td><td align="right">是</td></tr><tr><td align="left">可重复读(Repeatable red)</td><td align="center">事务级</td><td align="right">否</td><td align="right">否</td><td align="right">是</td></tr><tr><td align="left">可序列化(Serializable)</td><td align="center">最高级别，事务级</td><td align="right">否</td><td align="right">否</td><td align="right">否</td></tr></tbody></table></li></ol><ol start="7"><li><p>全文索引</p><p> MyISAM：支持 FULLTEXT类型的全文索引<br> InnoDB：不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。</p></li><li><p>表主键</p><p> MyISAM：允许没有任何索引和主键的表存在，索引都是保存行的地址。<br> InnoDB：如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值。</p></li><li><p>表的具体行数</p><p> MyISAM：保存有表的总行数，如果select count( * ) from table;会直接取出出该值。<br> InnoDB：没有保存表的总行数，如果使用select count( * ) from table；就会遍历整个表，消耗相当大，但是在加了wehre条件后，myisam和innodb处理的方式都一样。</p></li><li><p>CURD操作</p><p>MyISAM：如果执行大量的SELECT，MyISAM是更好的选择。<br>InnoDB：如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表。DELETE 从性能上InnoDB更优，但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除，在innodb上如果要清空保存有大量数据的表，最好使用truncate table这个命令。</p></li><li><p>外键</p><p>MyISAM：不支持<br>InnoDB：支持<br>通过上述的分析，基本上可以考虑使用InnoDB来替代MyISAM引擎了，原因是InnoDB自身很多良好的特点，比如事务支持、存储 过程、视图、行级锁定等等，在并发很多的情况下，相信InnoDB的表现肯定要比MyISAM强很多。另外，任何一种表都不是万能的，只用恰当的针对业务类型来选择合适的表类型，才能最大的发挥MySQL的性能优势。如果不是很复杂的Web应用，非关键应用，还是可以继续考虑MyISAM的，这个具体情况可以自己斟酌。</p></li></ol><h3 id="应用场景："><a href="#应用场景：" class="headerlink" title="应用场景："></a>应用场景：</h3><ol><li>MyISAM管理非事务表。它提供高速存储和检索，以及全文搜索能力。如果应用中需要执行大量的SELECT查询，那么MyISAM是更好的选择。</li><li>InnoDB用于事务处理应用程序，具有众多特性，包括ACID事务支持。如果应用中需要执行大量的INSERT或UPDATE操作，则应该使用InnoDB，这样可以提高多用户并发操作的性能。</li></ol><h3 id="开发的注意事项"><a href="#开发的注意事项" class="headerlink" title="开发的注意事项"></a>开发的注意事项</h3><ol><li><p>可以用 show create table tablename 命令看表的引擎类型。</p></li><li><p>对不支持事务的表做start/commit操作没有任何效果，在执行commit前已经提交。</p></li><li><p>可以执行以下命令来切换非事务表到事务（数据不会丢失），innodb表比myisam表更安全：alter table tablename type=innodb;或者使用 alter table tablename engine = innodb;</p></li><li><p>默认innodb是开启自动提交的，如果你按照myisam的使用方法来编写代码页不会存在错误，只是性能会很低。如何在编写代码时候提高数据库性能呢？</p></li><li><p>尽量将多个语句绑到一个事务中，进行提交，避免多次提交导致的数据库开销。</p></li><li><p>在一个事务获得排他锁或者意向排他锁以后，如果后面还有需要处理的sql语句，在这两条或者多条sql语句之间程序应尽量少的进行逻辑运算和处理，减少锁的时间。</p></li><li><p>尽量避免死锁</p></li><li><p>sql语句如果有where子句一定要使用索引，尽量避免获取意向排他锁。</p></li><li><p>针对我们自己的数据库环境，日志系统是直插入，不修改的，所以我们使用混合引擎方式，ZION_LOG_DB照旧使用myisam存储引擎，只有ZION_GAME_DB，ZION_LOGIN_DB，DAUM_BILLING使用Innodb引擎。</p></li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>在大数据量，高并发量的互联网业务场景下，对于MyISAM和InnoDB</p><p>有where条件，count(*)两个存储引擎性能差不多<br>不要使用全文索引，应当使用《索引外置》的设计方案<br>事务影响性能，强一致性要求才使用事务<br>不用外键，由应用程序来保证完整性<br>不命中索引，InnoDB也不能用行锁</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>在大数据量，高并发量的互联网业务场景下，请使用InnoDB:</p><p>行锁，对提高并发帮助很大<br>事务，对数据一致性帮助很大<br>这两个点，是InnoDB最吸引人的地方。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://blog.csdn.net/wjtlht928/article/details/46641865">https://blog.csdn.net/wjtlht928/article/details/46641865</a></p><p><a href="https://blog.csdn.net/perfectsorrow/article/details/80150672">https://blog.csdn.net/perfectsorrow/article/details/80150672</a></p><p><a href="http://www.hao124.net/article/111">http://www.hao124.net/article/111</a></p><p><a href="https://blog.csdn.net/aaa123524457/article/details/54375341">https://blog.csdn.net/aaa123524457/article/details/54375341</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Golang 内存对齐问题</title>
      <link href="/2018/10/16/data-struct-alignment/"/>
      <url>/2018/10/16/data-struct-alignment/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是内存对齐？"><a href="#什么是内存对齐？" class="headerlink" title="什么是内存对齐？"></a>什么是内存对齐？</h3><p>CPU把内存当成是一块一块的，块的大小可以是2，4，8，16字节大小，因此CPU在读取内存时是一块一块进行读取的。块大小成为memory access granularity（粒度）。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-31a8410aa48ca94f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="sda.png"></p><p>假设CPU访问粒度是4，也就是一次性可以读取内存中的四个字节内容；当我们不采用内存对齐策略，如果需要访问A中的b元素，CPU需要先取出0-3四个字节的内容，发现没有读取完，还需要再次读取，一共需要进行两次访问内存的操作；而有了内存对齐，参考左图，可一次性取出4-7四个字节的元素也即是b，这样就只需要进行一次访问内存的操作。所以操作系统这样做的原因也就是所谓的拿空间换时间，提高效率。</p><h3 id="为什么要内存对齐？"><a href="#为什么要内存对齐？" class="headerlink" title="为什么要内存对齐？"></a>为什么要内存对齐？</h3><p>会了关于结构体内存大小的计算，可是为什么系统要对于结构体数据进行内存对齐呢，很明显所占用的空间大小要更多。原因可归纳如下：</p><ol><li>平台原因(移植原因)：不是所有的硬件平台都能访问任意地址上的任意数据的；某些硬件平台只能在某些地址处取某些特定类型的数据，否则抛出硬件异常。</li><li>性能原因：数据结构(尤其是栈)应该尽可能地在自然边界上对齐。原因在于，为了访问未对齐的内存，处理器需要作两次内存访问；而对齐的内存访问仅需要一次访问。</li></ol><h3 id="Golang-字节对齐"><a href="#Golang-字节对齐" class="headerlink" title="Golang 字节对齐"></a>Golang 字节对齐</h3><p>最近在做一个需求的时候，有个场景，需要一个线程定时去更新一个全局变量指针地址，然后在另外的线程可以读取这个变量的数据，同事在帮忙Review代码的时候，问这个多线程操作这个全局指针变量时候需不需要加锁，因为在C/C++中有内存对齐问题，如果指针是内存对齐的，是可以不加锁的(<strong>但是这里其实是有线程可见性的问题</strong>)。所以下面测试下golang的内存是否会做自动对齐的操作。</p><p>测试一</p><pre><code>    //输出长度为1   fmt.Printf("%d",unsafe.Sizeof(struct {       i8  int8   }{}))   </code></pre><p>测试二</p><pre><code>    //输出长度为16   fmt.Printf("%d",unsafe.Sizeof(struct {       i8  int8       p   *int8   }{}))</code></pre><p>在测试二中可以看出， 在后面申明一个指针以后，内存空间自动扩容为16了，说明编译自动帮我们做了内存对齐。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p><a href="http://www.cppblog.com/snailcong/archive/2009/03/16/76705.html">http://www.cppblog.com/snailcong/archive/2009/03/16/76705.html</a></p><p><a href="https://www.zhihu.com/question/27862634">https://www.zhihu.com/question/27862634</a></p><p><a href="https://blog.csdn.net/sssssuuuuu666/article/details/75175108">https://blog.csdn.net/sssssuuuuu666/article/details/75175108</a></p><p><a href="https://my.oschina.net/u/2950272/blog/1829197">https://my.oschina.net/u/2950272/blog/1829197</a></p><p><a href="https://www.jianshu.com/p/cb40c746bf9e">https://www.jianshu.com/p/cb40c746bf9e</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Golang </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>服务发现之Consul</title>
      <link href="/2018/10/12/service-discovery-consul/"/>
      <url>/2018/10/12/service-discovery-consul/</url>
      
        <content type="html"><![CDATA[<p>consul是一个可以提供服务发现，健康检查，多数据中心，Key/Value存储等功能的分布式服务框架</p><p>用于实现分布式系统的服务发现与配置。与其他分布式服务注册与发现的方案，Consul的方案更”一站式”，内置了服务注册与发现框架、分布一致性协议实现、健康检查、Key/Value存储、多数据中心方案，不再需要依赖其他工具（比如ZooKeeper等）。使用起来也较为简单。Consul用Golang实现，因此具有天然可移植性(支持Linux、Windows和Mac OS X)；安装包仅包含一个可执行文件，方便部署，与Docker等轻量级容器可无缝配合。</p><h2 id="Consul-的使用场景"><a href="#Consul-的使用场景" class="headerlink" title="Consul 的使用场景"></a>Consul 的使用场景</h2><ul><li>docker 实例的注册与配置共享</li><li>coreos 实例的注册与配置共享</li><li>vitess 集群</li><li>SaaS 应用的配置共享</li><li>与 confd 服务集成，动态生成 nginx 和 haproxy 配置文件</li></ul><h2 id="Consul-的优势"><a href="#Consul-的优势" class="headerlink" title="Consul 的优势"></a>Consul 的优势</h2><ul><li>使用 Raft 算法来保证一致性, 比复杂的 Paxos 算法更直接. 相比较而言, zookeeper 采用的是 Paxos, 而 etcd 使用的则是 Raft.</li><li>支持多数据中心，内外网的服务采用不同的端口进行监听。 多数据中心集群可以避免单数据中心的单点故障,而其部署则需要考虑网络延迟, 分片等情况等. zookeeper 和 etcd 均不提供多数据中心功能的支持.</li><li>支持健康检查. etcd 不提供此功能.</li><li>支持 http 和 dns 协议接口. zookeeper 的集成较为复杂, etcd 只支持 http 协议.</li><li>官方提供web管理界面, etcd 无此功能.</li></ul><p>综合比较, Consul 作为服务注册和配置管理的新星, 比较值得关注和研究.</p><h2 id="Consul-的角色"><a href="#Consul-的角色" class="headerlink" title="Consul 的角色"></a>Consul 的角色</h2><ul><li>client: 客户端, 无状态, 将 HTTP 和 DNS 接口请求转发给局域网内的服务端集群. </li><li>server: 服务端, 保存配置信息, 高可用集群, 在局域网内与本地客户端通讯, 通过广域网与其他数据中心通讯. 每个数据中心的 server 数量推荐为 3 个或是 5 个.</li></ul><h2 id="Consul-基础组件"><a href="#Consul-基础组件" class="headerlink" title="Consul 基础组件"></a>Consul 基础组件</h2><ul><li><p>Agent: 在consul集群上每个节点运行的后台进程，在服务端模式和客户端模式都需要运行该进程。</p></li><li><p>client: 客户端是无状态的，负责把RPC请求转发给服务端， 占用资源和带宽比较少</p></li><li><p>server: 维持集群状态， 相应rpc请求， 选举算法</p></li><li><p>Datacenter：数据中心，支持多个数据中心</p></li><li><p>Consensus：一致性协议</p></li><li><p>Gossip protocol： consul是基于Serf, Serf为成员规则， 失败检测， 节点通信提供了一套协议，</p></li><li><p>LAN Gossip： 在同一个局域网或者数据中心中所有的节点</p><p>  Refers to the LAN gossip pool which contains nodes that are all located on the same local area network or datacenter.</p></li><li><p>Server和Client。客户端不存储配置数据，官方建议每个Consul Cluster至少有3个或5个运行在Server模式的Agent，Client节点不限，如下图</p></li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-eb86407e43a42dbf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="consul-framework.png"></p><ol><li><p>支持多个数据中心， 上图有两个数据中心</p></li><li><p>每个数据中心一般有3-5个服务器，服务器数目要在可用性和性能上进行平衡，客户端数量没有限制。分布在不同的物理机上</p></li><li><p>一个集群中的所有节点都添加到 gossip protocol（通过这个协议进行成员管理和信息广播）中， a客户端不用知道服务地址， b节点失败检测是分布式的， 不用只在服务端完成；c</p></li><li><p>数据中心的所有服务端节点组成一个raft集合， 他们会选举出一个leader，leader服务所有的请求和事务， 如果非leader收到请求， 会转发给leader. leader通过一致性协议（consensus protocol），把所有的改变同步(复制)给非leader.</p></li><li><p>所有数据中心的服务器组成了一个WAN gossip pool，他存在目的就是使数据中心可以相互交流，增加一个数据中心就是加入一个WAN gossip pool，</p></li><li><p>当一个服务端节点收到其他数据中心的请求， 会转发给对应数据中心的服务端。</p></li></ol><h2 id="保持一致性-Raft协议"><a href="#保持一致性-Raft协议" class="headerlink" title="保持一致性-Raft协议"></a>保持一致性-Raft协议</h2><p><a href="http://johng.cn/cluster-algorithm-raft/">分布式系统的Raft算法</a></p><p><a href="http://thesecretlivesofdata.com/raft/">英文动画演示Raft</a></p><h2 id="Consul-Agent-容错"><a href="#Consul-Agent-容错" class="headerlink" title="Consul Agent 容错"></a>Consul Agent 容错</h2><blockquote><p>引用内网WIKI</p><ol><li><p>前提：agent 向 server 发 RPC 请求数据，以目前一个机房 5 台 servers 的部署架构，宕 2 台 servers 对集群无影响。agent 默认的 stale 模式会向任一 server 请求数据，这样只要集群中仍存在 server，而不论其角色是否为 leader 时都能工作。</p></li><li><p>重试：RPC 调用失败的话，换另一台 server 再重试一次。</p></li><li><p>快照：将缓存数据（无论其是否过期）定期 dump 到本地存储，当 servers 全部宕掉时 agent 可以从本地 snapshot 取数据，以保证尚能返回部分热点数据。</p></li><li><p>封禁：agent 给 RPC 调用失败的 servers 扣分，根据不同的分值决定将 servers 封禁一定时长。</p></li><li><p>兜底：agent 宕掉的话，客户端尝试直连同集群的其它 agent 返回数据，仍然失败的话，尝试从本地存储中的 snapshot 中取数据。</p></li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-caf7eba2f5214d2f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="consul-agent-failover.png"></p></blockquote><h2 id="consul灾备方案"><a href="#consul灾备方案" class="headerlink" title="consul灾备方案"></a>consul灾备方案</h2><blockquote><p>引用内网WIKI<br>定时从consul读取要调用服务的host+port到db或者redis。如果从consul读取有问题并且重试后还有问题，说明consul出现了问题lark通知被调用服务负责人暂时先不要重启该服务以保证存储的服务host:port是最新的。</p><p>然后就是怎么从db或者redis在超低延迟以及高并发情况下获取服务的host:port了</p><p>此方案瓶颈就是怎么保证低延迟高并发从存储中读取host:port的配置，想了下加cache或者从redis读应该能满足</p></blockquote><h2 id="几种服务发现工具"><a href="#几种服务发现工具" class="headerlink" title="几种服务发现工具"></a>几种服务发现工具</h2><p>zookeeper：<a href="https://zookeeper.apache.org/">https://zookeeper.apache.org/</a></p><p>etcd：<a href="https://coreos.com/etcd/">https://coreos.com/etcd/</a></p><p>consul：<a href="https://www.consul.io/">https://www.consul.io/</a></p><p><a href="http://dockone.io/article/667">服务发现：Zookeeper vs etcd vs Consul</a></p><p><a href="https://www.jianshu.com/p/6160d414dd5e">服务发现之 Etcd VS Consul</a></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="http://www.liangxiansen.cn/2017/04/06/consul/">http://www.liangxiansen.cn/2017/04/06/consul/</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
            <tag> Consul </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式id几种生成方案</title>
      <link href="/2018/09/30/idgenerator/"/>
      <url>/2018/09/30/idgenerator/</url>
      
        <content type="html"><![CDATA[<h2 id="一、UUID"><a href="#一、UUID" class="headerlink" title="一、UUID"></a>一、UUID</h2><p>UUID 是 通用唯一识别码（Universally Unique Identifier）的缩写，是一种软件建构的标准，亦为开放软件基金会组织在分布式计算环境领域的一部分。其目的，是让分布式系统中的所有元素，都能有唯一的辨识信息，而不需要通过中央控制端来做辨识信息的指定。如此一来，每个人都可以创建不与其它人冲突的UUID。在这样的情况下，就不需考虑数据库创建时的名称重复问题。目前最广泛应用的UUID，是微软公司的全局唯一标识符（GUID），而其他重要的应用，则有Linux ext2/ext3文件系统、LUKS加密分区、GNOME、KDE、Mac OS X等等。另外我们也可以在e2fsprogs包中的UUID库找到实现。</p><p>UUID的标准形式包含32个16进制数字，以连字号分为五段，形式为8-4-4-4-12的32个字符，如：550e8400-e19b-41d4-a716-446655440000。</p><ul><li><p>Version 1：基于时间的UUID基于时间的UUID通过计算当前时间戳、随机数和机器MAC地址得到。由于在算法中使用了MAC地址，这个版本的UUID可以保证在全球范围的唯一性。但与此同时，使用MAC地址会带来安全性问题，这就是这个版本UUID受到批评的地方。如果应用只是在局域网中使用，也可以使用退化的算法，以IP地址来代替MAC地址－－Java的UUID往往是这样实现的（当然也考虑了获取MAC的难度）。</p></li><li><p>Version 2：DCE安全的UUIDDCE（Distributed Computing Environment）安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到。</p></li><li><p>Version 3：基于名字的UUID（MD5）基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的。</p></li><li><p>Version 4：随机UUID根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的，但随机的东西就像是买彩票：你指望它发财是不可能的，但狗屎运通常会在不经意中到来。</p></li><li><p>Version 5：基于名字的UUID（SHA1）和版本3的UUID算法类似，只是散列值计算使用SHA1（Secure<br>Hash Algorithm 1）算法。</p></li></ul><hr><p>UUID的优点:</p><p>通过本地生成，没有经过网络I/O，性能较快</p><p>无序，无法预测他的生成顺序。(当然这个也是他的缺点之一)</p><p> UUID的缺点:</p><p>128位二进制一般转换成36位的16进制，太长了只能用String存储，空间占用较多。</p><p>不能生成递增有序的数字</p><h2 id="二、数据库主键自增"><a href="#二、数据库主键自增" class="headerlink" title="二、数据库主键自增"></a>二、数据库主键自增</h2><p>优点：</p><ol><li>自增，趋势自增，作为聚集索引，提升查询效率。</li><li>节省磁盘空间。500W数据，UUID占5.4G,自增ID占2.5G.</li><li>查询，写入效率高：查询略优。写入效率自增ID是UUID的四倍。</li></ol><p>缺点：</p><ol><li>导入旧数据时，可能会ID重复，导致导入失败。</li><li>分布式架构，多个Mysql实例可能会导致ID重复。</li></ol><p>PS：</p><ol><li>单实例，单节点，由于InnoDB的特性，自增ID效率大于UUID.</li><li>20个节点一下小型分布式架构：为了实现快速部署，主键不重复，可以采用UUID</li><li>20到200个节点：可以采用自增ID+步长的较快速方案。</li><li>200个以上节点的分布式架构：可以采用twitter的雪花算法全局自增ID</li></ol><h2 id="三、Redis"><a href="#三、Redis" class="headerlink" title="三、Redis"></a>三、Redis</h2><p>当使用数据库来生成ID性能不够要求的时候，我们可以尝试使用Redis来生成ID。这主要依赖于Redis是单线程的，所以也可以用生成全局唯一的ID。可以用Redis的原子操作 INCR和INCRBY来实现。</p><p>优点：</p><ol><li>不依赖于数据库，灵活方便，且性能优于数据库。</li><li>数字ID天然排序，对分页或者需要排序的结果很有帮助。</li></ol><p>缺点：</p><ol><li>由于redis是内存的KV数据库，即使有AOF和RDB，但是依然会存在数据丢失，有可能会造成ID重复。</li><li>依赖于redis，redis要是不稳定，会影响ID生成。</li></ol><h2 id="四、Zookeeper"><a href="#四、Zookeeper" class="headerlink" title="四、Zookeeper"></a>四、Zookeeper</h2><p> zookeeper做分布式一致性，没有啥好说的。</p><h2 id="五、数据库分段-服务缓存ID"><a href="#五、数据库分段-服务缓存ID" class="headerlink" title="五、数据库分段+服务缓存ID"></a>五、数据库分段+服务缓存ID</h2><p><img src="https://upload-images.jianshu.io/upload_images/12321605-04f4d4feb09b608e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="fenduan.jpg"></p><p>优点:</p><ul><li><p>比主键递增性能高，能保证趋势递增。</p></li><li><p>如果DB宕机，proxServer由于有缓存依然可以坚持一段时间。</p></li></ul><p>缺点:</p><ul><li><p>和主键递增一样，容易被人猜测。</p></li><li><p>DB宕机，虽然能支撑一段时间但是仍然会造成系统不可用。</p></li></ul><p>适用场景:需要趋势递增，并且ID大小可控制的，可以使用这套方案。</p><p>当然这个方案也可以通过一些手段避免被人猜测，把ID变成是无序的，比如把我们生成的数据是一个递增的long型，把这个Long分成几个部分，比如可以分成几组三位数，几组四位数，然后在建立一个映射表，将我们的数据变成无序。</p><h2 id="六、雪花算法-Snowflake"><a href="#六、雪花算法-Snowflake" class="headerlink" title="六、雪花算法-Snowflake"></a>六、雪花算法-Snowflake</h2><p>Snowflake是Twitter提出来的一个算法，其目的是生成一个64bit的整数:</p><p>雪花算法简单描述： </p><ul><li>最高位是符号位，始终为0，不可用。 </li><li>41位的时间序列，精确到毫秒级，41位的长度可以使用69年。时间位还有一个很重要的作用是可以根据时间进行排序。 </li><li>10位的机器标识，10位的长度最多支持部署1024个节点。 </li><li>12位的计数序列号，序列号即一系列的自增id，可以支持同一节点同一毫秒生成多个ID序号，12位的计数序列号支持每个节点每毫秒产生4096个ID序号。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fea5834df99d0777.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Snowflake.jpg"></p><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://blog.csdn.net/distance_nba/article/details/78967932">https://blog.csdn.net/distance_nba/article/details/78967932</a></p><p><a href="https://mp.weixin.qq.com/s/KfoLFClRwDXlcTDmhCEdaQ">https://mp.weixin.qq.com/s/KfoLFClRwDXlcTDmhCEdaQ</a></p><p><a href="https://blog.csdn.net/u011499747/article/details/78254990">https://blog.csdn.net/u011499747/article/details/78254990</a></p><p><a href="https://blog.csdn.net/tuzhihai/article/details/80988816">https://blog.csdn.net/tuzhihai/article/details/80988816</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Middleware </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Go 执行Lua脚本和JS脚本测试</title>
      <link href="/2018/08/30/go-lua-js/"/>
      <url>/2018/08/30/go-lua-js/</url>
      
        <content type="html"><![CDATA[<p>最近有个需求需要在Go项目里面执行动态脚本，github上有好几个lua执行解释器，但是有很多要不就很久没维护了，要不就没有什么文档，经过几个对比我最后用的是 <a href="https://github.com/yuin/gopher-lua%E3%80%82JS%E8%A7%A3%E6%9E%90%E5%99%A8%E7%94%A8%E7%9A%84github.com/robertkrimen/otto%E3%80%82">https://github.com/yuin/gopher-lua。JS解析器用的github.com/robertkrimen/otto。</a></p><p>具体测试代码如下，给有需求的朋友参考。</p><p><a href="https://github.com/fanlv/runJsAndrLuaInGo">github地址</a></p><pre><code>package mainimport (    "fmt"    "github.com/robertkrimen/otto"    "github.com/yuin/gluamapper"    "github.com/yuin/gopher-lua"    "time")//function add(a, b)//return a+b//endvar luaCode = `function testFun(tab)    result = {}    result["key"] = "test"    result["key1"] = "val2"    if(tab["user"]=="test")then        result["title"]="good"    end    if(tab["os"]=="ios")then        result["url"]="http://www.google.com"    else        result["url"]="http://www.baidu.com"    end        return resultend`func main() {    dic := make(map[string]string)    dic["user"] = "test"    dic["os"] = "ios"    dic["version"] = "1.0"    start0 := time.Now()    count := 10000    for i := 0; i &lt; count; i++ {        LuaTest(dic)    }    tmp1 := time.Since(start0).Nanoseconds() / 1000 / 1000    start1 := time.Now()    for i := 0; i &lt; count; i++ {        JsTest(dic)    }    tmp2 := time.Since(start1).Nanoseconds() / 1000 / 1000    fmt.Printf("LuaTest : %d,JsTest : %d", tmp1, tmp2)}func LuaTest(dic map[string]string) {    L := lua.NewState()    defer L.Close()    if err := L.DoString(luaCode); err != nil {        panic(err)    }    table := L.NewTable()    for k, v := range dic {        L.SetTable(table, lua.LString(k), lua.LString(v))    }    if err := L.CallByParam(lua.P{        Fn:      L.GetGlobal("testFun"),        NRet:    1,        Protect: true,    }, table); err != nil {        panic(err)    }    ret := L.Get(-1) // returned value    L.Pop(1)         // remove received value    obj := gluamapper.ToGoValue(ret, gluamapper.Option{NameFunc: printTest})    fmt.Println(obj)}func printTest(s string) string {    return s}func JsTest(dic map[string]string) {    vm := otto.New()    v, err := vm.Run(`function testFun(tab) {    result = {}    result["key"] = "test"    result["key1"] = "val2"     if(tab["user"]=="test"){       result["title"]="good"    }    if(tab["os"]=="ios"){        result["url"]="http://www.google.com"    }else{        result["url"]="http://www.baidu.com"    }    return result}`)    if err == nil {        fmt.Println(v)    }    jsa, err := vm.ToValue(dic)    if err != nil {        panic(err)    }    result, err := vm.Call("testFun", nil, jsa)    tmpR, err := result.Export()    fmt.Println("object: ", tmpR)}</code></pre>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Golang </tag>
            
            <tag> Lua </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>测试Protobuf在Http传输测试</title>
      <link href="/2018/08/19/protobuf-on-http/"/>
      <url>/2018/08/19/protobuf-on-http/</url>
      
        <content type="html"><![CDATA[<p>Demo：<a href="https://github.com/fanlv/ProtobufOnHttpGo">https://github.com/fanlv/ProtobufOnHttpGo</a></p><h2 id="一、编写Proto文件"><a href="#一、编写Proto文件" class="headerlink" title="一、编写Proto文件"></a>一、编写Proto文件</h2><pre><code>syntax = "proto3";// 生成go代码//protoc --go_out=. user.proto// 生成oc代码//protoc --objc_out=. user.protopackage user;message LoginRequest {  string username = 1;  string password = 2;}message BaseResponse{  int64 code = 1;  string msg = 2;}message User{    string uid = 1;    string name = 2;    string logo = 3;}message LoginResponse {    User user = 1;    BaseResponse baseResp = 255;}</code></pre><h2 id="二、生成目标项目代码"><a href="#二、生成目标项目代码" class="headerlink" title="二、生成目标项目代码"></a>二、生成目标项目代码</h2><pre><code>// cd 到user.proto文件目录// 生成go代码//protoc --go_out=. user.proto// 生成oc代码//protoc --objc_out=. user.proto</code></pre><h2 id="三、服务端测试代码"><a href="#三、服务端测试代码" class="headerlink" title="三、服务端测试代码"></a>三、服务端测试代码</h2><pre><code>r.POST("/login", func(c *gin.Context) {    body, err := c.GetRawData()    if err == nil {        req := &amp;user.LoginRequest{}        err = proto.Unmarshal(body, req)        if err == nil {            if req.Username == "admin" &amp;&amp; req.Password == "123456" {                err = nil            } else {                err = errors.New("login fail")            }        } else {            fmt.Print(err.Error())        }    }    var req *user.LoginResponse    if err == nil {        req = &amp;user.LoginResponse{            User: &amp;user.User{                Uid:  "0010",                Name: "admin",                Logo: "url",            },            BaseResp: &amp;user.BaseResponse{                Code: 1,                Msg:  "ok",            },        }    } else {        req = &amp;user.LoginResponse{            User: nil,            BaseResp: &amp;user.BaseResponse{                Code: 100,                Msg:  "login fail",            },        }    }    out, err := proto.Marshal(req)    if err == nil {        c.Data(200, "application/x-protobuf", out)    }})</code></pre><h2 id="四、客户端测试代码"><a href="#四、客户端测试代码" class="headerlink" title="四、客户端测试代码"></a>四、客户端测试代码</h2><pre><code>NSDate *startDate = [NSDate date];LoginRequest *req = [[LoginRequest alloc] init];req.username = @"admin";req.password = @"123456";[self postUrl:@"http://127.0.0.1:8080/login" dataBody:[req data] Completetion:^(id result, NSError *error) {    if (!error &amp;&amp; [result isKindOfClass:[NSData class]]) {        NSData *data = (NSData *)result;        NSError *pError;        LoginResponse *resp = [[LoginResponse alloc] initWithData:data error:&amp;pError];        if (!pError) {            NSDate *endDate1 = [NSDate date];            _infolabel.text = [NSString stringWithFormat:@"数据大小 ： %.3f KB, 请求耗时：%f",[data length]/1000.0,[endDate1 timeIntervalSinceDate:startDate]];            _textView.text = resp.description;        }    }}];</code></pre><h2 id="Done"><a href="#Done" class="headerlink" title="Done"></a>Done</h2>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Protobuf </tag>
            
            <tag> Net </tag>
            
            <tag> HTPP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二叉树、2-3 树、红黑树</title>
      <link href="/2018/08/12/binary-tree/"/>
      <url>/2018/08/12/binary-tree/</url>
      
        <content type="html"><![CDATA[<blockquote><p>有序数组的优势在于二分查找，链表的优势在于数据项的插入和数据项的删除。但是在有序数组中插入数据就会很慢，同样在链表中查找数据项效率就很低。综合以上情况，二叉树可以利用链表和有序数组的优势，同时可以合并有序数组和链表的优势，二叉树也是一种常用的数据结构。</p></blockquote><h2 id="一、满二叉树"><a href="#一、满二叉树" class="headerlink" title="一、满二叉树"></a>一、满二叉树</h2><p>一个二叉树，如果每一个层的结点数都达到最大值，则这个二叉树就是满二叉树。也就是说，如果一个二叉树的层数为 K，且结点总数是(2^k) -1 ，则它就是满二叉树。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-cb32d1d0f03e0d3c.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="mbt.jpg"></p><h2 id="二、完全二叉树"><a href="#二、完全二叉树" class="headerlink" title="二、完全二叉树"></a>二、完全二叉树</h2><p>若设二叉树的深度为 h，除第 h 层外，其它各层 (1 ～ h-1) 的结点数都达到最大个数，第 h 层所有的结点都连续集中在最左边，这就是完全二叉树。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-776d7b0f8a8a6747.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="cbt.jpg"></p><h2 id="三、二叉查找树"><a href="#三、二叉查找树" class="headerlink" title="三、二叉查找树"></a>三、二叉查找树</h2><p>二叉查找树（Binary Search Tree），（又：二叉搜索树，二叉排序树）它或者是一棵空树，或者是具有下列性质的二叉树： 若它的左子树不空，则左子树上所有节点的值均小于它的根节点的值； 若它的右子树不空，则右子树上所有节点的值均大于它的根节点的值； 它的左、右子树也分别为二叉排序树。“中序遍历”可以让节点有序。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-58121e9e2ff697da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="bst.png"></p><h2 id="四、平衡二叉树"><a href="#四、平衡二叉树" class="headerlink" title="四、平衡二叉树"></a>四、平衡二叉树</h2><p>平衡二叉树（Balanced Binary Tree）是二叉查找树的一个进化体，也是第一个引入平衡概念的二叉树。1962 年，G.M. Adelson-Velsky 和 E.M. Landis 发明了这棵树，所以它又叫 AVL 树。平衡二叉树要求对于每一个节点来说，它的左右子树的高度之差不能超过 1，如果插入或者删除一个节点使得高度之差大于 1，就要进行节点之间的旋转，将二叉树重新维持在一个平衡状态。这个方案很好的解决了二叉查找树退化成链表的问题，把插入，查找，删除的时间复杂度最好情况和最坏情况都维持在 O(logN)。但是频繁旋转会使插入和删除牺牲掉 O(logN)左右的时间，不过相对二叉查找树来说，时间上稳定了很多。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4642d4482a16a894.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="bbt.jpg"></p><h3 id="4-1-插入原理"><a href="#4-1-插入原理" class="headerlink" title="4.1 插入原理"></a>4.1 插入原理</h3><p>根据二叉平衡树的定义，一定保持左右子树深度绝对值小于 1.在平衡二叉树插入工作一定考虑深度差，在 AVL 树进行插入工作时候，困难在于可能破坏 AVL 树的平衡属性。需要根据树的实际结构进行几种简单的旋转（rotation）操作就可以让树恢复 AVL 树的平衡性质</p><h3 id="4-2-旋转问题"><a href="#4-2-旋转问题" class="headerlink" title="4.2 旋转问题"></a>4.2 旋转问题</h3><p>对于一个平衡的节点，由于任意节点最多有两个儿子，因此高度不平衡时，此节点的两颗子树的高度差 2.容易看出，这种不平衡出现在下面四种情况：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1e5eaae43f6fd0be.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="nbbt.jpg"></p><ol><li><p>6 节点的左子树 3 节点高度比右子树 7 节点大 2，左子树 3 节点的左子树 1 节点高度大于右子树 4 节点，这种情况成为左左。</p></li><li><p>6 节点的左子树 2 节点高度比右子树 7 节点大 2，左子树 2 节点的左子树 1 节点高度小于右子树 4 节点，这种情况成为左右。</p></li><li><p>2 节点的左子树 1 节点高度比右子树 5 节点小 2，右子树 5 节点的左子树 3 节点高度大于右子树 6 节点，这种情况成为右左。</p></li><li><p>2 节点的左子树 1 节点高度比右子树 4 节点小 2，右子树 4 节点的左子树 3 节点高度小于右子树 6 节点，这种情况成为右右。</p></li></ol><p>从图 2 中可以可以看出，1 和 4 两种情况是对称的，这两种情况的旋转算法是一致的，只需要经过一次旋转就可以达到目标，我们称之为单旋转。2 和 3 两种情况也是对称的，这两种情况的旋转算法也是一致的，需要进行两次旋转，我们称之为双旋转。</p><h3 id="4-3-旋转操作"><a href="#4-3-旋转操作" class="headerlink" title="4.3 旋转操作"></a>4.3 旋转操作</h3><h4 id="4-3-1-单旋转"><a href="#4-3-1-单旋转" class="headerlink" title="4.3.1 单旋转"></a>4.3.1 单旋转</h4><p><strong>单旋转是针对于左左和右右这两种情况的解决方案</strong>，这两种情况是对称的，只要解决了左左这种情况，右右就很好办了。图 3 是左左情况的解决方案，节点 k2 不满足平衡特性，因为它的左子树 k1 比右子树 Z 深 2 层，而且 k1 子树中，更深的一层的是 k1 的左子树 X 子树，所以属于左左情况。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-724ef0b89acd93bc.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="dxz.jpg"></p><h4 id="4-3-2-双旋转"><a href="#4-3-2-双旋转" class="headerlink" title="4.3.2 双旋转"></a>4.3.2 双旋转</h4><p><strong>对于左右和右左这两种情况，单旋转不能使它达到一个平衡状态，要经过两次旋转。双旋转是针对于这两种情况的解决方案</strong>，同样的，这样两种情况也是对称的，只要解决了左右这种情况，右左就很好办了。图 4 是左右情况的解决方案，节点 k3 不满足平衡特性，因为它的左子树 k1 比右子树 Z 深 2 层，而且 k1 子树中，更深的一层的是 k1 的右子树 k2 子树，所以属于左右情况。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-12f2746f02f7952d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="ddxz.jpg"></p><h2 id="五、平衡查找树之-2-3-查找树-2-3-Search-Tree"><a href="#五、平衡查找树之-2-3-查找树-2-3-Search-Tree" class="headerlink" title="五、平衡查找树之 2-3 查找树(2-3 Search Tree)"></a>五、平衡查找树之 2-3 查找树(2-3 Search Tree)</h2><p>2-3 树是最简单的 B-树（或-树）结构，<strong>其每个非叶节点都有两个或三个子女，而且所有叶都在统一层上。2-3 树不是二叉树</strong>，其节点可拥有 3 个孩子。不过，2-3 树与满二叉树相似。若某棵 2-3 树不包含 3-节点，则看上去像满二叉树，其所有内部节点都可有两个孩子，所有的叶子都在同一级别。另一方面，2-3 树的一个内部节点确实有 3 个孩子，故比相同高度的满二叉树的节点更多。高为 h 的 2-3 树包含的节点数大于等于高度为 h 的满二叉树的节点数，即至少有 2^h-1 个节点。换一个角度分析，包含 n 的节点的 2-3 树的高度不大于 log2(n+1) (即包含 n 个节点的二叉树的最小高度)。</p><p>为了保证查找树的平衡性，我们需要一些灵活性，因此在这里我们允许树中的一个结点保存多个键。</p><p>2- 结点，含有一个键（及其对应的值）和两条链接，左链接指向的 2-3 树中的键都小于该结点，右链接指向的 2-3 树中的键都大于该结点，右链接指向的 203 树中的键都大于该结点</p><p>3-结点：含有两个键(及值)和三条链接，左链接指向的 2-3 树中的键都小于该结点，中链接指向的 2-3 树中的键都位于该结点的两个键之间，右链接指向的 2-3 树中的键都大于该结点。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ba63f849e9092326.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2-3node.jpg"></p><h3 id="5-1-将数据项放入-2-3-树节点中的规则是："><a href="#5-1-将数据项放入-2-3-树节点中的规则是：" class="headerlink" title="5.1 将数据项放入 2-3 树节点中的规则是："></a>5.1 将数据项放入 2-3 树节点中的规则是：</h3><p>先找插入结点，若结点有空(即 2-结点)，则直接插入。如结点没空(即 3-结点)，则插入使其临时容纳这个元素，然后分裂此结点，把中间元素移到其父结点中。对父结点亦如此处理。（中键一直往上移，直到找到空位，在此过程中没有空位就先搞个临时的，再分裂。）</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-49abdc47d5f492db.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2-3nodei.jpg"></p><h3 id="5-2-构造-2-3-树"><a href="#5-2-构造-2-3-树" class="headerlink" title="5.2 构造 2-3 树"></a>5.2 构造 2-3 树</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-46901e9bcf22ad93.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="2-3nodeic.jpg"></p><h4 id="5-2-1-优点"><a href="#5-2-1-优点" class="headerlink" title="5.2.1 优点"></a>5.2.1 优点</h4><p>2-3 树在最坏情况下仍有较好的性能。每个操作中处理每个结点的时间都不会超过一个很小的常数，且这两个操作都只会访问一条路径上的结点，所以任何查找或者插入的成本都肯定不会超过对数级别。</p><p>完美平衡的 2-3 树要平展的多。例如，含有 10 亿个结点的一颗 2-3 树的高度仅在 19 到 30 之间。我们最多只需要访问 30 个结点就能在 10 亿个键中进行任意查找和插入操作。</p><h4 id="5-2-2-缺点"><a href="#5-2-2-缺点" class="headerlink" title="5.2.2 缺点"></a>5.2.2 缺点</h4><p>我们需要维护两种不同类型的结点，查找和插入操作的实现需要大量的代码，而且它们所产生的额外开销可能会使算法比标准的二叉查找树更慢。</p><p>平衡一棵树的初衷是为了消除最坏情况，但我们希望这种保障所需的代码能够越少越好。</p><h2 id="六、红黑树"><a href="#六、红黑树" class="headerlink" title="六、红黑树"></a>六、红黑树</h2><h3 id="6-1-什么是红黑树"><a href="#6-1-什么是红黑树" class="headerlink" title="6.1 什么是红黑树"></a>6.1 什么是红黑树</h3><p>理解红黑树一句话就够了：<strong>红黑树就是用红链接表示 3-结点的 2-3 树</strong>。那么红黑树的插入、构造就可转化为 2-3 树的问题，即：在脑中用 2-3 树来操作，得到结果，再把结果中的 3-结点转化为红链接即可。而 2-3 树的插入，前面已有详细图文，实际也很简单：有空则插，没空硬插，再分裂。 这样，我们就不用记那么复杂且让人头疼的红黑树插入旋转的各种情况了。只要清楚 2-3 树的插入方式即可。</p><p>红黑树的另一种定义是满足下列条件的二叉查找树：</p><ol><li>红链接均为左链接。</li><li>没有任何一个结点同时和两条红链接相连。</li><li>该树是完美黑色平衡的，即任意空链接到根结点的路径上的黑链接数量相同。</li></ol><p>如果我们将<strong>一颗红黑树中的红链接画平</strong>，那么所有的空链接到根结点的距离都将是相同的。<strong>如果我们将由红链接相连的结点合并，得到的就是一颗 2-3 树</strong>。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-77e4bcb8a1f79d5b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="rba23t.jpg"></p><h3 id="6-2-红黑树的本质："><a href="#6-2-红黑树的本质：" class="headerlink" title="6.2 红黑树的本质："></a>6.2 红黑树的本质：</h3><p>★ 红黑树是对 2-3 查找树的改进，它能用一种统一的方式完成所有变换。</p><p>★ 红黑树背后的思想是用标准的二叉查找树（完全由 2-结点构成）和一些额外的信息（替换 3-结点）来表示 2-3 树。</p><h3 id="6-3-红黑树链接类型"><a href="#6-3-红黑树链接类型" class="headerlink" title="6.3 红黑树链接类型"></a>6.3 红黑树链接类型</h3><p>我们将树中的链接分为两种类型：红链接将两个 2-结点连接起来构成一个 3-结点，黑链接则是 2-3 树中的普通链接。确切地说，我们将 3-结点表示为由一条左斜的红色链接相连的两个 2-结点。</p><p>这种表示法的一个优点是，我们无需修改就可以直接使用标准二叉查找树的 get()方法。对于任意的 2-3 树，只要对结点进行转换，我们都可以立即派生出一颗对应的二叉查找树。我们将用这种方式表示 2-3 树的二叉查找树称为红黑树。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f2031959f8dba913.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="rbt.jpg"></p><h3 id="6-4-红黑树颜色表示"><a href="#6-4-红黑树颜色表示" class="headerlink" title="6.4 红黑树颜色表示"></a>6.4 红黑树颜色表示</h3><p>因为每个结点都只会有一条指向自己的链接（从它的父结点指向它），我们将链接的颜色保存在表示结点的 Node 数据类型的布尔变量 color 中（若指向它的链接是红色的，那么该变量为 true，黑色则为 false）。</p><p>当我们提到一个结点颜色时，我们指的是指向该结点的链接的颜色。</p><h3 id="6-5-红黑树旋转"><a href="#6-5-红黑树旋转" class="headerlink" title="6.5 红黑树旋转"></a>6.5 红黑树旋转</h3><p>在我们实现的某些操作中可能会出现红色右链接或者两条连续的红链接，但在操作完成前这些情况都会被小心地旋转并修复。</p><h3 id="6-7-红黑树插入"><a href="#6-7-红黑树插入" class="headerlink" title="6.7 红黑树插入"></a>6.7 红黑树插入</h3><p>在插入时我们可以使用旋转操作帮助我们保证 2-3 树和红黑树之间的一一对应关系，因为旋转操作可以保持红黑树的两个重要性质：<strong>有序性和完美平衡性</strong>。</p><ol><li> 向一个只含有一个 2-结点的 2-3 树中插入新键后，2-结点变为 3-结点。我们再把这个 3-结点转化为红结点即可）</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-44aca235dec34ec4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="rbti1.jpg"></p><ol start="2"><li>向一颗双键树（即一个 3-结点）中插入新键<br> （向红黑树中插入操作时，想想 2-3 树的插入操作。你把红黑树当做 2-3 树来处理插入，一切都变得简单了）<br> （向 2-3 树中的一个 3-结点插入新键，这个 3 结点临时成为 4-结点，然后分裂成 3 个 2 结点）</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e1d5fed10b3da7d9.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="rbti2.jpg"></p><h3 id="6-7-一颗红黑树的构造全过程"><a href="#6-7-一颗红黑树的构造全过程" class="headerlink" title="6.7 一颗红黑树的构造全过程"></a>6.7 一颗红黑树的构造全过程</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-2d355d154b11a395.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="rbtc.png"></p><h3 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h3><p><a href="http://www.cnblogs.com/polly333/p/4798944.html">浅谈数据结构-平衡二叉树</a></p><p><a href="https://blog.csdn.net/yang_yulei/article/details/26066409">查找（一）史上最简单清晰的红黑树讲解</a></p><p><a href="http://www.cnblogs.com/yangecnu/p/Introduce-2-3-Search-Tree.html">浅谈算法和数据结构: 八 平衡查找树之 2-3 树</a></p>]]></content>
      
      
      <categories>
          
          <category> Arithmetic </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataStructure </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高并发服务器IO模型</title>
      <link href="/2018/07/15/server-io-model/"/>
      <url>/2018/07/15/server-io-model/</url>
      
        <content type="html"><![CDATA[<p>服务端IO模型总结 草稿</p><h2 id="网络框架视角"><a href="#网络框架视角" class="headerlink" title="网络框架视角"></a>网络框架视角</h2><h3 id="零、Nginx"><a href="#零、Nginx" class="headerlink" title="零、Nginx"></a>零、Nginx</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-ee3acec698831d00?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="一、Netty（主从Reactor）"><a href="#一、Netty（主从Reactor）" class="headerlink" title="一、Netty（主从Reactor）"></a>一、Netty（主从Reactor）</h3><pre><code>MainReactor负责客户端的连接请求，并将请求转交给SubReactorSubReactor负责相应通道的IO读写请求非IO请求（具体逻辑处理）的任务则会直接写入队列，等待worker threads进行处理</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-9216883059354e4b?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-58af79a163dc0002?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="二、GRPC-GO-（Goroutine-Per-Connection）"><a href="#二、GRPC-GO-（Goroutine-Per-Connection）" class="headerlink" title="二、GRPC-GO （Goroutine Per Connection）"></a>二、GRPC-GO （Goroutine Per Connection）</h3><pre><code>net.Listen -&gt; Serve() -&gt; lis.Accept() net库的accept -&gt; 一个连接开个一个goroutine -&gt; s.handleRawConn(rawConn) -&gt; newHTTP2Transport(conn, authInfo) -&gt;  newHTTP2Server-&gt; 开个gorutine for 循环检查 是否有 数据没有发送=&gt; t.loopy.run(); -&gt; go t.keepalive() -&gt; 设置读取鉴权信息、超时配置、Http2Transport、一堆有的没的配置 最后生成st对象 -&gt; serveStreams(st)-&gt; 收到Request 、 HandleStreams (这个方法里面会for{} 不停读写消息内容)-&gt; ReadFrame() 读取数据 -&gt; 判断是什么帧类型数据http2.MetaHeadersFrame、http2.DataFrame、http2.RSTStreamFrame、http2.SettingsFramehttp2.PingFrame、http2.WindowUpdateFrame -&gt; t.operateHeaders 处理数据 -&gt; s.handleStream-&gt; 解析出service和method 找到对应的handle方法 -&gt; processUnaryRPC -&gt;  md.Handler-&gt;  执行对应方法的handle _Greeter_Login_Handler -&gt; s.getCodec 解码出req数据-&gt; srv.(GreeterServer).Login 执行对应的函数 -&gt; sendResponse -&gt; encode、compress、Write-&gt; writeHeaderLocked-&gt; dataFrame -&gt; writeQuota 剑去包大小 -&gt; t.controlBuf.put(df)-&gt;  executeAndPut -&gt; c.list.enqueue(it) -&gt; WriteStatus -&gt; 另个有个gorutine 会调用 t.loopy.run()-&gt; l.processData() -&gt; str.itl.peek().(*dataFrame)  -&gt;  l.framer.fr.WriteData </code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b95dc7af4b943476?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="三、Thrift-GO-（-Goroutine-Per-Connection）"><a href="#三、Thrift-GO-（-Goroutine-Per-Connection）" class="headerlink" title="三、Thrift-GO （ Goroutine Per Connection）"></a>三、Thrift-GO （ Goroutine Per Connection）</h3><pre><code>Serve() -&gt; Listen() -&gt; AcceptLoop() -&gt; for 循环接受连接请求 {innerAccept()} 一个连接开一个goroutine-&gt; 拿到net.coon -&gt; client = NewTSocketFromConnTimeout(coon,timeout)-&gt; processRequests(cleint) -&gt; TProcessor(interface包含:Process、ProcessorMap、AddToProcessorMap )-&gt;  调用 TransportFactory.GetTransport(client)拿到 inputTransport,outputTransport-&gt; ProtocolFactory.GetProtocol(inputTransport)拿到 inputProtocol和outputProtocol-&gt; for { ReadFrame processor.Process } 这里循环读取数据，读出请求，然后返回resp，然后再继续读-&gt; 调用到IDL生成的代码中对应方法的Process(inputProtocol和outputProtocol) -&gt;name, _, seqId, err := iprot.ReadMessageBegin() (seqId回复的数据包要回写回去)-&gt; 根据name 找到对应方法的 Process，调用对应的Process-&gt; args.Read(iprot) -&gt; iprot.ReadMessageEnd() -&gt; handler.xxxx 方法拿到结果 -&gt;oprot.WriteMessageBegin-&gt; response字段.Write(oprot) -&gt; oprot.WriteMessageEnd() -&gt; oprot.Flush() -&gt; 判断是否 ErrAbandonRequest 是的话关闭连接，不是的话继续读 processor.Process</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d19e6b183ef7baac?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="四、Kite-（Goroutine-Per-Connection）"><a href="#四、Kite-（Goroutine-Per-Connection）" class="headerlink" title="四、Kite （Goroutine Per Connection）"></a>四、Kite （Goroutine Per Connection）</h3><pre><code>kite跟Thrift Go Server 流程基本一样，只是在整理流程中加入了一些服务治理的东西，比如判断的连接是否过载，加一些中间件（打点、熔断、限流、ACL、压测、定时拉取ms配置）等等。</code></pre><pre><code>kite.Run() -&gt; RPCServer.ListenAndServe -&gt; CreateListener() -&gt; Serve() -&gt; for {Accept()} 一个连接开一个goroutine  -&gt; processRequests-&gt; for 循环 { processor.Process} -&gt; Process(in, out TProtocol)-&gt; name, _, seqId, err := iprot.ReadMessageBegin() (seqId回复的数据包要回写回去)-&gt; 根据name 找到对应方法的 Process，调用对应的Process-&gt; args.Read(iprot) -&gt; iprot.ReadMessageEnd() -&gt; handler.xxxx 方法拿到结果 -&gt;oprot.WriteMessageBegin-&gt; response字段.Write(oprot) -&gt; oprot.WriteMessageEnd() -&gt;oprot.Flush() -&gt; 判断是否 ErrAbandonRequest 是的话关闭连接，不是的话继续读 processor.Process</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3bd0241472e2566f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="五、Kitex-Netpoll（主从Reactor）"><a href="#五、Kitex-Netpoll（主从Reactor）" class="headerlink" title="五、Kitex-Netpoll（主从Reactor）"></a>五、Kitex-Netpoll（主从Reactor）</h3><p>其实为了解决 <a href="https://mp.weixin.qq.com/s/wSaJYg-HqnYY4SdLA2Zzaw">字节跳动在 Go 网络库上的实践</a>提到的“<strong>Go 调度导致的延迟问题</strong>” 最新的Netpoll已经改成了单Reactor模式。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-902e074e38161b26?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p>随便说下Go自身的Net库没有这个问题，是因为Golang的网络请求都是在GO自己的一个<a href="https://github.com/golang/go/blob/master/src/runtime/proc.go#L5099">Sysmon监控线程维护</a>的，<strong>Sysmon线程不需绑定P</strong>，首次休眠20us，每次执行完后，增加一倍的休眠时间，但是最多休眠10ms。</p><p>Sysmon主要做以下几件事</p><pre><code>1\. 释放闲置超过5分钟的span物理内存2\. 如果超过两分钟没有执行垃圾回收，则强制执行3\. 将长时间未处理的netpoll结果添加到任务队列4\. 向长时间运行的g进行抢占5\. 收回因为syscall而长时间阻塞的p</code></pre><p><a href="https://github.com/golang/go/blob/master/src/runtime/netpoll_epoll.go#L106">Golang runtime的netpoll函数</a>主要做的是就是调用epollwait拿到活跃的FD，然后唤醒相关阻塞的gorotine。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-b7f607bde828cf3a?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-59a9682ef60f1a64?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p><strong>Goroutine Pool ：</strong> 减少gorotine 调度开销， 最大10000</p><p><strong>LinkBuffer</strong> : 用来“分离网络IO”和“业务处理过程” ， 减少序列化和传递过程中字节的拷贝</p><p><strong>Codec</strong>：Kitex 支持自定义Codec。默认支持Thrift和ProtoBuf 两种Codec。</p><pre><code>/*kitex server-&gt; 启动netpoll初始化netpool init函数 -&gt; 初始化 loops数量 -&gt; SetNumLoops() -&gt;  m.Run()-&gt; openPoll()这个里面调用syscall.EpollCreate1 -&gt;  go poll.Wait() 这里调用 epollwait Reactor-&gt; subReactor 数量 = CPUNUM() / 20 + 1-&gt; 初始化ServerLarkSvcDemo.NewServer -&gt; server.NewServer -&gt; 初始化中间件 -&gt; RegisterService -&gt; Run()-&gt; richRemoteOption() -&gt; 初始化opt.RemoteOpt 属性 -&gt; addBoundHandlers-&gt; 添加入流量和出流量处理类 In/Out boundsHandler(ServerBaseHandler、ServerTTHeaderHandler)-&gt; Start() -&gt; buildListener -&gt; netpoll.CreateListener -&gt; go s.transSvr.BootstrapServer()-&gt; netpoll server.Run() -&gt; pollmanager.Pick() 找个一个 epoll 出来。-&gt; 注册 listen fd 的 onRead 事件里面主要是Accept Socket ，可以通过 OnRead != nil 判断这个fd是不是listen的FD-&gt; 等待接受链接-&gt; newConnectionWithPrepare 初始化链接相关 设置回调函数，添加链接到epoll、保存FD-&gt;connection关系-&gt; connection.init -&gt; 设置 Fd 为 noblocking -&gt; 初始化inputBuffer outputBuffer = NewLinkBuffer()-&gt; 设置 supportZeroCopy -&gt; onEvent . onPrepare 设置熟悉 -&gt; 这里onRequest 就是 transServer.onConnRead-&gt; onEvent .process = onRequest -&gt; onPrepare 就是 transServer.onConnActive -&gt; ts.connCount.Inc()-&gt; OnActive 新建立连接是触发 -&gt; inboundHdrls 执行OnActive -&gt; svrTransHandler OnActive 初始化RCPinfo-&gt; register -&gt; pollmanager.Pick() -&gt; 添加Fd到 epoll -&gt; s.connections.Store(fd, connection)-&gt; 等待接受客户端数据-&gt; epollWait 返回活跃链接 -&gt; operator.Inputs -&gt; Book 这个主要是判断是否需要扩大buffer-&gt; syscall.SYS_READV -&gt; inputAck -&gt; MallocAck(n)  linkbuffer的malloc += n ,buf = [:n]-&gt; 读取完以后处理数据-&gt; onEvent.onRequest -&gt; gopool.CtxGo(on.ctx, task) 新建一个task丢pool里面去让worker处理-&gt; 执行task -&gt; handler -&gt; transServer. onConnRead -&gt; transMetaHandler.OnRead -&gt; svrTransHandler.OnRead-&gt; NewMessageWithNewer -&gt; SetPayloadCodec -&gt; NewReadByteBuffer(ctx, conn, recvMsg)-&gt; Decode -&gt; flagBuf = Peek(8) 先读8个字节出来 , 根据前8个字节判断是不是TTHeader或者MeshHeader编码的-&gt; IsTTHeader -&gt; isMeshHeader -&gt; checkPayload 这个是没有header的编码 -&gt; isThriftBinary-&gt; 得到编码数据codecType是Thrift还是PB, transProto 是Framed还是transport.TTHeaderFramed-&gt; decodePayload -&gt; GetPayloadCodec 拿解码器，可以自定义codec，默认支持thrift和PB-&gt; pCodec.Unmarshal -&gt; thriftCodec.Unmarshal-&gt; methodName, msgType, seqID, err := tProt.ReadMessageBegin()-&gt; req.Read(tProt) 读取数据解码到 request -&gt; tProt.ReadMessageEnd() -&gt; tProt.Recycle()-&gt; sendMsg = remote.NewMessage -&gt; transPipe.OnMessage -&gt; TransPipeline.OnMessage-&gt; transMetaHandler.OnMessage -&gt; serverBaseHandler.ReadMeta 这里主要是设置logID、caller、GetExtra-&gt; serverTTHeaderHandler.ReadMeta -&gt; svrTransHandler.OnMessage -&gt; NewServerBytedTraceMW-&gt; NewStatusCheckMW -&gt; NewRPCConfigUpdateMW -&gt; NewACLMiddleware -&gt; invokeHandleEndpoint-&gt; getConfigDemoHandler 到业务方的代码handler -&gt; 执行完业务代码拿到response-&gt; 把拿到的response回复出去-&gt; transPipe.Write -&gt; outboundHdrls.Write -&gt; transMetaHandler.Write -&gt; serverTTHeaderHandler.WriteMeta-&gt; svrTransHandler.Write -&gt; NewWriteByteBuffer -&gt; codec.Encode -&gt; defaultCodec.Encode-&gt; getPayloadBuffer -&gt; encodePayload -&gt; pCodec.Marshal -&gt; thriftCodec.Marshal-&gt; tProt.WriteMessageBegin() BinaryProtocol -&gt; BinaryProtocol 底层内存用的是LinkBuffer 减少一次Copy？-&gt; msg.Write(tProt) -&gt; tProt.WriteMessageEnd() -&gt; bufWriter.Flush() -&gt; connection.flush-&gt; atomic.StoreInt32(&amp;c.writing, 2) 加锁 -&gt; sendmsg -&gt; syscall SYS_SENDMSG-&gt; outputAck() -&gt; 调整底层LinkBuffer的指针*/</code></pre><h2 id="IO模型视角"><a href="#IO模型视角" class="headerlink" title="IO模型视角"></a>IO模型视角</h2><h3 id="一、同步阻塞IO"><a href="#一、同步阻塞IO" class="headerlink" title="一、同步阻塞IO"></a><strong>一、同步阻塞IO</strong></h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7dd0f91bbe4f0680?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="二、同步非阻塞IO"><a href="#二、同步非阻塞IO" class="headerlink" title="二、同步非阻塞IO"></a><strong>二、同步非阻塞IO</strong></h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-fca24d340f5ba47f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><pre><code>func SetNonblock(fd int, nonblocking bool) (err error) {   flag, err := fcntl(fd, F_GETFL, 0)   if err != nil {      return err   }   if nonblocking {      flag |= O_NONBLOCK   } else {      flag &amp;^= O_NONBLOCK   }   _, err = fcntl(fd, F_SETFL, flag)   return err}</code></pre><h3 id="三、IO多路复用-（epoll、kqueue、select）"><a href="#三、IO多路复用-（epoll、kqueue、select）" class="headerlink" title="三、IO多路复用 （epoll、kqueue、select）"></a><strong>三、IO多路复用 （epoll、kqueue、select）</strong></h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3cfef77b51bb63dc?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="四、信号驱动"><a href="#四、信号驱动" class="headerlink" title="四、信号驱动"></a>四、信号驱动</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-a09ac98836a4daef?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="五、异步IO"><a href="#五、异步IO" class="headerlink" title="五、异步IO"></a>五<strong>、异步IO</strong></h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-356652fa26c0b0f1?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-6983d49e972be5e1?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h2 id="线程模型视角"><a href="#线程模型视角" class="headerlink" title="线程模型视角"></a>线程模型视角</h2><h3 id="一、线程模型-Thread-Per-Connection"><a href="#一、线程模型-Thread-Per-Connection" class="headerlink" title="一、线程模型 Thread Per Connection"></a>一、<strong>线程模型</strong> Thread Per Connection</h3><p><strong>生产环境基本没有使用这种模型的</strong></p><pre><code>采用阻塞式 I/O 模型获取输入数据；每个连接都需要独立的线程完成数据输入，业务处理，数据返回的完整操作。缺点：当并发数较大时，需要创建大量线程来处理连接，系统资源占用较大；连接建立后，如果当前线程暂时没有数据可读，则线程就阻塞在 Read 操作上，造成线程资源浪费。</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-714c4431d96deb48?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="二、单Reactor单线程"><a href="#二、单Reactor单线程" class="headerlink" title="二、单Reactor单线程"></a>二、单Reactor单线程</h3><pre><code>优点：简单，没有多线程，没有进程通信缺点：性能，无法发挥多核的极致，一个handler卡死，导致当前进程无法使用，IO和CPU不匹配场景：客户端有限，业务处理快，比如redis</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-d410a6277eecae1a?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="三、单Reactor多线程"><a href="#三、单Reactor多线程" class="headerlink" title="三、单Reactor多线程"></a>三、单Reactor多线程</h3><pre><code>优点：充分利用的CPU缺点：进程通信，复杂，Reactor承放了太多业务，高并发下可能成为性能瓶颈</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-df0beafafd370a62?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="四、主从Reactor多线程"><a href="#四、主从Reactor多线程" class="headerlink" title="四、主从Reactor多线程"></a>四、主从Reactor多线程</h3><pre><code>主Reactor负责建立连接，建立连接后的句柄丢给子Reactor，子Reactor负责监听所有事件进行处理优点：职责明确，分摊压力Nginx/netty/memcached都是使用的这</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4e21da74445bf72e?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="五、Proactor-模型（异步IO）"><a href="#五、Proactor-模型（异步IO）" class="headerlink" title="五、Proactor 模型（异步IO）"></a>五、Proactor 模型（异步IO）</h3><pre><code>  编程复杂性，由于异步操作流程的事件的初始化和事件完成在时间和空间上都是相互分离的，因此开发异步应用程序更加复杂。应用程序还可能因为反向的流控而变得更加难以 Debug；  内存使用，缓冲区在读或写操作的时间段内必须保持住，可能造成持续的不确定性，并且每个并发操作都要求有独立的缓存，相比 Reactor 模式，在 Socket 已经准备好读或写前，是不要求开辟缓存的；  操作系统支持，Windows 下通过 IOCP 实现了真正的异步 I/O，而在 Linux 系统下，Linux 2.6 才引入，目前异步 I/O 还不完善。</code></pre><p><img src="https://upload-images.jianshu.io/upload_images/12321605-940d12d2acc853d5?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IO </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>iOS之GRPC 测试（附代码）</title>
      <link href="/2018/07/13/ios-grpc/"/>
      <url>/2018/07/13/ios-grpc/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近在用gRPC框架测试，想起去年调研Protocol Buffer在HTTP的时候传输，了解过这个框架，当时没深入。这次做gRPC服务器端，随便看下iOS这边实现方式，附上测试代码。</p><p>demo地址： <a href="https://github.com/fanlv/gRPCDemo">https://github.com/fanlv/gRPCDemo</a></p><h2 id="proto文件"><a href="#proto文件" class="headerlink" title="proto文件"></a>proto文件</h2><pre><code>package user;message LoginRequest {  string username = 1;  string password = 2;}message BaseResponse{  int64 code = 1;  string msg = 2;}message User{    string uid = 1;    string name = 2;    string logo = 3;}message LoginResponse {    User user = 1;    BaseResponse baseResp = 255;}//service 名称，客户端会用这个去调用对应方法service Greeter {    //提供的RPC方法  rpc Login (LoginRequest) returns (LoginResponse) {}}</code></pre><h4 id="生成go代码"><a href="#生成go代码" class="headerlink" title="生成go代码"></a>生成go代码</h4><pre><code>--objc_out=plugins=grpc:. user.proto</code></pre><h4 id="生成oc代码"><a href="#生成oc代码" class="headerlink" title="生成oc代码"></a>生成oc代码</h4><pre><code>protoc --objc_out=. --grpc_out=. --plugin=protoc-gen-grpc=/usr/local/bin/grpc_objective_c_plugin user.proto</code></pre><h2 id="服务器代码实现（Go）"><a href="#服务器代码实现（Go）" class="headerlink" title="服务器代码实现（Go）"></a>服务器代码实现（Go）</h2><pre><code>package mainimport (    pb "gitee.com/xxxx/proto"//执行你生成的user.pb.go位置    "golang.org/x/net/context"    "net"    "google.golang.org/grpc"    "google.golang.org/grpc/reflection"    "log")const (    port = ":50051")type server struct{}func (s *server) Login(ctx context.Context, in *pb.LoginRequest) (*pb.LoginResponse, error) {    var resp *pb.LoginResponse    if in.Username =="test" &amp;&amp; in.Password == "123456" {        resp = &amp;pb.LoginResponse{            User:&amp;pb.User{                Uid:"001",                Name:"test",                Logo:"https://test.com/test.png",            },            BaseResp:&amp;pb.BaseResponse{                Code:0,                Msg:"ok",            },        }    }else {        resp = &amp;pb.LoginResponse{            User:nil,            BaseResp:&amp;pb.BaseResponse{                Code:1,                Msg:"login fail",            },        }    }    return resp,nil}func main() {    lis, err := net.Listen("tcp", port)    if err != nil {        log.Fatalf("failed to listen: %v", err)    }    s := grpc.NewServer()    pb.RegisterGreeterServer(s, &amp;server{})    // Register reflection service on gRPC server.    reflection.Register(s)    if err := s.Serve(lis); err != nil {        log.Fatalf("failed to serve: %v", err)    }}</code></pre><h2 id="客户端端代码（iOS）"><a href="#客户端端代码（iOS）" class="headerlink" title="客户端端代码（iOS）"></a>客户端端代码（iOS）</h2><p>可以先去官网下一个Demo项目，地址：<a href="https://grpc.io/docs/tutorials/basic/objective-c.html#try-it-out">grpc.io - objective-c</a></p><p>里面有三个demo，我这里借用的helloworld的demo，路径：<code>grpc/examples/objective-c/helloworld</code>。</p><p>执行pod install，主要会用到下面几个库</p><pre><code>Installing !ProtoCompiler (3.5.0)Installing !ProtoCompiler-gRPCPlugin (1.13.0)Installing BoringSSL (10.0.5)Installing Protobuf (3.6.0)Installing gRPC (1.13.0)Installing gRPC-Core (1.13.0)Installing gRPC-ProtoRPC (1.13.0)Installing gRPC-RxLibrary (1.13.0)Installing nanopb (0.3.8)</code></pre><p>把生成的四个pb文件（<code>User.pbobjc.h</code>、<code>User.pbobjc.m</code>、<code>User.pbrpc.h</code>、<code>User.pbrpc.m</code>）添加到项目中去。然后在main.m中添加下面的测试代码</p><pre><code>  Greeter *userClient = [[Greeter alloc] initWithHost:kHostAddress];  LoginRequest *req = [[LoginRequest alloc] init];  req.username = @"test";  req.password = @"123456";  [userClient loginWithRequest:req handler:^(LoginResponse * _Nullable response, NSError * _Nullable error) {      if (!error) {          if (response.baseResp.code == 0) {              NSLog(@"%@",response.user.name);          }else{              NSLog(@"error :%@",response.baseResp.msg);          }      }else{          NSLog(@"%@",error);      }  }];</code></pre><p>调用上面方法可以看到能够正常返回数据。Over</p>]]></content>
      
      
      <categories>
          
          <category> Frontend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> iOS </tag>
            
            <tag> GRPC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>跨域请求的几种解决方案</title>
      <link href="/2018/07/12/csrf/"/>
      <url>/2018/07/12/csrf/</url>
      
        <content type="html"><![CDATA[<h2 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h2><p>最近做的Apigate优化，前端的同学要求能在配置后台页面上加上一键测试接口的功能，但是由于浏览器的<a href="https://baike.baidu.com/item/%E5%90%8C%E6%BA%90%E7%AD%96%E7%95%A5/3927875?fr=aladdin">同源策略</a>防止<a href="https://baike.baidu.com/item/CSRF/2735433">跨域攻击</a>，所以前端的页面默认是不能请求其他域名的接口。</p><h2 id="方案一-Nginx配置代理"><a href="#方案一-Nginx配置代理" class="headerlink" title="方案一 Nginx配置代理"></a>方案一 Nginx配置代理</h2><pre><code>location /proxy {   if ($arg_url) {      proxy_pass $arg_url?;    } }</code></pre><p>最开始为了简单就配置了一个简单的代理，通过url传入想要访问的接口例如：</p><pre><code>http://nginxserver/proxy?url=http://10.23.39.140:8080/app/list</code></pre><p>这样前端需要什么测试什么接口只需要通过url传过来，Nginx会方向代理到对应的url上返回结果。</p><p>但是这个方法有个问题，url中的地址支持IP访问，不支持域名的接口访问，在测试环境还可以，线上环境就不支持了，所以Pass了。</p><h2 id="方案二-JSONP"><a href="#方案二-JSONP" class="headerlink" title="方案二 JSONP"></a>方案二 JSONP</h2><blockquote><p>JSONP(JSON with Padding)是JSON的一种“使用模式”，可用于解决主流浏览器的跨域数据访问的问题。由于同源策略，一般来说位于 server1.example.com 的网页无法与不是 server1.example.com的服务器沟通，而 HTML的<code>&lt;script&gt;</code>元素是一个例外。利用<code> &lt;script&gt;</code> 元素的这个开放策略，网页可以得到从其他来源动态产生的 JSON 资料，而这种使用模式就是所谓的 JSONP。用 JSONP 抓到的资料并不是 JSON，而是任意的JavaScript，用 JavaScript 直译器执行而不是用 JSON 解析器解析。</p></blockquote><p>说白了就是利用    <code>&lt;script&gt;</code>的<code>src</code>可以跨域的属性，使用接口返回js函数包装的数据</p><pre><code>&lt;script type="text/javascript" src="http://www.yiwuku.com/myService.aspx?jsonp=callbackFunction"&gt;&lt;/script&gt;</code></pre><p>假设正常数据返回 { “age” : 15, “name”: “John”, }<br>JSONP 就返回一个js包装数据的函数 jsonhandle({ “age” : 15, “name”: “John”, })</p><p>这种方案需要修改现有接口，Apigate的接口都是对外提供的，肯定不能改成这种格式，所以Pass。</p><h2 id="方案三-Access-Control-Allow-Origin"><a href="#方案三-Access-Control-Allow-Origin" class="headerlink" title="方案三 Access-Control-Allow-Origin"></a>方案三 Access-Control-Allow-Origin</h2><h4 id="Nginx配置"><a href="#Nginx配置" class="headerlink" title="Nginx配置"></a>Nginx配置</h4><p>只需要在Nginx的配置文件中配置以下参数：</p><pre><code>location / {    add_header Access-Control-Allow-Origin *;  add_header Access-Control-Allow-Headers "Origin, X-Requested-With, Content-Type, Accept";  add_header Access-Control-Allow-Methods "GET, POST, OPTIONS";} </code></pre><ol><li><p>Access-Control-Allow-Origin<br>服务器默认是不被允许跨域的。给Nginx服务器配置Access-Control-Allow-Origin *后，表示服务器可以接受所有的请求源（Origin）,即接受所有跨域的请求。</p></li><li><p>Access-Control-Allow-Headers 是为了防止出现以下错误：<br>Request header field Content-Type is not allowed by Access-Control-Allow-Headers in preflight response.这个错误表示当前请求Content-Type的值不被支持。其实是我们发起了”application/json”的类型请求导致的。这里涉及到一个概念：预检请求（preflight request）,请看下面”预检请求”的介绍。</p></li><li><p>Access-Control-Allow-Methods 是为了防止出现以下错误：<br>Content-Type is not allowed by Access-Control-Allow-Headers in preflight response.<br>发送”预检请求”时，需要用到方法 OPTIONS ,所以服务器需要允许该方法。</p></li></ol><h5 id="代码中控制"><a href="#代码中控制" class="headerlink" title="代码中控制"></a>代码中控制</h5><p>在代码层面，我们可以控制什么接口允许跨域，什么接口不允许跨域，这样对测试层面来说更灵活一些。</p><pre><code>// 在正式跨域的请求前，浏览器会根据需要，发起一个“PreFlight”//（也就是Option请求），用来让服务端返回允许的方法（如get、post），// 被跨域访问的Origin（来源，或者域），还有是否需要Credentials(认证信息）r.OPTIONS("/*allpath", func(c *gin.Context) {    origin := c.GetHeader("Origin")    c.Header("Access-Control-Allow-Origin", origin)    c.Header("Access-Control-Allow-Headers", "Origin, X-Requested-With, Content-Type, Accept")    c.Header("Access-Control-Allow-Methods", "GET, POST, OPTIONS")    c.Header("Access-Control-Allow-Credentials", "true")    c.String(http.StatusOK, "ok")})router.GET("/", func(c *gin.Context) {    origin := c.GetHeader("Origin")    c.Header("Access-Control-Allow-Origin", origin)    c.Header("Access-Control-Allow-Headers", "Origin, X-Requested-With, Content-Type, Accept")    c.Header("Access-Control-Allow-Methods", "GET, POST, OPTIONS")    c.Header("Access-Control-Allow-Credentials", "true")    c.String(http.StatusOK, "Hello World")})</code></pre><p>比如上面我只在测试环境下允许所有的Apigate接口跨域。</p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Golang </tag>
            
            <tag> HTTP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>概率论基础</title>
      <link href="/2018/06/23/probability/"/>
      <url>/2018/06/23/probability/</url>
      
        <content type="html"><![CDATA[<h2 id="样本空间与事件"><a href="#样本空间与事件" class="headerlink" title="样本空间与事件"></a>样本空间与事件</h2><p>A⊂B 事件 B 包含了事件 A，A 被包含于 B</p><p>A∩B A 与 B 的交集，表示事件 A 和事件 B 同时发生</p><p>A∪B A 与 B 的并集，表示事件 A 或事件 B 或他们二者同事发生</p><p>P(A|B) 在事件 B 发生下，事件 A 发生的概率</p><p><img src="/2018/06/23/probability/glsj.png"></p><h2 id="对偶公式"><a href="#对偶公式" class="headerlink" title="对偶公式"></a>对偶公式</h2><p><img src="/2018/06/23/probability/dogs.png"></p><h2 id="随机事件运算"><a href="#随机事件运算" class="headerlink" title="随机事件运算"></a>随机事件运算</h2><p>（1）交换律：A∪B=B∪A、AB=BA</p><p>（2）结合律：( A∪B )∪C=A∪( B∪C )</p><p>（3）分配律：A∪( BC )=( A∪B )( A∪C )A( B∪C )=( AB )∪( AC )</p><p>（4）摩根律：A B=A∪B、A ∪ B=A B</p><h2 id="概率的三个基本性质"><a href="#概率的三个基本性质" class="headerlink" title="概率的三个基本性质"></a>概率的三个基本性质</h2><p><img src="/2018/06/23/probability/gltz.png"></p><h2 id="二项系数"><a href="#二项系数" class="headerlink" title="二项系数"></a>二项系数</h2><p><img src="/2018/06/23/probability/exs2.png"></p><h2 id="蒙特卡罗（Monte-Carlo）"><a href="#蒙特卡罗（Monte-Carlo）" class="headerlink" title="蒙特卡罗（Monte Carlo）"></a>蒙特卡罗（Monte Carlo）</h2><p><img src="/2018/06/23/probability/mtkl.png"></p><h2 id="乘法公式"><a href="#乘法公式" class="headerlink" title="乘法公式"></a>乘法公式</h2><p>P(AB)=P(B)P(A|B) = P(A)P(B|A)</p><h2 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a>全概率公式</h2><p><img src="/2018/06/23/probability/qggs.png"></p><h2 id="贝叶斯公式（Bayes）"><a href="#贝叶斯公式（Bayes）" class="headerlink" title="贝叶斯公式（Bayes）"></a>贝叶斯公式（Bayes）</h2><p><img src="/2018/06/23/probability/bysgs.png"></p>]]></content>
      
      
      <categories>
          
          <category> Maths </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Probability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>微积分基础</title>
      <link href="/2018/06/22/calculus/"/>
      <url>/2018/06/22/calculus/</url>
      
        <content type="html"><![CDATA[<h2 id="导数-Derivatives"><a href="#导数-Derivatives" class="headerlink" title="导数(Derivatives)"></a>导数(Derivatives)</h2><blockquote><p>导数是函数的局部性质。一个函数在某一点的导数描述了这个函数在这一点附近的变化率。如果函数的自变量和取值都是实数的话，函数在某一点的导数就是该函数所代表的曲线在这一点上的切线斜率。导数的本质是通过极限的概念对函数进行局部的线性逼近。例如在运动学中，物体的位移对于时间的导数就是物体的瞬时速度。</p></blockquote><p>一阶导数为 0 的时候，对应的值不是最大值（局部）就是最小值（局部）。</p><h2 id="求导规则"><a href="#求导规则" class="headerlink" title="求导规则"></a>求导规则</h2><p>加（减）法则：<code>（f+g)'=f'+g'</code></p><p>乘法法则：<code>（f*g)'=f'*g+g'*f</code></p><p>除法法则：<code>（f/g)'=(f'*g-g'*f)/g^2</code></p><p><code>y = x^n =&gt; dy/dx = nx^n-1</code></p><p><code>y = sin(x) =&gt; dy/dx = cos(x)</code></p><p><code>y = e^x =&gt; dy/dx = e^x</code></p><p><code>y = lnx =&gt; dy/dx = 1/x</code></p><pre><code>(lnx)'=lim[h→0] [ln(x+h)-lnx]/h=lim[h→0] ln[(x+h)/x]/h=lim[h→0] ln(1+h/x)/hln(1+h/x)与h/x等价,用等价无穷小代换=lim[h→0] (h/x) / h=1/x</code></pre><ol><li><code>C'=0(C为常数)；</code></li><li><code>(Xn)'=nX(n-1) (n∈R)；</code></li><li><code>(sinX)'=cosX；</code></li><li><code>(cosX)'=-sinX；</code></li><li><code>(aX)'=aXIna （ln为自然对数)；</code></li><li><code>(logaX)'=（1/X)logae=1/(Xlna) (a&gt;0，且a≠1)；</code></li><li><code>(tanX)'=1/(cosX)2=(secX)2</code></li><li><code>(cotX)'=-1/(sinX)2=-(cscX)2</code></li><li><code>(secX)'=tanX secX；</code></li><li><code>(cscX)'=-cotX cscX； [1] </code></li></ol><h2 id="二阶导数-（Second-Derivative）"><a href="#二阶导数-（Second-Derivative）" class="headerlink" title="二阶导数 （Second Derivative）"></a>二阶导数 （Second Derivative）</h2><blockquote><p>二阶导数，是原函数导数的导数，将原函数进行二次求导。一般的，函数y=f（x）的导数y‘=f’（x）仍然是x的函数，则y’=f‘（x）的导数叫做函数y=f（x）的二阶导数。在图形上，它主要表现函数的凹凸性。 [1] </p></blockquote><p>dy/dx = cos(x) =&gt; -sin(x)</p><p>f’’ &gt; 0 定义为凸(convex)对应最小值(局部最小)，f’’&lt; 0 的时候定义为凹(concave)对应最大值（局部最大）</p><h3 id="二阶导数几何意义"><a href="#二阶导数几何意义" class="headerlink" title="二阶导数几何意义"></a>二阶导数几何意义</h3><p>（1）切线斜率变化的速度，表示的是一阶导数的变化率。</p><p>（2）函数的凹凸性（例如加速度的方向总是指向轨迹曲线凹的一侧）。 </p><pre><code>这里以物理学中的瞬时加速度为例:  根据定义有可如果加速度并不是恒定的，某点的加速度表达式就为：a=limΔt→0 Δv/Δt=dv/dt(即速度对时间的一阶导数)又因为v=dx/dt 所以就有：a=dv/dt=d²x/dt² 即元位移对时间的二阶导数将这种思想应用到函数中 即是数学所谓的二阶导数f'(x)=dy/dx (f(x)的一阶导数）f''(x)=d²y/dx²=d(dy/dx)/dx (f(x)的二阶导数)</code></pre><h2 id="拐点-（Inflection-Point）"><a href="#拐点-（Inflection-Point）" class="headerlink" title="拐点 （Inflection Point）"></a>拐点 （Inflection Point）</h2><blockquote><p>拐点，又称反曲点，在数学上指改变曲线向上或向下方向的点，直观地说拐点是使切线穿越曲线的点（即曲线的凹凸分界点）。若该曲线图形的函数在拐点有二阶导数，则二阶导数在拐点处异号（由正变负或由负变正）或不存在。</p></blockquote><p>拐点就是二阶导数为 0 的点</p><p>比较所有驻点（f’=0）处及边界点函数值，得到的最大或者最小值，即函数最值</p><h2 id="epsilon-delta"><a href="#epsilon-delta" class="headerlink" title="epsilon-delta"></a>epsilon-delta</h2><blockquote><p>就是数学分析（历史上称为“无穷小分析”）中用来严格定义极限概念的数学语言，它避免了早期微积分使用直观无穷小概念时在逻辑上产生的混乱，从而为微积分理论建立了坚实的逻辑基础。<br>ε-δ（epsilon-delta）语言的例子：<br>一元实函数在 x0 点“连续”概念的定义：<br>设 f(x)是实数集 R 上的函数，若对任意给定的数 ε &gt; 0，总存在数 δ &gt; 0，当<br>|x - x0| &lt;δ 时，有|f(x) - f(x0)| &lt;ε，则称函数 f(x)在 x0 点连续。 [1]<br>这种定义方法使得微积分的基本概念（如极限、连续、导数等）不再依赖于“无穷小”这个含混不清的说法，而是用不等式的语言确切地描述出来（并且是可验证的）。因而使微积分理论严密起来。<br>与 ε - δ 语言类似的是 N - δ 语言。它是用来定义数列极限的严密化语言，思想是完全相同的。</p></blockquote><blockquote><p>用“窄带”(narrow band)的说法通俗地讲解了极限和连续的概念。所谓极限存在，就是不管取多窄的窄带，数列足够靠后的数字，都会落在窄带(A+ε,A-ε)之内。所谓函数连续，就是只要x足够接近a，就能保证f(x)足够接近f(a)。</p></blockquote><h2 id="逆函数（反函数）"><a href="#逆函数（反函数）" class="headerlink" title="逆函数（反函数）"></a>逆函数（反函数）</h2><blockquote><p>设函数y=f(x)的定义域是D，值域是f(D)。如果对于值域f(D)中的每一个y，在D中有且只有一个x使得f(x)=y，则按此对应法则得到了一个定义在f(D)上的函数，并把该函数称为函数y=f(x)的反函数，记为<br><img src="/2018/06/22/calculus/nhs.png"></p></blockquote><h2 id="线性近似和牛顿法"><a href="#线性近似和牛顿法" class="headerlink" title="线性近似和牛顿法"></a>线性近似和牛顿法</h2><p>所谓线性近似，也叫线性逼近，主要作用是把一个复杂的非线性函数用一个简单的线性函数来表示。</p><p>假设一般函数上存在点(a, f(a))，当x接近a时，可以使用函数在a点的切线作为函数的近似线。函数L(x)≈f(a)+f’(a)(x-a)即称为函数f在a点的线性近似或切线近似</p><p>例如，有一个实数变量的可导函数f，根据n=1的泰勒公式，</p><p><img src="/2018/06/22/calculus/xxjs1.png"></p><p>其中  是余数。舍去余数就是线性近似：</p><p><img src="/2018/06/22/calculus/xxjs2.png"></p><p>当x无限接近于a的时候这个等式成立。</p><h3 id="常用线性近似公式"><a href="#常用线性近似公式" class="headerlink" title="常用线性近似公式"></a>常用线性近似公式</h3><p><img src="/2018/06/22/calculus/xxjs4.png"></p><h2 id="泰勒公式"><a href="#泰勒公式" class="headerlink" title="泰勒公式"></a>泰勒公式</h2><blockquote><p>泰勒公式是将一个在x=x0处具有n阶导数的函数f(x)利用关于(x-x0)的n次多项式来逼近函数的方法。<br>若函数f(x)在包含x0的某个闭区间[a,b]上具有n阶导数，且在开区间(a,b)上具有(n+1)阶导数，则对闭区间[a,b]上任意一点x，成立下式：<br><img src="/2018/06/22/calculus/tlgs.png"></p></blockquote><h2 id="二阶常系数线性微分方程"><a href="#二阶常系数线性微分方程" class="headerlink" title="二阶常系数线性微分方程"></a>二阶常系数线性微分方程</h2><p>二阶常系数线性微分方程是形如y’’+py’+qy=f(x)的微分方程，其中p，q是实常数。自由项f(x)为定义在区间I上的连续函数，即y’’+py’+qy=0时，称为二阶常系数齐次线性微分方程。若函数y1和y2之比为常数，称y1和y2是线性相关的；若函数y1和y2之比不为常数，称y1和y2是线性无关的。特征方程为：λ^2+pλ+q=0，然后根据特征方程根的情况对方程求解。</p>]]></content>
      
      
      <categories>
          
          <category> Maths </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Calculus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计学基础</title>
      <link href="/2018/06/19/statistics/"/>
      <url>/2018/06/19/statistics/</url>
      
        <content type="html"><![CDATA[<p>最近休息在家无聊，整理下之前看的统计学的一些基础知识，方便以后查阅吧。</p><h2 id="基础名词"><a href="#基础名词" class="headerlink" title="基础名词"></a>基础名词</h2><ul><li>均值 （Mean），所有数相加出去数量。</li><li>中位数 （Median）， 中间一个数，或者是中间的两个数相加除以 2</li><li>众数 （Mode），出现次数最多的数。</li><li>极差 （Range），最大值减去最小值（Max-Min）</li><li>中程数（Mid-Range） （Max+Min）/ 2</li><li>样本 （sameple）</li><li>总体 （population）</li><li>总体的均值 （mean of a population） 𝝻./</li><li>样本的均值 (mean of a sameple)</li><li>总体方差 （variance of a population）</li><li>样本方差（Sample variance）</li></ul><h2 id="基础概念和公式"><a href="#基础概念和公式" class="headerlink" title="基础概念和公式"></a>基础概念和公式</h2><h3 id="基础概念对应的数学符号："><a href="#基础概念对应的数学符号：" class="headerlink" title="基础概念对应的数学符号："></a>基础概念对应的数学符号：</h3><p><img src="/2018/06/19/statistics/mean0.png"></p><h3 id="总体均值（Population-Mean）和样本均值（Sample-Mean）公式："><a href="#总体均值（Population-Mean）和样本均值（Sample-Mean）公式：" class="headerlink" title="总体均值（Population Mean）和样本均值（Sample Mean）公式："></a>总体均值（Population Mean）和样本均值（Sample Mean）公式：</h3><p><img src="/2018/06/19/statistics/pm.png"></p><h3 id="总体方差（variance-of-a-population）公式："><a href="#总体方差（variance-of-a-population）公式：" class="headerlink" title="总体方差（variance of a population）公式："></a>总体方差（variance of a population）公式：</h3><p><img src="/2018/06/19/statistics/vp.jpg"></p><p><img src="/2018/06/19/statistics/vp2.jpg"></p><h3 id="样本方差（Sample-variance）"><a href="#样本方差（Sample-variance）" class="headerlink" title="样本方差（Sample variance）"></a>样本方差（Sample variance）</h3><p><img src="/2018/06/19/statistics/sv1.png"></p><h3 id="无偏样本方差-（unbiased-sameple-variance）"><a href="#无偏样本方差-（unbiased-sameple-variance）" class="headerlink" title="无偏样本方差 （unbiased sameple variance）"></a>无偏样本方差 （unbiased sameple variance）</h3><p>刚开始接触这个公式的话可能会有一个疑问就是：为什么样本方差要除以（n-1）而不是除以 n？为了解决这个疑惑，我们需要具备一点统计学的知识基础，关于总体、样本、期望（均值）、方差的定义以及统计估计量的评选标准。有了这些知识基础之后，我们会知道样本方差之所以要除以（n-1）是因为这样的方差估计量才是关于总体方差的无偏估计量。这个公式是通过修正下面的方差计算公式而来的：</p><p><img src="/2018/06/19/statistics/sv3.png"></p><p><a href="https://blog.csdn.net/Hearthougan/article/details/77859173">彻底理解样本方差为何除以 n-1</a></p><h3 id="总体标准差-（Population-Standard-Deviation）"><a href="#总体标准差-（Population-Standard-Deviation）" class="headerlink" title="总体标准差 （Population Standard Deviation）"></a>总体标准差 （Population Standard Deviation）</h3><p><img src="/2018/06/19/statistics/psd1.jpg"></p><h3 id="样本标准差-（Sample-Standard-Deviation）"><a href="#样本标准差-（Sample-Standard-Deviation）" class="headerlink" title="样本标准差 （Sample Standard Deviation）"></a>样本标准差 （Sample Standard Deviation）</h3><p><img src="/2018/06/19/statistics/psd2.png"></p><h3 id="样本均值的抽样分布"><a href="#样本均值的抽样分布" class="headerlink" title="样本均值的抽样分布"></a>样本均值的抽样分布</h3><p>无限总体，样本均值的方差为总体方差的 1/n，即 <img src="/2018/06/19/statistics/fc1.png"></p><p>有限总体，样本均值的方差为 <img src="/2018/06/19/statistics/fc2.png"> (x 为平均数)</p><p><a href="https://wapbaike.baidu.com/item/%E6%A0%B7%E6%9C%AC%E5%9D%87%E5%80%BC%E7%9A%84%E6%8A%BD%E6%A0%B7%E5%88%86%E5%B8%83">样本均值的抽样分布</a></p><p>ps：这里的样本均值，是指抽离多个样本的均值，不是单个样本的均值 ！！！</p><h3 id="随机变量-（Random-Variable）"><a href="#随机变量-（Random-Variable）" class="headerlink" title="随机变量 （Random Variable）"></a>随机变量 （Random Variable）</h3><p>随机变量（random variable）表示随机试验各种结果的实值单值函数。随机事件不论与数量是否直接有关，都可以数量化，即都能用数量化的方式表达</p><h3 id="概率密度函数-（probability-density-function）"><a href="#概率密度函数-（probability-density-function）" class="headerlink" title="概率密度函数 （probability density function）"></a>概率密度函数 （probability density function）</h3><p>在数学中，连续型随机变量的概率密度函数（在不至于混淆时可以简称为密度函数）是一个描述这个随机变量的输出值，在某个确定的取值点附近的可能性的函数。而随机变量的取值落在某个区域之内的概率则为概率密度函数在这个区域上的积分。当概率密度函数存在的时候，累积分布函数是概率密度函数的积分。概率密度函数一般以小写标记。</p><h3 id="二项式定理-Binomial-theorem"><a href="#二项式定理-Binomial-theorem" class="headerlink" title="二项式定理 (Binomial theorem)"></a>二项式定理 (Binomial theorem)</h3><p><img src="/2018/06/19/statistics/exs1.png"></p><p><img src="/2018/06/19/statistics/exs3.png"></p><p><img src="/2018/06/19/statistics/exs2.png"></p><p>组合</p><p><img src="/2018/06/19/statistics/zh.png"></p><p>排列</p><p><img src="/2018/06/19/statistics/pl.png"></p><h3 id="二项式分布-（Binomial-coefficients）"><a href="#二项式分布-（Binomial-coefficients）" class="headerlink" title="二项式分布 （Binomial coefficients）"></a>二项式分布 （Binomial coefficients）</h3><p><img src="/2018/06/19/statistics/exsfb.jpg"></p><h3 id="期望-（Expected-value）"><a href="#期望-（Expected-value）" class="headerlink" title="期望 （Expected value）"></a>期望 （Expected value）</h3><p><strong>期望公式：</strong></p><p><img src="/2018/06/19/statistics/qw1.png"></p><p><img src="/2018/06/19/statistics/qw2.png"></p><p>E(X) = np (若 X 服从二项分布 B(n,p))</p><p><img src="/2018/06/19/statistics/exnp.png"></p><p><a href="https://blog.csdn.net/Michael_liuyu09/article/details/77943898">频率和概率以及均值和期望的联系区别</a></p><h3 id="泊松分布-poisson-distribution"><a href="#泊松分布-poisson-distribution" class="headerlink" title="泊松分布 (poisson distribution)"></a>泊松分布 (poisson distribution)</h3><p>泊松分布适合于描述单位时间内随机事件发生的次数的概率分布。如某一服务设施在一定时间内受到的服务请求的次数，电话交换机接到呼叫的次数、汽车站台的候客人数、机器出现的故障数、自然灾害发生的次数、DNA 序列的变异数、放射性原子核的衰变数等等。</p><p><img src="/2018/06/19/statistics/psfb1.png"></p><p><img src="/2018/06/19/statistics/psfb2.jpg"></p><p>泊松分布的参数 λ 是单位时间(或单位面积)内随机事件的平均发生率。 泊松分布适合于描述单位时间内随机事件发生的次数。</p><h3 id="泊松过程-（Poisson-process）"><a href="#泊松过程-（Poisson-process）" class="headerlink" title="泊松过程 （Poisson process）"></a>泊松过程 （Poisson process）</h3><p>实验结果满足泊松分布的实验即为泊松过程。</p><p><img src="/2018/06/19/statistics/psgc.png"></p><p><a href="https://zh.wikipedia.org/wiki/%E6%B3%8A%E6%9D%BE%E8%BF%87%E7%A8%8B">泊松过程</a></p><p><a href="https://www.cnblogs.com/jwmeng/p/7698651.html">泊松分布、泊松过程、泊松点过程</a></p><h3 id="大数定律-law-of-large-numbers"><a href="#大数定律-law-of-large-numbers" class="headerlink" title="大数定律(law of large numbers)"></a>大数定律(law of large numbers)</h3><p>是一种描述当试验次数很大时所呈现的概率性质的定律。但是注意到，大数定律并不是经验规律，而是在一些附加条件上经严格证明了的定理，它是一种自然规律因而通常不叫定理而是大数“定律”。而我们说的大数定理通常是经数学家证明并以数学家名字命名的大数定理，如伯努利大数定理 [2] 。<br>（抛硬币概率在测试次数很多的时候正反的概率应该都趋势与.5）</p><h3 id="正态分布（Normal-distribution-又名高斯分布（Gaussian-distribution"><a href="#正态分布（Normal-distribution-又名高斯分布（Gaussian-distribution" class="headerlink" title="正态分布（Normal distribution)又名高斯分布（Gaussian distribution)"></a>正态分布（Normal distribution)又名高斯分布（Gaussian distribution)</h3><p><strong>样本值落在两个标准差范围内的概率是 95.4%</strong></p><p><img src="/2018/06/19/statistics/ztfb.png"></p><p><img src="/2018/06/19/statistics/ztfb2.png"></p><h3 id="中心极限定理-（central-limit-theorem）"><a href="#中心极限定理-（central-limit-theorem）" class="headerlink" title="中心极限定理 （central limit theorem）"></a>中心极限定理 （central limit theorem）</h3><p>中心极限定理，是指概率论中讨论随机变量序列部分和分布渐近于正态分布的一类定理。这组定理是数理统计学和误差分析的理论基础，指出了大量随机变量近似服从正态分布的条件。它是概率论中最重要的一类定理，有广泛的实际应用背景。在自然界与生产中，一些现象受到许多相互独立的随机因素的影响，如果每个因素所产生的影响都很微小时，总的影响可以看作是服从正态分布的。中心极限定理就是从数学上证明了这一现象。最早的中心极限定理是讨论重点，伯努利试验中，事件 A 出现的次数渐近于正态分布的问题。</p><h3 id="伯努利分布"><a href="#伯努利分布" class="headerlink" title="伯努利分布"></a>伯努利分布</h3><p>伯努利分布亦称“零一分布”、“两点分布”。称随机变量 X 有伯努利分布, 参数为 p(0&lt;p&lt;1),如果它分别以概率 p 和 1-p 取 1 和 0 为值。EX= p,DX=p(1-p)。伯努利试验成功的次数服从伯努利分布,参数 p 是试验成功的概率。伯努利分布是一个离散型机率分布，是 N=1 时二项分布的特殊情况，为纪念瑞士科学家詹姆斯·伯努利(Jacob Bernoulli 或 James Bernoulli)而命名。</p><ul><li><p>其概率质量函数为：</p><p><img src="/2018/06/19/statistics/bnl1.png"></p></li></ul><ul><li>期望： E[x] = p<br><img src="/2018/06/19/statistics/bnl2.png"></li></ul><ul><li><p>方差 ： var[X]=p(1-p)</p><p><img src="/2018/06/19/statistics/bnl3.png"></p></li></ul><h3 id="Z-统计量-（Z-statistic）和-T-统计量-T-statistic"><a href="#Z-统计量-（Z-statistic）和-T-统计量-T-statistic" class="headerlink" title="Z 统计量 （Z-statistic）和 T 统计量(T-statistic)"></a>Z 统计量 （Z-statistic）和 T 统计量(T-statistic)</h3><p>公式: <img src="/2018/06/19/statistics/zgs.png"></p><p>样本数量大于 n &gt; 30 用 Z-table，反之用 T-Table</p><h3 id="第一类型错误（Type-1-Error）"><a href="#第一类型错误（Type-1-Error）" class="headerlink" title="第一类型错误（Type 1 Error）"></a>第一类型错误（Type 1 Error）</h3><p>在进行假设检验时，由于检验统计量是随机变量，有一定的波动性，即使原假设 H0 为真，在正常的情况下，计算的统计量仍有一定的概率 α(α 称为显著性水平)落入拒绝域内，因此也有可能会错误地拒绝原假设 H0，这种当原假设 H0 为真而拒绝原假设的错误，称为假设检验的第一类错误，又称为拒真错误。</p><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a><strong>线性回归</strong></h3><p>线性回归方程是利用数理统计中的回归分析，来确定两种或两种以上变数间相互依赖的定量关系的一种统计分析方法之一。线性回归也是回归分析中第一种经过严格研究并在实际应用中广泛使用的类型。按自变量个数可分为一元线性回归分析方程和多元线性回归分析方程。</p><p><img src="/2018/06/19/statistics/xxhg.jpg"></p><h3 id="R2-参数"><a href="#R2-参数" class="headerlink" title="R2 参数"></a>R2 参数</h3><p><img src="/2018/06/19/statistics/r2.png"></p><h3 id="协方差和回归线"><a href="#协方差和回归线" class="headerlink" title="协方差和回归线"></a>协方差和回归线</h3><p>在概率论和统计学中，协方差用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。 [1]</p><p>期望值分别为 E[X]与 E[Y]的两个实随机变量 X 与 Y 之间的协方差 Cov(X,Y)定义为：</p><p>Cov(x,y) = E[(x - E[x])(y - E(y))]<br>= E[xy] - 2E[y]E[x] + E[x]E[y]<br>= E[xy] - E[y]E[x]<br>从直观上来看，协方差表示的是两个变量总体误差的期望。</p><p>如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值时另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值；如果两个变量的变化趋势相反，即其中一个变量大于自身的期望值时另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。</p><p>如果 X 与 Y 是统计独立的，那么二者之间的协方差就是 0，因为两个独立的随机变量满足 E[XY]=E[X]E[Y]。</p><p>但是，反过来并不成立。即如果 X 与 Y 的协方差为 0，二者并不一定是统计独立的。</p><p>协方差 Cov(X,Y)的度量单位是 X 的协方差乘以 Y 的协方差。而取决于协方差的相关性，是一个衡量线性独立的无量纲的数。</p><p>协方差为 0 的两个随机变量称为是不相关的。</p><p>回归线斜率 ： m = Cov(X,Y)/Var(X)</p><h3 id="卡方检验-x-2-chi-square-分布"><a href="#卡方检验-x-2-chi-square-分布" class="headerlink" title="卡方检验 x^2 (chi square)分布"></a>卡方检验 x^2 (chi square)分布</h3><p>卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合，若两个值完全相等时，卡方值就为 0，表明理论值完全符合。</p><p><img src="/2018/06/19/statistics/chisquare.png"></p><h3 id="F-统计量-（F-statistic）"><a href="#F-统计量-（F-statistic）" class="headerlink" title="F 统计量 （F-statistic）"></a>F 统计量 （F-statistic）</h3><p>F 检验（F-test），最常用的别名叫做联合假设检验（英语：joint hypotheses test），此外也称方差比率检验、方差齐性检验。它是一种在零假设（null hypothesis, H0）之下，统计值服从 F-分布的检验。其通常是用来分析用了超过一个参数的统计模型，以判断该模型中的全部或一部分参数是否适合用来估计母体。</p>]]></content>
      
      
      <categories>
          
          <category> Maths </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Statistics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《四书五经入门》</title>
      <link href="/2018/06/10/si-shu-wu-jin/"/>
      <url>/2018/06/10/si-shu-wu-jin/</url>
      
        <content type="html"><![CDATA[<h1 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h1><p>儒家学说，简称儒学，它的核心内容是儒家思想，由春秋末期思想家孔子所创立。孔子在总结、概括和继承了夏、商、周三代传统文化的基础上形成了一个完整的思想体系，它的表现形式是一些道德行为准则。</p><ul><li>在政治学上，主张仁政、行王道，通过礼来理顺君臣、官民和各种社会关系；</li><li>在伦理学上，以“仁”为核心，强调通过提高自身修养来实现人与人之间和谐相处；</li><li>在历史学上，注重编修历史，并于记叙中体现微言大义；</li><li>在经济学上，主张重义轻利、重官轻商、重本抑末；</li><li>在教育学上，格外强调道德培养的重要性。</li></ul><p>并把“仁”作为最高道德准则……儒家学说的创立者以孔子和孟子为代表，其中的孔子被尊为一代宗师。后来的各代中虽各有发展，但都以孔子和孟子的思想为核心。</p><p>“四书五经”基本上都是由孔子与他的弟子及传袭者编纂完成，所体现的都是孔子所尊创的儒家思想。孔子所生活的年代，礼崩乐坏，诸侯纷争，社会动荡不安。孔子的心性中，有心怀天下的抱负，而且他在年少时，就对国家治理的各种问题发表见解，进行思考。他也曾希望通过仕途之路来为国效力，但始终于仕途上不得志，只做了些小官。后来，因鲁国发生内乱，他不得不逃往齐国。到了47岁时，他退出了官场开始修著《诗经》《尚书》《礼记》《乐》，并率领众多弟子走遍各国宣扬自己的儒家思想。到了68岁时，他才结束周游列国的行动。所以说，孔子的一生，都过着颠沛流离的生活。这些经历，也成就了孔子思想的迸发。由此，我们也就知道了“正心修身齐家治国平天下”的事业前行的阶梯，“天人合一”思想的精髓，“朝闻道，夕死可矣”的精神超脱……从另一个角度来看，得益于孔子，我们还能领略到纯真的生活之美。“关关雎鸠，在河之洲；窈窕淑女，君子好逑。”这是一种心灵之美。“桃枝夭夭，灼灼其华；之子于归，宜其室家。”这何尝不是一种情感之美！</p><h1 id="二、《中庸》：中者填下之正道，庸者填下之定理"><a href="#二、《中庸》：中者填下之正道，庸者填下之定理" class="headerlink" title="二、《中庸》：中者填下之正道，庸者填下之定理"></a>二、《中庸》：中者填下之正道，庸者填下之定理</h1><p>宋朝理学家程颐的胞弟程伊川说：“<code>不偏之谓中，不易之谓庸；中者，天下之正道；庸者，天下之定理。</code>“</p><p>这段话包括两层意思：其一，中不偏，庸不易；其二，中正平和，守成坚持。解读开来就是中庸的中心思想，即大家熟悉的中庸之道。中庸之道的理论基础是<code>天人合一</code>，主旨在于<code>修养人性</code>。既包括学习的方式：“<code>博学之，审问之，慎思之，明辨之，笃行之</code>”，也包括儒家做人的规范，如“<code>五达道</code>”“<code>三达德</code>”和“<code>九经</code>”等。</p><h2 id="五达道"><a href="#五达道" class="headerlink" title="五达道"></a>五达道</h2><p>中庸之道中的“<code>五达道</code>”，即五典，指的是<code>君臣</code>、<code>父子</code>、<code>夫妻</code>、<code>兄弟</code>和<code>朋友</code>这五种基本人际关系，语出《礼记·中庸》第二十章：“天下之达道五……曰君臣也、父子也、夫妇也、昆弟也、朋友之交也。五者天下之达道也。”在《中庸》的第十三章，论述了父子、君臣、兄弟、朋友之达道。“君子之道四，丘未能一焉：所求乎子，以事父，未能也；所求乎臣，以事君，未能也；所求乎弟，以事兄，未能也；所求乎朋友，先施之，未能也。庸德之行，庸言之谨；有所不足，不敢不勉；有余，不敢尽。言顾行，行顾言。君子胡不慥慥尔。</p><p>”这是孔子所说的话，他所要表达的意思是：君子所应遵从的中庸之道有四项，但他孔子连其中的一项也没能做到：这些“道”就是：</p><ul><li>儿子应对父母孝顺；</li><li>臣民应对君王忠诚；</li><li>弟弟要对哥哥恭敬；</li><li>对待朋友要先替人家着想；</li><li>一个人行走于社会上不改变品德，与人言谈时不改变谨慎；</li><li>自己有不足之处，就去勉励自己改正弥补；</li><li>有宽余的时候，不去挥霍。言谈时考虑自己的能力，行动时顾及自己的言谈，力求言行一致。</li></ul><p>这样的人就是合乎中庸之道的君子。</p><p>至此，所谓的<code>五达道</code>，就是指通过奉行中庸之道来调节君臣、父子、夫妻、兄弟和朋友这五种人际关系。这其中，又把夫妇关系放到了立世之本的高度。</p><p>如何去实践中庸之道，就是对内要加强<code>自身修养</code>，对外要保持<code>谨言慎行</code>。不过，可以想象的是，如果完全按照孔子的要求去说话办事的话，这社会中的人与人之间必定会相处在没有纷争的安宁环境中。只是，如此一来，谁都没了锋芒，没了直言不公的勇气，失去了铲除邪恶的雄心，失去了冒险的精神，而这样社会还能不能前进，那又另当别论了。</p><h2 id="三达德"><a href="#三达德" class="headerlink" title="三达德"></a>三达德</h2><p>五达道着意在“<code>道</code>”，属人道的范围，讲的是人与人共有的五伦人际关系，如何处理好这些关系，就需要三种品德：<code>智、仁和勇</code>，即三达德。</p><p>《礼记·中庸》紧承五达道，写出了三达德：“……五者天下之达道也。<code>智</code>、<code>仁</code>、<code>勇</code>三者，天下之达德也。”也就是说，调节五伦人际关系，要靠人的<code>智慧</code>、<code>善良</code>和<code>勇于</code>担当这三种品德。这三种品德不管是怎么来的，只要拥有就无所谓差别。比如说，有的人生来就知道它们，有的人通过后天的学习才知道它们，有的人通过磨难后才知道它们，但只要最终都知道了它们，也就是一样的了。又比如说，有的人自觉自愿地去实行它们，有的人为了某种好处才去实行它们，有的人勉勉强强地去实行，但只要他们最终都实行起来了，也就是一样的了。孔子说：“<code>喜欢学习就接近了智，努力实行就接近了仁，知道羞耻就接近了勇。知道这三点，就知道怎样修养自己；知道怎样修养自己，就知道怎样管理他人；知道怎样管理他人，就知道怎样治理天下和国家了。</code>”</p><h2 id="九经"><a href="#九经" class="headerlink" title="九经"></a>九经</h2><p>《礼记·中庸》紧承三达道后，又提出九经之说：“<code>凡为天下国家有九经，曰：修身也，尊贤也，亲亲也，敬大臣也，体群臣也，子庶民也，来百工也，柔远人也，怀诸侯也。</code>”</p><p>这段话的意思是：治理天下和国家有九条原则，那就是：<code>修养自身，尊崇贤人，亲爱亲族，敬重大臣，体恤群臣，爱民如子，招纳工匠，优待远客，安抚诸侯</code>。为什么要这么做呢？</p><p>《礼记·中庸》接着说：“<code>修身，则道立。尊贤，则不惑。亲亲，则诸父昆弟不怨。敬大臣，则不眩。体群臣，则士之报礼重。子庶民，则百姓劝。来百工，则财用足。柔远人，则四方归之。怀诸侯，则天下畏之。</code>”</p><p>这段话的意思是：<code>修养自身，就能确立正道。尊崇贤人，就不会思想困惑。亲爱亲族，就不会惹得叔伯兄弟怨恨。敬重大臣，就不会遇事无措。体恤群臣，士人们就会竭力报效。爱民如子，老百姓就会忠心耿耿。招纳工匠，财物就会充足。优待远客，四方百姓就会归顺。安抚诸侯，天下的人都会敬畏。</code>怎么做到这些呢？</p><h2 id="中庸之道”对人的修养提出了哪些要求？"><a href="#中庸之道”对人的修养提出了哪些要求？" class="headerlink" title="中庸之道”对人的修养提出了哪些要求？"></a>中庸之道”对人的修养提出了哪些要求？</h2><p>中庸之道的精神实质就是教育人们从<code>本我出发</code>，自觉地进行<code>自我修养</code>、<code>自我完善</code>，从而培养出合乎儒家道德和伦理观的理想人格，让天下治理达到“<code>太平和合</code>”的境界。</p><p>“<code>天命之谓性，率性之谓道，修道之谓教。</code>”这句话的意思是：上天所赋予人的本质特性叫作本性（天性），遵循着本性来做人和处事叫作道，圣人的教化，就是遵循本性，来修正过与不及的差别现象，使一切事物都能合于正道，这就叫作教化。</p><p>在修养人性方面如何教化呢？下面罗列出《中庸》中的一些观点：</p><ol><li><p><code>慎独</code>。《中庸》在第一章写道：“<code>君子戒慎乎其所不睹，恐惧乎其所不闻。莫见乎隐，莫显乎微，故君子慎其独也。</code>” 这段话的意思是：君子在没有人看到的地方，更是小心谨慎。在没有人听到的地方，更是恐惧害怕。最隐蔽的地方，也是最容易被发现的处所。最微细的事物，也是最容易显露的，因此君子在一个人独处的时候，更要特别谨慎。</p></li><li><p><code>言顾行，行顾言</code>。这包含两个方面的内容：其一，<code>言行一致</code>。俗话说，君子一言，驷马难追；又说，言必信，行必果。从做人方面来说，就是要做到表里如一；从社会角度来说，就是要讲诚信。其二，<code>谨言慎行</code>。所谓谨言慎行，即指人在说话办事的时候要小心谨慎。</p></li><li><p><code>内省不疚，无恶于志</code>。《中庸》第三十三章中写道：“<code>君子内省不疚，无恶于志。君子之所不可及者，其唯人之所不见乎！</code>”这段话的意思是：君子只求内省时没有过失，无愧于心。君子之所以让人佩服，觉得赶不上，正是在这种别人看不见的地方。曾子说：“<code>吾日三省吾身</code>”在内省中，如果有什么让良心感到不安的东西，那么，这必然是个人修养方面存在缺失。面对物欲横流，面对种种诱惑，心被吸引在所难免。怎样才能做到端正自身呢？自然是接受道德教育，提升个人修养。通过严于律己，如果在内省中感到无愧于心，那么，这修养的最高境界也就达到了。</p></li><li><p><code>致中和</code>。《中庸》第一章中写道：“<code>喜怒哀乐之未发，谓之中；发而皆中节，谓之和。中也者，天下之大本也，和也者，天下之达道也。致中和，天地位焉，万物育焉。</code>”这段话的意思是：人的喜怒哀乐的情感还没有发生的时候，心是平静无所偏倚的，这就叫“中”；如果感情的发生都能合乎节度，没有过与不及就叫作“和”。“中”是天下万事万物的根本，“和”是天下共行的大道。如果能够把中和的道理推而及之，达到圆满的境界，那么天地万物，都能各安其所、各遂其生了。这是人的修养中的又一种境界。</p><p> 人之所以会出现喜怒哀乐的情感变化，一定是自身的利益受到了侵犯或伤害。在侵犯或伤害面前，如果能做到理性加理智地去处理，不以其人之道还治其人之身，那么，这天下也就没有纷争，万物都会回到自身的位置，清静无忧地顺应自然界的规律活动。这也就是《中庸》所盼望的“中”“和”的圆满境界。</p><p> 但是，物质世界中，利益的纠葛不仅是无处不在，也是无止无境的。要让人们远离纷扰，单靠行政的手段、法律的手段，那是远远不够的。生存的需要，欲望的需要，随时都会让人对利益有着执着的追求，这就会让行政和法律的干预变得防不胜防和力不从心。这样也就只能从人的本身出发，让人于内心中首先构筑起一道防护堤，远离物欲。这就是道德教育。通过道德教育，提升人的修养，从而让“中”“和”深入人心。</p><p> “中”与“和”可当做人的心性中的一部分，是后天接受的一种优良品质。放到社会层面上，“中”又可理解为不偏不倚、公允执正；“和”可理解为相处融洽、彼此相安。因而，当个人修养达到“中”“和”境界时，人际关系必定会达到一个圆满的境界；当社会关系达到“中”“和”境界时，这个社会必定会达一个圆满的境界。</p></li><li><p>至诚。至诚是《中庸》所追求的修养的最高境界。《中庸》第二十章写道：“<code>诚者，天之道也。诚之者，人之道也。诚者，不勉而中，不思而得，从容中道，圣人也。诚之者，择善而固执之者也。博学之，审问之，慎思之，明辨之，笃行之。</code>”。诚是上天的原则。追求诚是做人的原则。天生诚的人，不用勉强就能做到，不用思考就能拥有，自然而然地符合上天的原则，这样的人是圣人。所以，诚就是选择<code>良善作为人生信条并坚守不改变</code>。然后在此基础上广泛学习，详细询问，周密思考，明确辨别，切实实行。这段话道出了诚的含义，就是<code>选择良善作为人生信条并坚守不改变</code>。信奉诚并终生坚守不改变就是至诚。诚，遵循的是天道，坚守诚是做人的原则。<code>要达到至诚，就必须广泛学习，详细询问，周密思考，明确辨别，切实实行</code>。可以看出，至诚，既是做人的一项最高原则，也是一项做学问的最高原则。</p></li></ol><p>为什么说至诚是中庸追求的修养的最高境界？</p><p>中庸之道的主要思想就是要把人培养成合乎天道的尽善尽美的人，关注的是人的本身。个人修养就是贯穿于这当中的理想人格的塑造。那么，这种人格塑造要达到什么程度才算理想呢？</p><p>《中庸》第二十二章中写道：“唯天下之至诚，为能尽其性，能尽其性，则能尽人之性；能尽人之性，则能尽物之性；能尽物之性，则可以赞天地之化育；可以赞天地之化育，则可以与天地参矣。”</p><p>这段话的意思是：只有至诚恳切的人，才能尽力发挥他天赋的本性，到达极致；能尽他自己的本性，就能尽知他人的本性；能尽知他人的本性，就能尽知万物的本性；能尽知万物的本性，就可以赞助天地万物的化育；能赞助天地万物的化育，就可以与天地并列为三才了。在这里，《中庸》把至诚的阶段归入了人的天性范畴。也就是说，诚完全融入了人的心性，成为人的天性中固有的成分。到了至诚阶段，这种诚就是纯粹的，里面不搀杂任何其他物质。并且，“至诚无息”。在人生之路上，追求人格完美永不停息——完全是在没有外力的情况下自觉自愿进行的。人格修炼到至诚阶段，在现实生活中，无论言谈举止还是立业做学问，都是顺乎心性地以诚相伴始终。与人交往时，以诚相待；涉及利益纠纷时，诚心以对；路遇不平时，顺乎心性地拔刀相助；扶危济困时，发自内心地伸手施援；不媚上，不欺下，不恃强凌弱，不暴殄天物……并且，到了这个阶段，就可以很清楚地观察出别人的本性，观察出世间万物的本性。这样，达到至诚的人，就能够归于圣人当中了。虽然没有量化的标准，但到此谁都明白，至诚，就是人格塑造的理想状态。到了这个阶段，至诚，已由当初人生追求中的一种最高信念变成了现实。那么，这时候的诚，就是人格的最高境界了！</p><h2 id="动中取衡，静中就重"><a href="#动中取衡，静中就重" class="headerlink" title="动中取衡，静中就重"></a>动中取衡，静中就重</h2><p>“<code>动中取衡，静中就重</code>”，从字面上来理解就是，在运动中取得平衡，在平静中抓住重心。与此异曲同工的说法还有“<code>闲中取趣，闹里安身</code>”。</p><p><code>动中取衡，静中就重</code>，不仅是一种处世态度，也是中庸之道在现实中的具体运用。这其中，既包含有柔<code>弱胜刚强</code>，静中能制动的生活哲理，更多还是对个人精神力量的推崇。这些精神力量，就是建立在道德基础上的人格魅力。道德可理解为人们在共同的社会生活中形成的具有<code>约束力的准则和规范</code>，在现实生活中调节规范的是人的行为，主要靠人们自觉的内心观念来维持。道德的本质是<code>抑恶扬善</code>。人格是人身上通过先天遗传和后天接受而形成的<code>一种稳定的心理特征</code>。通过教育，道德可以转化为<code>稳定的心理特征，成为人格的一部分</code>。儒家的道德伦理观就是要从精神层面，通过道德说教，把人培养成具有高尚品德和人格魅力的谦谦君子，乃至圣人。这颇有现代人所说的洗脑的意味，只不过其目的较纯罢了。而中庸之道，可说是儒家道德伦理观当中，让人的高尚品德和人格魅力得以塑造成功的教科书。</p><h2 id="如何做到至诚"><a href="#如何做到至诚" class="headerlink" title="如何做到至诚"></a>如何做到至诚</h2><p>要想将人格塑造到至诚的高度，在具体实践中，起码要做到以下几点</p><ol><li>择善。善的本意是完好、圆满，是人性中较美好的成分。择善，<code>就是在人生态度和立世标准的取舍过程中，对美好的选择</code>。</li><li>固执。这里的固执，并不是现代人理解的行动上的顽固不知变通的意思，而指的是<code>信念上的坚守不改变</code>的意思。选择善作为人生的一个处世态度倒不难，难的是长期如一日地坚持下去。现实世界是物质的，存在着许许多多与人的生存息息相关的各种诱惑。这些诱惑总是会与利益牵连在一起。这时，提高生存质量的需求，渴望出人头地的欲念，还有种种羁绊和幻想，都在拷问着人的心灵，也就很难让常人做到面对诱惑能坚守信念不动摇。所以，要想品德得以提升，从而向至诚迈进，就必须以善为立脚点，坚守诚的信念不动摇。</li><li>博学。博学，既指才识上的，也指一种求知态度，即<code>广泛地学习</code>。世界是千变万化、丰富多彩的，人的本质也同样如此。孔子说：“三人行，必有我师焉。”所以，学无先后之别，学无身份之别，往往取决于一个人的态度。道理明显的是，一个人如果满足于现状，甚至抱残守缺，那么，他纵有再诚的心，也只能成为阶段性人物。至诚阶段所要求的是，能“尽物之性”，能“赞天地之化育”，最终能“与天地参”。要达到这些要求，就必须怀着诚心广泛地学习。</li><li>审问。审问，讲求的是效果，而不是形式。对不明白的事物，<code>不是问个大概，而是要问个清楚明白，学会思考</code>，了解其中的来龙去脉。这既适用于做学问方面，同样适用于做人方面。世界既是千变万化、丰富多彩的，也是纷繁复杂、高深莫测的。人常说，在生活中多问几个为什么，就会活成个明白人。从求知的角度来看，问，既是获取知识的手段，也是一种方法。通过审问，才能“尽物之性”，进一步朝着至诚迈进。</li><li>明辨。自然现象也好，社会现象也好，更多时候呈现给人的都是一种表象。只有独具慧眼的人才能看清实质。明辨，<code>也就是辨伪识真，透过表象看清实质</code>。在社会万象面前，以诚的心态来辨别是非，这是诚的内在要求。心态端正固然好，但如果取得的效果不能合乎心性达到真的一面，那么，也就不能在还原事实面前做到诚。所以，明辨，既是由诚通向至诚路上对自身的一种要求，又是达到至诚的一种手段。</li><li>笃行。诚只有融入心性，才能达到至诚。这就要求做到，在具体实践过程中，<code>对人对事都必须一心一意，心无二念，不虚伪</code>。不能把做事情当作临时的应付，不能把做人当作人前的卖弄，而应是虚心、求真的态度，贯穿其中的就是已化为人生中的一种永恒信念的诚。诚融入心性，笃行就会成为处世态度，进而成为一种美德，而坚持下去，至诚的境界也就达到了。</li></ol><h2 id="达到至诚的境界，有什么实际意义呢？"><a href="#达到至诚的境界，有什么实际意义呢？" class="headerlink" title="达到至诚的境界，有什么实际意义呢？"></a>达到至诚的境界，有什么实际意义呢？</h2><p>《中庸》第三十二章写道：<code>“唯天下至诚，为能经纶天下之大经，立天下之大本，知天地之化育。夫焉有所倚？</code>”</p><p>这段话的意思是：只有以至诚之心心怀天下，才能成为治理天下的崇高典范，才能树立天下的根本法则，掌握天地化育万物的深刻道理。</p><p>其意义就在于，至诚能让人怀有诚挚的仁慈之心，拥有深邃的聪明智慧，具有通达天赋、像苍天一样广阔的美德，成就了圣人的禀赋和气质，能“尽物之性”“赞天地之化育”，最终能“与天地参”。由这样的人来“平天下”，这天下就能垂拱而治。这样的天下，必然是合乎天道和人道的完美天下。</p><h2 id="为什么说“中庸之道”的理论基础是“天人合一”？"><a href="#为什么说“中庸之道”的理论基础是“天人合一”？" class="headerlink" title="为什么说“中庸之道”的理论基础是“天人合一”？"></a>为什么说“中庸之道”的理论基础是“天人合一”？</h2><p>谁都有自己所遵循的做人和处事的准则，这就是道。在儒家看来，芸芸众生作为社会中的一员，他们所遵循的道，不过是常人之道。常人之道纯属物质范畴，人们的思想境界始终处在物欲的包裹中，也就禁不起现实中的种种诱惑，从而让人始终处于利益的纷争之中。这也就是世间因人祸而起的灾难和动乱的根源。怎样避免这些纷争呢？当然得靠教育。通过教育让人的思想境界上升到一个新的高度。比如说，遵从“<code>五达道</code>”，培养“<code>三达德</code>”，懂得“<code>九经</code>”这种治理天下的主张等。这就是君子之道。</p><p>按上面的方式来理解，君子之道，就是品德高尚的人所遵循的做人和处事的准则。它是儒家伦理道德观当中一种理想的自然人格标准。这种人格呈现出来就是，为人心地善良，讷言敏行，胸怀志向，内心充实，贫贱不移，威武不屈，富贵不淫，达则兼济天下，穷则独善其身……但是，《中庸》不满足于这些，还要将人的思想境界继续提升，这就是圣人之道。</p><p>现在再来回顾中庸之道的理论推衍。社会上通行的是常人之道，通过教育，让人的道德品质得以提升后，就能成为君子，从而让君子之道得以施行；君子通过持续不断地加强自身品德的修养，让自己的思想和精神升华到最高境界，他们就成为了圣人；圣人所行之道都是循天而来，因而他们所行之道就由人道上升为天道。而到这个阶段，天道和人道就合二为一了。由此再回过头来就可看出，《中庸》所提倡的不断加强个人修养的主张，它的理论依据是，圣人是可以通过自我修养来塑造成的；有了圣人，天道就可以在人间实施，这时，天道也就成为了人道。既然天道和人道能够融合，那么，《中庸》中的各种主张也就有了现实意义。反过来，它就能成为修炼人性的指导工具。到这时，就可以说，《中庸》中的中庸之道是建立在天道和人道能够融合——即“天人合一”的理论基础上的。</p><h2 id="如何达到天人合一"><a href="#如何达到天人合一" class="headerlink" title="如何达到天人合一"></a>如何达到天人合一</h2><ol><li><code>理智与情感合一</code>，中庸之道的本质思想是，倡导个人通过自觉的个人修养来提高道德品质，从而在现实生活中能用一种不偏不倚、执中公允的态度来处理问题。这种态度的实质是一种道德品质的力量。现实生活往往是功利的，在利益纠纷或是非判断面前，人性中的贪婪总是会迁就个人的情感而置事实于不顾，从而做出有利于自己私利的判断或决定。所以，只有道德修养达到一定境界的人，才会让理性占据心头，做出实事求是的判决。反过来就是，只有思想境界上升到中庸阶段的人，他才会自觉抑制世俗的缠绕，他的人格才可以继续塑造到更高境界，直到天人合一。</li><li><code>追求与完美合一</code>，毫无疑问，圣人阶段是人生的完美阶段。《中庸》第二十九章写道：“<code>故君子之道，本诸身，征诸庶民；考诸三王而不谬，建诸天地而不悖，质诸鬼神而无疑，知天也；百世以俟圣人而不惑，知人也。是故君子动而世为天下道，行而世为天下法，言而世为天下则。</code>”这段话的意思是：所以君子治理天下应该以自身的德行为根本，并从老百姓那里得到验证。考查夏、商、周三代先王的做法而没有背谬，立于天地之间而没有悖乱，质询于鬼神而没有疑问，百世以后待到圣人出现也没有什么不理解的地方。质询于鬼神而没有疑问，这是知道天理；百世以后待到圣人出现也没有什么不理解的地方，这是知道人意。所以君子的举止能世世代代成为天下的先导，行为能世世代代成为天下的法度，语言能世世代代成为天下准则。很明显，当人格塑造到君子阶段，对他的要求也就更高了。所作所为要“不谬”“不悖”“无疑”，在圣人面前能做到“不惑”，并且，他的举止能世世代代成为天下的先导，行为能世世代代成为天下的法度，语言能世世代代成为天下准则。当君子能做到这一点时，他离圣人也就不远了。所以，把追求与完美结合起来，既是一种高规格的要求，也是一种良好的鞭策。长期坚持下去，必能到达天人合一的那一层次。</li><li><code>内在与外在合一</code>，个人修养方面的“慎独”“言顾行，行顾言”之类的所要表达的都是要求人做到表里如一。《中庸》第二十五章揭示了这方面的关系：“<code>诚者，自成也；而道，自道也。诚者，物之终始，不诚无物。是故君子诚之为贵。诚者非自成而已也，所在成物也。成己，仁也；成物，知也。性之德也。合外内之道也。故时措之宜也。</code>”这段话的意思是：诚实是自我的完善，道是自我的引导。诚实是事物的发端和归宿，没有诚实就没有了事物。因此，君子以诚实为贵。不过，诚实并不是自我完善就够了，而是还要完善事物。自我完善是仁，完善事物是智。仁和智是出于本性的德行，是融合自身与外物的准则，所以任何时候施行都是适宜的。内在的仁和智，之所以能够形成本身的德行，就是自身与外在的事物相接触过程中形成的知与行的融通。也就是说，内在的仁和智的形成，是在没有受到外物干扰或无视外物干扰的情况下，主观自觉培养的结果。仁和智都从诚实中来，当诚实发展到至诚阶段，仁和智也就成为圣人天性中的一部分，这时，也就是天人合一的时候。</li></ol><h2 id="治国之道中包含有“无为而治”的思想吗？"><a href="#治国之道中包含有“无为而治”的思想吗？" class="headerlink" title="治国之道中包含有“无为而治”的思想吗？"></a>治国之道中包含有“无为而治”的思想吗？</h2><p>看到“<code>无为而治</code>”四个字，谁都知道是春秋末期道家的老子提出的。它是一种治国理论，认为世界的本原是无，只有无才符合道的原则。《道德经》第二章中写道：“<code>是故圣人处无为之事；行不言之教。</code>”</p><p>这句话的意思是：圣人治理国家的措施都是顺应天道而来；这样，老百姓也就因遵循天道而合乎天道，对他们也就不需要再去教育了。老子认为，“<code>我无为，而民自化；我好静，而民自正；我无事，而民自富；我无欲，而民自朴</code>”。</p><p>这句话的意思是：管理国家者顺应天道施政，老百姓必定普遍发自内心地拥护；管理者不去打扰，老百姓自己就能走上正道；管理者不去做些什么，老百姓自己就能创造财富；管理者不去索求什么，老百姓自己就能创造淳朴的风气。当然，老百姓所能达到的“化”“正”“富”“朴”的前提是，管理者施政必须顺应天道。</p><p>《中庸》的教育思想就是通过推行并让人接受中庸之道，<code>把人由常人变为君子，再由君子修身为圣人</code>。到了圣人阶段，天道与人道就合而为一。修到圣人后，是不是他们的人生目的就达到了，从此以后就可以远离红尘成为仙人，过着逍遥自在的生活呢？当然不是。儒家所推崇的圣人的终极目标就是让他们运用天道来“平天下”，即治理天下，让天下太平和合。《中庸》第一章中写道：“天命之谓性，率性之谓道。”</p><p>这句话的意思是：上天所赋予人的本质特性叫作本性（天性），遵循着本性来做人和处事叫作道。</p><p>而到了圣人阶段，圣人的“率性”全都承自天意，顺应天意，是“从容中道”——自然而然地符合上天的原则。既然圣人的本性已经与天性相合，由他们来施政，不就是在人间施行天道吗？天道在人间施行，天地和合，那些体现着人的意志的法规制度也就没必要了，那些统治者所推行的施政措施也就没必要了，一切都自然而然地得到治理，这不就是无为而治吗？</p><h2 id="儒家和道家"><a href="#儒家和道家" class="headerlink" title="儒家和道家"></a>儒家和道家</h2><p>道家的起源很早，可以上溯到上古时期。到了春秋时期，老子将古圣先贤的思想精华进行汇集和总结，形成了无为无不为的道德理论。这标志着道家思想的正式成型。从这以后，这种思想被广泛尊崇并被用作治国理念。道家以老子、文子、庄子，管子等为主要代表，以“道”为核心，认为天道无为，主张“虚无、自然、无为、守静、璞朴、与时迁移，应物变化”等政治和军事方面的思想观念。它的思想形成以总结、发展、著典籍为主要路径，经历了极其长时间的广纳众慧，因而在历史上具有顽强的生命力，对中国乃至世界的文化都产生了巨大的影响。</p><p>儒家思想来源于尧舜时期的“中道”思想，还有《易经》中的“德义”思想和西周时期的《尚书》《诗经》中的文化思想，并逐渐演变成一门奉孔子为宗师的学说。儒家学说的核心思想是“仁”和“礼”，崇尚等级制度和用“三纲五常”来维护统治，主张“礼、乐、仁、义”，提倡“忠恕”和“中庸”之道。主张“德治”和“仁政”，重视伦常关系。西汉时期汉武帝“罢黜百家，独尊儒术”以后，儒家学说逐渐成为了占统治地位的学说，信奉由孔子创立的维护周礼的主张，并成为历朝历代统治者统治天下的政治理念。自汉代以来，儒家学说成为了大多数朝代的主流意识流派、正统的官方思想，对中国的社会、政治产生了深刻的、全方位的影响。</p><p>不过，从内容上来看，儒、道两家确实存在不少相通之处。以《中庸》为例。《中庸》中说“率性之谓道”，要求人们知道什么是道；而道家要求做到“体道”，要求人们对道有切身的体悟。这可说是两者在起点上的相通之处。道家倡导“道法自然”，而《中庸》所追求的最高境界是圣人之道，同样是“率性”的，也就是完全合乎自然法则的天道。这可说是两者在追求目标上的相通之处。道家所倡导的“守一”与《中庸》所倡导的“固执”在要求上几乎相同，都是要求人们不改变心趣和志向的一种做人准则。还有道家的“齐物”“动静结合”的观点，与《中庸》中的“致中和”“动中取衡、静中就重”的观点有相通之处。其他类似的相通观点还有很多。由此，或许能够理解为什么当年“独尊儒术”后，道家后来仍然能够浮出水面并长盛不衰的原因了。</p><h1 id="三、《论语》：温故而知新，可以为师矣"><a href="#三、《论语》：温故而知新，可以为师矣" class="headerlink" title="三、《论语》：温故而知新，可以为师矣"></a>三、《论语》：温故而知新，可以为师矣</h1><p>《论语》是记载孔子言行的重要儒家经典，由孔子的弟子及再传弟子编辑而成。全书现存共20篇492章，是我国春秋时期的一部语录体散文集。南宋时，朱熹将它与《孟子》《大学》《中庸》合称为“四书”。书中较为集中地反映了孔子的思想，既有对社会、人生的描绘，又有对人性与政治、道德、文化方面的真知灼见，对我们今天仍具有很大的指导和启发意义。</p><h2 id="《论语》是如何体现孔子的“仁、义、礼、智”思想的？"><a href="#《论语》是如何体现孔子的“仁、义、礼、智”思想的？" class="headerlink" title="《论语》是如何体现孔子的“仁、义、礼、智”思想的？"></a>《论语》是如何体现孔子的“仁、义、礼、智”思想的？</h2><p>（1）仁。《论语》中，孔子对仁的阐释多达一百零九处，而体现仁的内涵的集中在《论语·颜渊篇》中。什么是仁？颜渊就问过孔子这个问题。在《论语·颜渊篇》中有这样的记载：“颜渊问仁，子曰：‘克己复礼为仁。一日克己复礼，天下归仁焉。为仁由己，而由人乎哉？’颜渊曰：‘请问其目？’子曰：‘非礼勿视，非礼勿听，非礼勿言，非礼勿动。’”</p><p>这段话的意思是：颜渊问什么是仁？孔子回答说：“能够控制自己的情感，一切照着礼的要求去做，这就是仁。一旦这样做了，天下的一切就都归于仁了。实行仁德，完全在于自己，难道还在于别人吗？”颜渊接着又问：“请问实行仁有哪些条目？”孔子回答说：“不合于礼的不要看，不合于礼的不要听，不合于礼的不要说，不合于礼的不要做。”怎样做才是仁呢？仲弓问了这个问题。孔子回答道：“出门如见大宾，使民如承大祭。己所不欲，勿施于人。在邦无怨，在家无怨。”孔子的意思是说，出门办事如同去接待贵宾，使唤百姓如同去进行重大的祭祀，都要认真严肃地对待；自己不愿意要的，不要强加于别人；做到在诸侯的朝廷上不招人怨恨自己，在卿大夫的封地里也没人怨恨自己。而司马牛也问了这个问题。孔子回答道：“仁者，其言也讱。”孔子的意思是说，仁人说话是慎重的。</p><p>仁的性质是一种美德，仁者能够安贫乐道，能敢爱敢恨，能无害于社会。这也就是孔子大加推崇仁的一个原因吧。</p><p>（2）义。在《论语·里仁篇》中孔子说：“君子之于天下也，无适也，无莫也，义之与比。”其意思是说，君子对于天下的人和事，没有固定的厚薄亲疏，只是按照义去做。这里，孔子把义当作了行为参考标准。</p><p>在孔子的心目中，义既是一种做人准则，又是一种美德。这样的人，为人处事一身正气，完全不受利益的诱惑和驱使，而是按照公认的行事法则行事，遇到不平或灾难时会及时挺身而出。</p><p>（3）礼。孔子讲仁时说：“克己复礼为仁。”那么，礼指的是什么呢？礼，是孔子构建“天下有道”的理想社会的一种调节剂，目的就是希望通过礼的施行，让整个社会等级分明、和谐有序。由于这种思想受到了统治者的推崇，礼教就大为盛行，可说上升到了国教。在《论语》中，就于多处体现了孔子礼的思想。</p><p>作为一种社会秩序，礼存在于社会的方方面面，也作用于社会的方方面面。所以，孔子所推行的礼，既是一种治国的理念，又是一种治国的手段。</p><p>（4）智。智由知来，具有知的能力，就是智。所以，《论语》中的智，是从知来，体现在以下几个方面：</p><ul><li>其一，学习方面。在《论语·为政篇》中，孔子说：“知之为知之，不知为不知，是知也。”孔子所要表达的意思很明显，知道就是知道，不要不懂装懂。还有如：“业精于勤荒于嬉”“敏而好学，不耻下问”“学而不思则罔，思而不学则殆”等。这些都是要求人们养成学习知识、运用知识的习惯。</li><li>其二，政治方面。在《论语·公冶长篇》中，孔子写道：“宁武子，邦有道则知，邦无道则愚。其知可及也，其愚不可及也。”其意思是说，宁武子这个人，当国家有道时，他就显得聪明，当国家无道时，他就装傻。他的那种聪明别人可以做得到，他的那种装傻别人就做不到了。这里，孔子用的是反意，指出智不应用在满足个人的私利上。这样的智，不值得推崇。当然，这是今人的理解。孔子的本意恐怕是在推崇宁武子的这种大智若愚。</li><li>其三，为人处事方面。在《论语·卫灵公篇》中，孔子说：“可与言而不与言，失人；不可与言而与之言，失言。知者不失人亦不失言。”其意思是说，可以同他谈的话，却不同他谈，这就是失掉了朋友；不可以同他谈的话，却同他谈，这就是说错了话。有智慧的人既不失去朋友，又不说错话；既要会说话，又不说错话。这同样是智。还有如“知者乐山，仁者乐水”之类审美情趣方面、生活情趣方面和心理活动方面的智。不过，孔子所提的智，并不是从个人学识的范畴，而是归于是非判断的理性精神范畴，所要宣扬的都是一种道德修养上的品格。</li></ul><h2 id="“朝闻道，夕死可矣”体现了孔子怎样的道德观？"><a href="#“朝闻道，夕死可矣”体现了孔子怎样的道德观？" class="headerlink" title="“朝闻道，夕死可矣”体现了孔子怎样的道德观？"></a>“朝闻道，夕死可矣”体现了孔子怎样的道德观？</h2><p>道德观指的是对人们在共同生活中形成的存于人的意识中的行为准则和规范的认识与所持的立场，核心是善恶观。主要内容涉及正义、良心、义务、荣誉、人格、幸福等观念。孔子生活于社会动荡的春秋时期，当他看到天下“礼崩乐坏，天下无道”时，就想用道德教育的方法使天下归于有道，强化以“仁”为核心的伦理道德思想价值体系。他希望通过重建君臣秩序，让社会从“天下无道”走向“天下有道”。当以他的学说为核心的儒家学说成为天下正统的政治思想时，他所提出的一系列的道德标准，也就成为了中国传统道德的基石，而他本人也就成为了我国传统道德的奠基人。</p><p>孔子所倡导的伦理道德，意在通过规范社会秩序，实现天下大治的目的。他说的“朝闻道，夕死可矣”出自《论语·里仁篇》。其意思是说，早晨得知了道，就是当天晚上死去也心甘。这其中，关键在一“道”字。这里的“道”指的是天道，也就是圣人之道。孔子的道德观是建立在政治层面上的，就是希望通过克己复礼，让统治者具有仁德，从而“内圣外王”，让天下获得良性发展。因而，孔子的道德观是以服务于政治为目的，通过以礼教的手段，从而把人格塑造成符合天下大治这一政治目标的人性认知。</p><h2 id="“君子无所争”等对人的修养提出了哪些要求？"><a href="#“君子无所争”等对人的修养提出了哪些要求？" class="headerlink" title="“君子无所争”等对人的修养提出了哪些要求？"></a>“君子无所争”等对人的修养提出了哪些要求？</h2><p>在《论语·八佾篇》中，孔子说：“君子无所争。必也射乎！揖让而升，下而饮。其争也君子。”</p><p>这段话的意思是：君子没有什么可与别人争的事情。如果有的话，那就是射箭比赛了！比赛时，先相互作揖谦让，然后上场。射完后，又相互作揖再退下来，然后登堂喝酒。这就是君子之争。</p><p>孔子所推崇的就是人格修炼，让人成为品德高尚的君子。在具体实践中，君子要做到“不争”：不争名利、不争财色、不争私欲。同时君子要说有什么可争的话，就是争守礼、明礼、行礼和敬礼：上场时作揖谦让，下场时作揖辞别。就好比足球比赛一样，上场时握手，下场时再握手，友谊第一，比赛第二。“修己以敬。修己以安人。修己以安百姓。”要做到这些，孔子针对个人的修养，提出了许多要求，大致包括以下几个方面的内容：</p><ol><li>见利思义。在《论语·宪问篇》中，孔子直接表达了这种观点：“见利思义，见危授命，久要不忘平生之言，亦可以为成人矣。” 这段话的意思是：见到财利想到义的要求，遇到危险能献出生命，长久处于穷困还不忘平日的诺言，这样也可以成为一位完美的人。世间的纷争可说都是由利来，见利忘义是一般人的共同心态。但是，作为君子，在这方面不但要洁身自好，不与人争，还要做到把义字摆中间，“不义而富且贵，于我如浮云”，并且遇到危险需要自身挺身而出时，就勇敢地上前面对，哪怕献出生命。所以孔子认为，这样的人，在长久的贫困之境中能够坚守气节的话，那他就是完人。</li><li>见贤思齐。在《论语·里仁篇》中，孔子要求做到：“见贤思齐焉。见不贤而内自省也。”其意思是说，见到贤人，就应该向他学习、看齐，见到不贤的人，就应该自我反省自己有没有与他一样犯相类似的错误。这实际上是一种崇尚美德的促进。人的学识、美德诸方面原本是参差不齐的。对于一个某处或处处比自己优秀的人，在现实中很有可能缘于他们的优秀而致自己的利益受到损毁，从而让人性中的一些阴间面冒出来，也就是让人产生诸如嫉妒、排斥、诽谤甚至是打击、诋毁人家之类的丑恶念头。这时，就要从源头上下功夫，通过“自省”加强自身的道德修养，让人从内心深处革除掉那些丑恶念头。不仅如此，还要产生向人学习的勇气。一个良性的社会，都是一个学习蔚然成风的社会。尊重贤能，就是尊重知识，归根结底，还是尊重劳动。</li><li>好德如好色。在《论语·子罕篇》中，孔子说：“吾未见好德如好色者也。”其意思是说，我没有见过像好色那样好德的人。受生理激素的支配，人的某些心性往往不受人的意志支配不请自来。比如说，好色。这种好色，既可理解为对自然界美色的偏爱，也可理解为对人本身的肉欲享受。好色，是人的一种心理特性，最终还应归于物质范畴。无论何时何地，物质世界都是个有限度的范围，没有什么物质能做到让人取之不尽，用之不竭。得者固然欢心，但失者的伤痛就会成为社会矛盾的爆发点。当无止境的欲求面对有限的物质时，纷争在所难免。那要如何消除纷争呢？孔子提出了道德伦理的观念。通过道德修养，辅之以礼教，让人修身成为正人君子。“君子无所争”。面对物质世界，他们能自觉自愿地从心头熄灭欲求的火花，进而以这种品德垂范社会，让崇尚道德成为一种社会风气。</li><li>匹夫不可夺志。在《论语·子罕篇》中，孔子说：“三军可夺帅也，匹夫不可夺志也。”其意思是说，一个国家的军队，可以夺去它的主帅；但是一个男子汉，他的志向是不能强迫改变的。志向属精神范畴，而且总是与气节操守联系在一起，指的是人决心在某个方面有所作为时而选择的努力方向。这个词在现代使用频率极高，而且无不与“远大”联系在一起。问题通常是，选择志向“远大”倒是非常容易的，但在通往志向的努力过程中，难的是始终如一地坚持。该怎么坚持呢？孔子在《论语·里仁篇》就举过一例：“士志于道。而耻恶衣恶食者。未足与议也。”其意思是说，士有学习和实行圣人之道的志向，但又以自己吃穿得不好为耻辱，对这种人，是不值得与他谈论道的。孔子的观点再明显不过了。志向和做人是统一的。作为君子，就要守得住清贫，抵得住诱惑，也就是安贫乐道。如果以物质世界来干扰身心，甚至用物质评判的标准来作为处世标准，这样的人，就不是一个脱俗之人，其品行也就是世俗化的，距君子的身价相去甚远。</li><li>以“九思”作为行事准则。在《论语·季氏篇》中，孔子说：“君子有九思：视思明；听思聪；色思温；貌思恭；言思忠；事思敬；疑思问；忿思难；见得思义。”其意思是说，君子有九种要思考的事：看的时候，要思考看清与否；听的时候，要思考是否听得清楚；自己的脸色，要思考是否温和，容貌要思考是否谦恭；言谈的时候，要思考是否忠诚；办事要思考是否谨慎严肃；遇到疑问，要思考是否应该向别人询问；愤怒时，要思考是否有后患，获取财利时，要思考是否合乎义的准则。</li></ol><p>所以，君子无所争，在名在利在欲；君子有所争，在礼。“动容貌”“正颜色”“出辞气”，人人首先从自己的一言一行做起，礼治天下的目标就能实现。</p><h2 id="怎样理解孔子“有教无类”的教育观？"><a href="#怎样理解孔子“有教无类”的教育观？" class="headerlink" title="怎样理解孔子“有教无类”的教育观？"></a>怎样理解孔子“有教无类”的教育观？</h2><ol><li>人性相近。在《论语·阳货篇》中孔子说：“性相近也，习相远也。”其意思是说，人的本性是相近的，由于生活中从事的内容和接触的对象不同才相互间有了差别。这句话包含人性发展中的先天和后天两个方面。</li><li>人皆可以为尧舜。“人皆可以为尧舜”的说法出自《孟子》。其意思是说人人都能成为有用之才，包括政治方面的。在《中庸》中，孔子将人格修炼分为三个档次，从常人之道修身为君子之道，再从君子之道修身到圣人之道的最高境界。如何修身，当然是通过受教育了。所以，人皆可以为尧舜，同孔子的修道的观点是相同的，都体现了人可以通过接受教育来成才的教育观。</li><li>泛爱众。在《论语·学而篇》中，孔子说：“弟子，入则孝，出则悌，谨而信，泛爱众，而亲仁。行有馀力，则以学文。”其意思是说，弟子们在父母跟前，就要孝顺父母；出门在外，要顺从师长，言行要谨慎，要诚实可信，寡言少语，要广泛地去爱众人，亲近那些有仁德的人。这样躬行实践之后，还有余力的话，就再去学习文献知识。这是孔子教育的内容，而换个角度来看，这恰又是孔子本人的教育思想。这种思想简单来说就是守礼知仁。在《论语·颜渊篇》中，孔子回答樊迟，仁就是“爱人”。礼在于尊重，仁在于爱。这种爱，不是偏爱，而是“泛爱”；不是私爱，而是“爱众”。泛爱众，就是以同等的态度看待受众，十分贴近于今人所说的博爱。这是孔子的又一种教育观，是从教育者本身的角度提出的，强调的是教育者本身的道德修养。</li></ol><h2 id="孔子的“文、行、忠、信”四教分别指的是什么？"><a href="#孔子的“文、行、忠、信”四教分别指的是什么？" class="headerlink" title="孔子的“文、行、忠、信”四教分别指的是什么？"></a>孔子的“文、行、忠、信”四教分别指的是什么？</h2><p>文，首先应该理解为文字。文字是思想的载体，是语言的记录符号。任何启蒙教育，首先都是从教人识字开始的。识字并了解字中的含意，应是孔子教学的内容。其次，文，指的是文本中的文章。在《论语·雍也篇》中孔子说：“博学于文，约之以礼，亦可以弗畔矣夫！”其意思是说，君子广泛地学习古代的文化典籍，又以礼来约束自己，也就可以不离经叛道了。从这里可窥出，孔子教授的课本中，应该含有古代的文化典籍。在《论语·颜渊篇》中，曾子就说：“君子以文会友，以友辅仁。”其意思是说，君子以文章学问来结交朋友，依靠朋友帮助自己培养仁德。做人做学问，都是通过文章来传递知识的。孔子的教育自然也绕不过这一环节。孔子曾撰修过《诗》《书》《礼》《乐》《周易》《春秋》，在《论语》中他就多次提到《诗经》等书。这些书，既有涉及思想道德的，也有人伦秩序规范的，还有自然和社会学方面的知识的。可以想象，孔子一定会将它们传授给自己的学生。所以，孔子教授的文，既指文章，也指文章的思想内涵。</p><p>另外，文，还应指的是学规之类的内容。在《论语·公冶长》中写道：“子贡问曰：‘孔文子何以谓之文也？’子曰：‘敏而好学，不耻下问，是以谓之文也。’”其意思是说，子贡问孔子为什么给孔文子一个“文”的谥号呢？孔子回答说：“他聪敏勤勉而好学，不以向他地位卑下的人请教为耻，所以给他谥号叫‘文’。”这其中，“敏而好学，不耻下问”应是一种从学规当中摘录的文字。</p><p>行，从《论语》中所提及的有关“行”的描述来看，行，应是孔子教科书当中有关如何参与社会实践的内容，大多还应是礼教方面的内容。人的视、听、动、言都属行的方面。在《论语·颜渊篇》中，颜渊问孔子仁的科目。孔子回答说：“非礼勿视，非礼勿听，非礼勿言，非礼勿动。”那么，怎样才能做到这些呢？在《论语·乡党篇》中，孔子就现身说法地展示了各种“行”的规范。</p><p>孔子在本乡的地方上表现得很温和恭敬，像是不会说话的样子。但他在宗庙里，在朝廷上，却很善于言辞，同时又表现得比较谨慎。孔子在上朝的时候，要是国君还没有到来，他同下大夫说话，显出温和而快乐的样子；同上大夫说话，显出正直而公正的样子；国君来了时，他就显出恭敬而心中不安的样子，但又仪态适中。国君召孔子去接待宾客，孔子脸色立即庄重起来，脚步也快起来。他向和他站在一起的人作揖，手向左或向右作揖，衣服前后摆动，却整齐不乱。快步走的时候，像鸟儿展开双翅一样。宾客走后，必定向君主回报说：“客人已经不回头张望了。”孔子走进朝廷的大门，显出谨慎而恭敬的样子，好像没有他的容身之地。站，他不站在门的中间；走，也不踩门坎。经过国君的座位时，他脸色立刻庄重起来，脚步也加快起来……</p><p>忠，从《论语》来看，孔子教科书上的“忠”的内容应该包括三个方面：交友、事君和治民。</p><p>在《论语·颜渊篇》中，子贡问怎样对待朋友，孔子说：“忠告而善道之，不可则止，毋自辱焉。”其意思是说，忠诚地劝告他，恰当地引导他，如果不听也就罢了，不要自取其辱。孔子的观点是，对待朋友要忠诚，不背弃。</p><p>在《论语·八佾》中，孔子说：“君使臣以礼，臣事君以忠。”其意思是说，君主应该按照礼的要求去使唤臣子，臣子应该以忠来事奉君主。这里，君主所做的是体表行动，而臣子所做的是内心活动。</p><p>在《论语·颜渊篇》中，子张问如何处理政事，孔子说：“居之无倦，行之以忠。”其意思是说，居于官位不懈怠，执行君令要忠实。在《论语·为政篇》中，孔子说：“临之以庄，则敬；孝慈，则忠；举善而教不能，则劝。”其意思是说，你用庄重的态度对待老百姓，他们就会尊敬你；你对父母孝顺、对子弟慈祥，百姓就会尽忠于你；你选用善良的人，又教育能力差的人，百姓就会互相勉励，加倍努力。</p><p>由以上的摘录文字中可以看出，忠，属于人的内心中的一种自觉自愿的为他人服务或效力的意识的外在表现，通常有为人正直、诚恳厚道、尽心尽力等内容。它是人的道德义务的体现，属道德范畴。</p><p>信，在《论语》中，信，指的就是诚信。在《子路篇》中，更是对它有明确的解析：“故君子名之必可言也，言之必可行也。君子于其言，无所苟而已矣。”其意思是说，君子做事时一定要定下一个名分，必须能够说得明白，说出来一定能够行得通。君子对于自己的言行，是从不马马虎虎对待的。这也就是常人说的，言必信，行必果。在该篇中，孔子又说：“上好信，则民莫敢不用情。”其意思是说，在上位的人只要诚实守信，老百姓就不敢不用真心实情来对待你。</p><p>“人而无信，不知其可也。”所以，信，同样属于道德范畴，构成人的基本品格之一，自然也就成了孔子教学的内容。</p><h2 id="怎样理解孔子所说的“刚、毅、木、讷近仁”？"><a href="#怎样理解孔子所说的“刚、毅、木、讷近仁”？" class="headerlink" title="怎样理解孔子所说的“刚、毅、木、讷近仁”？"></a>怎样理解孔子所说的“刚、毅、木、讷近仁”？</h2><p>刚，什么是刚呢？《论语·公冶长篇》中写道：“子曰：‘吾未见刚者。’或对曰：‘申枨。’子曰：‘枨也欲，焉得刚？’”这是段对话。孔子说：“我没有见过刚强的人。”有人回答说：“申枨就是刚强的。”孔子说：“申枨这个人欲望太多，怎么能刚强呢？”从孔子的回答中很明显地看出，他所说的“刚”，指的是人内心的意志。那什么样的意志才算是“刚”呢？无欲则刚。</p><p>在《论语·阳货篇》中，孔子说：“好仁不好学，其蔽也愚；好知不好学，其蔽也荡；好信不好学，其蔽也贼；好直不好学，其蔽也绞；好勇不好学，其蔽也乱；好刚不好学，其蔽也狂。”意思是说，爱好仁德而不爱好学习，它的弊病是受人愚弄；爱好智慧而不爱好学习，它的弊病是行为放荡；爱好诚信而不爱好学习，它的弊病是危害亲人；爱好直率却不爱好学习，它的弊病是说话尖刻；爱好勇敢却不爱好学习，它的弊病是犯上作乱；爱好刚强却不爱好学习，它的弊病是狂妄自大。这段话从反面来理解就是，“好仁”“好知”“好信”“好直”“好勇”“好刚”，这些品德都还只是培养仁德的起步阶段，只有通过学习才能得到提升。一个不爱好学习的人，是不能做到修身的。所以，它们都只属于儒家所倡导的伦理道德修炼的阶段，距仁德的修成还存在相当的距离。</p><p>毅，在《论语·泰伯篇》中，曾子说：“士不可以不弘毅，任重而道远。仁以为己任，不亦重乎？死而后已，不亦远乎？”其意思是说，士不可以不弘大刚强而有毅力，因为他责任重大，道路遥远。把实现仁作为自己的责任，难道还不重大吗？奋斗终身，死而后已，难道路程还不遥远吗？这里的“毅”与孔子在《中庸》中提到的“固执”以及《论语》中的“忠”和“信”在目标上是一致的，都要求人们执着、坚守。它们都属于意志范畴，属于人格修炼中的必要成分，但从起步上看，“毅”属于最初阶段，处在最基层。</p><p>在《论语》中所提到的“木”字，它的意思基本上都同花草树木中的“木”，指的是树被砍伐后形成的材质。在花草树木当中，前三者基本上都具有色彩，往往既醒目又悦目，有些还带有让人欣赏的芳香，所以能吸引人。而木除了实用外，没有格外吸引人之处，尤其是在人的感官方面。木的这种特点，如果拿来与人的品性来比拟的话，就是朴实。这也就是孔子的真实用意。</p><p>朴实，作为人的品性，当然是一个优点。但孔子所倡导的修炼人的品德的目的是为了治国平天下的，所以，朴实还仅是个人修身当中的一个可贵的成分，距仁德的全面修成还有很多路要走。</p><p>讷，在《论语·里仁篇》中孔子说：“君子欲讷于言而敏于行。”其意思是说，君子说话要谨慎，而行动要敏捷。讷的本意是钝于言，也就是说话没锋芒，不会伤害别人。这同《中庸》中所说的“行顾言，言顾行”的观点是一致的，属于谨言阶段。孔子一直把谨言慎行作为人格修炼的重要要求，他本人也是时刻这样身体力行的。无论在庙堂、在朝廷还是接见外宾，他都“便便言，唯谨尔”。</p><p>在刚、毅、木、讷四种品德当中，前二者基本上还处在内心修炼阶段，后二者已是内心修炼后的外在体现。孔子在《论语·学而篇》中说：“巧言令色，鲜矣仁！”意思是，花言巧语，装出和颜悦色的样子，这种人的仁心就很少了。这是从反面对木、讷两种品德的推崇。在《论语·公冶长篇》中，孔子也有同样的观点，花言巧语，装出好看的脸色，摆出逢迎的姿势，低三下四地过分恭敬，这种人是可耻的。不过孔子认为，仅具有刚、毅、木、讷四种品德还不够，它们都距修成仁德的完人相差很远，都还处于起步阶段，还没达到仁。</p><h1 id="四、《孟子》：性本善，施仁政，民贵君轻"><a href="#四、《孟子》：性本善，施仁政，民贵君轻" class="headerlink" title="四、《孟子》：性本善，施仁政，民贵君轻"></a>四、《孟子》：性本善，施仁政，民贵君轻</h1><p>《孟子》记录了孟子的治国思想、政治观点和政治行动。大体来说包括这些方面：在人性方面，孟子主张性善论，认为人生来就具备仁、义、礼、智四种品德；在社会政治观点方面，孟子突出仁政、王道的理论，而仁政就是对人民“省刑罚，薄税敛”，强调发展农业，体恤民众，关注民生，同时他又提出民贵君轻的主张，认为君主必须重视人民，“诸侯之宝三，土地、人民、政事”；在价值观方面，孟子强调舍身取义，强调要以“礼义”来约束个人的一言一行。</p><p>孟子本人和孔子的经历差不多，都是通过周游列国，去宣传自己的思想。而且，在思想上，孟子继承并发扬了孔子仁、义、善的主张，力主政治上施行“仁政”。他是孔子之后重要的儒家代表人物，著名的思想家、教育家和政治家。所以，后世将他尊称为继孔子大成至圣之后的亚圣，并把他们的思想主张合称为“孔孟之道”。</p><h2 id="为什么说“民为贵，社稷次之，君为轻”？"><a href="#为什么说“民为贵，社稷次之，君为轻”？" class="headerlink" title="为什么说“民为贵，社稷次之，君为轻”？"></a>为什么说“民为贵，社稷次之，君为轻”？</h2><p>《孟子·尽心章句下》第十四节的整体意思是，民最为宝贵，土神和谷神次要，君主为轻。因此，得到广大民众的承认者就可以成为天子，得到天子承认的就可以成为诸侯，得到诸侯承认的就可以成为大夫。诸侯危害社稷国家，就另外改立。用作祭祀的牲畜已经长成，用作祭祀的粮食已经洁净，就按时祭祀，但仍发生旱灾水灾，那么就另外改换土神和谷神。根据这整体的意思，有人就认为，孟子的这些话，并不是针对整个国家来说的，而是针对当时的诸侯国来说的。如果这样来理解的的话，也就表明孟子的国家观念中，并未脱离君权神授的观念，他的民本思想还是建立在皇权永固层面上的。正因如此，才有了“君子者，道法之总要也”和“天子之位也，是为治统”的观点。统览孟子的学说，说孟子的思想并未脱离皇权正统，应该是较为公允的。</p><h2 id="如何理解孟子所说的“夫仁政，必自经界始”？"><a href="#如何理解孟子所说的“夫仁政，必自经界始”？" class="headerlink" title="如何理解孟子所说的“夫仁政，必自经界始”？"></a>如何理解孟子所说的“夫仁政，必自经界始”？</h2><p>如何施行仁政？孟子要求滕文公做到：急民之所急，想民之所想，让老百姓拥有“恒产”。在老百姓都拥有“恒产”，物质上得到满足后，就要兴办学校，教育老百姓懂得人与人之间的伦理关系。所以，孟子要求滕国施行仁政的具体措施就是：以井田制养民；以学校教民。人民得到休养受到教育，社会秩序也就有了保障，国君的统治地位也就巩固了。如何做到这些呢？首先从田土划分着手</p><p>孟子继承了孔子的以德治国的思想，并将它发展为仁政学说。孟子认为，如果统治者实行仁政，就能够得到老百姓的衷心拥护。反之，如果不顾老百姓的死活，推行虐政，就会失去民心而变成独夫民贼，就会被老百姓推翻。仁政的具体内容很广泛，包括经济、政治、教育以及统一天下的途径等。而推行井田制，就是一项行之有效的治国安民的措施。通过让老百姓拥有“恒产”，安居乐业，他们就不会去触犯刑律，为非作歹。当老百姓的物质生活有了保障后，再去兴办学校，用孝悌的道理进行教化，引导老百姓人性向善，这就可以造成一种“亲亲”“长长”的良好道德风尚，即“人人亲其亲、长其长，而天下平”。</p><p>孟子继承了孔子的以德治国的思想，并将它发展为仁政学说。孟子认为，如果统治者实行仁政，就能够得到老百姓的衷心拥护。反之，如果不顾老百姓的死活，推行虐政，就会失去民心而变成独夫民贼，就会被老百姓推翻。仁政的具体内容很广泛，包括经济、政治、教育以及统一天下的途径等。而推行井田制，就是一项行之有效的治国安民的措施。通过让老百姓拥有“恒产”，安居乐业，他们就不会去触犯刑律，为非作歹。当老百姓的物质生活有了保障后，再去兴办学校，用孝悌的道理进行教化，引导老百姓人性向善，这就可以造成一种“亲亲”“长长”的良好道德风尚，即“人人亲其亲、长其长，而天下平”。</p><h2 id="如何理解“诚者，天之道也”中的人与天？"><a href="#如何理解“诚者，天之道也”中的人与天？" class="headerlink" title="如何理解“诚者，天之道也”中的人与天？"></a>如何理解“诚者，天之道也”中的人与天？</h2><p>“诚者，天之道也”，出自《孟子·离娄章句上》第十二节：“孟子曰：‘居下位而不获于上，民不可得而治也。获于上有道：不信于友，弗获于上矣；信于友有道：事亲弗悦，弗信于友矣；悦亲有道：反身不诚，不悦于亲矣；诚身有道：不明乎善，不诚其身矣。是故诚者，天之道也；思诚者，人之道也。至诚而不动者，未之有也；不诚，未有能动者也。’”该节文字除最后两句外，其余的几乎与孔子《中庸》第二十章中的几节文字完全相同，而所强调的还是诚的问题。所以，《孟子》这是在复述《中庸》里的话。但是，现代有的学者分析后认为，应该是《孟子》在先。孔孟之道本是一脉相承的。所以，此处不纠结于此，而是从《孟子》的角度来理解其中的“人”与“天”。首先来阐释一下孟子所说的话的意思。</p><p>职位低下而得不到上司的信任，是不能得到民的支持的，也就不能治理百姓。要获得上司的信任也有一定的道，如果不能得到朋友的信任，也就不能获得上司的信任。取信于朋友也有一定的道，如果侍奉父母而不能博得父母的欢心，也就不能得到朋友的信任。博得父母的欢心也有一定的道，如果反躬自问而不诚实，也就不能博得父母的欢心。要想诚实也有一定的道，如果不明白什么是善，也就不能做到诚实。因此，所谓的诚实，是天的道路；追求诚实，是人类的道路。有了至诚的心意而没有感动别人，是没用的。不诚实，要感动别人也是不可能的。</p><p>孟子生活的时代，与孔子生活的时代有过之而无不及。不但社会矛盾突出，社会持续秩序混乱，诸侯国间的纷争也已经升级，当时的一些诸侯国就想通过强力来征服天下，统一天下。所以，孔子的德治天下和德化天下的理想距离现实是越来越远的。孟子继承和丰富了孔子的思想，将道德规范概括为大家熟悉的四个方面：仁、义、礼、智。他认为，“仁、义、礼、智”是人的本性中固有的成分，不是从外界获得的。如何实现德治天下和德化天下的目的，孟子同孔子的想法一致，就是首先从自身做起，修身养性，提高道德素质，塑造完美的人格。如何塑造，要塑造到什么程度，这其间的衡量标准就是“诚”。诚的最高境界是至诚。人的品德修炼到至诚的地步，就合乎天的意旨，这时候，也就达到了天人合一。所以，孟子所说的“诚者，天之道也；思诚者，人之道也”中的天和人，分别指的是品德修炼的两个阶段，也可说两种境界，即天道和人道。</p><p>孟子继承了孔子的天命思想，他的哲学思想的最高范畴就是天。孟子把天人格化为一个具有道德属性的实体，并把“诚”这种道德品质归结为天的本性。所以，天能运行，它的运行就是天道。因而，天道是一个很抽象的名词，指的是客观的自然规律，是人类生产活动过程中必须遵守和服从的规则。按儒家的观点，人在天面前是无能为力的。任何欺天、逆天的行为都是不可取的，也是必定会遭受天的惩罚的。不过，也不用怕，只要顺从天意，合乎天道，就不会招来天怒或天谴。并且，人只要以一颗诚实的心修炼本性，提升道德素养，就能够让自己逐步接近天道，最终就能与天道合一。那如何修炼呢？或者说如何让人道与天道合而为一呢？</p><p>孟子认为，人性本善，而尽管各个社会成员之间因社会分工的不同有了工种和身份的差别，但是在人性方面大家都是一样的。他说：“故凡同类者，举相似也，何独至于人而疑之？圣人与我同类者。”其意思是说，既然大家都一样，凡夫俗子与圣人属同一品种，那么，从理论上来讲，人人都可以修身成为圣人。所以，在修身之路上，谁也不必在意身份等级的差别，一切遵从心性所需。</p><p>作为一般的人，有恶者要赶快弃恶从善，然后像别人一样，把诚实装入心间，并切实做到表里如一。这时候，他们的所作所为就称为常人之道。当他们将仁、义、礼、智四种道德化为自己身上与生俱来的成分，并且做到“动必有道”“语必有礼”“求必有义”“行必有正”时，他们就能称作为人称颂的谦谦君子了。这个时候，他们如果能够出面为国家的建设大计出谋划策，就又能被称作贤人了。君子修炼到至诚阶段，什么事不用思考就能得出答案，不用勉强就知道怎么做，所有的言行举止，全都自然而然地符合上天的原则，也就是天道，那么，这样的君子也就成了圣人了。</p><p>圣人是个什么样子呢？《中庸》中孔子做了说明：“唯天下至圣，为能聪、明、睿知、足以有临也；宽、裕、温、柔、足以有容也；发、强、刚、毅、足以有执也；齐、庄、中、正、足以有敬也；文、理、密、察、足以有别也。”其意思是说，只有天下崇高的圣人，才能做到聪明智慧，能够居上位而临下民；宽宏大量，温和柔顺，能够包容天下；奋发勇健，刚强坚毅，能够决断天下大事；威严庄重，忠诚正直，能够博得人们的尊敬；条理清晰，详辨明察，能够辨别是非邪正；他们的美德广博而又深厚，并且时常会表现出来；德性广博如天，德性深厚如渊；美德表现在仪容上，百姓没有谁不敬佩；表现在言谈中，百姓没有谁不信服；表现在行动上，百姓没有谁不喜悦。孔子说到做到，身体力行，所以被后人尊称为大成至圣。历史上的尧、舜、周公等，大概也是这么修炼来的。这就是榜样。</p><p>孟子继承了孔子的天命思想，所以，他的人、天理论中自然继承了《中庸》中的天人合一的理论。同孔子一样，他的各种思想和主张都是以服务于政治为目的的。所以，他的天，着意在道德范畴，与君权神授中的神所居的天有着本质的区别。他的天道，是一种强化提升了的不可抗的最高级的意志，人世间的朝代更替、君王易位，以及兴衰存亡、富贵穷达，都是由它主宰的结果。而人，只能是天意的承受者和感悟者，人对于天必须百依百顺——“顺天者昌，逆天者亡”。天的意志是不可抗拒的。不过，在天意面前，孟子并不是消极的，而是认为人可以掌握并利用天意，来为人本身服务。比如说，他就拿当年夏禹治水时根据水势总是向下游流动，可导不可遏的规律来说明如何尊重和利用规律的。</p><h2 id="如何理解孟子所说的“穷不失义，达不离道”？"><a href="#如何理解孟子所说的“穷不失义，达不离道”？" class="headerlink" title="如何理解孟子所说的“穷不失义，达不离道”？"></a>如何理解孟子所说的“穷不失义，达不离道”？</h2><p>“穷不失义，达不离道”出自《孟子·尽心章句上》第九节。孟子说：“尊德乐义，则可以嚣嚣矣。故士穷不失义，达不离道。穷不失义，故士得己焉；达不离道，故民不失望焉。古之人，得志，泽加于民；不得志，修身见于世。穷则独善其身，达则兼善天下。”其意思是说，尊崇道德以义为乐，就可以宣扬自己的主张了。所以士人再穷困也不要失去自己为人处事的操守，发达了也不要偏离自己的人生追求。穷困时不失去为人处事的操守，士人就能做到无愧于自己。发达了也不偏离自己的人生追求，百姓也就不会让他失去威望。古时候的人，如果得志，就会惠泽给百姓；如果不得志，就修养自身以作为表率。穷困时独自善待自身，发达时兼顾善待天下百姓。</p><p>孟子的这番话的重心在“道”和“义”。现在再回顾一下什么是道。“率性之谓道”，即遵从人的本性去从事各种活动。什么是义？“义者，宜也，尊贤为大”，“恻隐之心，人皆有之……羞恶之心，义也”，即尊重有贤能、有德行的人，行为合乎世道准则，懂得羞耻、善恶。道和义都是儒家推崇的个人自身修养方面的内容，终极目标就是把人修炼成具有内圣外王人格的完人。在这里，孟子针对的对象是“士”。</p><p>对君子做出具体论述的始于孔子。儒家推崇的君子，是一种理想化的人格，以行仁、行义为己任，并且是一种自觉行为。一般来说，孔子心目中的君子首先是一个知书达礼的读书人。而士，既可称为有职业的人士，也可理解为一个社会阶层。这个阶层从社会地位上来看，在我国商、周时期指的是贵族的最低一级，介于卿大夫和庶民之间的一个阶层；从儒家推崇的道德操守上来看，介于已经脱离小人趣味且具备一定品德操守的人与君子之间。他们距君子所差的就是没有将仁融入心性。士是整个社会中的一种不可替代的阶层，对于他们，儒家要想将自己的德治和礼治天下的目标实现，就不能绕过他们。对他们有何要求呢？与君子必须将仁融入心性不同，孟子对他们的要求是必须懂得什么是“道”，而且在懂得“道”的基础上再去行义。</p><p>首先要做到贫贱不能移，即在贫困卑贱的处境中不动摇和改变坚强的意志。贫穷是人性中排斥的成分，但又是人生际遇中往往排斥不掉的一种生活状况，其极端处能威胁人的生存。贫穷是人生的低潮部分，处在贫穷之中，人的心性无疑是受到压抑的。这时，人的选择无不是试图改变它们，即常言说的穷则思变。但是，现实往往是，不论人怀着怎样良好的愿望，付出多大的努力，始终无法改变贫穷。当这种状况无限拉长时，考验人的意志的时候就到了。这时，首先就让人看到了人生的负面，有了诸如人穷志短、坑蒙拐骗之类的不良现象。这时，如果是君子呢？“君子固穷”，所以不用担心会出现败德背道甚至丧尽天良的事会发生。对于士呢？他们本该是有一定操守、有一定志向的人，他们自然会有耐不住清贫的时候。所以，孟子就希望他们能做到“穷不失义”，要有羞恶之心，不去做违背世道准则的事。并且，还要学会“独善其身”，不去做违背良心的事，不与道德败坏的人同流合污。</p><p>其次要做到富贵不能淫，即身处富贵之中不能放纵身心，不能暴殄天物。荣华富贵是人性中向往的成分，也可说是人追求的生活目标的最高情形。如何对待富贵？在这方面，中国人似乎想偏了、走偏了，以致于把富贵当做了一种罪恶。儒家包括孔子的学说当中，从不排斥人们对富贵的追求，他们所反对的是追求手段上的不义——“不义而富且贵，于我如浮云”，和获取方式上的无道——“君子爱财，取之有道”。所以，追求富贵是符合人的心性的。那么，该怎样对待富贵呢？孟子认为，对待富贵“不能淫”，通俗地讲就是，有钱不能放纵，有钱不能任性，不能为富不仁。富贵了，就该把道义放在心头，不做败坏人伦道德的事，不做伤风败俗的事，不做丧权辱节的事，不让利欲熏心，而应该有悲天悯人的情怀，有同情不幸的良知，更要有兼济天下的心胸。这也是孟子对士的要求。</p><p>另外，还要做到的就是威武不能屈。在强权、强势面前，个人的实力往往是极其有限的。这时，出现的情形往往就是，要么顺从权势，要么坚守气节。做到前者，一时能获得荣华富贵，但气节有亏；做到后者，气节虽保全了，但往往会付出身家性命的代价。这时，该怎么取舍？孟子认为，应做到“舍生取义”。士可杀不可辱，宁为玉碎，不为瓦全。历史上，多少人做到了舍生取义，慷慨赴死，留下了多少可歌可泣的英雄事迹。荆轲别易水，舍身刺秦王；蔺相如大义凛然，完璧归赵；文天祥鄙视威逼利诱，英勇就义……</p><h2 id="“尽信书，则不如无书”反映了孟子什么样的思想？"><a href="#“尽信书，则不如无书”反映了孟子什么样的思想？" class="headerlink" title="“尽信书，则不如无书”反映了孟子什么样的思想？"></a>“尽信书，则不如无书”反映了孟子什么样的思想？</h2><p>“尽信书，则不如无书”出自《孟子·尽心章句下》第三节。孟子说：“尽信书，则不如无书。吾于武成，取二三策而已矣。仁人无敌于天下，以至仁伐至不仁，而何其血之流杵也？”其意思是说，一味地相信《尚书》，还不如没有《尚书》。我对于《武成》这篇文章，只不过取其中的二三个道理罢了。能爱民的人无敌于天下，以最爱民的政策征伐最不爱民的，怎么会血流成河把舂米的木棒都漂起来呢？</p><p>首先要明了，孟子所说的“书”，指的是《尚书》这本书籍。并且，他还举例提到了其中的《武成》一文。《武成》这篇文章，讲述的是周武王在灭商取得重大成功后的重要政事。</p><p>根据《史记·周本纪》记载，武王进殷都伐纣并杀了纣后，就命令召公把箕子从牢狱里释放出来；命令毕公释放了被囚禁的百姓，表彰商容的里巷，以褒扬他的德行；命令南宫括散发鹿台仓库的钱财，发放钜桥粮仓的粮食，赈济贫弱的民众；命令南宫括、史佚展示传国之宝九鼎和殷朝的宝玉；命令闳夭给比干的墓培土筑坟；命令主管祭祀的祝官在军中祭奠阵亡将士的亡灵。然后才撤兵回西方去。</p><p>路上武王巡视各诸侯国，记录政事，写下了《武成》，宣告灭殷武功已成。孟子所要表达的意思是，既然是记叙政事，就没有必要照本宣科，只要记住它的中心内容就行了。而《武成》的中心只有一个：爱民！以爱民为基础，以爱民为前提，这就是周文王、武王能成功的道理。所以武王伐纣，心中装着百姓，一切行动都是从爱民出发，这样的战争怎么会血流成河呢？这简直就是不流血的战争。而据《史记》记载，当时武王伐纣时，所率兵力不到三万人，而纣王派来对抗的军队多达七十万人。结果，武王的军队一到，纣王的军队无心抵抗，全都阵前倒戈投降了。这就是爱民者无敌于天下的最好范例。</p><p>到这里，孟子所说的“尽信书，不如无书”的用意就十分明朗起来：天下的书是读不尽的，没必要为读书而读书，只需知道书的中心思想就行了——如果这些中心思想中没有仁的主张，那这样的书也就没必要读了，而且这样的书还不如没有好。所以，孟子所说的这句话，目的是为了引出他的仁政主张。</p><p>孟子的仁政主张是从孔子仁学的思想上继承发展而来的，继而成为一门学说，扩充发展成包括政治、经济、文化等各个方面的施政纲领，目的是希望统治者能够宽厚待民，惠泽于民，从而安抚人心，安定天下。它的最基本的精神就是爱民。在《孟子·梁惠王章句上》第五节中，孟子明确提出仁政的主张：“王如施仁政于民，省刑罚，薄税敛，深耕易耨；壮者以暇日，修其孝悌忠信，入以事其父兄，出以事其长上，可使制梃以挞秦楚之坚甲利兵矣。故曰：‘仁者无敌’。”其意思是说，国王如果对老百姓施行仁政，减免刑罚，少收赋税，深耕细作，及时除草；让身强力壮的人抽出时间修养孝顺、尊敬、忠诚、守信的品德，在家侍奉父母兄长，出门尊敬长辈上级，这样就是让他们制作木棒也可以打击那些拥有坚实盔甲锐利刀枪的秦楚军队了。所以说：施行仁政的人是无敌于天下的。所以，孟子的仁政就是，让民有产，有业，对民省刑罚，薄税敛，让民有德识修养……一句话，一切为了爱民。</p><p>具体如何施行仁政呢？孟子认为，仁政的基础就是“制民之产”，让民有“恒产”。让老百姓在生活上有保障，能够安居乐业。这样，政权就能因老百姓的拥护而稳固。在具体措施上，有农业上的。比如，孟子主张施行井田制，让一度遭到破坏的井田制恢复过来，让流离失所的农民回归土地并拥有土地。有政策上的。比如，孟子主张轻徭薄赋，征发徭役时要不违农时，以减轻百姓负担。有教育上的。比如，孟子主张以礼来强化人伦秩序和社会秩序……所有的这一切，其出发点和动机都是爱民。这就是施仁政。</p><p>“仁者爱人”，其所取得的效果就是孟子所说的“爱人者人恒爱之，敬人者人恒敬之”。所以孟子说：“以佚道使民，虽劳不怨；以生道杀民，虽死不怨杀者。”其意思是说，以让老百姓安逸舒适为目的来使用老百姓，老百姓虽劳累却不埋怨；以让老百姓能生存为目的而杀该杀的人，老百姓即使知道自己会死也不会埋怨杀害自己的人。道理很明显，提倡、号召老百姓过上幸福的日子，国强民富，老百姓谁不愿意呢？孟子认为，人性本善，谁对自己好，自己就对谁好。所以，当统治者让自己的心性中融入仁德并以仁政行天下时，老百姓就会投桃报李，对统治者和国家也就充满着爱心，对他们的施政措施也就会认同也就会拥护，即使牺牲自己也在所不惜。这恐怕也是孟子憧憬和希望看到的结果。</p><p>不过应该看到，孟子推崇的仁政，并不同于现代意义上的博爱，也不同于让社会变成无阶级差别的民主社会的主张，而是一种让社会秩序回归到有利于统治的良性发展层面上的政治主张。毕竟，在形式上，他不过是模仿周制拟定了一套从天子到庶人的等级制度；在理论上，又把统治者和被统治者的关系比作父母对子女的关系。所以，他的仁政主张，并没有消弭阶段差别，并没有鼓动人民当家作主，而只是一种理想化的人治诉求：通过施行仁政，统治者就会像父母一样关心人民的疾苦，人民就像对待父母一样自觉自愿地去拥护和服侍统治者。</p><p>当然，也可以直接从字面上来理解“尽信书，则不如无书”。即，读书时应该加以分析思考，不能盲目地迷信书本。这其中最关键的就是反对教条主义。所谓的教条主义，指的是完全遵从书本上的规定，一切从定义、公式出发，照搬书本上的经验，不和现实中的具体情况相结合来处理问题的一种思想作风。其实，在自然科学领域，如果能时刻考虑环境因素的话，则只有人的思维的僵化，而无所谓的教条主义。所以，教条主义的危害多出现在社会学领域，尤其是政治和思想意识领域。</p><p>人的思想意识是千变万化的，随着时空的改变而改变。从社会存在的角度来看，每一种思想意识都是一种合理的现象，无所谓对错之分。错就错在，现实中有的人硬生生地把一家之言当做普遍的真理而凌驾于其他思想之上，并且让别人去膜拜。所以，思想领域的教条越多，接受他的人所犯的教条主义错误就越多。从这个角度来说，倒是宁愿看到人们做到“尽信书，则不如无书”。人人都做到这一点，那么，所犯的教条主义错误也就必然少之又少了。这样说来，孟子的这句话，在今天反而更有普遍意义，更有推广的必要了。</p><h1 id="五、《诗经》：诗三百，一言以蔽之，思无邪"><a href="#五、《诗经》：诗三百，一言以蔽之，思无邪" class="headerlink" title="五、《诗经》：诗三百，一言以蔽之，思无邪"></a>五、《诗经》：诗三百，一言以蔽之，思无邪</h1><p>《诗经》约成书于春秋时期，是我国文学史上第一部诗歌总集，其中最早的作品大约成于西周时期。《诗经》现存305首（既有标题又有文辞的），后人为方便就称它为“诗三百”。它收集了从西周初年至春秋中叶，约十五个诸侯国的民间歌曲，全面地展示了中国社会从西周、到春秋中期时的生活状况。有些诗，如《大雅》中的《生民》《公刘》《皇矣》等，还可上溯到后稷降生到武王伐纣时期，真实地反映了那个时期的历史面貌。</p><p>《诗经》在中国历史上，一直受到历代读书人的尊崇。孔子在他的学说中多处提及《诗经》，他对《诗经》的赞誉是：“诗三百，一言以蔽之，思无邪”，甚至说“不学诗，无以言”，并常用《诗经》来教育自己的弟子。所以，《诗经》对中国古代文学乃至社会产生的影响是久远而广泛的，可称为文学史上的源头之作。</p><h2 id="《诗经》的“六义”指的是什么？"><a href="#《诗经》的“六义”指的是什么？" class="headerlink" title="《诗经》的“六义”指的是什么？"></a>《诗经》的“六义”指的是什么？</h2><p>《诗经》在表现形式上属于诗歌类，它的句式一般以四字为主，其间也夹杂有二到九个字的，只有个别的诗不遵从这种安排，如《伐檀》。在内容上，有反映战争的，有揭露社会的，有描写爱情的，有叙述风俗的，十分丰富。在体裁上分为风、雅、颂三部分。其中“风”是地方民歌，有十五国风，共160首；“雅”主要是朝廷乐歌，分大雅和小雅，共105首；“颂”主要是宗庙乐歌，有40首。在写作的修辞手法上有赋、比、兴三种。人们将《诗经》体裁风、雅、颂和修辞手法赋、比、兴合称为《诗经》的“六义”。</p><h2 id="为什么说“不学诗，无以言”？"><a href="#为什么说“不学诗，无以言”？" class="headerlink" title="为什么说“不学诗，无以言”？"></a>为什么说“不学诗，无以言”？</h2><p>“不学《诗》，无以言”出自孔子《论语·季氏篇》中孔子对儿子孔鲤说的话。其意思是说，不学习《诗经》，就无法与别人交谈。为什么这么说？显然与《诗经》的特点和当时的时代背景有关。</p><p>《诗经》上的作品，基本上都具有现实主义风格，来于生活，反映生活。它的题材涉及面非常广，包括现实生活当中的政治、经济、文化、人的情感和民俗风情等各个领域，所描述的内容涉及农业生产、祭祀、战争、徭役、男女情爱和人生态度等各个方面。并且，作品的作者层次非常多，包括平民到贵族的各个社会阶层，他们本身也是反映现实生活的影子。所以，《诗经》的这些特点，首先对提升一个人的学识具有旁者无以达到的影响力。</p><p>在春秋战国的数百年间，诸侯国间的战事吞并连绵不断，礼崩乐坏，不仅给社会带来了动乱，也让百姓苦不堪言。如何结束这种乱世，让天下能够长治久安，让老百姓能够安居乐业，包括孔子在内的许多仁人志士都在寻找着救世良药。孔子在经历了半生的奔走，以屡屡碰壁的代价寻找到了以礼治国、德化天下的政治思想——可以简单地把他的政治思想归为精神文化范畴。所以，当看到《诗经》在百姓间广为流行，又直面现实，寓有历史的、传统的政治、道德价值思想在里面，而且它们都符合当时社会公认的道德原则，自然就倍为推崇了。</p><h1 id="六、《尚书》：明仁君治民，明贤臣事君"><a href="#六、《尚书》：明仁君治民，明贤臣事君" class="headerlink" title="六、《尚书》：明仁君治民，明贤臣事君"></a>六、《尚书》：明仁君治民，明贤臣事君</h1><p>《尚书》是一部上古史料汇编，是现存最早的中国古代政治历史文献合集，可以称之为古代君臣的政治教科书。《尚书》的内容包括：历代君王对臣民的训诫或誓辞；臣子对君王提出的谏言；上古到夏商周三代的零碎文献记录。</p><p>《尚书》里包含了浓厚的民本意识与公天下精神。自古以来，无论哪家哪派，都以公正廉平为执政大德，都以克灭私心为理民之要。“以公灭私，民其允怀”的思想，既在某种层面上论证了设立君主的合理性，又对君主行使权力设置了一些条件。最根本的一点就是，执政贵公，赢得民心。为了实现这点，《尚书》要求决策者要“无稽之言勿听，弗询之谋勿庸”。只有保持不偏不倚的中正态度，才能严谨而公平地处理政事，不至于被谣言糊弄，也不会头脑发热冒进求成。</p><p>《尚书》的民本思想主要体现在以下几个方面：第一，重视生产，安民利民。第二，唯民是保，明德慎罚。</p><h1 id="七、《礼记》：往而不来非礼也，来而不往亦非礼也"><a href="#七、《礼记》：往而不来非礼也，来而不往亦非礼也" class="headerlink" title="七、《礼记》：往而不来非礼也，来而不往亦非礼也"></a>七、《礼记》：往而不来非礼也，来而不往亦非礼也</h1><p>礼乐文明是古代中国的标志。所谓“礼”指的是礼仪风俗教化。以礼治国的思想源远流长，古人“以德治国”的理想，最终落实在繁琐细密的礼仪上。关于“礼”的学问，主要集中在《礼记》《仪礼》《周礼》三部礼学经典中。其中《礼记》最受古人重视，从汉朝就是官学制定教材，而到了科举制时代也成为出题范围之一。</p><p>春秋时代，礼崩乐坏。孔子对此深感痛心，他在答齐景公问政时指出，治国之要在于“君君，臣臣，父父，子子”。</p><p>这句话的大意是：社会上下各阶层都应该遵守自己所处位置的本分。具体而言，就是重新按照周礼的要求来为人处世。</p><p>按照儒家的传统观念，上古五帝采取的是禅让制，当时是“公天下”阶段，是高度理想化的大同世界。而后来君主从禅让变成了世袭，“家天下”时代开始了。原先“天下为公”的精神，也因大同世界的结束而沉沦。随着西周的灭亡，天下从此进入了礼崩乐坏的时代。在孔子看来，这就是境界远低于“大同世界”的“小康世界”。</p><p>三纲五常一直是封建礼教的核心内容。尽管每个时代的礼仪风俗各有差异，但本质上都没脱离这个范畴。不过，三纲五常虽贵为儒家伦理道德的大经大法，在现实中也不是绝对的。在古人的礼教中，“君为臣纲”与“为人臣之礼，三谏而不听，则逃之”是并行不悖的。</p><h2 id="“三纲五常”讲的是什么样的伦理？"><a href="#“三纲五常”讲的是什么样的伦理？" class="headerlink" title="“三纲五常”讲的是什么样的伦理？"></a>“三纲五常”讲的是什么样的伦理？</h2><p>所谓“三纲”指的是“君为臣纲，父为子纲，夫为妻纲”。它确立了封建等级秩序中的基本道德观念，即臣、子、妻要绝对服从于君、父、夫。</p><p>关于“五常”有几种说法。按照第一种说法，“五常”是五种做君子的准则，分别是“仁、义、礼、智、信”。唐朝学者孔颖达在注疏中说：“五常即五典，谓父义、母慈、兄友、弟恭、子孝。”还有一种解释认为，五常就是五伦，即君臣、父子、兄弟、夫妇、朋友五种人伦关系。也就是孟子所说的“父子有亲，君臣有义，夫妇有别，长幼有序，朋友有信”。</p><h1 id="八、《周易》：君子以俭德辟难，不可荣以禄"><a href="#八、《周易》：君子以俭德辟难，不可荣以禄" class="headerlink" title="八、《周易》：君子以俭德辟难，不可荣以禄"></a>八、《周易》：君子以俭德辟难，不可荣以禄</h1><p>《周易》也叫《易经》，简称《易》，是中国古代一部包含有哲学思想的书籍。书中以高度抽象的六十四卦的形式来反映现实中可能发生的各种各样的变化，并附以卦爻辞作简要说明。</p><p>《易经》被儒家尊为五经之首，它包括了古代的《连山》《归藏》和《周易》三部分。后来《连山》和《归藏》失传，《周易》就指代了《易经》。《周易》由周文王创作，从表现形式上来看，是一部占卜书。它由被称为“经”的六十四个卦组成，每个卦的内容包括卦画、卦名、卦辞、爻题、爻辞。《周易》内容成于周初，到了后来就越来越难以让人看懂。于是，就出现了对《周易》的解释书籍。后来在春秋时期，孔子的弟子在孔子对周易的理解的基础上著了《易传》。《易传》较前人更进一步，不仅对《周易》进行了解说，还发挥了评论。</p><h1 id="九、《春秋》：必尊以经而后读之，须怀以诚敬之心读之"><a href="#九、《春秋》：必尊以经而后读之，须怀以诚敬之心读之" class="headerlink" title="九、《春秋》：必尊以经而后读之，须怀以诚敬之心读之"></a>九、《春秋》：必尊以经而后读之，须怀以诚敬之心读之</h1><p>《春秋》指的是《春秋经》，又称《麟经》或《麟史》，是中国古代儒家典籍“六经”之一。记载了从鲁隐公元年（公元前722年）到鲁哀公十四年（公元前481年）的历史，也是中国现存最早的一部编年体史书兼历史散文集。它是鲁国的编年史，由孔子修订而成。</p><p>《春秋》的写作语言极为精练。但是因为文字过于简质，后人不易理解，所以就有人对它进行了诠释——对书中的记载进行解释和说明，并把这种诠释编成的书称为“传”。其中有左丘明诠释的《春秋左氏传》，公羊高诠释的《春秋公羊传》，谷梁赤诠释的《春秋谷梁传》。它们合称《春秋三传》，并列入儒家经典。据《汉书·艺文志》记载，为《春秋》作传的共5家：《左氏传》30卷、《公羊传》11卷、《谷梁传》11篇、《邹氏传》11卷、《夹氏传》11卷。其中后两种已经遗失不存。《公羊传》和《谷梁传》成书于西汉初年，用当时通行的隶书所写，称为今文。《左传》有两种，一种出于孔子旧居的墙壁之中，使用秦朝以前的古代字体写的，称为古文；一种是从战国时期的荀卿流传下来的。《公羊传》和《谷梁传》与《左传》有很大的不同。《公羊传》和《谷梁传》讲“微言大义”，希望试图阐述清楚孔子的本意。《左传》以史实为主，补充了《春秋》中没有记录的大事，而一些纪录和《春秋》有出入，有人认为左传的史料价值大于《公羊传》和《谷梁传》。</p><p>《春秋》是按编年的方式来记述历史的。作者在编订《春秋》的过程中，除了夹杂有少量评论性质的诠释外，基本上就是将史实进行系统性的罗列记叙。所以，无论从内容上还是记叙方式上，《春秋》都属于历史学范畴，即《春秋》就是一部历史学著作。</p>]]></content>
      
      
      <categories>
          
          <category> Literature </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入浅出HTTP</title>
      <link href="/2018/06/08/simple-http/"/>
      <url>/2018/06/08/simple-http/</url>
      
        <content type="html"><![CDATA[<h1 id="深入浅出HTTP"><a href="#深入浅出HTTP" class="headerlink" title="深入浅出HTTP"></a>深入浅出HTTP</h1><h2 id="一、什么是Http和TCP"><a href="#一、什么是Http和TCP" class="headerlink" title="一、什么是Http和TCP"></a>一、什么是Http和TCP</h2><blockquote><p>HTTP（HyperText Transfer Protocol)超文本传输协议，是互联网上应用最为广泛的一种网络协议。所有的WWW文件都必须遵守这个标准。设计HTTP最初的目的是为了提供一种发布和接收HTML页面的方法。1960年美国人Ted Nelson构思了一种通过计算机处理文本信息的方法，并称之为超文本（hypertext）,这成为了HTTP超文本传输协议标准架构的发展根基。Ted Nelson组织协调万维网协会（World Wide Web Consortium）和互联网工程工作小组（Internet Engineering Task Force ）共同合作研究，最终发布了一系列的RFC，其中著名的RFC 2616定义了HTTP 1.1。</p></blockquote><blockquote><p>TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议，由IETF的RFC 793定义。在简化的计算机网络OSI模型中，它完成第四层传输层所指定的功能，用户数据报协议（UDP）是同一层内 [1]  另一个重要的传输协议。在因特网协议族（Internet protocol suite）中，TCP层是位于IP层之上，应用层之下的中间层。不同主机的应用层之间经常需要可靠的、像管道一样的连接，但是IP层不提供这样的流机制，而是提供不可靠的包交换。 [1] </p></blockquote><h2 id="二、Http和Tcp关系"><a href="#二、Http和Tcp关系" class="headerlink" title="二、Http和Tcp关系"></a>二、Http和Tcp关系</h2><p><img src="https://upload-images.jianshu.io/upload_images/12321605-3cee389050c0b761.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="TCP-IP-model-vs-OSI-model.png"></p><p>由上面的图，我们看到TCP运行在TCP/IP模型的传输层（Transport），HTTP运行在应用层（Application）。简单来说就是HTTP是基于TCP的封装，HTTP运行在TCP之上。</p><p>当客户端发起一个HTTP请求服务器的时候，HTTP会通过TCP建立起一个到服务器的Socket链接，然后发送相关请求数据，服务器收到请求数据以后，会处理相关逻辑然后把数据回复给客户端，这就完成了一次HTTP请求的收发。</p><h2 id="三、Keep-Alive"><a href="#三、Keep-Alive" class="headerlink" title="三、Keep-Alive"></a>三、Keep-Alive</h2><p>在Http1.0的时候Http每次请求需要的数据完毕后，会立即将TCP连接断开，这个过程是很短的。所以Http连接是一种短连接，是一种无状态的连接。所谓的无状态，是指浏览器每次向服务器发起请求的时候，不是通过一个连接，而是每次都建立一个新的连接。如果是一个连接的话，服务器进程中就能保持住这个连接并且在内存中记住一些信息状态。而每次请求结束后，连接就关闭，相关的内容就释放了，所以记不住任何状态，成为无状态连接。</p><p>随着时间的推移，html页面变得复杂了，里面可能嵌入了很多图片，这时候每次访问图片都需要建立一次tcp连接就显得低效了。因此Keep-Alive被提出用来解决效率低的问题。从HTTP/1.1起，默认都开启了Keep-Alive，保持连接特性，简单地说，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。虽然这里使用TCP连接保持了一段时间，但是这个时间是有限范围的，到了时间点依然是会关闭的，所以我们还把其看做是每次连接完成后就会关闭。后来，通过Session, Cookie等相关技术，也能保持一些用户的状态。但是还是每次都使用一个连接，依然是无状态连接。</p><h3 id="Tcp工具验证Keep-Alive功能"><a href="#Tcp工具验证Keep-Alive功能" class="headerlink" title="Tcp工具验证Keep-Alive功能"></a>Tcp工具验证Keep-Alive功能</h3><p>上面说了Http就是基于Tcp的封装，我们完全可以找个Tcp客户端来验证上面概率的正确与否。</p><ol><li>首先我们开个Tcp工具，我这里用的是AppStore里面一个“网络助手”的工具先连上一个我一个现有Http服务器</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-778cc20869233eec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="http-step1.png"></p><ol start="2"><li><p>我们自己构造HTTP请求，用TCP方式手动发给送服务器，内如如下</p><pre><code> GET /app1/test2 HTTP/1.1 Host: 10.8.124.194 Accept: */* Cookie: Name=Alex User-Agent: CookieTest/1 CFNetwork/901.1 Darwin/17.6.0 Accept-Language: en-us Accept-Encoding: gzip, deflate Connection: keep-alive</code></pre><p> 在HTTP头中我们指定了<code>GET /app1/test2 HTTP/1.1</code>表示用GET方式访问 服务器 <code>/app1/test2</code> ，并指定连接方式  <code>Connection: keep-alive</code>，表示不断开连接。</p></li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-e6d037ff423dbd9b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="http-step2.jpg"></p><pre><code>这里我们看到服务器给我们返回数据以后Socket连接并没有立即断开。</code></pre><ol start="3"><li>我们在来设置下<code>Connection: close</code>发送给服务器测试下，结果如下：</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-faae2d39bfad802b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="http-step3.png"></p><p>   此时我们可以看到，服务器回复给客户端数据以后，立马就断 开了连接，服务器在回复的HTTP头中也指明了<code>Connection: close</code>。</p><h2 id="四、HTTP管道传输机制（pipelining）"><a href="#四、HTTP管道传输机制（pipelining）" class="headerlink" title="四、HTTP管道传输机制（pipelining）"></a>四、HTTP管道传输机制（pipelining）</h2><blockquote><p>HTTP pipelining is a technique in which multiple HTTP requests are sent on a single TCP connection without waiting for the corresponding responses.[1]</p></blockquote><p>HTTP1.1 版引入了管道机制（pipelining），即在同一个TCP连接里面，发送一个请求后，不需要等待服务器返回就可以继续发送HTTP请求。这样就进一步改进了HTTP协议的效率。举例来说，客户端需要请求两个资源。以前的做法是，在同一个TCP连接里面，先发送A请求，然后等待服务器做出回应，收到后再发出B请求。管道机制则是允许浏览器同时发出A请求和B请求，但是服务器还是按照顺序，先回应A请求，完成后再回应B请求。</p><p>如下图所示：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-87e151e8f6c453d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="HTTP_pipelining.png"></p><p><em><strong>PS：这里需要说明的是虽然客户端可以连续发送几个请求，但是服务器的数据返回必须是按客户端的请求顺序返回，假设客户端先后快速请求了API1和API2，假设API1响应需要10s，API2响应需要1s，服务器一定是先返回API1的数据，再返回API2的数据</strong></em></p><h3 id="TCP工具验证pipelining"><a href="#TCP工具验证pipelining" class="headerlink" title="TCP工具验证pipelining"></a>TCP工具验证pipelining</h3><ol><li><p>首先服务器段，我定义好了<code>/app1/test1</code>和<code>/app1/test2</code>两个接口，<code>/app1/test1</code>里面接收到客户端请求以后，sleep 5秒以后才返回，<code>/app1/test2</code> 则不sleep直接返回。</p><pre><code> @app.route('/app1/test1') def test1():     start_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')     time.sleep(5)     end_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')     res = 'test1 start_time %s,end_time %s  %s:%s' % (start_time, end_time, request.remote_addr,                                                       request.environ.get('REMOTE_PORT'))     return res   @app.route('/app1/test2') def test2():     end_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')     res = 'test2 end_time %s  %s:%s' % (end_time, request.remote_addr, request.environ.get('REMOTE_PORT'))     return res</code></pre></li><li><p>我们先用Tcp向服务器依次发送<code>/app1/test1</code>和<code>/app1/test2</code>两个请求</p><pre><code> GET /app1/test1 HTTP/1.1 Host: 10.8.124.194 Accept: */* Cookie: Test=local set User-Agent: CookieTest/1 CFNetwork/901.1 Darwin/17.6.0 Accept-Language: en-us Accept-Encoding: gzip, deflate Connection: keep-alive  GET /app1/test2 HTTP/1.1 Host: 10.8.124.194 Accept: */* Cookie: Test=local set User-Agent: CookieTest/1 CFNetwork/901.1 Darwin/17.6.0 Accept-Language: en-us Accept-Encoding: gzip, deflate Connection: keep-alive</code></pre></li></ol><p><img src="https://upload-images.jianshu.io/upload_images/12321605-1c3571b1f231b86f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="http_pipelining1.png"></p><p>如图，客户端请求了<code>/app1/test1</code>和<code>/app1/test2</code>两个请求以后，服务器端阻塞在<code>/app1/test1</code>请求中，过了5秒钟以后，服务器先返回<code>test1</code>的数据，然后再返回了<code>test2</code>的数据。</p><p>数据返回结果如图二所示:</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-bc29d216d4cfdc9d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="http_pipelining2.png"></p><h3 id="思维发散：客户端一个请求阻塞，会阻塞其他所有请求吗？"><a href="#思维发散：客户端一个请求阻塞，会阻塞其他所有请求吗？" class="headerlink" title="思维发散：客户端一个请求阻塞，会阻塞其他所有请求吗？"></a>思维发散：客户端一个请求阻塞，会阻塞其他所有请求吗？</h3><p>上面的测试可以知道在Http1.1的时候所有接口所有请求都是按顺序返回的，当有一个请求被阻塞，后面的那后面的请求也会被阻塞。那平时我们用的客户端App也会这样吗？平时我们App启动的时候可能瞬间请求10几个接口，如果中间一个接口被阻塞了，App是否会也会阻塞？</p><h4 id="iOS上测试"><a href="#iOS上测试" class="headerlink" title="iOS上测试"></a>iOS上测试</h4><p>我这里用for循环去执行10个异步的get请求代码如下</p><pre><code>      NSLog(@"开始请求接口");   for (int i = 0; i&lt;10; i++) {        [self getUrl:@"http://10.8.124.194/app1/test2" dataBody:nil Completetion:^(id result, NSError *error) {            NSLog(@"test1 : %@",result);        }];    }</code></pre><p>在iOS11.4下输入日志如下，接口里返回了客户端请求的IP地址和端口，从端口可以看出来，客户端最多只开了四个连接（52997，52998，52999，53000），前面四个接口都可以正常返回，后面所有的请求都超时了，当然我们可以把超时时间设置长一些，所有接口都能正常返回。<strong>通过这个结论我们可以知道：如果所有通道被阻塞了，后面所有的接口都会被阻塞</strong></p><pre><code>2018-06-08 14:31:16.649633+0800 CookieTest[77315:18959238] 开始请求接口2018-06-08 14:31:21.852816+0800 CookieTest[77315:18959238] test1 : test1 start_time 2018-06-08 14:31:16,end_time 2018-06-08 14:31:21  10.1.99.68:529982018-06-08 14:31:21.854052+0800 CookieTest[77315:18959238] test1 : test1 start_time 2018-06-08 14:31:16,end_time 2018-06-08 14:31:21  10.1.99.68:530002018-06-08 14:31:21.854627+0800 CookieTest[77315:18959238] test1 : test1 start_time 2018-06-08 14:31:16,end_time 2018-06-08 14:31:21  10.1.99.68:529992018-06-08 14:31:21.855119+0800 CookieTest[77315:18959238] test1 : test1 start_time 2018-06-08 14:31:16,end_time 2018-06-08 14:31:21  10.1.99.68:529972018-06-08 14:31:26.662885+0800 CookieTest[77315:18959320] Task &lt;60D5735A-2FA9-4471-83AC-8CA45331DA5C&gt;.&lt;5&gt; finished with error - code: -10012018-06-08 14:31:26.671603+0800 CookieTest[77315:18959320] Task &lt;19F56B62-AC38-4CAD-9DEE-41BDAF56F142&gt;.&lt;6&gt; finished with error - code: -10012018-06-08 14:31:28.920738+0800 CookieTest[77315:18959238] test1 : error2018-06-08 14:31:28.921320+0800 CookieTest[77315:18959238] test1 : error2018-06-08 14:31:28.921494+0800 CookieTest[77315:18959320] Task &lt;B05712A7-FCA1-49BE-9FE3-2B0CB268B9BC&gt;.&lt;7&gt; finished with error - code: -10012018-06-08 14:31:28.922015+0800 CookieTest[77315:18959320] Task &lt;1EA49B3B-7191-4CDA-9F9B-310E861BA478&gt;.&lt;8&gt; finished with error - code: -10012018-06-08 14:31:28.922101+0800 CookieTest[77315:18959238] test1 : error2018-06-08 14:31:28.922272+0800 CookieTest[77315:18959320] Task &lt;B4DD6C71-06FF-41BF-87AF-C474505DBA43&gt;.&lt;9&gt; finished with error - code: -10012018-06-08 14:31:28.922529+0800 CookieTest[77315:18959238] test1 : error2018-06-08 14:31:28.922857+0800 CookieTest[77315:18959238] test1 : error2018-06-08 14:31:28.922786+0800 CookieTest[77315:18959320] Task &lt;AAFDA4A5-C9EA-4CDE-921C-CB83D3BEB449&gt;.&lt;10&gt; finished with error - code: -10012018-06-08 14:31:28.923383+0800 CookieTest[77315:18959238] test1 : error</code></pre><p>上面的结果，只告诉我们接口阻塞超时了，但是没有体现出来端口复用。下面我用一个服务器不会阻塞的接口来测试，结果如下</p><pre><code>2018-06-08 15:10:23.713832+0800 CookieTest[77361:18975432] 开始请求接口2018-06-08 15:10:23.826531+0800 CookieTest[77361:18975432] test1 : test2 end_time 2018-06-08 15:10:23  10.1.99.68:530692018-06-08 15:10:23.827515+0800 CookieTest[77361:18975432] test1 : test2 end_time 2018-06-08 15:10:23  10.1.99.68:530672018-06-08 15:10:23.827683+0800 CookieTest[77361:18975432] test1 : test2 end_time 2018-06-08 15:10:23  10.1.99.68:530682018-06-08 15:10:23.827820+0800 CookieTest[77361:18975432] test1 : test2 end_time 2018-06-08 15:10:23  10.1.99.68:530662018-06-08 15:10:23.857233+0800 CookieTest[77361:18975432] test1 : test2 end_time 2018-06-08 15:10:23  10.1.99.68:530692018-06-08 15:10:23.861634+0800 CookieTest[77361:18975432] test1 : test2 end_time 2018-06-08 15:10:23  10.1.99.68:530682018-06-08 15:10:23.862247+0800 CookieTest[77361:18975432] test1 : test2 end_time 2018-06-08 15:10:23  10.1.99.68:530672018-06-08 15:10:23.862464+0800 CookieTest[77361:18975432] test1 : test2 end_time 2018-06-08 15:10:23  10.1.99.68:530662018-06-08 15:10:23.889165+0800 CookieTest[77361:18975432] test1 : test2 end_time 2018-06-08 15:10:23  10.1.99.68:530692018-06-08 15:10:23.892295+0800 CookieTest[77361:18975432] test1 : test2 end_time 2018-06-08 15:10:23  10.1.99.68:53068</code></pre><p>可以看的很清楚，这里在<strong>iOS系统里面复用的Socket连接是四个</strong>，客户端端口分别是<code>53066</code>、<code>53067</code>、<code>53068</code>、<code>53069</code></p><h4 id="Mac上测试"><a href="#Mac上测试" class="headerlink" title="Mac上测试"></a>Mac上测试</h4><p>在MAC 控制台程序下输出的日志如下 ，<strong>可以看出MAC有6个连接是复用的</strong>，端口分别是<code>58392</code>，<code>58393</code>，<code>58394</code>，<code>58395</code>，<code>58396</code>，<code>58397</code></p><pre><code>2018-06-08 15:08:56.900511+0800 HttpTestOnMac[71167:2100741] 开始请求接口2018-06-08 15:08:57.064507+0800 HttpTestOnMac[71167:2100741] test1 : test2 end_time 2018-06-08 15:08:57  10.1.99.86:583922018-06-08 15:08:57.064794+0800 HttpTestOnMac[71167:2100741] test1 : test2 end_time 2018-06-08 15:08:57  10.1.99.86:583932018-06-08 15:08:57.064858+0800 HttpTestOnMac[71167:2100741] test1 : test2 end_time 2018-06-08 15:08:57  10.1.99.86:583952018-06-08 15:08:57.064875+0800 HttpTestOnMac[71167:2100741] test1 : test2 end_time 2018-06-08 15:08:57  10.1.99.86:583942018-06-08 15:08:57.065229+0800 HttpTestOnMac[71167:2100741] test1 : test2 end_time 2018-06-08 15:08:57  10.1.99.86:583962018-06-08 15:08:57.065277+0800 HttpTestOnMac[71167:2100741] test1 : test2 end_time 2018-06-08 15:08:57  10.1.99.86:583972018-06-08 15:08:57.093141+0800 HttpTestOnMac[71167:2100741] test1 : test2 end_time 2018-06-08 15:08:57  10.1.99.86:583922018-06-08 15:08:57.093583+0800 HttpTestOnMac[71167:2100741] test1 : test2 end_time 2018-06-08 15:08:57  10.1.99.86:583932018-06-08 15:08:57.093646+0800 HttpTestOnMac[71167:2100741] test1 : test2 end_time 2018-06-08 15:08:57  10.1.99.86:583952018-06-08 15:08:57.095336+0800 HttpTestOnMac[71167:2100741] test1 : test2 end_time 2018-06-08 15:08:57  10.1.99.86:58394</code></pre><h4 id="Android上测试"><a href="#Android上测试" class="headerlink" title="Android上测试"></a>Android上测试</h4><p>测试代码如下</p><pre><code>private void test() {        OkHttpClient httpClient = new OkHttpClient();        for (int i = 0; i &lt; 100; i++) {            Request request = new Request.Builder().url("http://10.8.124.194/app1/test1").build();            httpClient.newCall(request).enqueue(new Callback() {                @Override                public void onFailure(Call call, IOException e) {                }                @Override                public void onResponse(Call call, Response response) throws IOException {                    Ln.e("----res==" + response.body().string());                }            });        }    }   </code></pre><p>返回结果如下：</p><pre><code>MainActivity.java:76): ----res==test1 start_time 2018-06-08 14:52:55,end_time 2018-06-08 14:53:00  10.1.99.17:3759606-08 14:53:01.281 24199-24278/demo.testbrand E/demo.testbrand.MainActivity$3.onResponse(MainActivity.java:76): ----res==test1 start_time 2018-06-08 14:52:55,end_time 2018-06-08 14:53:00  10.1.99.17:3759406-08 14:53:01.281 24199-24277/demo.testbrand E/demo.testbrand.MainActivity$3.onResponse(MainActivity.java:76): ----res==test1 start_time 2018-06-08 14:52:55,end_time 2018-06-08 14:53:00  10.1.99.17:3759706-08 14:53:01.283 24199-24276/demo.testbrand E/demo.testbrand.MainActivity$3.onResponse(MainActivity.java:76): ----res==test1 start_time 2018-06-08 14:52:55,end_time 2018-06-08 14:53:00  10.1.99.17:3759306-08 14:53:01.285 24199-24279/demo.testbrand E/demo.testbrand.MainActivity$3.onResponse(MainActivity.java:76): ----res==test1 start_time 2018-06-08 14:52:55,end_time 2018-06-08 14:53:00  10.1.99.17:3759506-08 14:53:06.395 24199-24276/demo.testbrand E/demo.testbrand.MainActivity$3.onResponse(MainActivity.java:76): ----res==test1 start_time 2018-06-08 14:53:01,end_time 2018-06-08 14:53:06  10.1.99.17:3759606-08 14:53:06.396 24199-24278/demo.testbrand E/demo.testbrand.MainActivity$3.onResponse(MainActivity.java:76): ----res==test1 start_time 2018-06-08 14:53:01,end_time 2018-06-08 14:53:06  10.1.99.17:3759306-08 14:53:06.396 24199-24309/demo.testbrand E/demo.testbrand.MainActivity$3.onResponse(MainActivity.java:76): ----res==test1 start_time 2018-06-08 14:53:01,end_time 2018-06-08 14:53:06  10.1.99.17:3759706-08 14:53:06.397 24199-24280/demo.testbrand E/demo.testbrand.MainActivity$3.onResponse(MainActivity.java:76): ----res==test1 start_time 2018-06-08 14:53:01,end_time 2018-06-08 14:53:06  10.1.99.17:3759406-08 14:53:06.397 24199-24310/demo.testbrand E/demo.testbrand.MainActivity$3.onResponse(MainActivity.java:76): ----res==test1 start_time 2018-06-08 14:53:01,end_time 2018-06-08 14:53:06  10.1.99.17:37595</code></pre><p><strong>可以看出Android的复用连接是5个</strong></p><h2 id="五、HTTP-MAX-Coonection"><a href="#五、HTTP-MAX-Coonection" class="headerlink" title="五、HTTP-MAX-Coonection"></a>五、HTTP-MAX-Coonection</h2><h3 id="浏览器的Http连接数限制"><a href="#浏览器的Http连接数限制" class="headerlink" title="浏览器的Http连接数限制"></a>浏览器的Http连接数限制</h3><p>查阅资料，我们可以指定各个浏览器Socket的并发数为下面这些数据，具体可以参考<a href="https://stackoverflow.com/questions/985431/max-parallel-http-connections-in-a-browser">Max parallel http connections in a browser?</a></p><pre><code>Firefox 2:  2Firefox 3+: 6Opera 9.26: 4Opera 12:   6Safari 3:   4Safari 5:   6IE 7:       2IE 8:       6IE 10:      8Chrome:     6</code></pre><h3 id="iOS和Mac的连接数限制"><a href="#iOS和Mac的连接数限制" class="headerlink" title="iOS和Mac的连接数限制"></a>iOS和Mac的连接数限制</h3><p>Google查了下相关iOS连接数限制，iOS有个<a href="https://developer.apple.com/documentation/foundation/nsurlsessionconfiguration/1407597-httpmaximumconnectionsperhost?language=objc">HTTPMaximumConnectionsPerHost</a><br>这个属性。它的描述如下</p><blockquote><p>Discussion<br>This property determines the maximum number of simultaneous connections made to each host by tasks within sessions based on this configuration.</p></blockquote><blockquote><p>This limit is per session, so if you use multiple sessions, your app as a whole may exceed this limit. Additionally, depending on your connection to the Internet, a session may use a lower limit than the one you specify.</p></blockquote><blockquote><p>The default value is 6 in macOS, or 4 in iOS.</p></blockquote><p>从官方文档的描述来看，在macOS上最大的连接数是6，在iOS上最大的并发数是4，这也符合我们之前的测试结果。</p><h4 id="iOS和Mac的连接数设置的问题"><a href="#iOS和Mac的连接数设置的问题" class="headerlink" title="iOS和Mac的连接数设置的问题"></a>iOS和Mac的连接数设置的问题</h4><p>然后我尝试对这个属性进行设置</p><pre><code>[NSURLSession sharedSession].configuration.HTTPMaximumConnectionsPerHost = 2;</code></pre><p>发现并没有生效。</p><p>最后Google搜了下，有朋友说这个属性已经失效</p><p><a href="https://github.com/aws/aws-sdk-ios/issues/606">HTTPMaximumConnectionsPerHost parameter for NSURLSessionConfiguration</a></p><blockquote><p>Hi, currently there is no way to set it to a higher number. I will take this as a feature request to the team.</p></blockquote><h2 id="六、数据传输在HTTP2中的优化"><a href="#六、数据传输在HTTP2中的优化" class="headerlink" title="六、数据传输在HTTP2中的优化"></a>六、数据传输在HTTP2中的优化</h2><ul><li> 单一连接：刚才也说到 1.1 在请求多的时候，会开启6-8个连接，而 HTTP2 只会开启一个连接，这样就减少握手带来的延迟。</li><li> 多路复用：也就是连接共享，刚才说到 HTTP1.1的 head of line blocking，那么在多路复用的情况下，blocking 已经不存在了。每个连接中 可以包含多个流，而每个流中交错包含着来自两端的帧。也就是说同一个连接中是来自不同流的数据包混合在一起，如下图所示，每一块代表帧，而相同颜色块来自同一个流，每个流都有自己的 ID，在接收端会根据 ID 进行重装组合，就是通过这样一种方式来实现多路复用。</li><li>HTTP 2.0 使用新的二进制格式：基本的协议单位是帧，每个帧都有不同的类型和用途，规范中定义了10种不同的帧。例如，报头(HEADERS)和数据(DATA)帧组成了基本的HTTP 请求和响应；其他帧例如 设置(SETTINGS),窗口更新(WINDOW_UPDATE), 和推送承诺(PUSH_PROMISE)是用来实现HTTP/2的其他功能。那些请求和响应的帧数据通过流来进行数据交换。新的二进制格式是流量控制、优先级、server push等功能的基础</li><li> 头部压缩：HTTP2.0 通过 HPACK 格式来压缩头部，使用了哈夫曼编码压缩、索引表来对头部大小做优化。索引表是把字符串和数字之间做一个匹配，比如method: GET对应索引表中的2，那么如果之前发送过这个值是，就会缓存起来，之后使用时发现之前发送过该Header字段，并且值相同，就会沿用之前的索引来指代那个Header值。具体实验数据可以参考这里：HTTP/2 头部压缩技术介绍</li><li>Server Push：就是服务端可以主动推送一些东西给客户端，也被称为缓存推送。推送的资源可以备客户端日后之需，需要的时候直接拿出来用，提升了速率。具体的实验可以参考这里：iOS HTTP/2 Server Push 探索</li></ul><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="http://www.cocoachina.com/ios/20170425/19118.html">http://www.cocoachina.com/ios/20170425/19118.html</a></p><p><a href="https://stackoverflow.com/questions/15089331/maximize-the-number-of-simultaneous-http-downloads">https://stackoverflow.com/questions/15089331/maximize-the-number-of-simultaneous-http-downloads</a></p><p><a href="https://blog.csdn.net/qq_28885149/article/details/52922107">https://blog.csdn.net/qq_28885149/article/details/52922107</a></p><p><a href="https://www.cnblogs.com/master-song/p/8820244.html">https://www.cnblogs.com/master-song/p/8820244.html</a></p><p><a href="https://www.jianshu.com/p/5c21f75d2120">https://www.jianshu.com/p/5c21f75d2120</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HTTP </tag>
            
            <tag> Net </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Protobuf On HTTP 技术预研 （附代码）</title>
      <link href="/2018/06/03/protobuf-on-http-study/"/>
      <url>/2018/06/03/protobuf-on-http-study/</url>
      
        <content type="html"><![CDATA[<h1 id="Protobuf-技术预研"><a href="#Protobuf-技术预研" class="headerlink" title="Protobuf 技术预研"></a>Protobuf 技术预研</h1><p>Demo地址：<a href="https://github.com/fanlv/ProtobufOnHttp">https://github.com/fanlv/ProtobufOnHttp</a></p><p>Demo地址：<a href="https://github.com/fanlv/ProtobufOnHttpGo">https://github.com/fanlv/ProtobufOnHttpGo</a></p><h2 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h2><p>现在客户端与服务器通讯主要通过Json来做数据交互，本次调研主要比较Protobuf项目中使用的优缺点，和可行性。</p><h2 id="二、Protobuf说明"><a href="#二、Protobuf说明" class="headerlink" title="二、Protobuf说明"></a>二、Protobuf说明</h2><h3 id="2-1-什么是Protobuf"><a href="#2-1-什么是Protobuf" class="headerlink" title="2.1 什么是Protobuf"></a>2.1 什么是Protobuf</h3><p>ProtocolBuffer(以下简称PB)是google 的一种数据交换的格式，它独立于语言，独立于平台。<br>大部分IM通讯协议都是使用PB来传输。具体代表性的有支付宝、微信等App。</p><p><em><strong>说白了，PB就是一种序列化协议，我们开发中想在什么场景中使用Protobuf做为数据交换的序列化协议，取决于自己的业务。</strong></em></p><h3 id="2-2-Protobuf优点"><a href="#2-2-Protobuf优点" class="headerlink" title="2.2 Protobuf优点"></a>2.2 Protobuf优点</h3><ol><li><p>性能好/效率高<br>时间开销： XML格式化（序列化）的开销还好；但是XML解析（反序列化）的开销就不敢恭维了。 但是protobuf在这个方面就进行了优化。可以使序列化和反序列化的时间开销都减短。比较Json的解析速度也快很多。<br>空间开销：protobuf也减少了很多。</p></li><li><p>消息格式所有端共用一个通用的Proto文件描述。</p></li><li><p>支持向后兼容和向前兼容，<br>当客户端和服务器同时使用一块协议的时候， 当客户端在协议中增加一个字节，并不会影响客户端的使用</p></li><li><p>支持多种编程语言 :<br>Java、PHP、C++、<a href="https://github.com/google/protobuf/tree/master/objectivec" title="Title">Objective-c</a> 等等</p></li></ol><h3 id="2-3-Protobuf缺点"><a href="#2-3-Protobuf缺点" class="headerlink" title="2.3 Protobuf缺点"></a>2.3 Protobuf缺点</h3><ol><li><p>二进制格式导致可读性差，为了提高性能，protobuf采用了二进制格式进行编码。这直接导致了可读性差。</p></li><li><p>缺乏自描述， 一般来说，XML是自描述的，而protobuf格式则不是。 给你一段二进制格式的协议内容，不配合你写的.proto文件是看不出来是什么作用的。</p></li><li><p>没有通用的解析方法，必须一个proto文件对应一个Model</p></li></ol><h2 id="三、Protobuf-相关框架"><a href="#三、Protobuf-相关框架" class="headerlink" title="三、Protobuf 相关框架"></a>三、Protobuf 相关框架</h2><h3 id="3-1-gRPC框架"><a href="#3-1-gRPC框架" class="headerlink" title="3.1 gRPC框架"></a>3.1 gRPC框架</h3><p>gRPC  是一个高性能、开源和通用的 RPC 框架，面向移动和 HTTP/2 设计。目前提供 C、Java 和 Go 语言版本，分别是：grpc, grpc-java, grpc-go. 其中 C 版本支持 C, C++, Node.js, Python, Ruby, Objective-C, PHP 和 C# 支持.</p><p>优点 ：gRPC 基于 HTTP/2 标准设计，带来诸如双向流、流控、头部压缩、单TCP连接上的多复用请求等特。这些特性使得其在移动设备上表现更好，更省电和节省空间占用。</p><p>缺点 ： 暂时没有在网上找到有使用这个框架的APP。这个框架的稳定性、对系统的要求，需要进一步调研</p><p>PS：百度2017年9月开源了一个bRPC框架，据说效率比gRPC还要高，但是网上资料很少，稳定性也不确定，能否支持移动端开发也没找到对应的资料。</p><h3 id="3-2-Thrift框架"><a href="#3-2-Thrift框架" class="headerlink" title="3.2 Thrift框架"></a>3.2 Thrift框架</h3><p>Thrift是一款由Fackbook开发的可伸缩、跨语言的服务开发框架，该框架已经开源并且加入的Apache项目。Thrift主要功能是：通过自定义的Interface Definition Language(IDL)，可以创建基于RPC的客户端和服务端的服务代码。数据和服务代码的生成是通过Thrift内置的代码生成器来实现的。Thrift 的跨语言性体现在，它可以生成C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, JavaScript, Node.js, Smalltalk, OCaml , Delphi等语言的代码，且它们之间可以进行透明的通信。</p><p>优点：</p><ol><li>One-stop shop，相对于protobuf，序列化和RPC支持一站式解决，如果是pb的话，还需要考虑选择RPC框架，现在Google是开源了gRpc，但是几年以前是没有第一方的标准解决方案的</li><li>特性丰富，idl层面支持map，protobuf应该是最近才支持的，map的key支持任意类型，avro只支持string，序列化支持自定义protocol, rpc支持thread pool, hsha, no blocking 多种形式，必有一款适合你，对于多语言的支持也非常丰富</li><li>RPC和序列化性能都不错，这个到处都有benchmark，并不是性能最好的，但是基本上不会成为瓶颈或者短板</li></ol><p>缺点：</p><ol><li>移动客户端开发相关的资料很少。</li><li>主要是服务端在使用，移动端使用Thrift的目前只找到有友盟SDK和印象笔记App。</li></ol><h2 id="四、Protobuf-HTTP抓包方案"><a href="#四、Protobuf-HTTP抓包方案" class="headerlink" title="四、Protobuf-HTTP抓包方案"></a>四、Protobuf-HTTP抓包方案</h2><p>Charles支持Protobuf数据的解析工作，只需要指定对应的proto文件就可以。</p><p>相关链接  <a href="https://www.charlesproxy.com/documentation/using-charles/protocol-buffers/">Protocol Buffers • Charles Web Debugging Proxy</a></p><p>点击查看大图<br><img src="https://upload-images.jianshu.io/upload_images/12321605-457d88fe1b7273bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="charles_protubuf_setting.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-8f49cff0356d6a66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="charles_protubuf.png"></p><h2 id="五、业界方案调研"><a href="#五、业界方案调研" class="headerlink" title="五、业界方案调研"></a>五、业界方案调研</h2><h3 id="5-1-Protobuf在Http中的使用-抓包分析"><a href="#5-1-Protobuf在Http中的使用-抓包分析" class="headerlink" title="5.1 Protobuf在Http中的使用(抓包分析)"></a>5.1 Protobuf在Http中的使用(抓包分析)</h3><p>通过抓包分析了支付宝、淘宝、QQ、微信（包括企业微信）、米家、哔哩哔哩等APP数据包。</p><p>1.支付宝公开申明使用了Protobuf，通过抓包发现，支付宝HTTP接口调用很少，大部分都是走的TCP。应该是TCP传输数据使用的是Protobuf。</p><p>2.微信和QQ，Http上没有看到使用Protobuf的接口。TCP抓包里面的数据(具体见下图)都是加密过的数据，具体内部他们是使用什么数据传输，我们这边不得而知。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-f669158830903691.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="weixin.png"></p><p>3.在手机上抓到Google统计的两个接口使用Protobuf，具体是哪个App里调用的没查出来（调用时机不确定）。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-27279c6d37e9fa3c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Protobuf1.png"></p><p>返回内容如下：</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-c36d59519cccd3aa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Protobuf2.png"></p><p>4.使用米家的App时候发现里面调用高德接口获取地理位置信息的接口是使用Protobuf传输的。</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-7861831b0749fcaa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="gaode1.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-df6f42f65119764e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="gaode2.png"></p><p><em><strong>PS：综上所述，可以看出两点，一、Protobuf在HTTP中传输方案上是可行的，也有公司（Google、高德）在这样使用，但是使用场景单一。二、在Http中使用纯的Protobuf协议做数据序列化的方案的公司不多（可能Json传输简单直接，能满足99%公司的需求）。</strong></em></p><h3 id="5-2-gRPC-框架"><a href="#5-2-gRPC-框架" class="headerlink" title="5.2 gRPC 框架"></a>5.2 gRPC 框架</h3><p><em><strong>暂时没找搜到有使用该方案的App</strong></em></p><h3 id="5-3-Thrift-框架"><a href="#5-3-Thrift-框架" class="headerlink" title="5.3 Thrift 框架"></a>5.3 Thrift 框架</h3><p>Thrift也是一种RPC调用框架，Thrift也有自己定义的Interface Definition Language(IDL)（类似Protobuf的proto描述文件），根据定义好的中间文件可以转换为各种语言对应的Model。可以快速的把Model转二进制数据，发送给远端，远端拿到二进制数据可以快速反序列化成Model。传输协议TCP、HTTP都有。</p><p>目前调研到移动端有在使用的Thrift的有： <em><strong>友盟SDK、印象笔记</strong></em></p><p>友盟SDK日志上报的协议使用的是Thrift</p><p><a href="http://bbs.umeng.com/thread-6705-1-1.html">友盟统计新版SDK使用Thrift库的说明</a></p><p>印象笔记移动App中所有的接口都是使用Thrift调用</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-0cbce223b8dbbd0d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="evernote.png"></p><h2 id="六、测试Protobuf在Http中传输Demo"><a href="#六、测试Protobuf在Http中传输Demo" class="headerlink" title="六、测试Protobuf在Http中传输Demo"></a>六、测试Protobuf在Http中传输Demo</h2><h3 id="6-1-proto文件编写"><a href="#6-1-proto文件编写" class="headerlink" title="6.1 proto文件编写"></a>6.1 proto文件编写</h3><p>这里找了一个线上的配置文件地址作为测试数据,下载这个Json格式数据，修改里面数据内容，把Array里面数据全部改成第一条数据，方便Protobuf模拟数据，修改后的json文件数据为：<a href="./images/fans_medal.json">测试数据.Json</a></p><p>按照这个数据格式开始编写Proto文件内容如下：</p><pre><code>    syntax = "proto3";        message RoomInfo {       string room_id = 1;     uint64 start_time = 2;     uint64 end_time = 3;     string md5 = 4;     string zipUrl = 5;     repeated string s_id = 6;        }         message FansMedal {     repeated RoomInfo rooms = 1;}</code></pre><p>保存到fansMedal.proto文件</p><h3 id="6-2-proto文件转Objective-C-Model"><a href="#6-2-proto文件转Objective-C-Model" class="headerlink" title="6.2 proto文件转Objective-C Model"></a>6.2 proto文件转Objective-C Model</h3><p>工具：使用Google 提供的<a href="https://github.com/google/protobuf/releases">https://github.com/google/protobuf/releases</a> Mac版protoc工具</p><p>这里因为我服务器和客户端都是Objective-C写的，服务器是Mac控制台程序，手机是iOS客户端端。所以只用把proto文件转成OC的model，服务器和客户端都用这个model就可以了。</p><p>把fansMedal.proto 复制到跟protoc 一个目录下（也可以不在一个目录protoc指向对应地址就行），执行命令：</p><pre><code>./protoc fansMedal.proto --objc_out=./</code></pre><p>然后会在当前目录下生成 <code>FansMedal.pbobjc.h</code> <code>FansMedal.pbobjc.m</code>两个model文件。copy到服务器和客户端两个项目中。</p><h3 id="6-3-服务器项目配置和实现"><a href="#6-3-服务器项目配置和实现" class="headerlink" title="6.3 服务器项目配置和实现"></a>6.3 服务器项目配置和实现</h3><p>服务器这边需要pod引入Protobuf框架</p><pre><code>pod 'Protobuf', '~&gt; 3.5.0'</code></pre><p>返回Protobuf结构的data数据代码如下：</p><pre><code> FansMedal *fansMedal = [[FansMedal alloc] init]; fansMedal.roomsArray = [[NSMutableArray alloc] init]; for (int i = 0; i&lt;=13 ; i++) {     RoomInfo *roomInfo = [[RoomInfo alloc] init];     roomInfo.md5 = @"437f4ea71386e873d6f5aa31abb9e873";     roomInfo.zipURL = @"https://staticlive.douyucdn.cn/storage/webpic_resources/upload/fans_medal_resource/17cd936c18ca95bf3acfd7068bec9818.zip";     roomInfo.startTime = 1515125290;     roomInfo.endTime = 1517846400;     roomInfo.roomId = @"special_47";     roomInfo.sIdArray = [[NSMutableArray alloc] init];     [roomInfo.sIdArray addObject:@"271934"];     [roomInfo.sIdArray addObject:@"606118"];     [roomInfo.sIdArray addObject:@"70231"];     [roomInfo.sIdArray addObject:@"530791"];     [roomInfo.sIdArray addObject:@"4809"];     [roomInfo.sIdArray addObject:@"677406"];     [roomInfo.sIdArray addObject:@"414818"];     [roomInfo.sIdArray addObject:@"549212"];     [roomInfo.sIdArray addObject:@"1047629"];     [roomInfo.sIdArray addObject:@"2400799"];     [fansMedal.roomsArray addObject:roomInfo]; } NSData *data = [fansMedal data]; return [GCDWebServerDataResponse responseWithData:data contentType:@"application/octet-stream"];</code></pre><p>返回Json格式的接口数据代码如下：</p><pre><code>NSString *path = [[NSBundle mainBundle] pathForResource:@"fans_medal" ofType:@"json"];NSString *content = [[NSString alloc] initWithContentsOfFile:path encoding:NSUTF8StringEncoding error:nil];return [GCDWebServerDataResponse responseWithHTML:content];</code></pre><h3 id="6-4-客户端项目配置和实现"><a href="#6-4-客户端项目配置和实现" class="headerlink" title="6.4 客户端项目配置和实现"></a>6.4 客户端项目配置和实现</h3><p>客户端也需要pod引入Protobuf框架</p><pre><code>pod 'Protobuf', '~&gt; 3.5.0'</code></pre><p>客户端请求Json数据代码如下：</p><pre><code>NSDate *startDate = [NSDate date];[self getUrl:@"http://192.168.2.1:8080/" dataBody:nil Completetion:^(id result, NSError *error) {    if (!error &amp;&amp; [result isKindOfClass:[NSData class]]) {        NSData *data = (NSData *)result;        NSError *pError;        id obj = [NSJSONSerialization JSONObjectWithData:data options:NSJSONReadingAllowFragments error:&amp;pError];        if (!pError) {            NSDate *endDate1 = [NSDate date];            _infolabel.text = [NSString stringWithFormat:@"数据大小 ： %.3f KB, 请求耗时：%f",[data length]/1000.0,[endDate1 timeIntervalSinceDate:startDate]];            _textView.text = [obj description];        }    }}];</code></pre><p>客户端请求Protobuf数据代码如下：</p><pre><code>NSDate *startDate = [NSDate date];[self getUrl:@"http://192.168.2.1:8080/Protobuf" dataBody:nil Completetion:^(id result, NSError *error) {    if (!error &amp;&amp; [result isKindOfClass:[NSData class]]) {        NSData *data = (NSData *)result;        NSError *pError;        FansMedal *fansMedal = [[FansMedal alloc] initWithData:data error:&amp;pError];        if (!pError) {            NSDate *endDate1 = [NSDate date];            _infolabel.text = [NSString stringWithFormat:@"数据大小 ： %.3f KB, 请求耗时：%f",[data length]/1000.0,[endDate1 timeIntervalSinceDate:startDate]];            _textView.text = fansMedal.description;        }    }}];</code></pre><h3 id="6-5-测试结果"><a href="#6-5-测试结果" class="headerlink" title="6.5 测试结果"></a>6.5 测试结果</h3><p><img src="https://upload-images.jianshu.io/upload_images/12321605-4d853b693ef910a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Json.png"><br><img src="https://upload-images.jianshu.io/upload_images/12321605-a85d6d0ddae93854.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Protobuf.png"></p><p><em><strong>同一段数据内容 Json格式的数据大小是4.852KB,Protobuf格式的数据大小是3.654KB</strong></em></p><p><em><strong>请求耗时，测试了调用100次耗时在2~6秒左右，每次都不等。有时候Protobuf快，有时候纯Json请求快，不太有参考意义，这里就不贴出相应的耗时数据</strong></em></p><h2 id="七、思考和讨论"><a href="#七、思考和讨论" class="headerlink" title="七、思考和讨论"></a>七、思考和讨论</h2><ol><li><p>因为TCP所有数据都是二进制数据流传输，需要自己去把二进制数据流转成自己需要的数据协议，Protobuf可以很好的支持这一点，所以Protobuf在TCP传输使用的场景比较多。</p></li><li><p>反观HTTP，HTTP是属于应用层的协议，底层传输使用的也是TCP。HTTP已经做了数据解封装操作，我们在使用get和post的时候，我们在开发中可以快速拿到客户端和服务器的传输的数据（一般使用Json），Json可读性好，也能在各个端也能快速的转成Model，所以基本已经满足了大部分公司99%的需求。使用Protobuf在HTTP传输好处是Protobuf解析快（这个应该对一般App来说没有这种性能瓶颈，所以一般都不在乎这种优势），数据压缩空间比Json大（没有Json那种多余的“{} 、：”字符等等）。大部分公司都是为了简单直接所以选择Json，Protobuf能节省多少流量也需要具体统计以后才知道。</p></li><li><p>gRPC和Thrift都是RPC框架，客户端做简单的数据拉取工作，个人感觉没必要使用RPC框架，一个是框架的稳定性不确定，二是对现有的项目结构是否也有影响。而且RPC框架一般是用户服务器端内网之间互相调用，很少有客户端远程RPC调用服务器接口。</p></li></ol><p>如果只是对数据解析和节省带宽有要求可以单纯使用Protobuf协议来序列化传输的数据即可。</p><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://www.jianshu.com/p/a969036c711a">Protobuf优缺点</a></p><p><a href="https://www.v2ex.com/t/186561">Json VS Protobuf</a></p><p><a href="http://doc.oschina.net/grpc?t=56831">gRPC 官方文档中文版 V1.0</a></p><p><a href="https://www.zhihu.com/question/30657506">用 thrift 或 gRPC 之类的框架做 app 和服务器的通信合适吗？</a></p><p><a href="http://chengxu.org/p/440.html">比较跨语言通讯框架：thrift和Protobuf</a></p><p><a href="https://neue.v2ex.com/t/276757">RPC 框架对移动客户端的开发有什么意义?</a></p><p><a href="https://www.zhihu.com/question/20189791">哪个互联网公司使用 facebook thrift 做底层架构，实现高性能、可扩展的web应用？引入thrift之后的优缺点是什么？</a></p><p><a href="https://www.jianshu.com/p/774b38306c30">gRPC初体验</a></p><p><a href="http://blog.csdn.net/jiyiqinlovexx/article/details/50478712">Thrift优缺点讨论</a></p>]]></content>
      
      
      <categories>
          
          <category> Backend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HTTP </tag>
            
            <tag> Protobuf </tag>
            
            <tag> Net </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>iOS Cookie 存储相关技术</title>
      <link href="/2018/06/02/ios-cookie/"/>
      <url>/2018/06/02/ios-cookie/</url>
      
        <content type="html"><![CDATA[<h1 id="iOS-Cookie-存储相关技术"><a href="#iOS-Cookie-存储相关技术" class="headerlink" title="iOS Cookie 存储相关技术"></a>iOS Cookie 存储相关技术</h1><h2 id="一、什么是Cookie"><a href="#一、什么是Cookie" class="headerlink" title="一、什么是Cookie"></a>一、什么是Cookie</h2><blockquote><p> Cookie，有时也用其复数形式 Cookies，指某些网站为了辨别用户身份、进行 session 跟踪而储存在用户本地终端上的数据（通常经过加密）。定义于 RFC2109 和 2965 中的都已废弃，最新取代的规范是 RFC6265 [1]  。（可以叫做浏览器缓存）<a href="https://baike.baidu.com/item/cookie/1119?fr=aladdin">来自百度百科</a></p></blockquote><p>说白了Cookie就是提供服务器存储相关数据到客户端的一种解决方案，服务器通过返回的Http头中告知客户端，我设置了Cookie，客户端收到请求以后，会读出Http响应的Header里面把对应的Cookie的key、value值持久化存到本地的Cookie文件，下次再请求服务器相关域名的接口中，会自动带上Cookie相关数据。当然客户端也可以主动设置、读取、删除这些Cookie。</p><p>Cookie一般用来存放用户相关的信息，这样用户每次访问同一个网站的时候就不用重复登录（这个只是Cookie的使用场景之一），由于它是序列化以后存在本地磁盘上的（iOS是存在沙箱文件夹下后面会说），所以Cookie有被伪造的风险，一般存储敏感信息在Cookie上的时候，服务器都会对相关数据进行加密</p><h2 id="二、Cookie在Http中的传输方式"><a href="#二、Cookie在Http中的传输方式" class="headerlink" title="二、Cookie在Http中的传输方式"></a>二、Cookie在Http中的传输方式</h2><p>客户端请求一个服务器接口，如果本地没有任何该域名相关的Cookie，客户端发起的网络请求头是中是不会带Cookie的，Http请求头如下</p><pre><code>GET /app1/set HTTP/1.1Host: 10.8.124.194Accept: */*User-Agent: CookieTest/1 CFNetwork/901.1 Darwin/17.6.0Accept-Language: en-usAccept-Encoding: gzip, deflateConnection: keep-alive</code></pre><p>这个里调用服务器的<code>/app1/set</code>接口，请求的Http头中没有任何Cookie相关的数据。这里面每个字段表示什么意思感兴趣的朋友可以自己去查询相关接口。</p><p>调用Set的接口，服务器接口服务器返回以下数据</p><pre><code>HTTP/1.1 200 OKServer: nginx/1.6.2Date: Sat, 02 Jun 2018 14:23:40 GMTContent-Type: text/html; charset=utf-8Content-Length: 98Set-Cookie: Name=Alex; Expires=Mon, 02-Jul-2018 22:23:40 GMT; Path=/Proxy-Connection: Keep-aliveset Cookie ok</code></pre><p>我们看到服务器返回的响应头里面多了一个<code>Set-Cookie</code>字段，里面有Name=Alex和Expires=超时。</p><p>我们再次调用接口调用该域名下的一个<code>/app1/get</code>接口，此时我们客户端发起的请求头如下</p><pre><code>GET /app1/get HTTP/1.1Host: 10.8.124.194Accept: */*Cookie: Name=AlexUser-Agent: CookieTest/1 CFNetwork/901.1 Darwin/17.6.0Accept-Language: en-usAccept-Encoding: gzip, deflateConnection: keep-alive</code></pre><p>这时的Http请求头里面已经自动带上了Cookie字段，并且自动的填上了<code>Name=Alex</code>,服务器那边可以直接读取请求头中Cookie的的内容。</p><p>当客户端或者服务器为对应域名设置了Cookie以后，该域名下所有的网络请求都会带上Cookie。即使请求该域名下的静态资源，或者通过src属性去请求静态资源，都是会自动带上Cookie的。</p><pre><code>//请求方式一[self.webview loadRequest:[NSURLRequest requestWithURL:[NSURL URLWithString:@"http://10.8.124.194/static/log.jpg"]]];//请求方式二[self.webview loadHTMLString:@"&lt;html&gt;&lt;body&gt;&lt;img src='http://10.8.124.194/app1/img/' /&gt;&lt;/body&gt;&lt;/html&gt;" baseURL:nil];</code></pre><h2 id="三、Cookie的存放位置"><a href="#三、Cookie的存放位置" class="headerlink" title="三、Cookie的存放位置"></a>三、Cookie的存放位置</h2><p>很多朋友有个误区，任务Cookie是与浏览器强相关的，我们平时用网络请求是不能携带Cookie的，这个是错误的，Cookie其实就是存在Http请求头中的一段数据，只要客户端发的是网络请求就可以设置和保存Cookie，当然客户端和浏览器也可以设置禁止服务器写入Cookie。</p><p>iOS 的Cookie文件是存在<code>沙箱文件夹/Library/Cookies/</code>下，所以APP与APP之前是不能共享Cookie数据的。其实这样也好理解，就像电脑上的浏览器一样，你在Chrome里面打开百度登录了你的账号，这个登录状态只能在Chrome里面保持，你用Safari打开百度还是未登录状态，就是因为每个浏览器Cookie的保存位置都不一样。</p><p><em><strong>这里需要单独拿出来说的是APP里面使用的WKWebView的所有Cookie都是单独存在一个文件中的，与本地调用NSURLSession存储的Cookie是区分开的（WebCore），WKWebView存储的Cookie文件名字是<code>Cookies.binarycookies</code>，NSURLSession和UIWebView 是共用一套Cookeie存储的名字一般是 <code>app的bundleid.binarycooimages</code></strong></em></p><p>随便说下<code>NSHTTPCookieStorage</code>只能操作NSURLSession相关的Cookie，要操作WKWebView的Cookie需要用JS方式来写入。具体可以参考下面的文章。</p><p><a href="https://www.jianshu.com/p/fd47847c53f9">iOS之WKWebView 的Cookie读取与注入 同步登陆番外篇</a></p><p>可以用二进制编辑器查看Cookie文件存储内容如下</p><p><img src="https://upload-images.jianshu.io/upload_images/12321605-222a7a294755281d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="CookieInfo.jpg"></p><p>可以看到，Cookie中存储了 域名、key/value值，Cookie的接受策略等等。</p><p>PS：客户端或者服务器写Cookie的时候，不会立马在磁盘上生成Cookie文件，一般会过1~5秒以后才会生成。如果没有生成Cookie文件，在App退到后台的时候回立马生成Cookie文件。</p><h2 id="四、如何操作Cookie"><a href="#四、如何操作Cookie" class="headerlink" title="四、如何操作Cookie"></a>四、如何操作Cookie</h2><h3 id="4-1-iOS客户端设置Cookie"><a href="#4-1-iOS客户端设置Cookie" class="headerlink" title="4.1 iOS客户端设置Cookie"></a>4.1 iOS客户端设置Cookie</h3><ul><li><p>设置Cookie实现如下</p><pre><code>  NSMutableDictionary *properties = [NSMutableDictionary dictionary];  [properties setObject:key forKey:NSHTTPCookieName];  [properties setObject:newValue forKey:NSHTTPCookieValue];  [properties setObject:domian forKey:NSHTTPCookieDomain];  [properties setObject:path forKey:NSHTTPCookiePath];  // 将可变字典转化为cookie  NSHTTPCookie *cookie = [NSHTTPCookie cookieWithProperties: properties];  // 获取cookieStorage  NSHTTPCookieStorage *cookieStorage = [NSHTTPCookieStorage sharedHTTPCookieStorage];      // 存储cookie  [cookieStorage setCookie:cookie];这个里面`NSHTTPCookieName`、`NSHTTPCookieValue`两个必须要设置这个表示Key=Value，服务端读取的时候会根据Key（`NSHTTPCookieName`）的值去读取Value（`NSHTTPCookieValue`）中的数据。`NSHTTPCookieDomain`表示请求的URL的域名，设置以后，客户端请求相关的域名的时候，Http请求Header中会自动带上Cookie中存的这些数据。</code></pre></li><li><p>读取Cookie数据</p><pre><code>  //获取所有cookies  NSHTTPCookieStorage *cookieStorage = [NSHTTPCookieStorage sharedHTTPCookieStorage];  for (NSHTTPCookie *cookie in [cookieStorage cookies]) {     NSLog(@"%@", cookie);  }</code></pre></li><li><p>删除Cookie</p><pre><code>   //删除cookies   NSHTTPCookieStorage *cookieStorage = [NSHTTPCookieStorage sharedHTTPCookieStorage];    NSArray *tempArray = [NSArray arrayWithArray:[cookieStorage cookies]];    for (NSHTTPCookie *cookiej in tempArray) {      [cookieStorage deleteCookie:cookie];    }  </code></pre></li></ul><h3 id="4-2-服务器设置Cookie（Python-Flask）"><a href="#4-2-服务器设置Cookie（Python-Flask）" class="headerlink" title="4.2 服务器设置Cookie（Python-Flask）"></a>4.2 服务器设置Cookie（Python-Flask）</h3><ul><li><p>读取Cookie</p><pre><code>  @app.route('/app1/get')  def get_cookie():      name = request.cookies.get('Name')      if name is None:          name = ""      return "name : %s " % (name)</code></pre></li><li><p>设置Cookie</p><pre><code>  @app.route('/app1/set')  def set_cookie():      outdate = datetime.datetime.today() + datetime.timedelta(days=30)//设置30天以后超时      response = make_response('set Cookie')      response.set_cookie('Name', 'Alex', expires=outdate)      return response</code></pre></li><li><p>删除Cookie</p><pre><code>  @app.route('/app1/del')  def del_cookie():      response = make_response('delete cookie')      response.set_cookie('Name', '', expires=0)      return response     </code></pre><p>  删除Cookie其实就是设置Cookie立即超时，客户端判断超时以后会主动删除本地的Cookie文件</p></li></ul><h2 id="存在的不足"><a href="#存在的不足" class="headerlink" title="存在的不足"></a>存在的不足</h2><ol><li>没有在多个系统版本上验证这个事。</li><li>系统自动设置Cookie到HttpHeader头里面，这部分逻辑在哪里实现的。</li><li>Cookie的策略。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Frontend </category>
          
      </categories>
      
      
        <tags>
            
            <tag> iOS </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
